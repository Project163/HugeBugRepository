diff --git a/tika-app/src/main/java/org/apache/tika/batch/DigestingAutoDetectParserFactory.java b/tika-app/src/main/java/org/apache/tika/batch/DigestingAutoDetectParserFactory.java
index 5f54ed49e..1e072fa1b 100644
--- a/tika-app/src/main/java/org/apache/tika/batch/DigestingAutoDetectParserFactory.java
+++ b/tika-app/src/main/java/org/apache/tika/batch/DigestingAutoDetectParserFactory.java
@@ -34,8 +34,13 @@ public class DigestingAutoDetectParserFactory extends ParserFactory {
             return p;
         }
         boolean skipContainerDocument = false;
-        if (config.getAutoDetectParserConfig().getDigesterFactory() != null) {
-            skipContainerDocument = config.getAutoDetectParserConfig().getDigesterFactory().isSkipContainerDocument();
+        if (config
+                .getAutoDetectParserConfig()
+                .getDigesterFactory() != null) {
+            skipContainerDocument = config
+                    .getAutoDetectParserConfig()
+                    .getDigesterFactory()
+                    .isSkipContainerDocument();
         }
         return new DigestingParser(p, digester, skipContainerDocument);
     }
diff --git a/tika-app/src/main/java/org/apache/tika/batch/builders/AppParserFactoryBuilder.java b/tika-app/src/main/java/org/apache/tika/batch/builders/AppParserFactoryBuilder.java
index bfacbd998..09f4d434c 100644
--- a/tika-app/src/main/java/org/apache/tika/batch/builders/AppParserFactoryBuilder.java
+++ b/tika-app/src/main/java/org/apache/tika/batch/builders/AppParserFactoryBuilder.java
@@ -39,14 +39,15 @@ public class AppParserFactoryBuilder implements IParserFactoryBuilder {
         ParserFactory pf = ClassLoaderUtil.buildClass(ParserFactory.class, className);
 
         if (localAttrs.containsKey("parseRecursively")) {
-            String bString = localAttrs.get("parseRecursively").toLowerCase(Locale.ENGLISH);
+            String bString = localAttrs
+                    .get("parseRecursively")
+                    .toLowerCase(Locale.ENGLISH);
             if (bString.equals("true")) {
                 pf.setParseRecursively(true);
             } else if (bString.equals("false")) {
                 pf.setParseRecursively(false);
             } else {
-                throw new RuntimeException(
-                        "parseRecursively must have value of \"true\" or \"false\": " + bString);
+                throw new RuntimeException("parseRecursively must have value of \"true\" or \"false\": " + bString);
             }
         }
         if (pf instanceof DigestingAutoDetectParserFactory) {
@@ -60,16 +61,14 @@ public class AppParserFactoryBuilder implements IParserFactoryBuilder {
 
         String readLimitString = localAttrs.get("digestMarkLimit");
         if (readLimitString == null) {
-            throw new IllegalArgumentException("Must specify \"digestMarkLimit\" for " +
-                    "the DigestingAutoDetectParserFactory");
+            throw new IllegalArgumentException("Must specify \"digestMarkLimit\" for " + "the DigestingAutoDetectParserFactory");
         }
         int readLimit = -1;
 
         try {
             readLimit = Integer.parseInt(readLimitString);
         } catch (NumberFormatException e) {
-            throw new IllegalArgumentException(
-                    "Parameter \"digestMarkLimit\" must be a parseable int: " + readLimitString);
+            throw new IllegalArgumentException("Parameter \"digestMarkLimit\" must be a parseable int: " + readLimitString);
         }
         String digestString = localAttrs.get("digest");
         try {
@@ -78,9 +77,7 @@ public class AppParserFactoryBuilder implements IParserFactoryBuilder {
             try {
                 return new BouncyCastleDigester(readLimit, digestString);
             } catch (IllegalArgumentException bcException) {
-                throw new IllegalArgumentException(
-                        "Tried both CommonsDigester (" + commonsException.getMessage() +
-                                ") and BouncyCastleDigester (" + bcException.getMessage() + ")",
+                throw new IllegalArgumentException("Tried both CommonsDigester (" + commonsException.getMessage() + ") and BouncyCastleDigester (" + bcException.getMessage() + ")",
                         bcException);
             }
         }
diff --git a/tika-app/src/main/java/org/apache/tika/cli/BatchCommandLineBuilder.java b/tika-app/src/main/java/org/apache/tika/cli/BatchCommandLineBuilder.java
index b6528d493..9379f9d45 100644
--- a/tika-app/src/main/java/org/apache/tika/cli/BatchCommandLineBuilder.java
+++ b/tika-app/src/main/java/org/apache/tika/cli/BatchCommandLineBuilder.java
@@ -77,10 +77,14 @@ class BatchCommandLineBuilder {
         boolean foundHeadlessOption = false;
         for (Map.Entry<String, String> e : jvmOpts.entrySet()) {
             fullCommand.add(e.getKey());
-            if (e.getValue().length() > 0) {
+            if (e
+                    .getValue()
+                    .length() > 0) {
                 fullCommand.add(commandLineSafe(e.getValue()));
             }
-            if (e.getKey().contains("java.awt.headless")) {
+            if (e
+                    .getKey()
+                    .contains("java.awt.headless")) {
                 foundHeadlessOption = true;
             }
         }
@@ -93,7 +97,9 @@ class BatchCommandLineBuilder {
         //now add the process commands
         for (Map.Entry<String, String> e : processArgs.entrySet()) {
             fullCommand.add(e.getKey());
-            if (e.getValue().length() > 0) {
+            if (e
+                    .getValue()
+                    .length() > 0) {
                 fullCommand.add(commandLineSafe(e.getValue()));
             }
         }
@@ -121,8 +127,7 @@ class BatchCommandLineBuilder {
      * @param commandLine args that should be part of the batch commandline
      * @param jvmArgs     args that belong as jvm arguments for the forked process
      */
-    private static void mapifyArgs(final String[] args, final Map<String, String> commandLine,
-                                   final Map<String, String> jvmArgs) {
+    private static void mapifyArgs(final String[] args, final Map<String, String> commandLine, final Map<String, String> jvmArgs) {
 
         if (args.length == 0) {
             return;
@@ -130,7 +135,9 @@ class BatchCommandLineBuilder {
 
         Matcher matcher = JVM_OPTS_PATTERN.matcher("");
         for (int i = 0; i < args.length; i++) {
-            if (matcher.reset(args[i]).find()) {
+            if (matcher
+                    .reset(args[i])
+                    .find()) {
                 String jvmArg = matcher.group(1) + matcher.group(2);
                 String v = "";
                 if (i < args.length - 1 && !args[i + 1].startsWith("-")) {
@@ -150,8 +157,7 @@ class BatchCommandLineBuilder {
         }
     }
 
-    private static void translateCommandLine(String[] args, Map<String, String> map)
-            throws IOException {
+    private static void translateCommandLine(String[] args, Map<String, String> map) throws IOException {
         //if there are only two args and they are both directories, treat the first
         //as input and the second as output.
         if (args.length == 2 && !args[0].startsWith("-") && !args[1].startsWith("-")) {
@@ -159,8 +165,7 @@ class BatchCommandLineBuilder {
             Path candOutput = Paths.get(args[1]);
 
             if (Files.isRegularFile(candOutput)) {
-                throw new IllegalArgumentException("Can't specify an existing file as the " +
-                        "second argument for the output directory of a batch process");
+                throw new IllegalArgumentException("Can't specify an existing file as the " + "second argument for the output directory of a batch process");
             }
 
             if (Files.isDirectory(candInput)) {
diff --git a/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java b/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java
index bd78d4338..4aa5361ba 100644
--- a/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java
+++ b/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java
@@ -82,8 +82,6 @@ import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.language.detect.LanguageHandler;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
 import org.apache.tika.mime.MediaType;
 import org.apache.tika.mime.MediaTypeRegistry;
 import org.apache.tika.mime.MimeType;
@@ -107,6 +105,8 @@ import org.apache.tika.sax.ExpandedTitleContentHandler;
 import org.apache.tika.sax.RecursiveParserWrapperHandler;
 import org.apache.tika.sax.WriteOutContentHandler;
 import org.apache.tika.sax.boilerpipe.BoilerpipeContentHandler;
+import org.apache.tika.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.xmp.XMPMetadata;
 
 /**
@@ -123,22 +123,12 @@ public class TikaCLI {
             return new DefaultHandler();
         }
     };
-
-    private final OutputType XML = new OutputType() {
-        @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
-            return getTransformerHandler(output, "xml", encoding, prettyPrint);
-        }
-    };
-
     private File extractDir = new File(".");
     private ParseContext context;
     private Detector detector;
     private Parser parser;
     private TikaConfig config;
     private String configFilePath;
-    private OutputType type = XML;
     private boolean recursiveJSON = false;
     private URI networkURI = null;
     /**
@@ -147,53 +137,46 @@ public class TikaCLI {
     private String encoding = null;
     private final OutputType TEXT = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
             return new BodyContentHandler(getOutputWriter(output, encoding));
         }
     };
     private final OutputType TEXT_MAIN = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
             return new BoilerpipeContentHandler(getOutputWriter(output, encoding));
         }
     };
     private final OutputType TEXT_ALL = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
             return new WriteOutContentHandler(getOutputWriter(output, encoding));
         }
     };
     private final OutputType METADATA = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
             final PrintWriter writer = new PrintWriter(getOutputWriter(output, encoding));
             return new NoDocumentMetHandler(metadata, writer);
         }
     };
     private final OutputType JSON = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
             final PrintWriter writer = new PrintWriter(getOutputWriter(output, encoding));
             return new NoDocumentJSONMetHandler(metadata, writer);
         }
     };
     private final OutputType XMP = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, final Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, final Metadata metadata) throws Exception {
             final PrintWriter writer = new PrintWriter(getOutputWriter(output, encoding));
             return new NoDocumentXMPMetaHandler(metadata, writer);
         }
     };
     private final OutputType LANGUAGE = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
             final PrintWriter writer = new PrintWriter(getOutputWriter(output, encoding));
             return new LanguageHandler() {
                 public void endDocument() {
@@ -205,10 +188,11 @@ public class TikaCLI {
     };
     private final OutputType DETECT = new OutputType() {
         @Override
-        public void process(InputStream stream, OutputStream output, Metadata metadata)
-                throws Exception {
+        public void process(InputStream stream, OutputStream output, Metadata metadata) throws Exception {
             PrintWriter writer = new PrintWriter(getOutputWriter(output, encoding));
-            writer.println(detector.detect(stream, metadata).toString());
+            writer.println(detector
+                    .detect(stream, metadata)
+                    .toString());
             writer.flush();
         }
     };
@@ -221,13 +205,17 @@ public class TikaCLI {
     private boolean pipeMode = true;
     private boolean fork = false;
     private boolean prettyPrint;
-
+    private final OutputType XML = new OutputType() {
+        @Override
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
+            return getTransformerHandler(output, "xml", encoding, prettyPrint);
+        }
+    };
+    private OutputType type = XML;
     private final OutputType HTML = new OutputType() {
         @Override
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
-            return new ExpandedTitleContentHandler(
-                    getTransformerHandler(output, "html", encoding, prettyPrint));
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
+            return new ExpandedTitleContentHandler(getTransformerHandler(output, "html", encoding, prettyPrint));
         }
     };
 
@@ -273,8 +261,7 @@ public class TikaCLI {
         }
     }
 
-    private static void async(String[] args)
-            throws Exception {
+    private static void async(String[] args) throws Exception {
         String tikaConfigPath = "";
         String config = "--config=";
         for (String arg : args) {
@@ -282,7 +269,7 @@ public class TikaCLI {
                 tikaConfigPath = arg.substring(config.length());
             }
         }
-        TikaAsyncCLI.main(new String[]{ tikaConfigPath});
+        TikaAsyncCLI.main(new String[]{tikaConfigPath});
     }
 
     /**
@@ -295,8 +282,7 @@ public class TikaCLI {
      * @throws UnsupportedEncodingException if the given encoding is not supported
      * @see <a href="https://issues.apache.org/jira/browse/TIKA-277">TIKA-277</a>
      */
-    private static Writer getOutputWriter(OutputStream output, String encoding)
-            throws UnsupportedEncodingException {
+    private static Writer getOutputWriter(OutputStream output, String encoding) throws UnsupportedEncodingException {
         if (encoding != null) {
             return new OutputStreamWriter(output, encoding);
         } else {
@@ -317,15 +303,19 @@ public class TikaCLI {
      * @throws TransformerConfigurationException if the transformer can not be created
      * @see <a href="https://issues.apache.org/jira/browse/TIKA-277">TIKA-277</a>
      */
-    private static TransformerHandler getTransformerHandler(OutputStream output, String method,
-                                                            String encoding, boolean prettyPrint)
-            throws TransformerConfigurationException {
+    private static TransformerHandler getTransformerHandler(OutputStream output, String method, String encoding, boolean prettyPrint) throws TransformerConfigurationException {
         SAXTransformerFactory factory = (SAXTransformerFactory) SAXTransformerFactory.newInstance();
         TransformerHandler handler = factory.newTransformerHandler();
-        handler.getTransformer().setOutputProperty(OutputKeys.METHOD, method);
-        handler.getTransformer().setOutputProperty(OutputKeys.INDENT, prettyPrint ? "yes" : "no");
+        handler
+                .getTransformer()
+                .setOutputProperty(OutputKeys.METHOD, method);
+        handler
+                .getTransformer()
+                .setOutputProperty(OutputKeys.INDENT, prettyPrint ? "yes" : "no");
         if (encoding != null) {
-            handler.getTransformer().setOutputProperty(OutputKeys.ENCODING, encoding);
+            handler
+                    .getTransformer()
+                    .setOutputProperty(OutputKeys.ENCODING, encoding);
         }
         handler.setResult(new StreamResult(output));
         return handler;
@@ -345,11 +335,8 @@ public class TikaCLI {
             PDFParserConfig pdfParserConfig = new PDFParserConfig();
             pdfParserConfig.setExtractInlineImages(true);
             pdfParserConfig.setParseIncrementalUpdates(true);
-            String warn = "As a convenience, TikaCLI has turned on extraction of\n" +
-                    "inline images and incremental updates for the PDFParser (TIKA-2374 and " +
-                    "TIKA-4017).\n" +
-                    "Aside from the -z option, this is not the default behavior\n" +
-                    "in Tika generally or in tika-server.";
+            String warn = "As a convenience, TikaCLI has turned on extraction of\n" + "inline images and incremental updates for the PDFParser (TIKA-2374 and " + "TIKA-4017).\n" +
+                    "Aside from the -z option, this is not the default behavior\n" + "in Tika generally or in tika-server.";
             LOG.info(warn);
             context.set(PDFParserConfig.class, pdfParserConfig);
         }
@@ -380,8 +367,7 @@ public class TikaCLI {
         } else if (arg.equals("--list-parser-detail") || arg.equals("--list-parser-details")) {
             pipeMode = false;
             displayParsers(true, false);
-        } else if (arg.equals("--list-parser-detail-apt") ||
-                arg.equals("--list-parser-details-apt")) {
+        } else if (arg.equals("--list-parser-detail-apt") || arg.equals("--list-parser-details-apt")) {
             pipeMode = false;
             displayParsers(true, true);
         } else if (arg.equals("--list-met-models")) {
@@ -459,11 +445,8 @@ public class TikaCLI {
             context.set(EmbeddedDocumentExtractor.class, new FileEmbeddedDocumentExtractor());
         } else if (arg.equals("-r") || arg.equals("--pretty-print")) {
             prettyPrint = true;
-        } else if (arg.equals("-p") || arg.equals("--port") || arg.equals("-s") ||
-                arg.equals("--server")) {
-            throw new IllegalArgumentException(
-                    "As of Tika 2.0, the server option is no longer supported in tika-app.\n" +
-                            "See https://wiki.apache.org/tika/TikaJAXRS for usage.");
+        } else if (arg.equals("-p") || arg.equals("--port") || arg.equals("-s") || arg.equals("--server")) {
+            throw new IllegalArgumentException("As of Tika 2.0, the server option is no longer supported in tika-app.\n" + "See https://wiki.apache.org/tika/TikaJAXRS for usage.");
         } else if (arg.startsWith("-c")) {
             networkURI = new URI(arg.substring("-c".length()));
         } else if (arg.startsWith("--client=")) {
@@ -473,15 +456,16 @@ public class TikaCLI {
             configure();
 
             if (arg.equals("-")) {
-                try (InputStream stream = TikaInputStream
-                        .get(CloseShieldInputStream.wrap(System.in))) {
+                try (InputStream stream = TikaInputStream.get(CloseShieldInputStream.wrap(System.in))) {
                     type.process(stream, System.out, new Metadata());
                 }
             } else {
                 URL url;
                 File file = new File(arg);
                 if (file.isFile()) {
-                    url = file.toURI().toURL();
+                    url = file
+                            .toURI()
+                            .toURL();
                 } else {
                     url = new URL(arg);
                 }
@@ -503,17 +487,13 @@ public class TikaCLI {
         configure();
         TikaConfig localConfig = (config == null) ? TikaConfig.getDefaultConfig() : config;
 
-        TikaConfigSerializer
-                .serialize(localConfig, mode, new OutputStreamWriter(System.out, UTF_8), UTF_8);
+        TikaConfigSerializer.serialize(localConfig, mode, new OutputStreamWriter(System.out, UTF_8), UTF_8);
     }
 
-    private void handleRecursiveJson(URL url, OutputStream output)
-            throws IOException, SAXException, TikaException {
+    private void handleRecursiveJson(URL url, OutputStream output) throws IOException, SAXException, TikaException {
         Metadata metadata = new Metadata();
         RecursiveParserWrapper wrapper = new RecursiveParserWrapper(parser);
-        RecursiveParserWrapperHandler handler =
-                new RecursiveParserWrapperHandler(getContentHandlerFactory(type), -1,
-                        config.getMetadataFilter());
+        RecursiveParserWrapperHandler handler = new RecursiveParserWrapperHandler(getContentHandlerFactory(type), -1, config.getMetadataFilter());
         try (InputStream input = TikaInputStream.get(url, metadata)) {
             wrapper.parse(input, handler, metadata, context);
         }
@@ -524,8 +504,7 @@ public class TikaCLI {
     }
 
     private ContentHandlerFactory getContentHandlerFactory(OutputType type) {
-        BasicContentHandlerFactory.HANDLER_TYPE handlerType =
-                BasicContentHandlerFactory.HANDLER_TYPE.IGNORE;
+        BasicContentHandlerFactory.HANDLER_TYPE handlerType = BasicContentHandlerFactory.HANDLER_TYPE.IGNORE;
         if (type.equals(HTML)) {
             handlerType = BasicContentHandlerFactory.HANDLER_TYPE.HTML;
         } else if (type.equals(XML)) {
@@ -553,8 +532,7 @@ public class TikaCLI {
         out.println("    -f  or --fork          Use Fork Mode for out-of-process extraction");
         out.println();
         out.println("    --config=<tika-config.xml>");
-        out.println(
-                "        TikaConfig file. Must be specified before -g, -s, -f or the dump-x-config !");
+        out.println("        TikaConfig file. Must be specified before -g, -s, -f or the dump-x-config !");
         out.println("    --dump-minimal-config  Print minimal TikaConfig");
         out.println("    --dump-current-config  Print current TikaConfig");
         out.println("    --dump-static-config   Print static config");
@@ -563,8 +541,7 @@ public class TikaCLI {
         out.println("    -x  or --xml           Output XHTML content (default)");
         out.println("    -h  or --html          Output HTML content");
         out.println("    -t  or --text          Output plain text content (body)");
-        out.println(
-                "    -T  or --text-main     Output plain text content (main content only via boilerpipe handler)");
+        out.println("    -T  or --text-main     Output plain text content (main content only via boilerpipe handler)");
         out.println("    -A  or --text-all      Output all text content");
         out.println("    -m  or --metadata      Output only metadata");
         out.println("    -j  or --json          Output metadata in JSON");
@@ -572,8 +549,7 @@ public class TikaCLI {
         out.println("    -J  or --jsonRecursive Output metadata and content from all");
         out.println("                           embedded files (choose content type");
         out.println("                           with -x, -h, -t or -m; default is -x)");
-        out.println("    -a  or --async         Run Tika in async mode; must specify details in a" +
-                " tikaConfig file");
+        out.println("    -a  or --async         Run Tika in async mode; must specify details in a" + " tikaConfig file");
         out.println("    -l  or --language      Output only language");
         out.println("    -d  or --detect        Detect document type");
         out.println("           --digest=X      Include digest X (md2, md5, sha1,");
@@ -582,8 +558,7 @@ public class TikaCLI {
         out.println("    -pX or --password=X    Use document password X");
         out.println("    -z  or --extract       Extract all attachements into current directory");
         out.println("    --extract-dir=<dir>    Specify target directory for -z");
-        out.println(
-                "    -r  or --pretty-print  For JSON, XML and XHTML outputs, adds newlines and");
+        out.println("    -r  or --pretty-print  For JSON, XML and XHTML outputs, adds newlines and");
         out.println("                           whitespace, for better readability");
         out.println();
         out.println("    --list-parsers");
@@ -591,8 +566,7 @@ public class TikaCLI {
         out.println("    --list-parser-details");
         out.println("         List the available document parsers and their supported mime types");
         out.println("    --list-parser-details-apt");
-        out.println(
-                "         List the available document parsers and their supported mime types in apt format.");
+        out.println("         List the available document parsers and their supported mime types in apt format.");
         out.println("    --list-detectors");
         out.println("         List the available document detectors");
         out.println("    --list-met-models");
@@ -602,8 +576,7 @@ public class TikaCLI {
         out.println();
         out.println();
         out.println("    --compare-file-magic=<dir>");
-        out.println(
-                "         Compares Tika's known media types to the File(1) tool's magic directory");
+        out.println("         Compares Tika's known media types to the File(1) tool's magic directory");
         out.println("Description:");
         out.println("    Apache Tika will parse the file(s) specified on the");
         out.println("    command line and output the extracted text content");
@@ -636,18 +609,15 @@ public class TikaCLI {
         out.println("    -numConsumers              Number of processing threads");
         out.println("    -bc                        Batch config file");
         out.println("    -maxRestarts               Maximum number of times the ");
-        out.println(
-                "                               watchdog process will restart the forked process.");
+        out.println("                               watchdog process will restart the forked process.");
         out.println("    -timeoutThresholdMillis    Number of milliseconds allowed to a parse");
-        out.println(
-                "                               before the process is terminated and restarted");
+        out.println("                               before the process is terminated and restarted");
         out.println("    -fileList                  List of files to process, with");
         out.println("                               paths relative to the input directory");
         out.println("    -includeFilePat            Regular expression to determine which");
         out.println("                               files to process, e.g. \"(?i)\\.pdf\"");
         out.println("    -excludeFilePat            Regular expression to determine which");
-        out.println(
-                "                               files to avoid processing, e.g. \"(?i)\\.pdf\"");
+        out.println("                               files to avoid processing, e.g. \"(?i)\\.pdf\"");
         out.println("    -maxFileSizeBytes          Skip files longer than this value");
         out.println();
         out.println("    Control the type of output with -x, -h, -t and/or -J.");
@@ -715,7 +685,9 @@ public class TikaCLI {
         for (Class<?> modelClass : modelClasses) {
             // we don't care about internal Tika met classes
             // if we do, then we can take this conditional out
-            if (!modelClass.getSimpleName().contains("Tika")) {
+            if (!modelClass
+                    .getSimpleName()
+                    .contains("Tika")) {
                 System.out.println(modelClass.getSimpleName());
                 Field[] keyFields = modelClass.getFields();
                 Arrays.sort(keyFields, Comparator.comparing(Field::getName));
@@ -731,8 +703,7 @@ public class TikaCLI {
      * If a parser is a composite parser, it will list the
      * sub parsers and their mime-types.
      */
-    private void displayParsers(boolean includeMimeTypes, boolean aptListFormat)
-            throws TikaException, IOException, SAXException {
+    private void displayParsers(boolean includeMimeTypes, boolean aptListFormat) throws TikaException, IOException, SAXException {
         configure();
         displayParser(parser, includeMimeTypes, aptListFormat, 3);
     }
@@ -746,18 +717,17 @@ public class TikaCLI {
         }
 
         boolean isComposite = (p instanceof CompositeParser);
-        String name = p.getClass().getName();
+        String name = p
+                .getClass()
+                .getName();
 
         if (apt) {
-            name = name.substring(0, name.lastIndexOf(".") + 1) + "{{{./api/" +
-                    name.replace(".", "/") + "}" + name.substring(name.lastIndexOf(".") + 1) + "}}";
+            name = name.substring(0, name.lastIndexOf(".") + 1) + "{{{./api/" + name.replace(".", "/") + "}" + name.substring(name.lastIndexOf(".") + 1) + "}}";
         } else if (decorated != null) {
             name += decorated;
         }
-        if ((apt && !isComposite) ||
-                !apt) {    // Don't display Composite parsers in the apt output.
-            System.out.println(indent(i) + ((apt) ? "* " : "") + name +
-                    (isComposite ? " (Composite Parser):" : ""));
+        if ((apt && !isComposite) || !apt) {    // Don't display Composite parsers in the apt output.
+            System.out.println(indent(i) + ((apt) ? "* " : "") + name + (isComposite ? " (Composite Parser):" : ""));
             if (apt) {
                 System.out.println();
             }
@@ -772,11 +742,9 @@ public class TikaCLI {
         }
 
         if (isComposite) {
-            Parser[] subParsers =
-                    sortParsers(invertMediaTypeMap(((CompositeParser) p).getParsers()));
+            Parser[] subParsers = sortParsers(invertMediaTypeMap(((CompositeParser) p).getParsers()));
             for (Parser sp : subParsers) {
-                displayParser(sp, includeMimeTypes, apt,
-                        i + ((apt) ? 0 : 3));  // Don't indent for Composites in apt.
+                displayParser(sp, includeMimeTypes, apt, i + ((apt) ? 0 : 3));  // Don't indent for Composites in apt.
             }
         }
     }
@@ -793,7 +761,9 @@ public class TikaCLI {
 
     private void displayDetector(Detector d, int i) {
         boolean isComposite = (d instanceof CompositeDetector);
-        String name = d.getClass().getName();
+        String name = d
+                .getClass()
+                .getName();
         System.out.println(indent(i) + name + (isComposite ? " (Composite Detector):" : ""));
         if (isComposite) {
             List<Detector> subDetectors = ((CompositeDetector) d).getDetectors();
@@ -809,10 +779,16 @@ public class TikaCLI {
 
     private Parser[] sortParsers(Map<Parser, Set<MediaType>> parsers) {
         // Get a nicely sorted list of the parsers
-        Parser[] sortedParsers = parsers.keySet().toArray(new Parser[0]);
+        Parser[] sortedParsers = parsers
+                .keySet()
+                .toArray(new Parser[0]);
         Arrays.sort(sortedParsers, (p1, p2) -> {
-            String name1 = p1.getClass().getName();
-            String name2 = p2.getClass().getName();
+            String name1 = p1
+                    .getClass()
+                    .getName();
+            String name2 = p2
+                    .getClass()
+                    .getName();
             return name1.compareTo(name2);
         });
         return sortedParsers;
@@ -824,7 +800,9 @@ public class TikaCLI {
             if (!parsers.containsKey(e.getValue())) {
                 parsers.put(e.getValue(), new HashSet<>());
             }
-            parsers.get(e.getValue()).add(e.getKey());
+            parsers
+                    .get(e.getValue())
+                    .add(e.getKey());
         }
         return parsers;
     }
@@ -849,9 +827,13 @@ public class TikaCLI {
             Parser p = parsers.get(type);
             if (p != null) {
                 if (p instanceof CompositeParser) {
-                    p = ((CompositeParser) p).getParsers().get(type);
+                    p = ((CompositeParser) p)
+                            .getParsers()
+                            .get(type);
                 }
-                System.out.println("  parser:    " + p.getClass().getName());
+                System.out.println("  parser:    " + p
+                        .getClass()
+                        .getName());
             }
         }
     }
@@ -869,24 +851,23 @@ public class TikaCLI {
 
         // Plausibility check
         File dir = new File(magicDir);
-        if ((new File(dir, "elf")).exists() && (new File(dir, "mime")).exists() &&
-                (new File(dir, "vorbis")).exists()) {
+        if ((new File(dir, "elf")).exists() && (new File(dir, "mime")).exists() && (new File(dir, "vorbis")).exists()) {
             // Looks plausible
         } else {
-            throw new IllegalArgumentException(
-                    magicDir + " doesn't seem to hold uncompressed file magic entries");
+            throw new IllegalArgumentException(magicDir + " doesn't seem to hold uncompressed file magic entries");
         }
 
         // Find all the mimetypes in the directory
         Set<String> fileMimes = new HashSet<>();
         for (File mf : dir.listFiles()) {
             if (mf.isFile()) {
-                BufferedReader r =
-                        new BufferedReader(new InputStreamReader(new FileInputStream(mf), UTF_8));
+                BufferedReader r = new BufferedReader(new InputStreamReader(new FileInputStream(mf), UTF_8));
                 String line;
                 while ((line = r.readLine()) != null) {
                     if (line.startsWith("!:mime") || line.startsWith("#!:mime")) {
-                        String mime = line.substring(7).trim();
+                        String mime = line
+                                .substring(7)
+                                .trim();
                         fileMimes.add(mime);
                     }
                 }
@@ -930,9 +911,7 @@ public class TikaCLI {
                         } else {
                             // Check the parent next
                             MediaType parent = registry.getSupertype(type.getType());
-                            if (parent == MediaType.APPLICATION_XML ||
-                                    parent == MediaType.TEXT_PLAIN ||
-                                    parent == MediaType.OCTET_STREAM) {
+                            if (parent == MediaType.APPLICATION_XML || parent == MediaType.TEXT_PLAIN || parent == MediaType.OCTET_STREAM) {
                                 // Stop checking parents if we hit a top level type
                                 parent = null;
                             }
@@ -958,23 +937,22 @@ public class TikaCLI {
         int tikaAliases = 0;
         for (MediaType type : registry.getTypes()) {
             tikaTypes++;
-            tikaAliases += registry.getAliases(type).size();
+            tikaAliases += registry
+                    .getAliases(type)
+                    .size();
         }
 
         // Report
         System.out.println("Tika knows about " + tikaTypes + " unique mime types");
-        System.out.println(
-                "Tika knows about " + (tikaTypes + tikaAliases) + " mime types including aliases");
-        System.out.println(
-                "The File Magic directory knows about " + fileMimes.size() + " unique mime types");
+        System.out.println("Tika knows about " + (tikaTypes + tikaAliases) + " mime types including aliases");
+        System.out.println("The File Magic directory knows about " + fileMimes.size() + " unique mime types");
         System.out.println();
         System.out.println("The following mime types are known to File but not Tika:");
         for (String mime : tikaLacking) {
             System.out.println("  " + mime);
         }
         System.out.println();
-        System.out.println(
-                "The following mime types from File have no Tika magic (but their children might):");
+        System.out.println("The following mime types from File have no Tika magic (but their children might):");
         for (String mime : tikaNoMagic) {
             System.out.println("  " + mime);
         }
@@ -1058,8 +1036,7 @@ public class TikaCLI {
     }
 
     private class OutputType {
-        public void process(InputStream input, OutputStream output, Metadata metadata)
-                throws Exception {
+        public void process(InputStream input, OutputStream output, Metadata metadata) throws Exception {
             Parser p = parser;
             if (fork) {
                 p = new ForkParser(TikaCLI.class.getClassLoader(), p);
@@ -1083,8 +1060,7 @@ public class TikaCLI {
             }
         }
 
-        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata)
-                throws Exception {
+        protected ContentHandler getContentHandler(OutputStream output, Metadata metadata) throws Exception {
             throw new UnsupportedOperationException();
         }
 
@@ -1093,17 +1069,14 @@ public class TikaCLI {
     private class FileEmbeddedDocumentExtractor implements EmbeddedDocumentExtractor {
 
         private final TikaConfig config = TikaConfig.getDefaultConfig();
-        private final EmbeddedStreamTranslator embeddedStreamTranslator =
-                new DefaultEmbeddedStreamTranslator();
+        private final EmbeddedStreamTranslator embeddedStreamTranslator = new DefaultEmbeddedStreamTranslator();
         private int count = 0;
 
         public boolean shouldParseEmbedded(Metadata metadata) {
             return true;
         }
 
-        public void parseEmbedded(InputStream inputStream, ContentHandler contentHandler,
-                                  Metadata metadata, boolean outputHtml)
-                throws SAXException, IOException {
+        public void parseEmbedded(InputStream inputStream, ContentHandler contentHandler, Metadata metadata, boolean outputHtml) throws SAXException, IOException {
 
             if (!inputStream.markSupported()) {
                 inputStream = TikaInputStream.get(inputStream);
@@ -1128,8 +1101,7 @@ public class TikaCLI {
 
             try (FileOutputStream os = new FileOutputStream(outputFile)) {
                 if (embeddedStreamTranslator.shouldTranslate(inputStream, metadata)) {
-                    try (InputStream translated = embeddedStreamTranslator
-                            .translate(inputStream, metadata)) {
+                    try (InputStream translated = embeddedStreamTranslator.translate(inputStream, metadata)) {
                         IOUtils.copy(translated, os);
                     }
                 } else {
@@ -1139,9 +1111,7 @@ public class TikaCLI {
                 //
                 // being a CLI program messages should go to the stderr too
                 //
-                String msg = String.format(Locale.ROOT,
-                        "Ignoring unexpected exception trying to save embedded file %s (%s)", name,
-                        e.getMessage());
+                String msg = String.format(Locale.ROOT, "Ignoring unexpected exception trying to save embedded file %s (%s)", name, e.getMessage());
                 LOG.warn(msg, e);
             }
         }
@@ -1177,15 +1147,19 @@ public class TikaCLI {
             //if file already exists, prepend uuid
             if (outputFile.exists()) {
                 String fileName = FilenameUtils.getName(normalizedName);
-                outputFile = new File(extractDir, UUID.randomUUID().toString() + "-" + fileName);
+                outputFile = new File(extractDir, UUID
+                        .randomUUID()
+                        .toString() + "-" + fileName);
             }
             return outputFile;
         }
 
         private String getExtension(MediaType contentType) {
             try {
-                String ext =
-                        config.getMimeRepository().forName(contentType.toString()).getExtension();
+                String ext = config
+                        .getMimeRepository()
+                        .forName(contentType.toString())
+                        .getExtension();
                 if (ext == null) {
                     return ".bin";
                 } else {
diff --git a/tika-app/src/main/java/org/apache/tika/gui/ParsingTransferHandler.java b/tika-app/src/main/java/org/apache/tika/gui/ParsingTransferHandler.java
index 721ae13b8..debc24e5f 100644
--- a/tika-app/src/main/java/org/apache/tika/gui/ParsingTransferHandler.java
+++ b/tika-app/src/main/java/org/apache/tika/gui/ParsingTransferHandler.java
@@ -78,8 +78,7 @@ class ParsingTransferHandler extends TransferHandler {
 
     public boolean canImport(JComponent component, DataFlavor[] flavors) {
         for (DataFlavor flavor : flavors) {
-            if (flavor.equals(DataFlavor.javaFileListFlavor) || flavor.equals(uriListFlavor) ||
-                    flavor.equals(urlListFlavor)) {
+            if (flavor.equals(DataFlavor.javaFileListFlavor) || flavor.equals(uriListFlavor) || flavor.equals(urlListFlavor)) {
                 return true;
             }
         }
@@ -90,8 +89,7 @@ class ParsingTransferHandler extends TransferHandler {
     public boolean importData(JComponent component, Transferable transferable) {
         try {
             if (transferable.isDataFlavorSupported(DataFlavor.javaFileListFlavor)) {
-                importFiles(
-                        (List<File>) transferable.getTransferData(DataFlavor.javaFileListFlavor));
+                importFiles((List<File>) transferable.getTransferData(DataFlavor.javaFileListFlavor));
             } else if (transferable.isDataFlavorSupported(urlListFlavor)) {
                 Object data = transferable.getTransferData(urlListFlavor);
                 tika.openURL(new URL(data.toString()));
@@ -114,8 +112,7 @@ class ParsingTransferHandler extends TransferHandler {
         delegate.exportAsDrag(arg0, arg1, arg2);
     }
 
-    public void exportToClipboard(JComponent arg0, Clipboard arg1, int arg2)
-            throws IllegalStateException {
+    public void exportToClipboard(JComponent arg0, Clipboard arg1, int arg2) throws IllegalStateException {
         delegate.exportToClipboard(arg0, arg1, arg2);
     }
 
diff --git a/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java b/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java
index b58b39257..6b1e65966 100644
--- a/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java
+++ b/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java
@@ -75,7 +75,6 @@ import org.apache.tika.extractor.DocumentSelector;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
 import org.apache.tika.mime.MediaType;
 import org.apache.tika.parser.AutoDetectParser;
 import org.apache.tika.parser.DigestingParser;
@@ -90,6 +89,7 @@ import org.apache.tika.sax.RecursiveParserWrapperHandler;
 import org.apache.tika.sax.TeeContentHandler;
 import org.apache.tika.sax.XHTMLContentHandler;
 import org.apache.tika.sax.boilerpipe.BoilerpipeContentHandler;
+import org.apache.tika.serialization.JsonMetadataList;
 
 /**
  * Simple Swing GUI for Apache Tika. You can drag and drop files on top
@@ -198,9 +198,8 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
         UIManager.setLookAndFeel(UIManager.getSystemLookAndFeelClassName());
         final TikaConfig finalConfig = config;
         SwingUtilities.invokeLater(() -> new TikaGUI(
-                new DigestingParser(new AutoDetectParser(finalConfig),
-                        new CommonsDigester(MAX_MARK, CommonsDigester.DigestAlgorithm.MD5,
-                                CommonsDigester.DigestAlgorithm.SHA256), false)).setVisible(true));
+                new DigestingParser(new AutoDetectParser(finalConfig), new CommonsDigester(MAX_MARK, CommonsDigester.DigestAlgorithm.MD5, CommonsDigester.DigestAlgorithm.SHA256),
+                        false)).setVisible(true));
     }
 
     private void addMenuBar() {
@@ -248,15 +247,16 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
                 openFile(chooser.getSelectedFile());
             }
         } else if ("openurl".equals(command)) {
-            Object rv = JOptionPane
-                    .showInputDialog(this, "Enter the URL of the resource to be parsed:",
-                            "Open URL", JOptionPane.PLAIN_MESSAGE, null, null, "");
-            if (rv != null && rv.toString().length() > 0) {
+            Object rv = JOptionPane.showInputDialog(this, "Enter the URL of the resource to be parsed:", "Open URL", JOptionPane.PLAIN_MESSAGE, null, null, "");
+            if (rv != null && rv
+                    .toString()
+                    .length() > 0) {
                 try {
-                    openURL(new URL(rv.toString().trim()));
+                    openURL(new URL(rv
+                            .toString()
+                            .trim()));
                 } catch (MalformedURLException exception) {
-                    JOptionPane.showMessageDialog(this, "The given string is not a valid URL",
-                            "Invalid URL", JOptionPane.ERROR_MESSAGE);
+                    JOptionPane.showMessageDialog(this, "The given string is not a valid URL", "Invalid URL", JOptionPane.ERROR_MESSAGE);
                 }
             }
         } else if ("html".equals(command)) {
@@ -274,7 +274,9 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
         } else if ("about".equals(command)) {
             textDialog("About Apache Tika", TikaGUI.class.getResource("about.html"));
         } else if ("exit".equals(command)) {
-            Toolkit.getDefaultToolkit().getSystemEventQueue()
+            Toolkit
+                    .getDefaultToolkit()
+                    .getSystemEventQueue()
                     .postEvent(new WindowEvent(this, WindowEvent.WINDOW_CLOSING));
         }
     }
@@ -309,8 +311,7 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
         StringBuilder metadataBuffer = new StringBuilder();
 
         ContentHandler handler =
-                new TeeContentHandler(getHtmlHandler(htmlBuffer), getTextContentHandler(textBuffer),
-                        getTextMainContentHandler(textMainBuffer), getXmlContentHandler(xmlBuffer));
+                new TeeContentHandler(getHtmlHandler(htmlBuffer), getTextContentHandler(textBuffer), getTextMainContentHandler(textMainBuffer), getXmlContentHandler(xmlBuffer));
 
         context.set(DocumentSelector.class, new ImageDocumentSelector());
 
@@ -363,14 +364,12 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
             input.reset();
             isReset = true;
         } catch (IOException e) {
-            setText(json, "Error during stream reset.\n" + "There's a limit of " + MAX_MARK +
-                    " bytes for this type of processing in the GUI.\n" +
+            setText(json, "Error during stream reset.\n" + "There's a limit of " + MAX_MARK + " bytes for this type of processing in the GUI.\n" +
                     "Try the app with command line argument of -J.");
         }
         if (isReset) {
             RecursiveParserWrapperHandler recursiveParserWrapperHandler =
-                    new RecursiveParserWrapperHandler(new BasicContentHandlerFactory(
-                            BasicContentHandlerFactory.HANDLER_TYPE.BODY, -1), -1);
+                    new RecursiveParserWrapperHandler(new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.BODY, -1), -1);
             RecursiveParserWrapper wrapper = new RecursiveParserWrapper(parser);
             wrapper.parse(input, recursiveParserWrapperHandler, new Metadata(), new ParseContext());
             StringWriter jsonBuffer = new StringWriter();
@@ -384,7 +383,10 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
     private void handleError(String name, Throwable t) {
         StringWriter writer = new StringWriter();
         writer.append("Apache Tika was unable to parse the document\n");
-        writer.append("at ").append(name).append(".\n\n");
+        writer
+                .append("at ")
+                .append(name)
+                .append(".\n\n");
         writer.append("The full exception stack trace is included below:\n\n");
         t.printStackTrace(new PrintWriter(writer));
 
@@ -406,8 +408,7 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
             editor.setContentType("text/html");
             editor.setEditable(false);
             editor.setBackground(Color.WHITE);
-            editor.setTransferHandler(
-                    new ParsingTransferHandler(editor.getTransferHandler(), this));
+            editor.setTransferHandler(new ParsingTransferHandler(editor.getTransferHandler(), this));
             panel.add(new JScrollPane(editor), name);
         } catch (IOException e) {
             e.printStackTrace();
@@ -445,8 +446,7 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
             try {
                 URL url = e.getURL();
                 try (InputStream stream = url.openStream()) {
-                    JEditorPane editor =
-                            new JEditorPane("text/plain", IOUtils.toString(stream, UTF_8));
+                    JEditorPane editor = new JEditorPane("text/plain", IOUtils.toString(stream, UTF_8));
                     editor.setEditable(false);
                     editor.setBackground(Color.WHITE);
                     editor.setCaretPosition(0);
@@ -495,12 +495,13 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
     private ContentHandler getHtmlHandler(Writer writer) throws TransformerConfigurationException {
         SAXTransformerFactory factory = (SAXTransformerFactory) SAXTransformerFactory.newInstance();
         TransformerHandler handler = factory.newTransformerHandler();
-        handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "html");
+        handler
+                .getTransformer()
+                .setOutputProperty(OutputKeys.METHOD, "html");
         handler.setResult(new StreamResult(writer));
         return new ContentHandlerDecorator(handler) {
             @Override
-            public void startElement(String uri, String localName, String name, Attributes atts)
-                    throws SAXException {
+            public void startElement(String uri, String localName, String name, Attributes atts) throws SAXException {
                 if (XHTMLContentHandler.XHTML.equals(uri)) {
                     uri = null;
                 }
@@ -520,11 +521,12 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
                                     String filename = src.substring(src.indexOf(':') + 1);
                                     try {
                                         File img = imageParser.requestSave(filename);
-                                        String newSrc = img.toURI().toString();
+                                        String newSrc = img
+                                                .toURI()
+                                                .toString();
                                         newAttrs.setValue(i, newSrc);
                                     } catch (IOException e) {
-                                        System.err.println(
-                                                "Error creating temp image file " + filename);
+                                        System.err.println("Error creating temp image file " + filename);
                                         // The html viewer will show a broken image too to alert them
                                     }
                                 }
@@ -565,11 +567,12 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
         return new BoilerpipeContentHandler(writer);
     }
 
-    private ContentHandler getXmlContentHandler(Writer writer)
-            throws TransformerConfigurationException {
+    private ContentHandler getXmlContentHandler(Writer writer) throws TransformerConfigurationException {
         SAXTransformerFactory factory = (SAXTransformerFactory) SAXTransformerFactory.newInstance();
         TransformerHandler handler = factory.newTransformerHandler();
-        handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "xml");
+        handler
+                .getTransformer()
+                .setOutputProperty(OutputKeys.METHOD, "xml");
         handler.setResult(new StreamResult(writer));
         return handler;
     }
@@ -598,7 +601,9 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
             this.downstreamParser = downstreamParser;
 
             try {
-                File t = Files.createTempFile("tika", ".test").toFile();
+                File t = Files
+                        .createTempFile("tika", ".test")
+                        .toFile();
                 tmpDir = t.getParentFile();
             } catch (IOException e) {
                 //swallow
@@ -613,7 +618,9 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
                 embeddedName.substring(splitAt);
             }
 
-            File tmp = Files.createTempFile("tika-embedded-", suffix).toFile();
+            File tmp = Files
+                    .createTempFile("tika-embedded-", suffix)
+                    .toFile();
             wanted.put(embeddedName, tmp);
             return tmp;
         }
@@ -622,8 +629,7 @@ public class TikaGUI extends JFrame implements ActionListener, HyperlinkListener
             return downstreamParser.getSupportedTypes(context);
         }
 
-        public void parse(InputStream stream, ContentHandler handler, Metadata metadata,
-                          ParseContext context) throws IOException, SAXException, TikaException {
+        public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context) throws IOException, SAXException, TikaException {
             String name = metadata.get(TikaCoreProperties.RESOURCE_NAME_KEY);
             if (name != null && wanted.containsKey(name)) {
                 FileOutputStream out = new FileOutputStream(wanted.get(name));
diff --git a/tika-app/src/test/java/org/apache/tika/cli/TikaCLIAsyncTest.java b/tika-app/src/test/java/org/apache/tika/cli/TikaCLIAsyncTest.java
index d9f6d053f..4913f96cd 100644
--- a/tika-app/src/test/java/org/apache/tika/cli/TikaCLIAsyncTest.java
+++ b/tika-app/src/test/java/org/apache/tika/cli/TikaCLIAsyncTest.java
@@ -39,19 +39,12 @@ public class TikaCLIAsyncTest extends TikaCLITest {
     @BeforeAll
     public static void setUpClass() throws Exception {
         ASYNC_CONFIG = Files.createTempFile(ASYNC_OUTPUT_DIR, "async-config-", ".xml");
-        String xml = "<properties>" + "<async>" + "<numClients>3</numClients>" +
-                "<tikaConfig>" + ASYNC_CONFIG.toAbsolutePath() + "</tikaConfig>" +
-                "</async>" + "<fetchers>" +
-                "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" +
-                "<name>fsf</name>" + "<basePath>" + TEST_DATA_FILE.getAbsolutePath() +
-                "</basePath>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
-                "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" +
-                "<name>fse</name>" + "<basePath>" + ASYNC_OUTPUT_DIR.toAbsolutePath() +
-                "</basePath>" + "<prettyPrint>true</prettyPrint>" + "</emitter>" + "</emitters>" +
-                "<pipesIterator class=\"org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator\">" +
-                "<basePath>" + TEST_DATA_FILE.getAbsolutePath() + "</basePath>" +
-                "<fetcherName>fsf</fetcherName>" + "<emitterName>fse</emitterName>" +
-                "</pipesIterator>" + "</properties>";
+        String xml = "<properties>" + "<async>" + "<numClients>3</numClients>" + "<tikaConfig>" + ASYNC_CONFIG.toAbsolutePath() + "</tikaConfig>" + "</async>" + "<fetchers>" +
+                "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" + "<name>fsf</name>" + "<basePath>" + TEST_DATA_FILE.getAbsolutePath() + "</basePath>" +
+                "</fetcher>" + "</fetchers>" + "<emitters>" + "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" + "<name>fse</name>" + "<basePath>" +
+                ASYNC_OUTPUT_DIR.toAbsolutePath() + "</basePath>" + "<prettyPrint>true</prettyPrint>" + "</emitter>" + "</emitters>" +
+                "<pipesIterator class=\"org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator\">" + "<basePath>" + TEST_DATA_FILE.getAbsolutePath() + "</basePath>" +
+                "<fetcherName>fsf</fetcherName>" + "<emitterName>fse</emitterName>" + "</pipesIterator>" + "</properties>";
         Files.write(ASYNC_CONFIG, xml.getBytes(UTF_8));
     }
 
@@ -60,10 +53,16 @@ public class TikaCLIAsyncTest extends TikaCLITest {
         String content = getParamOutContent("-a", "--config=" + ASYNC_CONFIG.toAbsolutePath());
 
         int json = 0;
-        for (File f : ASYNC_OUTPUT_DIR.toFile().listFiles()) {
-            if (f.getName().endsWith(".json")) {
+        for (File f : ASYNC_OUTPUT_DIR
+                .toFile()
+                .listFiles()) {
+            if (f
+                    .getName()
+                    .endsWith(".json")) {
                 //check one file for pretty print
-                if (f.getName().equals("coffee.xls.json")) {
+                if (f
+                        .getName()
+                        .equals("coffee.xls.json")) {
                     checkForPrettyPrint(f);
                 }
                 json++;
@@ -76,10 +75,9 @@ public class TikaCLIAsyncTest extends TikaCLITest {
         String json = FileUtils.readFileToString(f, UTF_8);
         int previous = json.indexOf("Content-Length");
         assertTrue(previous > -1);
-        for (String k : new String[]{"Content-Type", "dc:creator",
-                "dcterms:created", "dcterms:modified", "X-TIKA:content\""}) {
+        for (String k : new String[]{"Content-Type", "dc:creator", "dcterms:created", "dcterms:modified", "X-TIKA:content\""}) {
             int i = json.indexOf(k);
-            assertTrue( i > -1, "should have found " + k);
+            assertTrue(i > -1, "should have found " + k);
             assertTrue(i > previous, "bad order: " + k + " at " + i + " not less than " + previous);
             previous = i;
         }
diff --git a/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchCommandLineTest.java b/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchCommandLineTest.java
index 3328db596..d25b35cfd 100644
--- a/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchCommandLineTest.java
+++ b/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchCommandLineTest.java
@@ -59,9 +59,10 @@ public class TikaCLIBatchCommandLineTest {
         } catch (IOException e) {
             throw new RuntimeException("Couldn't open testFile");
         }
-        testInputPathForCommandLine = testInput.toAbsolutePath().toString();
-        escapedInputPathForCommandLine =
-                BatchCommandLineBuilder.commandLineSafe(testInputPathForCommandLine);
+        testInputPathForCommandLine = testInput
+                .toAbsolutePath()
+                .toString();
+        escapedInputPathForCommandLine = BatchCommandLineBuilder.commandLineSafe(testInputPathForCommandLine);
     }
 
     @AfterEach
@@ -84,15 +85,16 @@ public class TikaCLIBatchCommandLineTest {
 
     @Test
     public void testJVMOpts() throws Exception {
-        String[] params = {"-JXmx1g", "-JDlog4j.configuration=batch_process_log4j.xml", "-inputDir",
-                testInputPathForCommandLine, "-outputDir", "testout-output"};
+        String[] params = {"-JXmx1g", "-JDlog4j.configuration=batch_process_log4j.xml", "-inputDir", testInputPathForCommandLine, "-outputDir", "testout-output"};
 
 
         String[] commandLine = BatchCommandLineBuilder.build(params);
         StringBuilder sb = new StringBuilder();
 
         for (String s : commandLine) {
-            sb.append(s).append(" ");
+            sb
+                    .append(s)
+                    .append(" ");
         }
         String s = sb.toString();
         int classInd = s.indexOf("org.apache.tika.batch.fs.FSBatchProcessCLI");
@@ -110,8 +112,7 @@ public class TikaCLIBatchCommandLineTest {
 
     @Test
     public void testBasicMappingOfArgs() throws Exception {
-        String[] params = {"-JXmx1g", "-JDlog4j.configuration=batch_process_log4j.xml", "-bc",
-                "batch-config.xml", "-J", "-h", "-inputDir", testInputPathForCommandLine};
+        String[] params = {"-JXmx1g", "-JDlog4j.configuration=batch_process_log4j.xml", "-bc", "batch-config.xml", "-J", "-h", "-inputDir", testInputPathForCommandLine};
 
         String[] commandLine = BatchCommandLineBuilder.build(params);
         Map<String, String> attrs = mapify(commandLine);
@@ -163,8 +164,7 @@ public class TikaCLIBatchCommandLineTest {
         String outputRoot = "outputRoot";
         String configPath = "c:/somewhere/someConfig.xml";
 
-        String[] params = {"--inputDir", testInputPathForCommandLine, "--outputDir", outputRoot,
-                "--config=" + configPath};
+        String[] params = {"--inputDir", testInputPathForCommandLine, "--outputDir", outputRoot, "--config=" + configPath};
         String[] commandLine = BatchCommandLineBuilder.build(params);
         Map<String, String> attrs = mapify(commandLine);
         assertEquals(escapedInputPathForCommandLine, attrs.get("-inputDir"));
@@ -177,7 +177,9 @@ public class TikaCLIBatchCommandLineTest {
     public void testOneDirOneFileException() throws Exception {
         boolean ex = false;
         try {
-            String path = testFile.toAbsolutePath().toString();
+            String path = testFile
+                    .toAbsolutePath()
+                    .toString();
             path = ProcessUtils.escapeCommandLine(path);
             String[] params = {testInputPathForCommandLine, path};
 
diff --git a/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchIntegrationTest.java b/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchIntegrationTest.java
index b482db13e..d42a9795f 100644
--- a/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchIntegrationTest.java
+++ b/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchIntegrationTest.java
@@ -40,7 +40,7 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadataList;
 
 public class TikaCLIBatchIntegrationTest {
 
@@ -67,13 +67,20 @@ public class TikaCLIBatchIntegrationTest {
         err = System.err;
         System.setOut(outWriter);
         System.setErr(errWriter);
-        testInputDirForCommandLine = testInputDir.toAbsolutePath().toString();
-        tempOutputDirForCommandLine = tempOutputDir.toAbsolutePath().toString();
+        testInputDirForCommandLine = testInputDir
+                .toAbsolutePath()
+                .toString();
+        tempOutputDirForCommandLine = tempOutputDir
+                .toAbsolutePath()
+                .toString();
         customBatchLogging = tempOutputDir.resolve(propsFileName);
         configFile = Files.createTempFile("tika-app-batch-", ".xml");
-        Files.copy(this.getClass().getResourceAsStream("/" + propsFileName), customBatchLogging);
-        Files.copy(this.getClass().getResourceAsStream("/test-data/tika-config1.xml"),
-                configFile, StandardCopyOption.REPLACE_EXISTING);
+        Files.copy(this
+                .getClass()
+                .getResourceAsStream("/" + propsFileName), customBatchLogging);
+        Files.copy(this
+                .getClass()
+                .getResourceAsStream("/test-data/tika-config1.xml"), configFile, StandardCopyOption.REPLACE_EXISTING);
     }
 
     @AfterEach
@@ -95,18 +102,17 @@ public class TikaCLIBatchIntegrationTest {
 
     @Test
     public void testTikaConfig() throws Exception {
-        String[] params = {
-                "-i", testInputDirForCommandLine,
-                "-o", tempOutputDirForCommandLine,
-                "--config=" + configFile.toAbsolutePath().toString() };
+        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine, "--config=" + configFile
+                .toAbsolutePath()
+                .toString()};
         TikaCLI.main(params);
         assertFileExists(tempOutputDir.resolve("bad_xml.xml.xml"));
         assertFileExists(tempOutputDir.resolve("coffee.xls.xml"));
     }
+
     @Test
     public void testBasicBatchIntegration() throws Exception {
-        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine,
-                "-numConsumers", "2"};
+        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine, "-numConsumers", "2"};
         TikaCLI.main(params);
 
         assertFileExists(tempOutputDir.resolve("bad_xml.xml.xml"));
@@ -115,8 +121,7 @@ public class TikaCLIBatchIntegrationTest {
 
     @Test
     public void testJsonRecursiveBatchIntegration() throws Exception {
-        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine,
-                "-numConsumers", "10", "-J", //recursive Json
+        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine, "-numConsumers", "10", "-J", //recursive Json
                 "-t" //plain text in content
         };
         TikaCLI.main(params);
@@ -125,15 +130,16 @@ public class TikaCLIBatchIntegrationTest {
         try (Reader reader = Files.newBufferedReader(jsonFile, UTF_8)) {
             List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
             assertEquals(12, metadataList.size());
-            assertTrue(metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT)
+            assertTrue(metadataList
+                    .get(6)
+                    .get(TikaCoreProperties.TIKA_CONTENT)
                     .contains("human events"));
         }
     }
 
     @Test
     public void testStreamingJsonRecursiveBatchIntegration() throws Exception {
-        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine,
-                "-numConsumers", "10", "-J", //recursive Json
+        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine, "-numConsumers", "10", "-J", //recursive Json
                 "-t", //plain text in content
                 "-streamOut"};
         TikaCLI.main(params);
@@ -142,17 +148,20 @@ public class TikaCLIBatchIntegrationTest {
         try (Reader reader = Files.newBufferedReader(jsonFile, UTF_8)) {
             List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
             assertEquals(12, metadataList.size());
-            assertTrue(metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT)
+            assertTrue(metadataList
+                    .get(6)
+                    .get(TikaCoreProperties.TIKA_CONTENT)
                     .contains("human events"));
             //test that the last written object has been bumped to the first by JsonMetadataList.fromJson()
-            assertNull(metadataList.get(0).get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH));
+            assertNull(metadataList
+                    .get(0)
+                    .get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH));
         }
     }
 
     @Test
     public void testProcessLogFileConfig() throws Exception {
-        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine,
-                "-numConsumers", "2", "-JDlog4j.configurationFile=" + customBatchLogging.toUri()};
+        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine, "-numConsumers", "2", "-JDlog4j.configurationFile=" + customBatchLogging.toUri()};
         TikaCLI.main(params);
 
         assertFileExists(tempOutputDir.resolve("bad_xml.xml.xml"));
@@ -182,8 +191,7 @@ public class TikaCLIBatchIntegrationTest {
             IOUtils.closeQuietly(reader);
         }
 */
-        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine,
-                "-numConsumers", "10", "-J", //recursive Json
+        String[] params = {"-i", testInputDirForCommandLine, "-o", tempOutputDirForCommandLine, "-numConsumers", "10", "-J", //recursive Json
                 "-t", //plain text in content
                 "-digest", "sha512"};
         TikaCLI.main(params);
@@ -192,15 +200,18 @@ public class TikaCLIBatchIntegrationTest {
 
             List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
             assertEquals(12, metadataList.size());
-            assertNotNull(metadataList.get(0).get("X-TIKA:digest:SHA512"));
-            assertTrue(metadataList.get(0).get("X-TIKA:digest:SHA512")
+            assertNotNull(metadataList
+                    .get(0)
+                    .get("X-TIKA:digest:SHA512"));
+            assertTrue(metadataList
+                    .get(0)
+                    .get("X-TIKA:digest:SHA512")
                     .startsWith("ee46d973ee1852c01858"));
         }
     }
 
     private void assertFileExists(Path path) {
-        assertTrue(Files.isRegularFile(path),
-                "File doesn't exist: " + path.toAbsolutePath());
+        assertTrue(Files.isRegularFile(path), "File doesn't exist: " + path.toAbsolutePath());
     }
 
 }
diff --git a/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java b/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java
index fa16e124a..0e55ac7db 100644
--- a/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java
+++ b/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java
@@ -44,11 +44,9 @@ import org.apache.tika.utils.ProcessUtils;
 public class TikaCLITest {
 
     static final File TEST_DATA_FILE = new File("src/test/resources/test-data");
-
-
+    private final URI testDataURI = TEST_DATA_FILE.toURI();
     @TempDir
     private Path extractDir;
-    private final URI testDataURI = TEST_DATA_FILE.toURI();
     /* Test members */
     private ByteArrayOutputStream outContent = null;
     private ByteArrayOutputStream errContent = null;
@@ -57,15 +55,13 @@ public class TikaCLITest {
     private String resourcePrefix;
 
 
-
     protected static void assertExtracted(Path p, String allFiles) throws IOException {
 
         assertTrue(Files.exists(p), "File " + p.getFileName() + " not found in " + allFiles);
 
         assertFalse(Files.isDirectory(p), "File " + p.getFileName() + " is a directory!");
 
-        assertTrue(Files.size(p) > 0,
-                "File " + p.getFileName() + " wasn't extracted with " + "contents");
+        assertTrue(Files.size(p) > 0, "File " + p.getFileName() + " wasn't extracted with " + "contents");
     }
 
     /**
@@ -144,8 +140,7 @@ public class TikaCLITest {
         assertTrue(content.contains("?xml version=\"1.0\" encoding=\"UTF-8\"?"));
 
         content = getParamOutContent("-x", "--digest=SHA256", resourcePrefix + "alice.cli.test");
-        assertTrue(content.contains(
-                "<meta name=\"X-TIKA:digest:SHA256\" content=\"e90779adbac09c4ee"));
+        assertTrue(content.contains("<meta name=\"X-TIKA:digest:SHA256\" content=\"e90779adbac09c4ee"));
 
     }
 
@@ -158,12 +153,10 @@ public class TikaCLITest {
     public void testHTMLOutput() throws Exception {
         String content = getParamOutContent("-h", resourcePrefix + "alice.cli.test");
         assertTrue(content.contains("html xmlns=\"http://www.w3.org/1999/xhtml"));
-        assertTrue(content.contains("<title></title>"),
-                "Expanded <title></title> element should be present");
+        assertTrue(content.contains("<title></title>"), "Expanded <title></title> element should be present");
 
         content = getParamOutContent("-h", "--digest=SHA384", resourcePrefix + "alice.cli.test");
-        assertTrue(content.contains(
-                "<meta name=\"X-TIKA:digest:SHA384\" content=\"c69ea023f5da95a026"));
+        assertTrue(content.contains("<meta name=\"X-TIKA:digest:SHA384\" content=\"c69ea023f5da95a026"));
     }
 
     /**
@@ -224,8 +217,7 @@ public class TikaCLITest {
      */
     @Test
     public void testJsonMetadataOutput() throws Exception {
-        String json = getParamOutContent("--json", "--digest=MD2",
-                resourcePrefix + "testJsonMultipleInts.html");
+        String json = getParamOutContent("--json", "--digest=MD2", resourcePrefix + "testJsonMultipleInts.html");
         //TIKA-1310
         assertTrue(json.contains("\"fb:admins\":\"1,2,3,4\","));
         assertTrue(json.contains("\"X-TIKA:digest:MD2\":"));
@@ -238,12 +230,9 @@ public class TikaCLITest {
      */
     @Test
     public void testJsonMetadataPrettyPrintOutput() throws Exception {
-        String json =
-                getParamOutContent("--json", "-r", resourcePrefix + "testJsonMultipleInts.html");
+        String json = getParamOutContent("--json", "-r", resourcePrefix + "testJsonMultipleInts.html");
 
-        assertTrue(json.contains(
-                "\"X-TIKA:Parsed-By\" : [ \"org.apache.tika.parser.DefaultParser\", " +
-                        "\"org.apache.tika.parser.html.JSoupParser\" ],"));
+        assertTrue(json.contains("\"X-TIKA:Parsed-By\" : [ \"org.apache.tika.parser.DefaultParser\", " + "\"org.apache.tika.parser.html.JSoupParser\" ],"));
         //test pretty-print alphabetic sort of keys
         int enc = json.indexOf("\"Content-Encoding\"");
         int fb = json.indexOf("fb:admins");
@@ -292,16 +281,13 @@ public class TikaCLITest {
      */
     @Test
     public void testListSupportedTypes() throws Exception {
-        String content =
-                getParamOutContent("--list-supported-types", resourcePrefix + "alice.cli.test");
+        String content = getParamOutContent("--list-supported-types", resourcePrefix + "alice.cli.test");
         assertTrue(content.contains("supertype: application/octet-stream"));
     }
 
     @Test
     public void testExtractSimple() throws Exception {
-        String[] expectedChildren =
-                new String[]{"MBD002B040A.cdx", "file4.png", "MBD002B0FA6.bin", "MBD00262FE3.txt",
-                        "file0.emf"};
+        String[] expectedChildren = new String[]{"MBD002B040A.cdx", "file4.png", "MBD002B0FA6.bin", "MBD00262FE3.txt", "file0.emf"};
         testExtract("/coffee.xls", expectedChildren, 8);
     }
 
@@ -330,21 +316,21 @@ public class TikaCLITest {
         testExtract("testZip_zeroByte.zip", expectedChildren);
     }
 
-    private void testExtract(String targetFile, String[] expectedChildrenFileNames)
-            throws Exception {
+    private void testExtract(String targetFile, String[] expectedChildrenFileNames) throws Exception {
         testExtract(targetFile, expectedChildrenFileNames, expectedChildrenFileNames.length);
     }
 
-    private void testExtract(String targetFile, String[] expectedChildrenFileNames,
-                             int expectedLength) throws Exception {
+    private void testExtract(String targetFile, String[] expectedChildrenFileNames, int expectedLength) throws Exception {
 
-        String[] params = {"--extract-dir=" +
-                ProcessUtils.escapeCommandLine(extractDir.toAbsolutePath().toString()), "-z",
-                resourcePrefix + "/" + targetFile};
+        String[] params = {"--extract-dir=" + ProcessUtils.escapeCommandLine(extractDir
+                .toAbsolutePath()
+                .toString()), "-z", resourcePrefix + "/" + targetFile};
 
         TikaCLI.main(params);
 
-        String[] tempFileNames = extractDir.toFile().list();
+        String[] tempFileNames = extractDir
+                .toFile()
+                .list();
         assertNotNull(tempFileNames);
         assertEquals(expectedLength, tempFileNames.length);
         String allFiles = String.join(" : ", tempFileNames);
@@ -358,12 +344,13 @@ public class TikaCLITest {
     public void testExtractTgz() throws Exception {
         //TIKA-2564
 
-        String[] params = {"--extract-dir=" + extractDir.toAbsolutePath(), "-z",
-                resourcePrefix + "/test-documents.tgz"};
+        String[] params = {"--extract-dir=" + extractDir.toAbsolutePath(), "-z", resourcePrefix + "/test-documents.tgz"};
 
         TikaCLI.main(params);
 
-        String[] tempFileNames = extractDir.toFile().list();
+        String[] tempFileNames = extractDir
+                .toFile()
+                .list();
         assertNotNull(tempFileNames);
         String allFiles = String.join(" : ", tempFileNames);
 
@@ -387,8 +374,7 @@ public class TikaCLITest {
     public void testZipWithSubdirs() throws Exception {
         new File("subdir/foo.txt").delete();
         new File("subdir").delete();
-        String content = getParamOutContent("-z", "--extract-dir=target",
-                resourcePrefix + "testWithSubdirs.zip");
+        String content = getParamOutContent("-z", "--extract-dir=target", resourcePrefix + "testWithSubdirs.zip");
         assertTrue(content.contains("Extracting 'subdir/foo.txt'"));
         // clean up. TODO: These should be in target.
         new File("target/subdir/foo.txt").delete();
@@ -397,12 +383,13 @@ public class TikaCLITest {
 
     @Test
     public void testExtractInlineImages() throws Exception {
-        String[] params = {"--extract-dir=" + extractDir.toAbsolutePath(), "-z",
-                resourcePrefix + "/testPDF_childAttachments.pdf"};
+        String[] params = {"--extract-dir=" + extractDir.toAbsolutePath(), "-z", resourcePrefix + "/testPDF_childAttachments.pdf"};
 
         TikaCLI.main(params);
 
-        String[] tempFileNames = extractDir.toFile().list();
+        String[] tempFileNames = extractDir
+                .toFile()
+                .list();
         assertNotNull(tempFileNames);
         String allFiles = String.join(" : ", tempFileNames);
 
@@ -434,18 +421,14 @@ public class TikaCLITest {
 
     @Test
     public void testConfig() throws Exception {
-        String content =
-                getParamOutContent("--config=" + TEST_DATA_FILE.toString() + "/tika-config1.xml",
-                        resourcePrefix + "bad_xml.xml");
+        String content = getParamOutContent("--config=" + TEST_DATA_FILE.toString() + "/tika-config1.xml", resourcePrefix + "bad_xml.xml");
         assertTrue(content.contains("apple"));
         assertTrue(content.contains("org.apache.tika.parser.html.JSoupParser"));
     }
 
     @Test
     public void testConfigIgnoreInit() throws Exception {
-        String content = getParamOutContent(
-                "--config=" + TEST_DATA_FILE.toString() + "/TIKA-2389-ignore-init-problems.xml",
-                resourcePrefix + "test_recursive_embedded.docx");
+        String content = getParamOutContent("--config=" + TEST_DATA_FILE.toString() + "/TIKA-2389-ignore-init-problems.xml", resourcePrefix + "test_recursive_embedded.docx");
         assertTrue(content.contains("embed_1a"));
         //TODO: add a real unit test that configures logging to a file to test that nothing is
         //written at the various logging levels
@@ -454,79 +437,62 @@ public class TikaCLITest {
 
     @Test
     public void testJsonRecursiveMetadataParserMetadataOnly() throws Exception {
-        String content = getParamOutContent("-m", "-J", "-r",
-                resourcePrefix + "test_recursive_embedded.docx");
+        String content = getParamOutContent("-m", "-J", "-r", resourcePrefix + "test_recursive_embedded.docx");
         assertTrue(content.contains("\"extended-properties:AppVersion\" : \"15.0000\","));
-        assertTrue(content.contains(
-                "\"extended-properties:Application\" : \"Microsoft Office Word\","));
+        assertTrue(content.contains("\"extended-properties:Application\" : \"Microsoft Office Word\","));
         assertTrue(content.contains("\"X-TIKA:embedded_resource_path\" : \"/embed1.zip\""));
         assertFalse(content.contains("X-TIKA:content"));
     }
 
     @Test
     public void testJsonRecursiveMetadataParserDefault() throws Exception {
-        String content =
-                getParamOutContent("-J", "-r", resourcePrefix + "test_recursive_embedded.docx");
-        assertTrue(content.contains(
-                "\"X-TIKA:content\" : \"<html xmlns=\\\"http://www.w3.org/1999/xhtml"));
+        String content = getParamOutContent("-J", "-r", resourcePrefix + "test_recursive_embedded.docx");
+        assertTrue(content.contains("\"X-TIKA:content\" : \"<html xmlns=\\\"http://www.w3.org/1999/xhtml"));
     }
 
     @Test
     public void testJsonRecursiveMetadataParserText() throws Exception {
-        String content = getParamOutContent("-J", "-r", "-t",
-                resourcePrefix + "test_recursive_embedded.docx");
+        String content = getParamOutContent("-J", "-r", "-t", resourcePrefix + "test_recursive_embedded.docx");
         assertTrue(content.contains("\\n\\nembed_4\\n"));
         assertTrue(content.contains("\\n\\nembed_0"));
     }
 
     @Test
     public void testDigestInJson() throws Exception {
-        String content = getParamOutContent("-J", "-r", "-t", "--digest=MD5",
-                resourcePrefix + "test_recursive_embedded.docx");
-        assertTrue(
-                content.contains("\"X-TIKA:digest:MD5\" : \"59f626e09a8c16ab6dbc2800c685f772\","));
-        assertTrue(
-                content.contains("\"X-TIKA:digest:MD5\" : \"f9627095ef86c482e61d99f0cc1cf87d\""));
+        String content = getParamOutContent("-J", "-r", "-t", "--digest=MD5", resourcePrefix + "test_recursive_embedded.docx");
+        assertTrue(content.contains("\"X-TIKA:digest:MD5\" : \"59f626e09a8c16ab6dbc2800c685f772\","));
+        assertTrue(content.contains("\"X-TIKA:digest:MD5\" : \"f9627095ef86c482e61d99f0cc1cf87d\""));
     }
 
     @Test
     public void testConfigSerializationStaticAndCurrent() throws Exception {
         String content = getParamOutContent("--dump-static-config");
         //make sure at least one detector is there
-        assertTrue(content.contains(
-                "<detector class=\"org.apache.tika.detect.microsoft.POIFSContainerDetector\"/>"));
+        assertTrue(content.contains("<detector class=\"org.apache.tika.detect.microsoft.POIFSContainerDetector\"/>"));
         //make sure Executable is there because follow on tests of custom config
         //test that it has been turned off.
-        assertTrue(content.contains(
-                "<parser class=\"org.apache.tika.parser.executable.ExecutableParser\"/>"));
+        assertTrue(content.contains("<parser class=\"org.apache.tika.parser.executable.ExecutableParser\"/>"));
 
         content = getParamOutContent("--dump-current-config");
         //make sure at least one detector is there
-        assertTrue(
-                content.contains("<detector class=\"org.apache.tika.detect.DefaultDetector\"/>"));
+        assertTrue(content.contains("<detector class=\"org.apache.tika.detect.DefaultDetector\"/>"));
         //and at least one parser
         assertTrue(content.contains("<parser class=\"org.apache.tika.parser.DefaultParser\"/>"));
     }
 
     @Test
     public void testConfigSerializationCustomMinimal() throws Exception {
-        String content =
-                getParamOutContent("--config=" + TEST_DATA_FILE.toString() + "/tika-config2.xml",
-                        "--dump-minimal-config").replaceAll("[\r\n\t ]+", " ");
-
-        String expected = "<parser class=\"org.apache.tika.parser.DefaultParser\">" +
-                " <mime-exclude>application/pdf</mime-exclude>" +
-                " <mime-exclude>image/jpeg</mime-exclude> " + "</parser> " +
-                "<parser class=\"org.apache.tika.parser.EmptyParser\">" +
-                " <mime>application/pdf</mime> " + "</parser>";
+        String content = getParamOutContent("--config=" + TEST_DATA_FILE.toString() + "/tika-config2.xml", "--dump-minimal-config").replaceAll("[\r\n\t ]+", " ");
+
+        String expected =
+                "<parser class=\"org.apache.tika.parser.DefaultParser\">" + " <mime-exclude>application/pdf</mime-exclude>" + " <mime-exclude>image/jpeg</mime-exclude> " +
+                        "</parser> " + "<parser class=\"org.apache.tika.parser.EmptyParser\">" + " <mime>application/pdf</mime> " + "</parser>";
         assertTrue(content.contains(expected));
     }
 
     @Test
     public void testConfigSerializationCustomStatic() throws Exception {
-        String content =
-                getParamOutContent("--config=" + TEST_DATA_FILE.toString() + "/tika-config2.xml",
-                        "--dump-static-config");
+        String content = getParamOutContent("--config=" + TEST_DATA_FILE.toString() + "/tika-config2.xml", "--dump-static-config");
         assertFalse(content.contains("org.apache.tika.parser.executable.Executable"));
     }
 
diff --git a/tika-app/src/test/java/org/apache/tika/extractor/TestEmbeddedDocumentUtil.java b/tika-app/src/test/java/org/apache/tika/extractor/TestEmbeddedDocumentUtil.java
index 3306f6f23..bcd31dffa 100644
--- a/tika-app/src/test/java/org/apache/tika/extractor/TestEmbeddedDocumentUtil.java
+++ b/tika-app/src/test/java/org/apache/tika/extractor/TestEmbeddedDocumentUtil.java
@@ -37,9 +37,7 @@ public class TestEmbeddedDocumentUtil {
         Parser p = new AutoDetectParser();
         ParseContext parseContext = new ParseContext();
         parseContext.set(Parser.class, p);
-        Parser txtParser = EmbeddedDocumentUtil
-                .tryToFindExistingLeafParser(org.apache.tika.parser.csv.TextAndCSVParser.class,
-                        parseContext);
+        Parser txtParser = EmbeddedDocumentUtil.tryToFindExistingLeafParser(org.apache.tika.parser.csv.TextAndCSVParser.class, parseContext);
         assertNotNull(txtParser);
         assertEquals(org.apache.tika.parser.csv.TextAndCSVParser.class, txtParser.getClass());
 
@@ -51,9 +49,7 @@ public class TestEmbeddedDocumentUtil {
         RecursiveParserWrapper wrapper = new RecursiveParserWrapper(d, true);
         ParseContext parseContext = new ParseContext();
         parseContext.set(Parser.class, wrapper);
-        Parser txtParser = EmbeddedDocumentUtil
-                .tryToFindExistingLeafParser(org.apache.tika.parser.csv.TextAndCSVParser.class,
-                        parseContext);
+        Parser txtParser = EmbeddedDocumentUtil.tryToFindExistingLeafParser(org.apache.tika.parser.csv.TextAndCSVParser.class, parseContext);
         assertNotNull(txtParser);
         assertEquals(org.apache.tika.parser.csv.TextAndCSVParser.class, txtParser.getClass());
     }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/BatchProcess.java b/tika-batch/src/main/java/org/apache/tika/batch/BatchProcess.java
index b9287caae..b6a991933 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/BatchProcess.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/BatchProcess.java
@@ -73,13 +73,14 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
     private int maxAliveTimeSeconds = -1;
     private boolean alreadyExecuted = false;
 
-    public BatchProcess(FileResourceCrawler fileResourceCrawler, ConsumersManager consumersManager,
-                        StatusReporter reporter, Interrupter interrupter) {
+    public BatchProcess(FileResourceCrawler fileResourceCrawler, ConsumersManager consumersManager, StatusReporter reporter, Interrupter interrupter) {
         this.fileResourceCrawler = fileResourceCrawler;
         this.consumersManager = consumersManager;
         this.reporter = reporter;
         this.interrupter = interrupter;
-        timedOuts = new ArrayBlockingQueue<>(consumersManager.getConsumers().size());
+        timedOuts = new ArrayBlockingQueue<>(consumersManager
+                .getConsumers()
+                .size());
         this.consumersManagerMaxMillis = consumersManager.getConsumersManagerMaxMillis();
     }
 
@@ -108,7 +109,9 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
 
         ParallelFileProcessingResult result = null;
         try {
-            int numConsumers = consumersManager.getConsumers().size();
+            int numConsumers = consumersManager
+                    .getConsumers()
+                    .size();
             // fileResourceCrawler, statusReporter, the Interrupter, timeoutChecker
             int numNonConsumers = 2;
             if (interrupter != null) {
@@ -119,17 +122,14 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
             }
 
             ExecutorService ex = Executors.newFixedThreadPool(numConsumers + numNonConsumers);
-            CompletionService<IFileProcessorFutureResult> completionService =
-                    new ExecutorCompletionService<>(ex);
+            CompletionService<IFileProcessorFutureResult> completionService = new ExecutorCompletionService<>(ex);
             TimeoutChecker timeoutChecker = new TimeoutChecker();
 
             try {
                 startConsumersManager();
             } catch (BatchNoRestartError e) {
-                return new ParallelFileProcessingResult(0, 0, 0, 0, 0,
-                        BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE,
-                        CAUSE_FOR_TERMINATION.CONSUMERS_MANAGER_DIDNT_INIT_IN_TIME_NO_RESTART
-                                .toString());
+                return new ParallelFileProcessingResult(0, 0, 0, 0, 0, BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE,
+                        CAUSE_FOR_TERMINATION.CONSUMERS_MANAGER_DIDNT_INIT_IN_TIME_NO_RESTART.toString());
 
             }
 
@@ -142,8 +142,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         return result;
     }
 
-    private State mainLoop(CompletionService<IFileProcessorFutureResult> completionService,
-                           TimeoutChecker timeoutChecker) {
+    private State mainLoop(CompletionService<IFileProcessorFutureResult> completionService, TimeoutChecker timeoutChecker) {
         alreadyExecuted = true;
         State state = new State();
         LOG.info("BatchProcess starting up");
@@ -162,13 +161,14 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
             completionService.submit(consumer);
         }
 
-        state.numConsumers = consumersManager.getConsumers().size();
+        state.numConsumers = consumersManager
+                .getConsumers()
+                .size();
         CAUSE_FOR_TERMINATION causeForTermination = null;
         //main processing loop
         while (true) {
             try {
-                Future<IFileProcessorFutureResult> futureResult =
-                        completionService.poll(1, TimeUnit.SECONDS);
+                Future<IFileProcessorFutureResult> futureResult = completionService.poll(1, TimeUnit.SECONDS);
 
                 if (futureResult != null) {
                     state.removed++;
@@ -212,9 +212,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         return state;
     }
 
-    private ParallelFileProcessingResult shutdown(ExecutorService ex,
-                                                  CompletionService<IFileProcessorFutureResult> completionService,
-                                                  TimeoutChecker timeoutChecker, State state) {
+    private ParallelFileProcessingResult shutdown(ExecutorService ex, CompletionService<IFileProcessorFutureResult> completionService, TimeoutChecker timeoutChecker, State state) {
 
         if (reporter != null) {
             reporter.setIsShuttingDown(true);
@@ -248,8 +246,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         //has shut down by now.
         LOG.trace("About to shutdownNow()");
         List<Runnable> neverCalled = ex.shutdownNow();
-        LOG.trace("TERMINATED {} : {} : {}", ex.isTerminated(), state.consumersRemoved,
-                state.crawlersRemoved);
+        LOG.trace("TERMINATED {} : {} : {}", ex.isTerminated(), state.consumersRemoved, state.crawlersRemoved);
 
         int end = state.numConsumers + state.numNonConsumers - state.removed - neverCalled.size();
 
@@ -271,16 +268,11 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
                     FileConsumerFutureResult consumerResult = (FileConsumerFutureResult) result;
                     FileStarted fileStarted = consumerResult.getFileStarted();
                     LOG.trace("file started " + fileStarted);
-                    if (fileStarted != null &&
-                            fileStarted.getElapsedMillis() > timeoutThresholdMillis) {
-                        LOG.warn(
-                                "{} caused a file processor to hang or crash. You may need to remove " +
-                                        "this file from your input set and rerun.",
-                                fileStarted.getResourceId());
+                    if (fileStarted != null && fileStarted.getElapsedMillis() > timeoutThresholdMillis) {
+                        LOG.warn("{} caused a file processor to hang or crash. You may need to remove " + "this file from your input set and rerun.", fileStarted.getResourceId());
                     }
                 } else if (result instanceof FileResourceCrawlerFutureResult) {
-                    FileResourceCrawlerFutureResult crawlerResult =
-                            (FileResourceCrawlerFutureResult) result;
+                    FileResourceCrawlerFutureResult crawlerResult = (FileResourceCrawlerFutureResult) result;
                     considered += crawlerResult.getConsidered();
                     added += crawlerResult.getAdded();
                 } //else ...we don't care about anything else stopping at this point
@@ -292,8 +284,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         }
         //do we need to restart?
         String restartMsg = null;
-        if (state.causeForTermination == CAUSE_FOR_TERMINATION.PARENT_SHUTDOWN ||
-                state.causeForTermination == CAUSE_FOR_TERMINATION.MAIN_LOOP_EXCEPTION_NO_RESTART) {
+        if (state.causeForTermination == CAUSE_FOR_TERMINATION.PARENT_SHUTDOWN || state.causeForTermination == CAUSE_FOR_TERMINATION.MAIN_LOOP_EXCEPTION_NO_RESTART) {
             //do not restart!!!
         } else if (state.causeForTermination == CAUSE_FOR_TERMINATION.MAIN_LOOP_EXCEPTION) {
             restartMsg = "Uncaught consumer throwable";
@@ -301,8 +292,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
             if (areResourcesPotentiallyRemaining()) {
                 restartMsg = "Consumer timed out with resources remaining";
             }
-        } else if (state.causeForTermination ==
-                CAUSE_FOR_TERMINATION.BATCH_PROCESS_ALIVE_TOO_LONG) {
+        } else if (state.causeForTermination == CAUSE_FOR_TERMINATION.BATCH_PROCESS_ALIVE_TOO_LONG) {
             restartMsg = BATCH_CONSTANTS.BATCH_PROCESS_EXCEEDED_MAX_ALIVE_TIME.toString();
         } else if (state.causeForTermination == CAUSE_FOR_TERMINATION.CRAWLER_TIMED_OUT) {
             restartMsg = "Crawler timed out.";
@@ -320,8 +310,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         timeoutChecker.checkForTimedOutConsumers();
 
         for (FileStarted fs : timedOuts) {
-            LOG.warn("A parser was still working on >{}< for {} milliseconds after it started. " +
-                            "This exceeds the maxTimeoutMillis parameter", fs.getResourceId(),
+            LOG.warn("A parser was still working on >{}< for {} milliseconds after it started. " + "This exceeds the maxTimeoutMillis parameter", fs.getResourceId(),
                     fs.getElapsedMillis());
         }
         double elapsed = ((double) System.currentTimeMillis() - (double) state.start) / 1000.0;
@@ -332,8 +321,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
             numExceptions += c.getNumHandledExceptions();
         }
         LOG.trace("returning " + state.causeForTermination);
-        return new ParallelFileProcessingResult(considered, added, processed, numExceptions,
-                elapsed, exitStatus, state.causeForTermination.toString());
+        return new ParallelFileProcessingResult(considered, added, processed, numExceptions, elapsed, exitStatus, state.causeForTermination.toString());
     }
 
     private void startConsumersManager() {
@@ -356,8 +344,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         }
         if (timed.isAlive()) {
             LOG.error("ConsumersManager did not start within {}ms", consumersManagerMaxMillis);
-            throw new BatchNoRestartError(
-                    "ConsumersManager did not start within " + consumersManagerMaxMillis + "ms");
+            throw new BatchNoRestartError("ConsumersManager did not start within " + consumersManagerMaxMillis + "ms");
         }
     }
 
@@ -380,9 +367,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         }
         if (timed.isAlive()) {
             LOG.error("ConsumersManager was still alive during shutdown!");
-            throw new BatchNoRestartError(
-                    "ConsumersManager did not shutdown within: " + consumersManagerMaxMillis +
-                            "ms");
+            throw new BatchNoRestartError("ConsumersManager did not shutdown within: " + consumersManagerMaxMillis + "ms");
         }
     }
 
@@ -406,9 +391,7 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
             }
             long elapsed = System.currentTimeMillis() - start;
             if (pauseOnEarlyTerminationMillis > -1 && elapsed > pauseOnEarlyTerminationMillis) {
-                LOG.warn(
-                        "Waited after an early termination for {}ms, but there was at least one active consumer",
-                        elapsed);
+                LOG.warn("Waited after an early termination for {}ms, but there was at least one active consumer", elapsed);
                 return;
             }
         }
@@ -429,17 +412,14 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         }
 
         if (restartMsg != null) {
-            if (restartMsg
-                    .equals(BATCH_CONSTANTS.BATCH_PROCESS_EXCEEDED_MAX_ALIVE_TIME.toString())) {
+            if (restartMsg.equals(BATCH_CONSTANTS.BATCH_PROCESS_EXCEEDED_MAX_ALIVE_TIME.toString())) {
                 LOG.warn(restartMsg);
             } else {
                 LOG.error(restartMsg);
             }
 
             //send over stdout wrapped in outputStreamWriter
-            outputStreamWriter.println(
-                    BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString() + " >> " +
-                            restartMsg);
+            outputStreamWriter.println(BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString() + " >> " + restartMsg);
             outputStreamWriter.flush();
             return BatchProcessDriverCLI.PROCESS_RESTART_EXIT_CODE;
         }
@@ -522,9 +502,8 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
     }
 
     private enum CAUSE_FOR_TERMINATION {
-        COMPLETED_NORMALLY, MAIN_LOOP_EXCEPTION_NO_RESTART,
-        CONSUMERS_MANAGER_DIDNT_INIT_IN_TIME_NO_RESTART, MAIN_LOOP_EXCEPTION, CRAWLER_TIMED_OUT,
-        TIMED_OUT_CONSUMER, PARENT_SHUTDOWN, BATCH_PROCESS_ALIVE_TOO_LONG,
+        COMPLETED_NORMALLY, MAIN_LOOP_EXCEPTION_NO_RESTART, CONSUMERS_MANAGER_DIDNT_INIT_IN_TIME_NO_RESTART, MAIN_LOOP_EXCEPTION, CRAWLER_TIMED_OUT, TIMED_OUT_CONSUMER,
+        PARENT_SHUTDOWN, BATCH_PROCESS_ALIVE_TOO_LONG,
     }
 
     private static class State {
@@ -537,6 +516,20 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
         CAUSE_FOR_TERMINATION causeForTermination = null;
     }
 
+    private static class TimeoutFutureResult implements IFileProcessorFutureResult {
+        //used to be used when more than one timeout was allowed
+        //TODO: get rid of this?
+        private final int timedOutCount;
+
+        private TimeoutFutureResult(final int timedOutCount) {
+            this.timedOutCount = timedOutCount;
+        }
+
+        protected int getTimedOutCount() {
+            return timedOutCount;
+        }
+    }
+
     private class TimeoutChecker implements Callable<IFileProcessorFutureResult> {
         @Override
         public TimeoutFutureResult call() throws Exception {
@@ -567,18 +560,4 @@ public class BatchProcess implements Callable<ParallelFileProcessingResult> {
             }
         }
     }
-
-    private static class TimeoutFutureResult implements IFileProcessorFutureResult {
-        //used to be used when more than one timeout was allowed
-        //TODO: get rid of this?
-        private final int timedOutCount;
-
-        private TimeoutFutureResult(final int timedOutCount) {
-            this.timedOutCount = timedOutCount;
-        }
-
-        protected int getTimedOutCount() {
-            return timedOutCount;
-        }
-    }
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/BatchProcessDriverCLI.java b/tika-batch/src/main/java/org/apache/tika/batch/BatchProcessDriverCLI.java
index a3e66e6f4..6d9e236ef 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/BatchProcessDriverCLI.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/BatchProcessDriverCLI.java
@@ -78,7 +78,9 @@ public class BatchProcessDriverCLI {
         final BatchProcessDriverCLI runner = new BatchProcessDriverCLI(args);
 
         //make absolutely certain that the forked process is terminated
-        Runtime.getRuntime().addShutdownHook(new Thread(runner::stop));
+        Runtime
+                .getRuntime()
+                .addShutdownHook(new Thread(runner::stop));
 
         runner.execute();
         System.out.println("FSBatchProcessDriver has gracefully completed");
@@ -91,15 +93,13 @@ public class BatchProcessDriverCLI {
             String arg = commandLine[i];
             if (arg.equals("-maxRestarts")) {
                 if (i == commandLine.length - 1) {
-                    throw new IllegalArgumentException(
-                            "Must specify an integer after \"-maxRestarts\"");
+                    throw new IllegalArgumentException("Must specify an integer after \"-maxRestarts\"");
                 }
                 String restartNumString = commandLine[i + 1];
                 try {
                     maxProcessRestarts = Integer.parseInt(restartNumString);
                 } catch (NumberFormatException e) {
-                    throw new IllegalArgumentException(
-                            "Must specify an integer after \"-maxRestarts\" arg.");
+                    throw new IllegalArgumentException("Must specify an integer after \"-maxRestarts\" arg.");
                 }
                 i++;
             } else {
@@ -141,26 +141,20 @@ public class BatchProcessDriverCLI {
             //if we've gotten the message via stdout to restart
             //but the process hasn't exited yet, give it another
             //chance
-            if (receivedRestartMsg && exit == null &&
-                    loopsAfterRestartMessageReceived <= waitNumLoopsAfterRestartMessage) {
+            if (receivedRestartMsg && exit == null && loopsAfterRestartMessageReceived <= waitNumLoopsAfterRestartMessage) {
                 loopsAfterRestartMessageReceived++;
-                LOG.warn("Must restart, still not exited; loops after restart: {}",
-                        loopsAfterRestartMessageReceived);
+                LOG.warn("Must restart, still not exited; loops after restart: {}", loopsAfterRestartMessageReceived);
                 continue;
             }
             if (loopsAfterRestartMessageReceived > waitNumLoopsAfterRestartMessage) {
-                LOG.trace("About to try to restart because: exit={} receivedRestartMsg={}", exit,
-                        receivedRestartMsg);
-                LOG.warn("Restarting after exceeded wait loops waiting for exit: {}",
-                        loopsAfterRestartMessageReceived);
+                LOG.trace("About to try to restart because: exit={} receivedRestartMsg={}", exit, receivedRestartMsg);
+                LOG.warn("Restarting after exceeded wait loops waiting for exit: {}", loopsAfterRestartMessageReceived);
                 boolean restarted = restart(exit, receivedRestartMsg);
                 if (!restarted) {
                     break;
                 }
-            } else if (exit != null && exit != BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE &&
-                    exit != BatchProcessDriverCLI.PROCESS_COMPLETED_SUCCESSFULLY) {
-                LOG.trace("About to try to restart because: exit={} receivedRestartMsg={}", exit,
-                        receivedRestartMsg);
+            } else if (exit != null && exit != BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE && exit != BatchProcessDriverCLI.PROCESS_COMPLETED_SUCCESSFULLY) {
+                LOG.trace("About to try to restart because: exit={} receivedRestartMsg={}", exit, receivedRestartMsg);
 
                 if (exit == BatchProcessDriverCLI.PROCESS_RESTART_EXIT_CODE) {
                     LOG.info("Restarting on expected restart code");
@@ -171,8 +165,7 @@ public class BatchProcessDriverCLI {
                 if (!restarted) {
                     break;
                 }
-            } else if (exit != null && (exit == PROCESS_COMPLETED_SUCCESSFULLY ||
-                    exit == BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE)) {
+            } else if (exit != null && (exit == PROCESS_COMPLETED_SUCCESSFULLY || exit == BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE)) {
                 LOG.trace("Will not restart: {}", exit);
                 break;
             }
@@ -203,8 +196,7 @@ public class BatchProcessDriverCLI {
                     //swallow
                 }
             }
-            LOG.error("Process didn't stop after 60 seconds after shutdown. " +
-                    "I am forcefully terminating it.");
+            LOG.error("Process didn't stop after 60 seconds after shutdown. " + "I am forcefully terminating it.");
         }
         interruptWatcherThread.interrupt();
     }
@@ -229,8 +221,7 @@ public class BatchProcessDriverCLI {
             stop();
             return false;
         }
-        LOG.warn("Must restart process (exitValue={} numRestarts={} receivedRestartMessage={})",
-                exitValue, numRestarts, receivedRestartMsg);
+        LOG.warn("Must restart process (exitValue={} numRestarts={} receivedRestartMessage={})", exitValue, numRestarts, receivedRestartMsg);
         stop();
         start();
         numRestarts++;
@@ -343,8 +334,7 @@ public class BatchProcessDriverCLI {
         protected boolean running = true;
 
         private StreamGobbler(InputStream is) {
-            this.reader =
-                    new BufferedReader(new InputStreamReader(new BufferedInputStream(is), UTF_8));
+            this.reader = new BufferedReader(new InputStreamReader(new BufferedInputStream(is), UTF_8));
         }
 
         @Override
@@ -384,9 +374,7 @@ public class BatchProcessDriverCLI {
             try {
                 LOG.trace("watcher starting to read");
                 while ((line = reader.readLine()) != null && this.running) {
-                    if (line.startsWith(
-                            BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART
-                                    .toString())) {
+                    if (line.startsWith(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString())) {
                         receivedRestartMsg = true;
                     }
                     LOG.info("BatchProcess: " + line);
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/FileResourceConsumer.java b/tika-batch/src/main/java/org/apache/tika/batch/FileResourceConsumer.java
index d38533b57..21295aa42 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/FileResourceConsumer.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/FileResourceConsumer.java
@@ -49,6 +49,7 @@ import org.apache.tika.sax.ToXMLContentHandler;
  */
 public abstract class FileResourceConsumer implements Callable<IFileProcessorFutureResult> {
     protected static final Logger LOG = LoggerFactory.getLogger(FileResourceConsumer.class);
+    private static final AtomicInteger numConsumers = new AtomicInteger(-1);
     public static String TIMED_OUT = "timed_out";
     public static String OOM = "oom";
     public static String IO_IS = "io_on_inputstream";
@@ -56,18 +57,17 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
     public static String PARSE_ERR = "parse_err";
     public static String PARSE_EX = "parse_ex";
     public static String ELAPSED_MILLIS = "elapsedMS";
-    private static final AtomicInteger numConsumers = new AtomicInteger(-1);
     private final ArrayBlockingQueue<FileResource> fileQueue;
     private final int consumerId;
     //used to lock checks on state to prevent
     private final Object lock = new Object();
     private final long MAX_CONSEC_WAIT_IN_MILLIS = 10 * 60 * 1000;// 10 minutes
+    private final AtomicInteger numResourcesConsumed = new AtomicInteger(0);
+    private final AtomicInteger numHandledExceptions = new AtomicInteger(0);
     //this records the file that is currently
     //being processed.  It is null if no file is currently being processed.
     //no need for volatile because of lock for checkForStales
     private FileStarted currentFile = null;
-    private final AtomicInteger numResourcesConsumed = new AtomicInteger(0);
-    private final AtomicInteger numHandledExceptions = new AtomicInteger(0);
     //after this has been set to ACTIVELY_CONSUMING,
     //this should only be set by setEndedState.
     private volatile STATE currentState = STATE.NOT_YET_STARTED;
@@ -85,8 +85,7 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
             while (fileResource != null) {
                 LOG.trace("file consumer is about to process: {}", fileResource.getResourceId());
                 boolean consumed = _processFileResource(fileResource);
-                LOG.trace("file consumer has finished processing: {}",
-                        fileResource.getResourceId());
+                LOG.trace("file consumer has finished processing: {}", fileResource.getResourceId());
 
                 if (consumed) {
                     numResourcesConsumed.incrementAndGet();
@@ -136,13 +135,13 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
      * @return whether this consumer is still active
      */
     public boolean isStillActive() {
-        if (Thread.currentThread().isInterrupted()) {
+        if (Thread
+                .currentThread()
+                .isInterrupted()) {
             return false;
-        } 
-        
-        return currentState == STATE.NOT_YET_STARTED ||
-                      currentState == STATE.ACTIVELY_CONSUMING ||
-                      currentState == STATE.ASKED_TO_SHUTDOWN;
+        }
+
+        return currentState == STATE.NOT_YET_STARTED || currentState == STATE.ACTIVELY_CONSUMING || currentState == STATE.ASKED_TO_SHUTDOWN;
     }
 
     private boolean _processFileResource(FileResource fileResource) {
@@ -220,8 +219,7 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
         }
         synchronized (lock) {
             //check again once the lock has been obtained
-            if (currentState != STATE.ACTIVELY_CONSUMING &&
-                    currentState != STATE.ASKED_TO_SHUTDOWN) {
+            if (currentState != STATE.ACTIVELY_CONSUMING && currentState != STATE.ASKED_TO_SHUTDOWN) {
                 return null;
             }
             FileStarted tmp = currentFile;
@@ -230,8 +228,7 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
             }
             if (tmp.getElapsedMillis() > staleThresholdMillis) {
                 setEndedState(STATE.TIMED_OUT);
-                LOG.error("{}", getXMLifiedLogMsg(TIMED_OUT, tmp.getResourceId(), ELAPSED_MILLIS,
-                        Long.toString(tmp.getElapsedMillis())));
+                LOG.error("{}", getXMLifiedLogMsg(TIMED_OUT, tmp.getResourceId(), ELAPSED_MILLIS, Long.toString(tmp.getElapsedMillis())));
                 return tmp;
             }
         }
@@ -250,8 +247,7 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
      * @param t          throwable can be null
      * @param attrs      (array of key0, value0, key1, value1, etc.)
      */
-    protected String getXMLifiedLogMsg(String type, String resourceId, Throwable t,
-                                       String... attrs) {
+    protected String getXMLifiedLogMsg(String type, String resourceId, Throwable t, String... attrs) {
 
         ContentHandler toXML = new ToXMLContentHandler();
         SafeContentHandler handler = new SafeContentHandler(toXML);
@@ -269,7 +265,9 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
                 t.printStackTrace(printWriter);
                 printWriter.flush();
                 stackWriter.flush();
-                char[] chars = stackWriter.toString().toCharArray();
+                char[] chars = stackWriter
+                        .toString()
+                        .toCharArray();
                 handler.characters(chars, 0, chars.length);
             }
             handler.endElement("", type, type);
@@ -285,7 +283,9 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
         long start = System.currentTimeMillis();
         while (fileResource == null) {
             //check to see if thread is interrupted before polling
-            if (Thread.currentThread().isInterrupted()) {
+            if (Thread
+                    .currentThread()
+                    .isInterrupted()) {
                 setEndedState(STATE.THREAD_INTERRUPTED);
                 LOG.debug("Consumer thread was interrupted.");
                 break;
@@ -306,8 +306,7 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
                 }
                 break;
             }
-            LOG.debug("{} is waiting for file and the queue size is: {}", consumerId,
-                    fileQueue.size());
+            LOG.debug("{} is waiting for file and the queue size is: {}", consumerId, fileQueue.size());
 
             long elapsed = System.currentTimeMillis() - start;
             if (MAX_CONSEC_WAIT_IN_MILLIS > 0 && elapsed > MAX_CONSEC_WAIT_IN_MILLIS) {
@@ -348,8 +347,7 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
     //to set will be ignored!!!
     private void setEndedState(STATE cause) {
         synchronized (lock) {
-            if (currentState == STATE.NOT_YET_STARTED || currentState == STATE.ACTIVELY_CONSUMING ||
-                    currentState == STATE.ASKED_TO_SHUTDOWN) {
+            if (currentState == STATE.NOT_YET_STARTED || currentState == STATE.ACTIVELY_CONSUMING || currentState == STATE.ASKED_TO_SHUTDOWN) {
                 currentState = cause;
             }
         }
@@ -367,9 +365,8 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
      * @param parseContext parse context
      * @throws Throwable (logs and then throws whatever was thrown (if anything)
      */
-    protected void parse(final String resourceId, final Parser parser, InputStream is,
-                         final ContentHandler handler, final Metadata m,
-                         final ParseContext parseContext) throws Throwable {
+    protected void parse(final String resourceId, final Parser parser, InputStream is, final ContentHandler handler, final Metadata m, final ParseContext parseContext)
+            throws Throwable {
 
         try {
             parser.parse(is, handler, m, parseContext);
@@ -389,8 +386,7 @@ public abstract class FileResourceConsumer implements Callable<IFileProcessorFut
     }
 
     private enum STATE {
-        NOT_YET_STARTED, ACTIVELY_CONSUMING, SWALLOWED_POISON, THREAD_INTERRUPTED,
-        EXCEEDED_MAX_CONSEC_WAIT_MILLIS, ASKED_TO_SHUTDOWN, TIMED_OUT, CONSUMER_EXCEPTION,
+        NOT_YET_STARTED, ACTIVELY_CONSUMING, SWALLOWED_POISON, THREAD_INTERRUPTED, EXCEEDED_MAX_CONSEC_WAIT_MILLIS, ASKED_TO_SHUTDOWN, TIMED_OUT, CONSUMER_EXCEPTION,
         CONSUMER_ERROR, COMPLETED
     }
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/FileResourceCrawler.java b/tika-batch/src/main/java/org/apache/tika/batch/FileResourceCrawler.java
index 5d071d442..615c880ab 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/FileResourceCrawler.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/FileResourceCrawler.java
@@ -28,8 +28,7 @@ import org.apache.tika.extractor.DocumentSelector;
 import org.apache.tika.metadata.Metadata;
 
 public abstract class FileResourceCrawler implements Callable<IFileProcessorFutureResult> {
-    protected static final Logger LOG =
-            LoggerFactory.getLogger(FileResourceCrawler.class.toString());
+    protected static final Logger LOG = LoggerFactory.getLogger(FileResourceCrawler.class.toString());
 
     protected final static int SKIPPED = 0;
     protected final static int ADDED = 1;
@@ -108,20 +107,19 @@ public abstract class FileResourceCrawler implements Callable<IFileProcessorFutu
         boolean isAdded = false;
         if (select(fileResource.getMetadata())) {
             long start = System.currentTimeMillis();
-            while (queue.offer(fileResource, PAUSE_INCREMENT_MILLIS, TimeUnit.MILLISECONDS) ==
-                    false) {
+            while (queue.offer(fileResource, PAUSE_INCREMENT_MILLIS, TimeUnit.MILLISECONDS) == false) {
                 long elapsed = System.currentTimeMillis() - start;
-                LOG.info("FileResourceCrawler is pausing. Queue is full: {} after {} ms",
-                        queue.size(), elapsed);
+                LOG.info("FileResourceCrawler is pausing. Queue is full: {} after {} ms", queue.size(), elapsed);
 
                 if (maxConsecWaitInMillis > -1 && elapsed > maxConsecWaitInMillis) {
                     timedOut = true;
-                    String msg = "FileResourceCrawler had to wait longer (" + elapsed +
-                            " ms) than allowed (" + maxConsecWaitInMillis + " ms)";
+                    String msg = "FileResourceCrawler had to wait longer (" + elapsed + " ms) than allowed (" + maxConsecWaitInMillis + " ms)";
                     LOG.error(msg);
                     throw new InterruptedException(msg);
                 }
-                if (Thread.currentThread().isInterrupted()) {
+                if (Thread
+                        .currentThread()
+                        .isInterrupted()) {
                     LOG.info("FileResourceCrawler shutting down because of interrupted thread.");
                     throw new InterruptedException("FileResourceCrawler interrupted.");
                 }
@@ -149,7 +147,9 @@ public abstract class FileResourceCrawler implements Callable<IFileProcessorFutu
                 LOG.debug("quitting the poison loop because shutDownNoPoison is now true");
                 return;
             }
-            if (Thread.currentThread().isInterrupted()) {
+            if (Thread
+                    .currentThread()
+                    .isInterrupted()) {
                 LOG.debug("thread interrupted while trying to add poison");
                 return;
             }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/ParallelFileProcessingResult.java b/tika-batch/src/main/java/org/apache/tika/batch/ParallelFileProcessingResult.java
index 9950e03bd..3a2892d3e 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/ParallelFileProcessingResult.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/ParallelFileProcessingResult.java
@@ -26,9 +26,7 @@ public class ParallelFileProcessingResult {
     private final int exitStatus;
     private final String causeForTermination;
 
-    public ParallelFileProcessingResult(int considered, int added, int consumed,
-                                        int numberHandledExceptions, double secondsElapsed,
-                                        int exitStatus, String causeForTermination) {
+    public ParallelFileProcessingResult(int considered, int added, int consumed, int numberHandledExceptions, double secondsElapsed, int exitStatus, String causeForTermination) {
         this.considered = considered;
         this.added = added;
         this.consumed = consumed;
@@ -92,9 +90,7 @@ public class ParallelFileProcessingResult {
 
     @Override
     public String toString() {
-        return "ParallelFileProcessingResult{" + "considered=" + considered + ", added=" + added +
-                ", consumed=" + consumed + ", numberHandledExceptions=" + numberHandledExceptions +
-                ", secondsElapsed=" + secondsElapsed + ", exitStatus=" + exitStatus +
-                ", causeForTermination='" + causeForTermination + '\'' + '}';
+        return "ParallelFileProcessingResult{" + "considered=" + considered + ", added=" + added + ", consumed=" + consumed + ", numberHandledExceptions=" +
+                numberHandledExceptions + ", secondsElapsed=" + secondsElapsed + ", exitStatus=" + exitStatus + ", causeForTermination='" + causeForTermination + '\'' + '}';
     }
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/StatusReporter.java b/tika-batch/src/main/java/org/apache/tika/batch/StatusReporter.java
index e6f9dace1..cb106a1a0 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/StatusReporter.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/StatusReporter.java
@@ -86,19 +86,14 @@ public class StatusReporter implements Callable<IFileProcessorFutureResult> {
                 double elapsedSecs = (double) elapsed / (double) 1000;
                 int avg = (elapsedSecs > 5 || cnt > 100) ? (int) ((double) cnt / elapsedSecs) : -1;
 
-                String elapsedString =
-                        DurationFormatUtils.formatMillis(System.currentTimeMillis() - start);
-                String docsPerSec = avg > -1 ?
-                        String.format(Locale.ROOT, " (%s docs per sec)", numberFormat.format(avg)) :
-                        "";
-                String msg = String.format(Locale.ROOT, "Processed %s documents in %s%s.",
-                        numberFormat.format(cnt), elapsedString, docsPerSec);
+                String elapsedString = DurationFormatUtils.formatMillis(System.currentTimeMillis() - start);
+                String docsPerSec = avg > -1 ? String.format(Locale.ROOT, " (%s docs per sec)", numberFormat.format(avg)) : "";
+                String msg = String.format(Locale.ROOT, "Processed %s documents in %s%s.", numberFormat.format(cnt), elapsedString, docsPerSec);
                 report(msg);
                 if (exceptions == 1) {
                     msg = "There has been one handled exception.";
                 } else {
-                    msg = String.format(Locale.ROOT, "There have been %s handled exceptions.",
-                            numberFormat.format(exceptions));
+                    msg = String.format(Locale.ROOT, "There have been %s handled exceptions.", numberFormat.format(exceptions));
                 }
                 report(msg);
 
@@ -108,8 +103,7 @@ public class StatusReporter implements Callable<IFileProcessorFutureResult> {
                 if (stillAlive == 1) {
                     msg = "There is one file processor still active.";
                 } else {
-                    msg = "There are " + numberFormat.format(stillAlive) +
-                            " file processors still active.";
+                    msg = "There are " + numberFormat.format(stillAlive) + " file processors still active.";
                 }
                 report(msg);
 
@@ -118,14 +112,12 @@ public class StatusReporter implements Callable<IFileProcessorFutureResult> {
                 if (crawled == 1) {
                     msg = "The directory crawler has considered 1 file,";
                 } else {
-                    msg = "The directory crawler has considered " + numberFormat.format(crawled) +
-                            " files, ";
+                    msg = "The directory crawler has considered " + numberFormat.format(crawled) + " files, ";
                 }
                 if (added == 1) {
                     msg += "and it has added 1 file.";
                 } else {
-                    msg += "and it has added " + numberFormat.format(crawler.getAdded()) +
-                            " files.";
+                    msg += "and it has added " + numberFormat.format(crawler.getAdded()) + " files.";
                 }
                 msg += "\n";
                 report(msg);
@@ -175,8 +167,7 @@ public class StatusReporter implements Callable<IFileProcessorFutureResult> {
             long elapsed = fs.getElapsedMillis();
             if (elapsed > staleThresholdMillis) {
                 String elapsedString = Double.toString((double) elapsed / (double) 1000);
-                report("A thread has been working on " + fs.getResourceId() + " for " +
-                        elapsedString + " seconds.");
+                report("A thread has been working on " + fs.getResourceId() + " for " + elapsedString + " seconds.");
             }
         }
     }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/AbstractConsumersBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/AbstractConsumersBuilder.java
index fd84bbad7..2a99ec9a7 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/AbstractConsumersBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/AbstractConsumersBuilder.java
@@ -27,12 +27,13 @@ import org.apache.tika.batch.FileResource;
 public abstract class AbstractConsumersBuilder {
 
     public static int getDefaultNumConsumers() {
-        int n = Runtime.getRuntime().availableProcessors() - 1;
+        int n = Runtime
+                .getRuntime()
+                .availableProcessors() - 1;
         return Math.max(n, 1);
     }
 
-    public abstract ConsumersManager build(Node node, Map<String, String> runtimeAttributes,
-                                           ArrayBlockingQueue<FileResource> queue);
+    public abstract ConsumersManager build(Node node, Map<String, String> runtimeAttributes, ArrayBlockingQueue<FileResource> queue);
 
 
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/BatchProcessBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/BatchProcessBuilder.java
index 1ee13d5ca..a5d8c4ed1 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/BatchProcessBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/BatchProcessBuilder.java
@@ -92,8 +92,7 @@ public class BatchProcessBuilder {
      * @return batch process
      * @throws java.io.IOException
      */
-    public BatchProcess build(InputStream is, Map<String, String> runtimeAttributes)
-            throws IOException {
+    public BatchProcess build(InputStream is, Map<String, String> runtimeAttributes) throws IOException {
         Document doc = null;
         try {
             DocumentBuilder docBuilder = XMLReaderUtils.getDocumentBuilder();
@@ -118,14 +117,10 @@ public class BatchProcessBuilder {
     public BatchProcess build(Node docElement, Map<String, String> incomingRuntimeAttributes) {
 
         //key components
-        long timeoutThresholdMillis =
-                XMLDOMUtil.getLong("timeoutThresholdMillis", incomingRuntimeAttributes, docElement);
-        long timeoutCheckPulseMillis = XMLDOMUtil
-                .getLong("timeoutCheckPulseMillis", incomingRuntimeAttributes, docElement);
-        long pauseOnEarlyTerminationMillis = XMLDOMUtil
-                .getLong("pauseOnEarlyTerminationMillis", incomingRuntimeAttributes, docElement);
-        int maxAliveTimeSeconds =
-                XMLDOMUtil.getInt("maxAliveTimeSeconds", incomingRuntimeAttributes, docElement);
+        long timeoutThresholdMillis = XMLDOMUtil.getLong("timeoutThresholdMillis", incomingRuntimeAttributes, docElement);
+        long timeoutCheckPulseMillis = XMLDOMUtil.getLong("timeoutCheckPulseMillis", incomingRuntimeAttributes, docElement);
+        long pauseOnEarlyTerminationMillis = XMLDOMUtil.getLong("pauseOnEarlyTerminationMillis", incomingRuntimeAttributes, docElement);
+        int maxAliveTimeSeconds = XMLDOMUtil.getInt("maxAliveTimeSeconds", incomingRuntimeAttributes, docElement);
 
         FileResourceCrawler crawler = null;
         ConsumersManager consumersManager = null;
@@ -138,8 +133,7 @@ public class BatchProcessBuilder {
          * supplies the numConsumers from the commandline (if it exists) or from the config file
          * At least this creates an unmodifiable defensive copy of incomingRuntimeAttributes...
          */
-        Map<String, String> runtimeAttributes =
-                setNumConsumersInRuntimeAttributes(docElement, incomingRuntimeAttributes);
+        Map<String, String> runtimeAttributes = setNumConsumersInRuntimeAttributes(docElement, incomingRuntimeAttributes);
 
         //build queue
         ArrayBlockingQueue<FileResource> queue = buildQueue(docElement, runtimeAttributes);
@@ -155,21 +149,17 @@ public class BatchProcessBuilder {
             keyNodes.put(nodeName, child);
         }
         //build consumers
-        consumersManager =
-                buildConsumersManager(keyNodes.get("consumers"), runtimeAttributes, queue);
+        consumersManager = buildConsumersManager(keyNodes.get("consumers"), runtimeAttributes, queue);
 
         //build crawler
         crawler = buildCrawler(queue, keyNodes.get("crawler"), runtimeAttributes);
 
         if (keyNodes.containsKey("reporter")) {
-            reporter = buildReporter(crawler, consumersManager, keyNodes.get("reporter"),
-                    runtimeAttributes);
+            reporter = buildReporter(crawler, consumersManager, keyNodes.get("reporter"), runtimeAttributes);
         }
 
         if (keyNodes.containsKey("interrupter")) {
-            interrupter =
-                    buildInterrupter(keyNodes.get("interrupter"), pauseOnEarlyTerminationMillis,
-                            runtimeAttributes);
+            interrupter = buildInterrupter(keyNodes.get("interrupter"), pauseOnEarlyTerminationMillis, runtimeAttributes);
         }
         BatchProcess proc = new BatchProcess(crawler, consumersManager, reporter, interrupter);
 
@@ -188,38 +178,32 @@ public class BatchProcessBuilder {
         return proc;
     }
 
-    private Interrupter buildInterrupter(Node node, long pauseOnEarlyTermination,
-                                         Map<String, String> runtimeAttributes) {
+    private Interrupter buildInterrupter(Node node, long pauseOnEarlyTermination, Map<String, String> runtimeAttributes) {
         Map<String, String> attrs = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
         String className = attrs.get("builderClass");
         if (className == null) {
             throw new RuntimeException("Need to specify class name in interrupter element");
         }
-        InterrupterBuilder builder =
-                ClassLoaderUtil.buildClass(InterrupterBuilder.class, className);
+        InterrupterBuilder builder = ClassLoaderUtil.buildClass(InterrupterBuilder.class, className);
 
         return builder.build(node, pauseOnEarlyTermination, runtimeAttributes);
 
     }
 
-    private StatusReporter buildReporter(FileResourceCrawler crawler,
-                                         ConsumersManager consumersManager, Node node,
-                                         Map<String, String> runtimeAttributes) {
+    private StatusReporter buildReporter(FileResourceCrawler crawler, ConsumersManager consumersManager, Node node, Map<String, String> runtimeAttributes) {
 
         Map<String, String> attrs = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
         String className = attrs.get("builderClass");
         if (className == null) {
             throw new RuntimeException("Need to specify class name in reporter element");
         }
-        StatusReporterBuilder builder =
-                ClassLoaderUtil.buildClass(StatusReporterBuilder.class, className);
+        StatusReporterBuilder builder = ClassLoaderUtil.buildClass(StatusReporterBuilder.class, className);
 
         return builder.build(crawler, consumersManager, node, runtimeAttributes);
 
     }
 
-    private Map<String, String> setNumConsumersInRuntimeAttributes(Node docElement,
-                                                                   Map<String, String> incomingRuntimeAttributes) {
+    private Map<String, String> setNumConsumersInRuntimeAttributes(Node docElement, Map<String, String> incomingRuntimeAttributes) {
         Map<String, String> runtimeAttributes = new HashMap<>();
 
         for (Map.Entry<String, String> e : incomingRuntimeAttributes.entrySet()) {
@@ -230,7 +214,9 @@ public class BatchProcessBuilder {
         if (runtimeAttributes.containsKey(NUM_CONSUMERS_KEY)) {
             return Collections.unmodifiableMap(runtimeAttributes);
         }
-        Node ncNode = docElement.getAttributes().getNamedItem("numConsumers");
+        Node ncNode = docElement
+                .getAttributes()
+                .getNamedItem("numConsumers");
         int numConsumers = -1;
         String numConsumersString = ncNode.getNodeValue();
         try {
@@ -247,13 +233,14 @@ public class BatchProcessBuilder {
     }
 
     //tries to get maxQueueSize from main element
-    private ArrayBlockingQueue<FileResource> buildQueue(Node docElement,
-                                                        Map<String, String> runtimeAttributes) {
+    private ArrayBlockingQueue<FileResource> buildQueue(Node docElement, Map<String, String> runtimeAttributes) {
         int maxQueueSize = DEFAULT_MAX_QUEUE_SIZE;
         String szString = runtimeAttributes.get(MAX_QUEUE_SIZE_KEY);
 
         if (szString == null) {
-            Node szNode = docElement.getAttributes().getNamedItem(MAX_QUEUE_SIZE_KEY);
+            Node szNode = docElement
+                    .getAttributes()
+                    .getNamedItem(MAX_QUEUE_SIZE_KEY);
             if (szNode != null) {
                 szString = szNode.getNodeValue();
             }
@@ -274,23 +261,20 @@ public class BatchProcessBuilder {
         return new ArrayBlockingQueue<>(maxQueueSize);
     }
 
-    private ConsumersManager buildConsumersManager(Node node, Map<String, String> runtimeAttributes,
-                                                   ArrayBlockingQueue<FileResource> queue) {
+    private ConsumersManager buildConsumersManager(Node node, Map<String, String> runtimeAttributes, ArrayBlockingQueue<FileResource> queue) {
 
         Map<String, String> attrs = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
         String className = attrs.get("builderClass");
         if (className == null) {
             throw new RuntimeException("Need to specify class name in consumers element");
         }
-        AbstractConsumersBuilder builder =
-                ClassLoaderUtil.buildClass(AbstractConsumersBuilder.class, className);
+        AbstractConsumersBuilder builder = ClassLoaderUtil.buildClass(AbstractConsumersBuilder.class, className);
 
         return builder.build(node, runtimeAttributes, queue);
     }
 
 
-    private FileResourceCrawler buildCrawler(ArrayBlockingQueue<FileResource> queue, Node node,
-                                             Map<String, String> runtimeAttributes) {
+    private FileResourceCrawler buildCrawler(ArrayBlockingQueue<FileResource> queue, Node node, Map<String, String> runtimeAttributes) {
         Map<String, String> attrs = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
         String className = attrs.get("builderClass");
         if (className == null) {
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/CommandLineParserBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/CommandLineParserBuilder.java
index 38656b7ea..40bcb4104 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/CommandLineParserBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/CommandLineParserBuilder.java
@@ -88,11 +88,17 @@ public class CommandLineParserBuilder {
         String longOpt = getString(map, "longOpt", "");
         boolean isRequired = getBoolean(map, "required", false);
         boolean hasArg = getBoolean(map, "hasArg", false);
-        if (opt.trim().length() == 0 || description.trim().length() == 0) {
+        if (opt
+                .trim()
+                .length() == 0 || description
+                .trim()
+                .length() == 0) {
             throw new IllegalArgumentException("Must specify at least option and description");
         }
         Option option = new Option(opt, description);
-        if (longOpt.trim().length() > 0) {
+        if (longOpt
+                .trim()
+                .length() > 0) {
             option.setLongOpt(longOpt);
         }
         if (isRequired) {
@@ -114,9 +120,15 @@ public class CommandLineParserBuilder {
             return defaultValue;
         }
 
-        if (n.getNodeValue().toLowerCase(Locale.ROOT).equals("true")) {
+        if (n
+                .getNodeValue()
+                .toLowerCase(Locale.ROOT)
+                .equals("true")) {
             return true;
-        } else if (n.getNodeValue().toLowerCase(Locale.ROOT).equals("false")) {
+        } else if (n
+                .getNodeValue()
+                .toLowerCase(Locale.ROOT)
+                .equals("false")) {
             return false;
         }
         return defaultValue;
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/DefaultContentHandlerFactoryBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/DefaultContentHandlerFactoryBuilder.java
index 4d8511598..926a9b093 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/DefaultContentHandlerFactoryBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/DefaultContentHandlerFactoryBuilder.java
@@ -39,8 +39,7 @@ public class DefaultContentHandlerFactoryBuilder implements IContentHandlerFacto
         Map<String, String> attributes = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
         BasicContentHandlerFactory.HANDLER_TYPE type = null;
         String handlerTypeString = attributes.get("basicHandlerType");
-        type = BasicContentHandlerFactory
-                .parseHandlerType(handlerTypeString, BasicContentHandlerFactory.HANDLER_TYPE.TEXT);
+        type = BasicContentHandlerFactory.parseHandlerType(handlerTypeString, BasicContentHandlerFactory.HANDLER_TYPE.TEXT);
         int writeLimit = -1;
         String writeLimitString = attributes.get("writeLimit");
         if (writeLimitString != null) {
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/ICrawlerBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/ICrawlerBuilder.java
index 88842fa0c..726218156 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/ICrawlerBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/ICrawlerBuilder.java
@@ -26,7 +26,6 @@ import org.apache.tika.batch.FileResourceCrawler;
 
 public interface ICrawlerBuilder extends ObjectFromDOMAndQueueBuilder<FileResourceCrawler> {
 
-    public FileResourceCrawler build(Node node, Map<String, String> attributes,
-                                     ArrayBlockingQueue<FileResource> queue);
+    public FileResourceCrawler build(Node node, Map<String, String> attributes, ArrayBlockingQueue<FileResource> queue);
 
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/InterrupterBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/InterrupterBuilder.java
index 24fdefd4a..43baa9a4a 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/InterrupterBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/InterrupterBuilder.java
@@ -27,8 +27,7 @@ import org.apache.tika.batch.Interrupter;
  */
 public class InterrupterBuilder {
 
-    public Interrupter build(Node n, long pauseOnEarlyTermination,
-                             Map<String, String> commandlineArguments) {
+    public Interrupter build(Node n, long pauseOnEarlyTermination, Map<String, String> commandlineArguments) {
         return new Interrupter(pauseOnEarlyTermination);
     }
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/ObjectFromDOMAndQueueBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/ObjectFromDOMAndQueueBuilder.java
index 6578f5154..fd6e30a66 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/ObjectFromDOMAndQueueBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/ObjectFromDOMAndQueueBuilder.java
@@ -32,7 +32,6 @@ import org.apache.tika.batch.FileResource;
  */
 public interface ObjectFromDOMAndQueueBuilder<T> {
 
-    public T build(Node node, Map<String, String> runtimeAttributes,
-                   ArrayBlockingQueue<FileResource> resourceQueue);
+    public T build(Node node, Map<String, String> runtimeAttributes, ArrayBlockingQueue<FileResource> resourceQueue);
 
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/ParserFactoryBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/ParserFactoryBuilder.java
index 5d3c0956c..48d933590 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/ParserFactoryBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/ParserFactoryBuilder.java
@@ -35,14 +35,15 @@ public class ParserFactoryBuilder implements IParserFactoryBuilder {
         ParserFactory pf = ClassLoaderUtil.buildClass(ParserFactory.class, className);
 
         if (localAttrs.containsKey("parseRecursively")) {
-            String bString = localAttrs.get("parseRecursively").toLowerCase(Locale.ENGLISH);
+            String bString = localAttrs
+                    .get("parseRecursively")
+                    .toLowerCase(Locale.ENGLISH);
             if (bString.equals("true")) {
                 pf.setParseRecursively(true);
             } else if (bString.equals("false")) {
                 pf.setParseRecursively(false);
             } else {
-                throw new RuntimeException(
-                        "parseRecursively must have value of \"true\" or \"false\": " + bString);
+                throw new RuntimeException("parseRecursively must have value of \"true\" or \"false\": " + bString);
             }
         }
         return pf;
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/SimpleLogReporterBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/SimpleLogReporterBuilder.java
index b1e80f6ce..8647e0d8c 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/SimpleLogReporterBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/SimpleLogReporterBuilder.java
@@ -29,13 +29,11 @@ import org.apache.tika.util.XMLDOMUtil;
 public class SimpleLogReporterBuilder implements StatusReporterBuilder {
 
     @Override
-    public StatusReporter build(FileResourceCrawler crawler, ConsumersManager consumersManager,
-                                Node n, Map<String, String> commandlineArguments) {
+    public StatusReporter build(FileResourceCrawler crawler, ConsumersManager consumersManager, Node n, Map<String, String> commandlineArguments) {
 
         Map<String, String> attributes = XMLDOMUtil.mapifyAttrs(n, commandlineArguments);
         long sleepMillis = PropsUtil.getLong(attributes.get("reporterSleepMillis"), 1000L);
-        long staleThresholdMillis =
-                PropsUtil.getLong(attributes.get("reporterStaleThresholdMillis"), 500000L);
+        long staleThresholdMillis = PropsUtil.getLong(attributes.get("reporterStaleThresholdMillis"), 500000L);
         StatusReporter reporter = new StatusReporter(crawler, consumersManager);
         reporter.setSleepMillis(sleepMillis);
         reporter.setStaleThresholdMillis(staleThresholdMillis);
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/builders/StatusReporterBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/builders/StatusReporterBuilder.java
index 9cf6bc650..a879a5d5a 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/builders/StatusReporterBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/builders/StatusReporterBuilder.java
@@ -26,6 +26,5 @@ import org.apache.tika.batch.StatusReporter;
 
 public interface StatusReporterBuilder {
 
-    public StatusReporter build(FileResourceCrawler crawler, ConsumersManager consumers, Node n,
-                                Map<String, String> commandlineArguments);
+    public StatusReporter build(FileResourceCrawler crawler, ConsumersManager consumers, Node n, Map<String, String> commandlineArguments);
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/AbstractFSConsumer.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/AbstractFSConsumer.java
index d5648c960..dc161eae6 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/AbstractFSConsumer.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/AbstractFSConsumer.java
@@ -41,8 +41,7 @@ public abstract class AbstractFSConsumer extends FileResourceConsumer {
      * @param fileResource used by the OSFactory to create the stream
      * @return the OutputStream or null if the output file already exists
      */
-    protected OutputStream getOutputStream(OutputStreamFactory fsOSFactory,
-                                           FileResource fileResource) {
+    protected OutputStream getOutputStream(OutputStreamFactory fsOSFactory, FileResource fileResource) {
         OutputStream os = null;
         try {
             os = fsOSFactory.getOutputStream(fileResource.getMetadata());
@@ -50,9 +49,7 @@ public abstract class AbstractFSConsumer extends FileResourceConsumer {
             //This can happen if the disk has run out of space,
             //or if there was a failure with mkdirs in fsOSFactory
             LOG.error("{}", getXMLifiedLogMsg(IO_OS, fileResource.getResourceId(), e));
-            throw new BatchNoRestartError(
-                    "IOException trying to open output stream for " + fileResource.getResourceId(),
-                    e);
+            throw new BatchNoRestartError("IOException trying to open output stream for " + fileResource.getResourceId(), e);
         }
         return os;
     }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/BasicTikaFSConsumer.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/BasicTikaFSConsumer.java
index 8e0afc875..8763fde8b 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/BasicTikaFSConsumer.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/BasicTikaFSConsumer.java
@@ -58,18 +58,15 @@ public class BasicTikaFSConsumer extends AbstractFSConsumer {
      * Parser, ContentHandlerFactory, OutputStreamFactory)}
      */
     @Deprecated
-    public BasicTikaFSConsumer(ArrayBlockingQueue<FileResource> queue, ParserFactory parserFactory,
-                               ContentHandlerFactory contentHandlerFactory,
-                               OutputStreamFactory fsOSFactory, TikaConfig tikaConfig) {
+    public BasicTikaFSConsumer(ArrayBlockingQueue<FileResource> queue, ParserFactory parserFactory, ContentHandlerFactory contentHandlerFactory, OutputStreamFactory fsOSFactory,
+                               TikaConfig tikaConfig) {
         super(queue);
         this.parser = parserFactory.getParser(tikaConfig);
         this.contentHandlerFactory = contentHandlerFactory;
         this.fsOSFactory = fsOSFactory;
     }
 
-    public BasicTikaFSConsumer(ArrayBlockingQueue<FileResource> queue, Parser parser,
-                               ContentHandlerFactory contentHandlerFactory,
-                               OutputStreamFactory fsOSFactory) {
+    public BasicTikaFSConsumer(ArrayBlockingQueue<FileResource> queue, Parser parser, ContentHandlerFactory contentHandlerFactory, OutputStreamFactory fsOSFactory) {
         super(queue);
         this.parser = parser;
         this.contentHandlerFactory = contentHandlerFactory;
@@ -88,7 +85,9 @@ public class BasicTikaFSConsumer extends AbstractFSConsumer {
         //os can be null if fsOSFactory is set to skip processing a file if the output
         //file already exists
         if (os == null) {
-            LOG.debug("Skipping: {}", fileResource.getMetadata().get(FSProperties.FS_REL_PATH));
+            LOG.debug("Skipping: {}", fileResource
+                    .getMetadata()
+                    .get(FSProperties.FS_REL_PATH));
             return false;
         }
 
@@ -104,8 +103,7 @@ public class BasicTikaFSConsumer extends AbstractFSConsumer {
         //now actually call parse!
         Throwable thrown = null;
         try {
-            parse(fileResource.getResourceId(), parser, is, handler, fileResource.getMetadata(),
-                    context);
+            parse(fileResource.getResourceId(), parser, is, handler, fileResource.getMetadata(), context);
         } catch (Error t) {
             throw t;
         } catch (Throwable t) {
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSBatchProcessCLI.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSBatchProcessCLI.java
index 975f773bb..559bbae5f 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSBatchProcessCLI.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSBatchProcessCLI.java
@@ -66,8 +66,7 @@ public class FSBatchProcessCLI {
             cli.execute(args);
         } catch (Throwable t) {
             t.printStackTrace();
-            LOG.error(MarkerFactory.getMarker("FATAL"),
-                    "Fatal exception from FSBatchProcessCLI: {}", t.getMessage(), t);
+            LOG.error(MarkerFactory.getMarker("FATAL"), "Fatal exception from FSBatchProcessCLI: {}", t.getMessage(), t);
             System.exit(BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE);
         }
     }
@@ -77,8 +76,7 @@ public class FSBatchProcessCLI {
         helpFormatter.printHelp("tika filesystem batch", options);
     }
 
-    private TikaInputStream getConfigInputStream(String[] args, boolean logDefault)
-            throws IOException {
+    private TikaInputStream getConfigInputStream(String[] args, boolean logDefault) throws IOException {
         TikaInputStream is = null;
         Path batchConfigFile = getConfigFile(args);
         if (batchConfigFile != null) {
@@ -87,17 +85,14 @@ public class FSBatchProcessCLI {
             is = TikaInputStream.get(batchConfigFile);
         } else {
             if (logDefault) {
-                LOG.info("No config file set via -bc, relying on tika-app-batch-config.xml " +
-                        "or default-tika-batch-config.xml");
+                LOG.info("No config file set via -bc, relying on tika-app-batch-config.xml " + "or default-tika-batch-config.xml");
             }
             //test to see if there's a tika-app-batch-config.xml on the path
             URL config = FSBatchProcessCLI.class.getResource("/tika-app-batch-config.xml");
             if (config != null) {
-                is = TikaInputStream.get(FSBatchProcessCLI.class
-                        .getResourceAsStream("/tika-app-batch-config.xml"));
+                is = TikaInputStream.get(FSBatchProcessCLI.class.getResourceAsStream("/tika-app-batch-config.xml"));
             } else {
-                is = TikaInputStream.get(FSBatchProcessCLI.class
-                        .getResourceAsStream("default-tika-batch-config.xml"));
+                is = TikaInputStream.get(FSBatchProcessCLI.class.getResourceAsStream("default-tika-batch-config.xml"));
             }
         }
         return is;
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDirectoryCrawler.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDirectoryCrawler.java
index 3fa52a3a5..c35795664 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDirectoryCrawler.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDirectoryCrawler.java
@@ -38,30 +38,28 @@ public class FSDirectoryCrawler extends FileResourceCrawler {
     private final Comparator<Path> pathComparator = new FileNameComparator();
     private CRAWL_ORDER crawlOrder;
 
-    public FSDirectoryCrawler(ArrayBlockingQueue<FileResource> fileQueue, int numConsumers,
-                              Path root, CRAWL_ORDER crawlOrder) {
+    public FSDirectoryCrawler(ArrayBlockingQueue<FileResource> fileQueue, int numConsumers, Path root, CRAWL_ORDER crawlOrder) {
         super(fileQueue, numConsumers);
         this.root = root;
         this.startDirectory = root;
         this.crawlOrder = crawlOrder;
         if (!Files.isDirectory(startDirectory)) {
-            throw new RuntimeException(
-                    "Crawler couldn't find this directory:" + startDirectory.toAbsolutePath());
+            throw new RuntimeException("Crawler couldn't find this directory:" + startDirectory.toAbsolutePath());
         }
 
     }
 
-    public FSDirectoryCrawler(ArrayBlockingQueue<FileResource> fileQueue, int numConsumers,
-                              Path root, Path startDirectory, CRAWL_ORDER crawlOrder) {
+    public FSDirectoryCrawler(ArrayBlockingQueue<FileResource> fileQueue, int numConsumers, Path root, Path startDirectory, CRAWL_ORDER crawlOrder) {
         super(fileQueue, numConsumers);
         this.root = root;
         this.startDirectory = startDirectory;
         this.crawlOrder = crawlOrder;
-        assert (startDirectory.toAbsolutePath().startsWith(root.toAbsolutePath()));
+        assert (startDirectory
+                .toAbsolutePath()
+                .startsWith(root.toAbsolutePath()));
 
         if (!Files.isDirectory(startDirectory)) {
-            throw new RuntimeException(
-                    "Crawler couldn't find this directory:" + startDirectory.toAbsolutePath());
+            throw new RuntimeException("Crawler couldn't find this directory:" + startDirectory.toAbsolutePath());
         }
     }
 
@@ -82,8 +80,7 @@ public class FSDirectoryCrawler extends FileResourceCrawler {
                 files.add(p);
             }
         } catch (IOException e) {
-            LOG.warn("FSFileAdder couldn't read {}: {}", directory.toAbsolutePath(), e.getMessage(),
-                    e);
+            LOG.warn("FSFileAdder couldn't read {}: {}", directory.toAbsolutePath(), e.getMessage(), e);
         }
         if (files.size() == 0) {
             LOG.info("Empty directory: {}", directory.toAbsolutePath());
@@ -100,7 +97,9 @@ public class FSDirectoryCrawler extends FileResourceCrawler {
         int numFiles = 0;
         List<Path> directories = new LinkedList<>();
         for (Path f : files) {
-            if (Thread.currentThread().isInterrupted()) {
+            if (Thread
+                    .currentThread()
+                    .isInterrupted()) {
                 throw new InterruptedException("file adder interrupted");
             }
             if (!Files.isReadable(f)) {
@@ -157,7 +156,12 @@ public class FSDirectoryCrawler extends FileResourceCrawler {
             if (f1 == null || f2 == null) {
                 return 0;
             }
-            return f1.getFileName().toString().compareTo(f2.getFileName().toString());
+            return f1
+                    .getFileName()
+                    .toString()
+                    .compareTo(f2
+                            .getFileName()
+                            .toString());
         }
     }
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDocumentSelector.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDocumentSelector.java
index 4955fbeaa..628bb0052 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDocumentSelector.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSDocumentSelector.java
@@ -42,8 +42,7 @@ public class FSDocumentSelector implements DocumentSelector {
     private final long maxFileSizeBytes;
     private final long minFileSizeBytes;
 
-    public FSDocumentSelector(Pattern includeFileName, Pattern excludeFileName,
-                              long minFileSizeBytes, long maxFileSizeBytes) {
+    public FSDocumentSelector(Pattern includeFileName, Pattern excludeFileName, long minFileSizeBytes, long maxFileSizeBytes) {
         this.includeFileName = includeFileName;
         this.excludeFileName = excludeFileName;
         this.minFileSizeBytes = minFileSizeBytes;
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSFileResource.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSFileResource.java
index 34d19a818..2f92b3f6b 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSFileResource.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSFileResource.java
@@ -57,12 +57,18 @@ public class FSFileResource implements FileResource {
         this.fullPath = fullPath;
         this.metadata = new Metadata();
         //child path must actually be a child
-        assert (fullPath.toAbsolutePath().startsWith(inputRoot.toAbsolutePath()));
-        this.relativePath = inputRoot.relativize(fullPath).toString();
+        assert (fullPath
+                .toAbsolutePath()
+                .startsWith(inputRoot.toAbsolutePath()));
+        this.relativePath = inputRoot
+                .relativize(fullPath)
+                .toString();
 
         //need to set these now so that the filter can determine
         //whether or not to crawl this file
-        metadata.set(TikaCoreProperties.RESOURCE_NAME_KEY, fullPath.getFileName().toString());
+        metadata.set(TikaCoreProperties.RESOURCE_NAME_KEY, fullPath
+                .getFileName()
+                .toString());
         long sz = -1;
         try {
             sz = Files.size(fullPath);
@@ -85,10 +91,14 @@ public class FSFileResource implements FileResource {
      * @return the lowercased extension or an empty string
      */
     private String getExtension(Path fullPath) {
-        String p = fullPath.getFileName().toString();
+        String p = fullPath
+                .getFileName()
+                .toString();
         int i = p.lastIndexOf(".");
         if (i > -1) {
-            return p.substring(i + 1).toLowerCase(Locale.ROOT);
+            return p
+                    .substring(i + 1)
+                    .toLowerCase(Locale.ROOT);
         }
         return "";
     }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSListCrawler.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSListCrawler.java
index a8326e5c8..f8befe93c 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSListCrawler.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSListCrawler.java
@@ -49,8 +49,7 @@ public class FSListCrawler extends FileResourceCrawler {
      * @param charset      charset of the file
      * @throws IOException
      */
-    public FSListCrawler(ArrayBlockingQueue<FileResource> fileQueue, int numConsumers, Path root,
-                         Path list, Charset charset) throws IOException {
+    public FSListCrawler(ArrayBlockingQueue<FileResource> fileQueue, int numConsumers, Path root, Path list, Charset charset) throws IOException {
         super(fileQueue, numConsumers);
         reader = Files.newBufferedReader(list, charset);
         this.root = root;
@@ -60,7 +59,9 @@ public class FSListCrawler extends FileResourceCrawler {
         String line = nextLine();
 
         while (line != null) {
-            if (Thread.currentThread().isInterrupted()) {
+            if (Thread
+                    .currentThread()
+                    .isInterrupted()) {
                 throw new InterruptedException("file adder interrupted");
             }
             Path f = Paths.get(root.toString(), line);
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSOutputStreamFactory.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSOutputStreamFactory.java
index 7760d188a..bcd0d89e5 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSOutputStreamFactory.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSOutputStreamFactory.java
@@ -37,8 +37,7 @@ public class FSOutputStreamFactory implements OutputStreamFactory {
     private final String suffix;
     private final COMPRESSION compression;
 
-    public FSOutputStreamFactory(Path outputRoot, FSUtil.HANDLE_EXISTING handleExisting,
-                                 COMPRESSION compression, String suffix) {
+    public FSOutputStreamFactory(Path outputRoot, FSUtil.HANDLE_EXISTING handleExisting, COMPRESSION compression, String suffix) {
         this.handleExisting = handleExisting;
         this.outputRoot = outputRoot;
         this.suffix = suffix;
@@ -65,8 +64,7 @@ public class FSOutputStreamFactory implements OutputStreamFactory {
     @Override
     public OutputStream getOutputStream(Metadata metadata) throws IOException {
         String initialRelativePath = metadata.get(FSProperties.FS_REL_PATH);
-        Path outputPath =
-                FSUtil.getOutputPath(outputRoot, initialRelativePath, handleExisting, suffix);
+        Path outputPath = FSUtil.getOutputPath(outputRoot, initialRelativePath, handleExisting, suffix);
         if (outputPath == null) {
             return null;
         }
@@ -74,8 +72,7 @@ public class FSOutputStreamFactory implements OutputStreamFactory {
             Files.createDirectories(outputPath.getParent());
             //TODO: shouldn't need this any more in java 7, right?
             if (!Files.isDirectory(outputPath.getParent())) {
-                throw new IOException(
-                        "Couldn't create parent directory for:" + outputPath.toAbsolutePath());
+                throw new IOException("Couldn't create parent directory for:" + outputPath.toAbsolutePath());
             }
         }
 
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSProperties.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSProperties.java
index dab246164..f31d6c825 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSProperties.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSProperties.java
@@ -24,6 +24,5 @@ public class FSProperties {
     /**
      * File's relative path (including file name) from a given source root
      */
-    public final static Property FS_REL_PATH =
-            Property.internalText(TIKA_BATCH_FS_NAMESPACE + ":relative_path");
+    public final static Property FS_REL_PATH = Property.internalText(TIKA_BATCH_FS_NAMESPACE + ":relative_path");
 }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSUtil.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSUtil.java
index 8777f0416..43f591b77 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/FSUtil.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/FSUtil.java
@@ -31,18 +31,23 @@ import java.util.regex.Pattern;
  */
 public class FSUtil {
 
-    private final static Pattern FILE_NAME_PATTERN =
-            Pattern.compile("\\A(.*?)(?:\\((\\d+)\\))?\\.([^\\.]+)\\Z");
+    private final static Pattern FILE_NAME_PATTERN = Pattern.compile("\\A(.*?)(?:\\((\\d+)\\))?\\.([^\\.]+)\\Z");
 
     @Deprecated
     public static boolean checkThisIsAncestorOfThat(File ancestor, File child) {
-        int ancLen = ancestor.getAbsolutePath().length();
-        int childLen = child.getAbsolutePath().length();
+        int ancLen = ancestor
+                .getAbsolutePath()
+                .length();
+        int childLen = child
+                .getAbsolutePath()
+                .length();
         if (childLen <= ancLen) {
             return false;
         }
 
-        String childBase = child.getAbsolutePath().substring(0, ancLen);
+        String childBase = child
+                .getAbsolutePath()
+                .substring(0, ancLen);
         return childBase.equals(ancestor.getAbsolutePath());
 
     }
@@ -86,11 +91,8 @@ public class FSUtil {
      * @see #getOutputPath(Path, String, HANDLE_EXISTING, String)
      */
     @Deprecated
-    public static File getOutputFile(File outputRoot, String initialRelativePath,
-                                     HANDLE_EXISTING handleExisting, String suffix)
-            throws IOException {
-        return getOutputPath(Paths.get(outputRoot.toURI()), initialRelativePath, handleExisting,
-                suffix).toFile();
+    public static File getOutputFile(File outputRoot, String initialRelativePath, HANDLE_EXISTING handleExisting, String suffix) throws IOException {
+        return getOutputPath(Paths.get(outputRoot.toURI()), initialRelativePath, handleExisting, suffix).toFile();
     }
 
     /**
@@ -122,9 +124,7 @@ public class FSUtil {
      * @return can return null
      * @throws IOException
      */
-    public static Path getOutputPath(Path outputRoot, String initialRelativePath,
-                                     HANDLE_EXISTING handleExisting, String suffix)
-            throws IOException {
+    public static Path getOutputPath(Path outputRoot, String initialRelativePath, HANDLE_EXISTING handleExisting, String suffix) throws IOException {
 
         String localSuffix = (suffix == null) ? "" : suffix;
         Path cand = FSUtil.resolveRelative(outputRoot, initialRelativePath + "." + localSuffix);
@@ -149,7 +149,9 @@ public class FSUtil {
         String fNameExt = "";
         //this doesn't include the addition of the localSuffix
         Path candOnly = FSUtil.resolveRelative(outputRoot, initialRelativePath);
-        Matcher m = FILE_NAME_PATTERN.matcher(candOnly.getFileName().toString());
+        Matcher m = FILE_NAME_PATTERN.matcher(candOnly
+                .getFileName()
+                .toString());
         if (m.find()) {
             fNameBase = m.group(1);
 
@@ -174,13 +176,11 @@ public class FSUtil {
         cnt = 0;
         while (Files.exists(cand) && cnt++ < 20000) {
             UUID uid = UUID.randomUUID();
-            cand = FSUtil.resolveRelative(outputParent,
-                    uid.toString() + fNameExt + "" + localSuffix);
+            cand = FSUtil.resolveRelative(outputParent, uid.toString() + fNameExt + "" + localSuffix);
         }
 
         if (Files.exists(cand)) {
-            throw new IOException(
-                    "Couldn't find candidate output file after trying " + "very, very hard");
+            throw new IOException("Couldn't find candidate output file after trying " + "very, very hard");
         }
         return cand;
     }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/RecursiveParserWrapperFSConsumer.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/RecursiveParserWrapperFSConsumer.java
index 542b69396..b3527dc40 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/RecursiveParserWrapperFSConsumer.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/RecursiveParserWrapperFSConsumer.java
@@ -30,11 +30,11 @@ import org.apache.tika.batch.FileResource;
 import org.apache.tika.batch.OutputStreamFactory;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.filter.MetadataFilter;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
 import org.apache.tika.parser.ParseContext;
 import org.apache.tika.parser.Parser;
 import org.apache.tika.sax.ContentHandlerFactory;
 import org.apache.tika.sax.RecursiveParserWrapperHandler;
+import org.apache.tika.serialization.JsonMetadataList;
 
 /**
  * This runs a RecursiveParserWrapper against an input file
@@ -55,9 +55,7 @@ public class RecursiveParserWrapperFSConsumer extends AbstractFSConsumer {
      * @param contentHandlerFactory
      * @param fsOSFactory
      */
-    public RecursiveParserWrapperFSConsumer(ArrayBlockingQueue<FileResource> queue, Parser parser,
-                                            ContentHandlerFactory contentHandlerFactory,
-                                            OutputStreamFactory fsOSFactory,
+    public RecursiveParserWrapperFSConsumer(ArrayBlockingQueue<FileResource> queue, Parser parser, ContentHandlerFactory contentHandlerFactory, OutputStreamFactory fsOSFactory,
                                             MetadataFilter metadataFilter) {
         super(queue);
         this.contentHandlerFactory = contentHandlerFactory;
@@ -75,7 +73,9 @@ public class RecursiveParserWrapperFSConsumer extends AbstractFSConsumer {
         OutputStream os = getOutputStream(fsOSFactory, fileResource);
 
         if (os == null) {
-            LOG.debug("Skipping: {}", fileResource.getMetadata().get(FSProperties.FS_REL_PATH));
+            LOG.debug("Skipping: {}", fileResource
+                    .getMetadata()
+                    .get(FSProperties.FS_REL_PATH));
             return false;
         }
 
@@ -92,8 +92,7 @@ public class RecursiveParserWrapperFSConsumer extends AbstractFSConsumer {
         Throwable thrown = null;
         List<Metadata> metadataList = null;
         Metadata containerMetadata = fileResource.getMetadata();
-        RecursiveParserWrapperHandler handler =
-                new RecursiveParserWrapperHandler(contentHandlerFactory, -1, metadataFilter);
+        RecursiveParserWrapperHandler handler = new RecursiveParserWrapperHandler(contentHandlerFactory, -1, metadataFilter);
         try {
             parse(fileResource.getResourceId(), parser, is, handler, containerMetadata, context);
         } catch (Throwable t) {
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/StreamOutRPWFSConsumer.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/StreamOutRPWFSConsumer.java
index 03c3ba14e..24228a9ea 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/StreamOutRPWFSConsumer.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/StreamOutRPWFSConsumer.java
@@ -35,11 +35,11 @@ import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
 import org.apache.tika.metadata.filter.MetadataFilter;
-import org.apache.tika.metadata.serialization.JsonStreamingSerializer;
 import org.apache.tika.parser.ParseContext;
 import org.apache.tika.parser.Parser;
 import org.apache.tika.sax.AbstractRecursiveParserWrapperHandler;
 import org.apache.tika.sax.ContentHandlerFactory;
+import org.apache.tika.serialization.JsonStreamingSerializer;
 
 /**
  * This uses the {@link JsonStreamingSerializer} to write out a
@@ -54,9 +54,8 @@ public class StreamOutRPWFSConsumer extends AbstractFSConsumer {
     private String outputEncoding = "UTF-8";
 
 
-    public StreamOutRPWFSConsumer(ArrayBlockingQueue<FileResource> queue, Parser parser,
-                                  ContentHandlerFactory contentHandlerFactory,
-                                  OutputStreamFactory fsOSFactory, MetadataFilter metadataFilter) {
+    public StreamOutRPWFSConsumer(ArrayBlockingQueue<FileResource> queue, Parser parser, ContentHandlerFactory contentHandlerFactory, OutputStreamFactory fsOSFactory,
+                                  MetadataFilter metadataFilter) {
         super(queue);
         this.contentHandlerFactory = contentHandlerFactory;
         this.fsOSFactory = fsOSFactory;
@@ -73,7 +72,9 @@ public class StreamOutRPWFSConsumer extends AbstractFSConsumer {
         OutputStream os = getOutputStream(fsOSFactory, fileResource);
 
         if (os == null) {
-            LOG.debug("Skipping: {}", fileResource.getMetadata().get(FSProperties.FS_REL_PATH));
+            LOG.debug("Skipping: {}", fileResource
+                    .getMetadata()
+                    .get(FSProperties.FS_REL_PATH));
             return false;
         }
 
@@ -88,11 +89,9 @@ public class StreamOutRPWFSConsumer extends AbstractFSConsumer {
         }
 
         Metadata containerMetadata = fileResource.getMetadata();
-        JsonStreamingSerializer writer =
-                new JsonStreamingSerializer(new OutputStreamWriter(os, StandardCharsets.UTF_8));
+        JsonStreamingSerializer writer = new JsonStreamingSerializer(new OutputStreamWriter(os, StandardCharsets.UTF_8));
 
-        WriteoutRPWHandler handler =
-                new WriteoutRPWHandler(contentHandlerFactory, writer, metadataFilter);
+        WriteoutRPWHandler handler = new WriteoutRPWHandler(contentHandlerFactory, writer, metadataFilter);
         Throwable thrown = null;
         try {
             parse(fileResource.getResourceId(), parser, is, handler, containerMetadata, context);
@@ -137,16 +136,14 @@ public class StreamOutRPWFSConsumer extends AbstractFSConsumer {
         private final JsonStreamingSerializer jsonWriter;
         private final MetadataFilter metadataFilter;
 
-        public WriteoutRPWHandler(ContentHandlerFactory contentHandlerFactory,
-                                  JsonStreamingSerializer writer, MetadataFilter metadataFilter) {
+        public WriteoutRPWHandler(ContentHandlerFactory contentHandlerFactory, JsonStreamingSerializer writer, MetadataFilter metadataFilter) {
             super(contentHandlerFactory);
             this.jsonWriter = writer;
             this.metadataFilter = metadataFilter;
         }
 
         @Override
-        public void endEmbeddedDocument(ContentHandler contentHandler, Metadata metadata)
-                throws SAXException {
+        public void endEmbeddedDocument(ContentHandler contentHandler, Metadata metadata) throws SAXException {
             metadata.add(TikaCoreProperties.TIKA_CONTENT, contentHandler.toString());
             try {
                 metadataFilter.filter(metadata);
@@ -161,8 +158,7 @@ public class StreamOutRPWFSConsumer extends AbstractFSConsumer {
         }
 
         @Override
-        public void endDocument(ContentHandler contentHandler, Metadata metadata)
-                throws SAXException {
+        public void endDocument(ContentHandler contentHandler, Metadata metadata) throws SAXException {
             endEmbeddedDocument(contentHandler, metadata);
         }
     }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/BasicTikaFSConsumersBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/BasicTikaFSConsumersBuilder.java
index 9818f7477..94a031390 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/BasicTikaFSConsumersBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/BasicTikaFSConsumersBuilder.java
@@ -57,22 +57,19 @@ import org.apache.tika.util.XMLDOMUtil;
 public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
 
     @Override
-    public ConsumersManager build(Node node, Map<String, String> runtimeAttributes,
-                                  ArrayBlockingQueue<FileResource> queue) {
+    public ConsumersManager build(Node node, Map<String, String> runtimeAttributes, ArrayBlockingQueue<FileResource> queue) {
 
         //figure out if we're building a recursiveParserWrapper
         boolean recursiveParserWrapper = false;
         String recursiveParserWrapperString = runtimeAttributes.get("recursiveParserWrapper");
         if (recursiveParserWrapperString != null) {
-            recursiveParserWrapper =
-                    PropsUtil.getBoolean(recursiveParserWrapperString, recursiveParserWrapper);
+            recursiveParserWrapper = PropsUtil.getBoolean(recursiveParserWrapperString, recursiveParserWrapper);
         } else {
-            Node recursiveParserWrapperNode =
-                    node.getAttributes().getNamedItem("recursiveParserWrapper");
+            Node recursiveParserWrapperNode = node
+                    .getAttributes()
+                    .getNamedItem("recursiveParserWrapper");
             if (recursiveParserWrapperNode != null) {
-                recursiveParserWrapper = PropsUtil
-                        .getBoolean(recursiveParserWrapperNode.getNodeValue(),
-                                recursiveParserWrapper);
+                recursiveParserWrapper = PropsUtil.getBoolean(recursiveParserWrapperNode.getNodeValue(), recursiveParserWrapper);
             }
         }
 
@@ -81,7 +78,9 @@ public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
         if (streamOutString != null) {
             streamOut = PropsUtil.getBoolean(streamOutString, streamOut);
         } else {
-            Node streamOutNode = node.getAttributes().getNamedItem("streamout");
+            Node streamOutNode = node
+                    .getAttributes()
+                    .getNamedItem("streamout");
             if (streamOutNode != null) {
                 streamOut = PropsUtil.getBoolean(streamOutNode.getNodeValue(), streamOut);
             }
@@ -93,11 +92,11 @@ public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
         if (consumersManagerMaxMillisString != null) {
             consumersManagerMaxMillis = PropsUtil.getLong(consumersManagerMaxMillisString, null);
         } else {
-            Node consumersManagerMaxMillisNode =
-                    node.getAttributes().getNamedItem("consumersManagerMaxMillis");
+            Node consumersManagerMaxMillisNode = node
+                    .getAttributes()
+                    .getNamedItem("consumersManagerMaxMillis");
             if (consumersManagerMaxMillis == null && consumersManagerMaxMillisNode != null) {
-                consumersManagerMaxMillis =
-                        PropsUtil.getLong(consumersManagerMaxMillisNode.getNodeValue(), null);
+                consumersManagerMaxMillis = PropsUtil.getLong(consumersManagerMaxMillisNode.getNodeValue(), null);
             }
         }
 
@@ -105,7 +104,9 @@ public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
         String tikaConfigPath = runtimeAttributes.get("c");
 
         if (tikaConfigPath == null) {
-            Node tikaConfigNode = node.getAttributes().getNamedItem("tikaConfig");
+            Node tikaConfigNode = node
+                    .getAttributes()
+                    .getNamedItem("tikaConfig");
             if (tikaConfigNode != null) {
                 tikaConfigPath = PropsUtil.getString(tikaConfigNode.getNodeValue(), null);
             }
@@ -144,17 +145,12 @@ public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
             }
         }
 
-        if (contentHandlerFactoryNode == null || parserFactoryNode == null ||
-                outputStreamFactoryNode == null) {
-            throw new RuntimeException("You must specify a ContentHandlerFactory, " +
-                    "a ParserFactory and an OutputStreamFactory");
+        if (contentHandlerFactoryNode == null || parserFactoryNode == null || outputStreamFactoryNode == null) {
+            throw new RuntimeException("You must specify a ContentHandlerFactory, " + "a ParserFactory and an OutputStreamFactory");
         }
-        ContentHandlerFactory contentHandlerFactory =
-                getContentHandlerFactory(contentHandlerFactoryNode, runtimeAttributes);
+        ContentHandlerFactory contentHandlerFactory = getContentHandlerFactory(contentHandlerFactoryNode, runtimeAttributes);
         ParserFactory parserFactory = getParserFactory(parserFactoryNode, runtimeAttributes);
-        OutputStreamFactory outputStreamFactory =
-                getOutputStreamFactory(outputStreamFactoryNode, runtimeAttributes,
-                        contentHandlerFactory, recursiveParserWrapper);
+        OutputStreamFactory outputStreamFactory = getOutputStreamFactory(outputStreamFactoryNode, runtimeAttributes, contentHandlerFactory, recursiveParserWrapper);
         Parser parser = parserFactory.getParser(config);
         if (recursiveParserWrapper) {
             MetadataFilter metadataFilter = config.getMetadataFilter();
@@ -163,19 +159,15 @@ public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
             for (int i = 0; i < numConsumers; i++) {
                 FileResourceConsumer c = null;
                 if (streamOut) {
-                    c = new StreamOutRPWFSConsumer(queue, parser, contentHandlerFactory,
-                            outputStreamFactory, metadataFilter);
+                    c = new StreamOutRPWFSConsumer(queue, parser, contentHandlerFactory, outputStreamFactory, metadataFilter);
                 } else {
-                    c = new RecursiveParserWrapperFSConsumer(queue, parser, contentHandlerFactory,
-                            outputStreamFactory, metadataFilter);
+                    c = new RecursiveParserWrapperFSConsumer(queue, parser, contentHandlerFactory, outputStreamFactory, metadataFilter);
                 }
                 consumers.add(c);
             }
         } else {
             for (int i = 0; i < numConsumers; i++) {
-                FileResourceConsumer c =
-                        new BasicTikaFSConsumer(queue, parser, contentHandlerFactory,
-                                outputStreamFactory);
+                FileResourceConsumer c = new BasicTikaFSConsumer(queue, parser, contentHandlerFactory, outputStreamFactory);
                 consumers.add(c);
             }
         }
@@ -186,30 +178,25 @@ public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
         return manager;
     }
 
-    private ContentHandlerFactory getContentHandlerFactory(Node node,
-                                                           Map<String, String> runtimeAttributes) {
+    private ContentHandlerFactory getContentHandlerFactory(Node node, Map<String, String> runtimeAttributes) {
 
         Map<String, String> localAttrs = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
         String className = localAttrs.get("builderClass");
         if (className == null) {
             throw new RuntimeException("Must specify builderClass for contentHandler");
         }
-        IContentHandlerFactoryBuilder builder =
-                ClassLoaderUtil.buildClass(IContentHandlerFactoryBuilder.class, className);
+        IContentHandlerFactoryBuilder builder = ClassLoaderUtil.buildClass(IContentHandlerFactoryBuilder.class, className);
         return builder.build(node, runtimeAttributes);
     }
 
     private ParserFactory getParserFactory(Node node, Map<String, String> runtimeAttributes) {
         Map<String, String> localAttrs = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
         String className = localAttrs.get("builderClass");
-        IParserFactoryBuilder builder =
-                ClassLoaderUtil.buildClass(IParserFactoryBuilder.class, className);
+        IParserFactoryBuilder builder = ClassLoaderUtil.buildClass(IParserFactoryBuilder.class, className);
         return builder.build(node, runtimeAttributes);
     }
 
-    private OutputStreamFactory getOutputStreamFactory(Node node,
-                                                       Map<String, String> runtimeAttributes,
-                                                       ContentHandlerFactory contentHandlerFactory,
+    private OutputStreamFactory getOutputStreamFactory(Node node, Map<String, String> runtimeAttributes, ContentHandlerFactory contentHandlerFactory,
                                                        boolean useRecursiveParserWrapper) {
         Map<String, String> attrs = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
 
@@ -255,12 +242,10 @@ public class BasicTikaFSConsumersBuilder extends AbstractConsumersBuilder {
         //TODO: possibly open up the different handle-existings in the future
         //but for now, lock it down to require skip.  Too dangerous otherwise
         //if the driver restarts and this is set to overwrite...
-        return new FSOutputStreamFactory(outputDir, FSUtil.HANDLE_EXISTING.SKIP, compression,
-                suffix);
+        return new FSOutputStreamFactory(outputDir, FSUtil.HANDLE_EXISTING.SKIP, compression, suffix);
     }
 
-    private void appendCompression(FSOutputStreamFactory.COMPRESSION compression,
-                                   StringBuilder sb) {
+    private void appendCompression(FSOutputStreamFactory.COMPRESSION compression, StringBuilder sb) {
         switch (compression) {
             case NONE:
                 break;
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/FSCrawlerBuilder.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/FSCrawlerBuilder.java
index b1bf9feec..56300b921 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/FSCrawlerBuilder.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/builders/FSCrawlerBuilder.java
@@ -61,8 +61,7 @@ public class FSCrawlerBuilder implements ICrawlerBuilder {
     private final static String EXCLUDE_FILE_PAT_ATTR = "excludeFilePat";
 
     @Override
-    public FileResourceCrawler build(Node node, Map<String, String> runtimeAttributes,
-                                     ArrayBlockingQueue<FileResource> queue) {
+    public FileResourceCrawler build(Node node, Map<String, String> runtimeAttributes, ArrayBlockingQueue<FileResource> queue) {
 
         Map<String, String> attributes = XMLDOMUtil.mapifyAttrs(node, runtimeAttributes);
 
@@ -77,20 +76,17 @@ public class FSCrawlerBuilder implements ICrawlerBuilder {
                 System.err.println("randomCrawl attribute is ignored by FSListCrawler");
             }
             Path fileList = PropsUtil.getPath(attributes.get("fileList"), null);
-            String encodingString =
-                    PropsUtil.getString(attributes.get("fileListEncoding"), "UTF-8");
+            String encodingString = PropsUtil.getString(attributes.get("fileListEncoding"), "UTF-8");
 
             try {
                 Charset encoding = Charset.forName(encodingString);
                 crawler = new FSListCrawler(queue, numConsumers, inputDir, fileList, encoding);
             } catch (FileNotFoundException e) {
-                throw new RuntimeException(
-                        "fileList file not found for FSListCrawler: " + fileList.toAbsolutePath());
+                throw new RuntimeException("fileList file not found for FSListCrawler: " + fileList.toAbsolutePath());
             } catch (UnsupportedEncodingException e) {
                 throw new RuntimeException("fileList encoding not supported: " + encodingString);
             } catch (IOException e) {
-                throw new RuntimeException(
-                        "IOException while trying to open fileList: " + e.getMessage(), e);
+                throw new RuntimeException("IOException while trying to open fileList: " + e.getMessage(), e);
             }
         } else {
             FSDirectoryCrawler.CRAWL_ORDER crawlOrder = getCrawlOrder(attributes.get(CRAWL_ORDER));
@@ -98,13 +94,11 @@ public class FSCrawlerBuilder implements ICrawlerBuilder {
             if (startDir == null) {
                 crawler = new FSDirectoryCrawler(queue, numConsumers, inputDir, crawlOrder);
             } else {
-                crawler =
-                        new FSDirectoryCrawler(queue, numConsumers, inputDir, startDir, crawlOrder);
+                crawler = new FSDirectoryCrawler(queue, numConsumers, inputDir, startDir, crawlOrder);
             }
         }
 
-        crawler.setMaxFilesToConsider(
-                PropsUtil.getInt(attributes.get(MAX_FILES_TO_CONSIDER_ATTR), -1));
+        crawler.setMaxFilesToConsider(PropsUtil.getInt(attributes.get(MAX_FILES_TO_CONSIDER_ATTR), -1));
         crawler.setMaxFilesToAdd(PropsUtil.getInt(attributes.get(MAX_FILES_TO_ADD_ATTR), -1));
 
         DocumentSelector selector = buildSelector(attributes);
@@ -112,17 +106,22 @@ public class FSCrawlerBuilder implements ICrawlerBuilder {
             crawler.setDocumentSelector(selector);
         }
 
-        crawler.setMaxConsecWaitInMillis(
-                PropsUtil.getLong(attributes.get(MAX_CONSEC_WAIT_MILLIS), 300000L));//5 minutes
+        crawler.setMaxConsecWaitInMillis(PropsUtil.getLong(attributes.get(MAX_CONSEC_WAIT_MILLIS), 300000L));//5 minutes
         return crawler;
     }
 
     private FSDirectoryCrawler.CRAWL_ORDER getCrawlOrder(String s) {
-        if (s == null || s.trim().length() == 0 || s.equals("os")) {
+        if (s == null || s
+                .trim()
+                .length() == 0 || s.equals("os")) {
             return FSDirectoryCrawler.CRAWL_ORDER.OS_ORDER;
-        } else if (s.toLowerCase(Locale.ROOT).contains("rand")) {
+        } else if (s
+                .toLowerCase(Locale.ROOT)
+                .contains("rand")) {
             return FSDirectoryCrawler.CRAWL_ORDER.RANDOM;
-        } else if (s.toLowerCase(Locale.ROOT).contains("sort")) {
+        } else if (s
+                .toLowerCase(Locale.ROOT)
+                .contains("sort")) {
             return FSDirectoryCrawler.CRAWL_ORDER.SORTED;
         } else {
             return FSDirectoryCrawler.CRAWL_ORDER.OS_ORDER;
@@ -134,10 +133,8 @@ public class FSCrawlerBuilder implements ICrawlerBuilder {
         String excludeString = attributes.get(EXCLUDE_FILE_PAT_ATTR);
         long maxFileSize = PropsUtil.getLong(attributes.get(MAX_FILE_SIZE_BYTES_ATTR), -1L);
         long minFileSize = PropsUtil.getLong(attributes.get(MIN_FILE_SIZE_BYTES_ATTR), -1L);
-        Pattern includePat = (includeString != null && includeString.length() > 0) ?
-                Pattern.compile(includeString) : null;
-        Pattern excludePat = (excludeString != null && excludeString.length() > 0) ?
-                Pattern.compile(excludeString) : null;
+        Pattern includePat = (includeString != null && includeString.length() > 0) ? Pattern.compile(includeString) : null;
+        Pattern excludePat = (excludeString != null && excludeString.length() > 0) ? Pattern.compile(excludeString) : null;
 
         return new FSDocumentSelector(includePat, excludePat, minFileSize, maxFileSize);
     }
diff --git a/tika-batch/src/main/java/org/apache/tika/batch/fs/strawman/StrawManTikaAppDriver.java b/tika-batch/src/main/java/org/apache/tika/batch/fs/strawman/StrawManTikaAppDriver.java
index ef7668532..95f7ee395 100644
--- a/tika-batch/src/main/java/org/apache/tika/batch/fs/strawman/StrawManTikaAppDriver.java
+++ b/tika-batch/src/main/java/org/apache/tika/batch/fs/strawman/StrawManTikaAppDriver.java
@@ -58,8 +58,7 @@ public class StrawManTikaAppDriver implements Callable<Integer> {
     private final Path fileList;
     private final String[] args;
 
-    public StrawManTikaAppDriver(Path inputRoot, Path outputRoot, int totalThreads, Path fileList,
-                                 String[] args) {
+    public StrawManTikaAppDriver(Path inputRoot, Path outputRoot, int totalThreads, Path fileList, String[] args) {
         this.inputRoot = inputRoot;
         this.outputRoot = outputRoot;
         this.fileList = fileList;
@@ -94,16 +93,15 @@ public class StrawManTikaAppDriver implements Callable<Integer> {
         }
 
         int initialParams = (fileList == null) ? 3 : 4;
-        List<String> commandLine =
-                new ArrayList<>(Arrays.asList(args).subList(initialParams, args.length));
+        List<String> commandLine = new ArrayList<>(Arrays
+                .asList(args)
+                .subList(initialParams, args.length));
         totalThreads = Math.max(totalThreads, 1);
         ExecutorService ex = Executors.newFixedThreadPool(totalThreads);
         ExecutorCompletionService<Integer> completionService = new ExecutorCompletionService<>(ex);
 
         for (int i = 0; i < totalThreads; i++) {
-            StrawManTikaAppDriver driver =
-                    new StrawManTikaAppDriver(inputDir, outputDir, totalThreads, fileList,
-                            commandLine.toArray(new String[0]));
+            StrawManTikaAppDriver driver = new StrawManTikaAppDriver(inputDir, outputDir, totalThreads, fileList, commandLine.toArray(new String[0]));
             completionService.submit(driver);
         }
 
@@ -129,15 +127,13 @@ public class StrawManTikaAppDriver implements Callable<Integer> {
         TikaVisitor v = new TikaVisitor();
         if (fileList != null) {
             TikaVisitor tikaVisitor = new TikaVisitor();
-            try (BufferedReader reader = Files
-                    .newBufferedReader(fileList, StandardCharsets.UTF_8)) {
+            try (BufferedReader reader = Files.newBufferedReader(fileList, StandardCharsets.UTF_8)) {
                 String line = reader.readLine();
                 while (line != null) {
                     Path inputFile = inputRoot.resolve(line.trim());
                     if (Files.isReadable(inputFile)) {
                         try {
-                            tikaVisitor.visitFile(inputFile,
-                                    Files.readAttributes(inputFile, BasicFileAttributes.class));
+                            tikaVisitor.visitFile(inputFile, Files.readAttributes(inputFile, BasicFileAttributes.class));
                         } catch (IOException e) {
                             LOG.warn("Problem with: " + inputFile, e);
                         }
@@ -166,14 +162,16 @@ public class StrawManTikaAppDriver implements Callable<Integer> {
         @Override
         public FileVisitResult visitFile(Path file, BasicFileAttributes attr) {
             if (totalThreads > 1) {
-                int hashCode = file.toAbsolutePath().toString().hashCode();
+                int hashCode = file
+                        .toAbsolutePath()
+                        .toString()
+                        .hashCode();
                 if (Math.abs(hashCode % totalThreads) != threadNum) {
                     return FileVisitResult.CONTINUE;
                 }
             }
             if (!file.startsWith(inputRoot)) {
-                LOG.warn("File (" + file.toAbsolutePath() + ") doesn't start with input root (" +
-                        inputRoot.toAbsolutePath() + ")");
+                LOG.warn("File (" + file.toAbsolutePath() + ") doesn't start with input root (" + inputRoot.toAbsolutePath() + ")");
                 return FileVisitResult.CONTINUE;
             }
             Path relPath = inputRoot.relativize(file);
@@ -187,19 +185,21 @@ public class StrawManTikaAppDriver implements Callable<Integer> {
                     suffix = ".html";
                 }
             }
-            String fullPath = file.toAbsolutePath().toString();
+            String fullPath = file
+                    .toAbsolutePath()
+                    .toString();
             if (fullPath.contains(" ")) {
                 fullPath = "\"" + fullPath + "\"";
             }
             commandLine.add(fullPath);
 
-            Path outputFile =
-                    Paths.get(outputRoot.toAbsolutePath().toString(), relPath.toString() + suffix);
+            Path outputFile = Paths.get(outputRoot
+                    .toAbsolutePath()
+                    .toString(), relPath.toString() + suffix);
             try {
                 Files.createDirectories(outputFile.getParent());
             } catch (IOException e) {
-                LOG.error(MarkerFactory.getMarker("FATAL"), "parent directory for {} was not made!",
-                        outputFile);
+                LOG.error(MarkerFactory.getMarker("FATAL"), "parent directory for {} was not made!", outputFile);
                 throw new RuntimeException("couldn't make parent file for " + outputFile);
             }
             ProcessBuilder builder = new ProcessBuilder();
@@ -238,8 +238,12 @@ public class StrawManTikaAppDriver implements Callable<Integer> {
                 proc.destroyForcibly();
             }
             try {
-                proc.getOutputStream().flush();
-                proc.getOutputStream().close();
+                proc
+                        .getOutputStream()
+                        .flush();
+                proc
+                        .getOutputStream()
+                        .close();
             } catch (IOException e) {
                 LOG.warn("couldn't close process outputstream", e);
             }
diff --git a/tika-batch/src/main/java/org/apache/tika/util/ClassLoaderUtil.java b/tika-batch/src/main/java/org/apache/tika/util/ClassLoaderUtil.java
index bbefed089..6b0d76c97 100644
--- a/tika-batch/src/main/java/org/apache/tika/util/ClassLoaderUtil.java
+++ b/tika-batch/src/main/java/org/apache/tika/util/ClassLoaderUtil.java
@@ -28,12 +28,12 @@ public class ClassLoaderUtil {
         try {
             clazz = loader.loadClass(className);
             if (iface.isAssignableFrom(clazz)) {
-                return (T) clazz.getDeclaredConstructor().newInstance();
+                return (T) clazz
+                        .getDeclaredConstructor()
+                        .newInstance();
             }
-            throw new IllegalArgumentException(
-                    iface + " is not assignable from " + className);
-        } catch (ClassNotFoundException | InstantiationException | IllegalAccessException |
-                 NoSuchMethodException | InvocationTargetException e) {
+            throw new IllegalArgumentException(iface + " is not assignable from " + className);
+        } catch (ClassNotFoundException | InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {
             throw new RuntimeException(e);
         }
 
diff --git a/tika-batch/src/main/java/org/apache/tika/util/PropsUtil.java b/tika-batch/src/main/java/org/apache/tika/util/PropsUtil.java
index 805186a91..830479012 100644
--- a/tika-batch/src/main/java/org/apache/tika/util/PropsUtil.java
+++ b/tika-batch/src/main/java/org/apache/tika/util/PropsUtil.java
@@ -37,10 +37,14 @@ public class PropsUtil {
         if (v == null || v.length() == 0) {
             return defaultMissing;
         }
-        if (v.toLowerCase(Locale.ROOT).equals("true")) {
+        if (v
+                .toLowerCase(Locale.ROOT)
+                .equals("true")) {
             return true;
         }
-        if (v.toLowerCase(Locale.ROOT).equals("false")) {
+        if (v
+                .toLowerCase(Locale.ROOT)
+                .equals("false")) {
             return false;
         }
         return defaultMissing;
diff --git a/tika-batch/src/main/java/org/apache/tika/util/XMLDOMUtil.java b/tika-batch/src/main/java/org/apache/tika/util/XMLDOMUtil.java
index fe7faa958..e4ad51226 100644
--- a/tika-batch/src/main/java/org/apache/tika/util/XMLDOMUtil.java
+++ b/tika-batch/src/main/java/org/apache/tika/util/XMLDOMUtil.java
@@ -58,8 +58,7 @@ public class XMLDOMUtil {
      * @param docElement        correct element that should have specified attribute
      * @return specified int value
      */
-    public static int getInt(String attrName, Map<String, String> runtimeAttributes,
-                             Node docElement) {
+    public static int getInt(String attrName, Map<String, String> runtimeAttributes, Node docElement) {
         String stringValue = getStringValue(attrName, runtimeAttributes, docElement);
         if (stringValue != null) {
             try {
@@ -68,8 +67,7 @@ public class XMLDOMUtil {
                 //swallow
             }
         }
-        throw new RuntimeException("Need to specify a parseable int value in -- " + attrName +
-                " -- in commandline or in config file!");
+        throw new RuntimeException("Need to specify a parseable int value in -- " + attrName + " -- in commandline or in config file!");
     }
 
 
@@ -83,8 +81,7 @@ public class XMLDOMUtil {
      * @param docElement        correct element that should have specified attribute
      * @return specified long value
      */
-    public static long getLong(String attrName, Map<String, String> runtimeAttributes,
-                               Node docElement) {
+    public static long getLong(String attrName, Map<String, String> runtimeAttributes, Node docElement) {
         String stringValue = getStringValue(attrName, runtimeAttributes, docElement);
         if (stringValue != null) {
             try {
@@ -93,15 +90,15 @@ public class XMLDOMUtil {
                 //swallow
             }
         }
-        throw new RuntimeException("Need to specify a \"long\" value in -- " + attrName +
-                " -- in commandline or in config file!");
+        throw new RuntimeException("Need to specify a \"long\" value in -- " + attrName + " -- in commandline or in config file!");
     }
 
-    private static String getStringValue(String attrName, Map<String, String> runtimeAttributes,
-                                         Node docElement) {
+    private static String getStringValue(String attrName, Map<String, String> runtimeAttributes, Node docElement) {
         String stringValue = runtimeAttributes.get(attrName);
         if (stringValue == null) {
-            Node staleNode = docElement.getAttributes().getNamedItem(attrName);
+            Node staleNode = docElement
+                    .getAttributes()
+                    .getNamedItem(attrName);
             if (staleNode != null) {
                 stringValue = staleNode.getNodeValue();
             }
diff --git a/tika-batch/src/test/java/org/apache/tika/batch/RecursiveParserWrapperFSConsumerTest.java b/tika-batch/src/test/java/org/apache/tika/batch/RecursiveParserWrapperFSConsumerTest.java
index 86aaf661c..72484fc03 100644
--- a/tika-batch/src/test/java/org/apache/tika/batch/RecursiveParserWrapperFSConsumerTest.java
+++ b/tika-batch/src/test/java/org/apache/tika/batch/RecursiveParserWrapperFSConsumerTest.java
@@ -37,10 +37,10 @@ import org.apache.tika.config.TikaConfig;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
 import org.apache.tika.metadata.filter.NoOpFilter;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
 import org.apache.tika.parser.Parser;
 import org.apache.tika.parser.RecursiveParserWrapper;
 import org.apache.tika.sax.BasicContentHandlerFactory;
+import org.apache.tika.serialization.JsonMetadataList;
 
 public class RecursiveParserWrapperFSConsumerTest extends TikaTest {
 
@@ -72,27 +72,37 @@ public class RecursiveParserWrapperFSConsumerTest extends TikaTest {
         queue.add(new PoisonFileResource());
 
         MockOSFactory mockOSFactory = new MockOSFactory();
-        Parser p = new RecursiveParserWrapper(
-                new AutoDetectParserFactory().getParser(new TikaConfig()));
-        RecursiveParserWrapperFSConsumer consumer = new RecursiveParserWrapperFSConsumer(queue, p,
-                new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.TEXT, -1),
-                mockOSFactory, NoOpFilter.NOOP_FILTER);
+        Parser p = new RecursiveParserWrapper(new AutoDetectParserFactory().getParser(new TikaConfig()));
+        RecursiveParserWrapperFSConsumer consumer =
+                new RecursiveParserWrapperFSConsumer(queue, p, new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.TEXT, -1), mockOSFactory,
+                        NoOpFilter.NOOP_FILTER);
 
         IFileProcessorFutureResult result = consumer.call();
-        mockOSFactory.getStreams().get(0).flush();
-        byte[] bytes = mockOSFactory.getStreams().get(0).toByteArray();
-        List<Metadata> results = JsonMetadataList
-                .fromJson(new InputStreamReader(new ByteArrayInputStream(bytes), UTF_8));
+        mockOSFactory
+                .getStreams()
+                .get(0)
+                .flush();
+        byte[] bytes = mockOSFactory
+                .getStreams()
+                .get(0)
+                .toByteArray();
+        List<Metadata> results = JsonMetadataList.fromJson(new InputStreamReader(new ByteArrayInputStream(bytes), UTF_8));
 
         assertEquals(4, results.size());
-        assertContains("another null pointer",
-                results.get(2).get(TikaCoreProperties.EMBEDDED_EXCEPTION));
+        assertContains("another null pointer", results
+                .get(2)
+                .get(TikaCoreProperties.EMBEDDED_EXCEPTION));
 
-        assertEquals("Nikolai Lobachevsky", results.get(0).get("author"));
+        assertEquals("Nikolai Lobachevsky", results
+                .get(0)
+                .get("author"));
         for (int i = 1; i < 4; i++) {
-            assertEquals("embeddedAuthor" + i, results.get(i).get("author"));
-            assertContains("some_embedded_content" + i,
-                    results.get(i).get(TikaCoreProperties.TIKA_CONTENT));
+            assertEquals("embeddedAuthor" + i, results
+                    .get(i)
+                    .get("author"));
+            assertContains("some_embedded_content" + i, results
+                    .get(i)
+                    .get(TikaCoreProperties.TIKA_CONTENT));
         }
     }
 
@@ -123,24 +133,34 @@ public class RecursiveParserWrapperFSConsumerTest extends TikaTest {
         queue.add(new PoisonFileResource());
 
         MockOSFactory mockOSFactory = new MockOSFactory();
-        Parser p = new RecursiveParserWrapper(
-                new AutoDetectParserFactory().getParser(new TikaConfig()));
-        RecursiveParserWrapperFSConsumer consumer = new RecursiveParserWrapperFSConsumer(queue, p,
-                new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.TEXT, -1),
-                mockOSFactory, NoOpFilter.NOOP_FILTER);
+        Parser p = new RecursiveParserWrapper(new AutoDetectParserFactory().getParser(new TikaConfig()));
+        RecursiveParserWrapperFSConsumer consumer =
+                new RecursiveParserWrapperFSConsumer(queue, p, new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.TEXT, -1), mockOSFactory,
+                        NoOpFilter.NOOP_FILTER);
 
         IFileProcessorFutureResult result = consumer.call();
-        mockOSFactory.getStreams().get(0).flush();
-        byte[] bytes = mockOSFactory.getStreams().get(0).toByteArray();
-        List<Metadata> results = JsonMetadataList
-                .fromJson(new InputStreamReader(new ByteArrayInputStream(bytes), UTF_8));
+        mockOSFactory
+                .getStreams()
+                .get(0)
+                .flush();
+        byte[] bytes = mockOSFactory
+                .getStreams()
+                .get(0)
+                .toByteArray();
+        List<Metadata> results = JsonMetadataList.fromJson(new InputStreamReader(new ByteArrayInputStream(bytes), UTF_8));
         assertEquals(2, results.size());
-        assertContains("another null pointer",
-                results.get(0).get(TikaCoreProperties.CONTAINER_EXCEPTION));
-        assertEquals("Nikolai Lobachevsky", results.get(0).get("author"));
-        assertEquals("embeddedAuthor", results.get(1).get("author"));
-        assertContains("some_embedded_content",
-                results.get(1).get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("another null pointer", results
+                .get(0)
+                .get(TikaCoreProperties.CONTAINER_EXCEPTION));
+        assertEquals("Nikolai Lobachevsky", results
+                .get(0)
+                .get("author"));
+        assertEquals("embeddedAuthor", results
+                .get(1)
+                .get("author"));
+        assertContains("some_embedded_content", results
+                .get(1)
+                .get(TikaCoreProperties.TIKA_CONTENT));
     }
 
 
diff --git a/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchDriverTest.java b/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchDriverTest.java
index 2643bd5c7..d1555875f 100644
--- a/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchDriverTest.java
+++ b/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchDriverTest.java
@@ -55,8 +55,7 @@ public class BatchDriverTest extends FSBatchTestBase {
         assertFalse(driver.isUserInterrupted());
         assertEquals(5, countChildren(outputDir));
 
-        assertContains("first test file",
-                readFileToString(outputDir.resolve("test2_ok.xml.xml"), UTF_8));
+        assertContains("first test file", readFileToString(outputDir.resolve("test2_ok.xml.xml"), UTF_8));
     }
 
     @Test
@@ -74,8 +73,7 @@ public class BatchDriverTest extends FSBatchTestBase {
         //could be one or two depending on timing
         assertTrue(driver.getNumRestarts() > 0);
         assertFalse(driver.isUserInterrupted());
-        assertContains("first test file",
-                readFileToString(outputDir.resolve("test6_ok.xml.xml"), UTF_8));
+        assertContains("first test file", readFileToString(outputDir.resolve("test6_ok.xml.xml"), UTF_8));
     }
 
     @Test
@@ -116,8 +114,7 @@ public class BatchDriverTest extends FSBatchTestBase {
         driver.execute();
         assertEquals(1, driver.getNumRestarts());
         assertFalse(driver.isUserInterrupted());
-        assertContains("first test file",
-                readFileToString(outputDir.resolve("test2_ok.xml.xml"), UTF_8));
+        assertContains("first test file", readFileToString(outputDir.resolve("test2_ok.xml.xml"), UTF_8));
     }
 
     @Test
@@ -135,8 +132,7 @@ public class BatchDriverTest extends FSBatchTestBase {
         driver.execute();
         assertEquals(3, driver.getNumRestarts());
         assertFalse(driver.isUserInterrupted());
-        assertContains("first test file",
-                readFileToString(outputDir.resolve("test6_ok.xml.xml"), UTF_8));
+        assertContains("first test file", readFileToString(outputDir.resolve("test6_ok.xml.xml"), UTF_8));
     }
 
     @Test
@@ -229,13 +225,11 @@ public class BatchDriverTest extends FSBatchTestBase {
         assertEquals(6, countChildren(outputDir));
         assertTrue(driver.getNumRestarts() > 1);
         for (int i = 0; i < 3; i++) {
-            assertEquals(0, Files.size(outputDir.resolve("test" + i + "_system_exit.xml.xml")),
-                    "problem with " + i);
+            assertEquals(0, Files.size(outputDir.resolve("test" + i + "_system_exit.xml.xml")), "problem with " + i);
         }
         //sys exit may prevent test3 from running successfully
         for (int i = 5; i < 6; i++) {
-            assertContains("first test file",
-                    readFileToString(outputDir.resolve("test" + i + "_ok.xml.xml"), UTF_8));
+            assertContains("first test file", readFileToString(outputDir.resolve("test" + i + "_ok.xml.xml"), UTF_8));
         }
     }
 
@@ -253,14 +247,11 @@ public class BatchDriverTest extends FSBatchTestBase {
         assertEquals(6, countChildren(outputDir));
 
         for (int i = 0; i < 3; i++) {
-            assertEquals(0,
-                    Files.size(outputDir.resolve("test" + i + "_thread_interrupt.xml.xml")),
-                    "problem with " + i);
+            assertEquals(0, Files.size(outputDir.resolve("test" + i + "_thread_interrupt.xml.xml")), "problem with " + i);
         }
         //sys exit may prevent test3 from running successfully
         for (int i = 5; i < 6; i++) {
-            assertContains("first test file",
-                    readFileToString(outputDir.resolve("test" + i + "_ok.xml.xml"), UTF_8));
+            assertContains("first test file", readFileToString(outputDir.resolve("test" + i + "_ok.xml.xml"), UTF_8));
         }
     }
 
diff --git a/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchProcessTest.java b/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchProcessTest.java
index d2cc83cfe..94b6da17e 100644
--- a/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchProcessTest.java
+++ b/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchProcessTest.java
@@ -56,8 +56,7 @@ public class BatchProcessTest extends FSBatchTestBase {
         Path hvyHang = outputDir.resolve("test0_heavy_hang.xml.xml");
         assertTrue(Files.exists(hvyHang));
         assertEquals(0, Files.size(hvyHang));
-        assertNotContained(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(),
-                streamStrings.getErrString());
+        assertNotContained(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(), streamStrings.getErrString());
     }
 
 
@@ -74,12 +73,9 @@ public class BatchProcessTest extends FSBatchTestBase {
         assertEquals(3, countChildren(outputDir));
         for (Path hvyHang : listPaths(outputDir)) {
             assertTrue(Files.exists(hvyHang));
-            assertEquals(0, Files.size(hvyHang),
-                    "file length for " + hvyHang.getFileName() + " should be 0, but is: " +
-                            Files.size(hvyHang));
+            assertEquals(0, Files.size(hvyHang), "file length for " + hvyHang.getFileName() + " should be 0, but is: " + Files.size(hvyHang));
         }
-        assertContains(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(),
-                streamStrings.getErrString());
+        assertContains(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(), streamStrings.getErrString());
     }
 
     @Test
@@ -97,13 +93,11 @@ public class BatchProcessTest extends FSBatchTestBase {
             assertTrue(Files.exists(hvyHang));
             assertEquals(0, Files.size(hvyHang));
         }
-        assertContains("This is tika-batch's first test file",
-                readFileToString(outputDir.resolve("test6_ok.xml.xml"), UTF_8));
+        assertContains("This is tika-batch's first test file", readFileToString(outputDir.resolve("test6_ok.xml.xml"), UTF_8));
 
         //key that the process realize that there were no more processable files
         //in the queue and does not ask for a restart!
-        assertNotContained(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(),
-                streamStrings.getErrString());
+        assertNotContained(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(), streamStrings.getErrString());
     }
 
     @Test
@@ -116,8 +110,7 @@ public class BatchProcessTest extends FSBatchTestBase {
         Map<String, String> args = getDefaultArgs("heavy_heavy_hangs", outputDir);
         args.put("numConsumers", "2");
         args.put("maxQueueSize", "2");
-        args.put("timeoutThresholdMillis",
-                "100000000");//make sure that the batch process doesn't time out
+        args.put("timeoutThresholdMillis", "100000000");//make sure that the batch process doesn't time out
         BatchProcessTestExecutor ex = new BatchProcessTestExecutor(args);
         StreamStrings streamStrings = ex.execute();
         assertEquals(2, countChildren(outputDir));
@@ -127,8 +120,7 @@ public class BatchProcessTest extends FSBatchTestBase {
             assertTrue(Files.exists(hvyHang));
             assertEquals(0, Files.size(hvyHang));
         }
-        assertContains(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(),
-                streamStrings.getErrString());
+        assertContains(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(), streamStrings.getErrString());
         assertContains("Crawler timed out", streamStrings.getErrString());
     }
 
@@ -151,11 +143,9 @@ public class BatchProcessTest extends FSBatchTestBase {
         StreamStrings streamStrings = ex.execute();
 
         assertEquals(4, countChildren(outputDir));
-        assertContains("This is tika-batch's first test file",
-                readFileToString(outputDir.resolve("test2_ok.xml.xml"), UTF_8));
+        assertContains("This is tika-batch's first test file", readFileToString(outputDir.resolve("test2_ok.xml.xml"), UTF_8));
 
-        assertContains(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(),
-                streamStrings.getErrString());
+        assertContains(BatchProcess.BATCH_CONSTANTS.BATCH_PROCESS_FATAL_MUST_RESTART.toString(), streamStrings.getErrString());
     }
 
 
@@ -175,10 +165,8 @@ public class BatchProcessTest extends FSBatchTestBase {
         assertTrue(Files.exists(test2), "test2_norestart.xml");
         Path test3 = outputDir.resolve("test3_ok.xml.xml");
         assertFalse(Files.exists(test3), "test3_ok.xml");
-        assertContains("exitStatus=" + BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE,
-                streamStrings.getOutString());
-        assertContains("causeForTermination='MAIN_LOOP_EXCEPTION_NO_RESTART'",
-                streamStrings.getOutString());
+        assertContains("exitStatus=" + BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE, streamStrings.getOutString());
+        assertContains("causeForTermination='MAIN_LOOP_EXCEPTION_NO_RESTART'", streamStrings.getOutString());
     }
 
     /**
@@ -203,13 +191,10 @@ public class BatchProcessTest extends FSBatchTestBase {
 
         StreamStrings streamStrings = ex.execute();
         assertEquals(1, countChildren(outputDir));
-        assertContains("<p>some content</p>",
-                readFileToString(outputDir.resolve("test0_sleep.xml.xml"), UTF_8));
+        assertContains("<p>some content</p>", readFileToString(outputDir.resolve("test0_sleep.xml.xml"), UTF_8));
 
-        assertContains("exitStatus=" + BatchProcessDriverCLI.PROCESS_RESTART_EXIT_CODE,
-                streamStrings.getOutString());
-        assertContains("causeForTermination='BATCH_PROCESS_ALIVE_TOO_LONG'",
-                streamStrings.getOutString());
+        assertContains("exitStatus=" + BatchProcessDriverCLI.PROCESS_RESTART_EXIT_CODE, streamStrings.getOutString());
+        assertContains("causeForTermination='BATCH_PROCESS_ALIVE_TOO_LONG'", streamStrings.getOutString());
     }
 
     @Test
@@ -228,10 +213,8 @@ public class BatchProcessTest extends FSBatchTestBase {
         List<Path> paths = listPaths(outputDir);
         assertEquals(1, paths.size());
         assertEquals(0, Files.size(paths.get(0)));
-        assertContains("exitStatus=" + BatchProcessDriverCLI.PROCESS_RESTART_EXIT_CODE,
-                streamStrings.getOutString());
-        assertContains("causeForTermination='BATCH_PROCESS_ALIVE_TOO_LONG'",
-                streamStrings.getOutString());
+        assertContains("exitStatus=" + BatchProcessDriverCLI.PROCESS_RESTART_EXIT_CODE, streamStrings.getOutString());
+        assertContains("causeForTermination='BATCH_PROCESS_ALIVE_TOO_LONG'", streamStrings.getOutString());
     }
 
     @Test
@@ -260,12 +243,10 @@ public class BatchProcessTest extends FSBatchTestBase {
         Map<String, String> args = getDefaultArgs("noisy_parsers", outputDir);
         args.put("numConsumers", "1");
         args.put("hangOnInit", "true");
-        BatchProcessTestExecutor ex =
-                new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml");
+        BatchProcessTestExecutor ex = new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml");
         StreamStrings streamStrings = ex.execute();
         assertEquals(BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE, ex.getExitValue());
-        assertContains("causeForTermination='CONSUMERS_MANAGER_DIDNT_INIT_IN_TIME_NO_RESTART'",
-                streamStrings.getOutString());
+        assertContains("causeForTermination='CONSUMERS_MANAGER_DIDNT_INIT_IN_TIME_NO_RESTART'", streamStrings.getOutString());
     }
 
     @Test
@@ -277,8 +258,7 @@ public class BatchProcessTest extends FSBatchTestBase {
         args.put("numConsumers", "1");
         args.put("hangOnShutdown", "true");
 
-        BatchProcessTestExecutor ex =
-                new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml");
+        BatchProcessTestExecutor ex = new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml");
         StreamStrings streamStrings = ex.execute();
         assertEquals(BatchProcessDriverCLI.PROCESS_NO_RESTART_EXIT_CODE, ex.getExitValue());
         assertContains("ConsumersManager did not shutdown within", streamStrings.getOutString());
@@ -293,11 +273,12 @@ public class BatchProcessTest extends FSBatchTestBase {
 
         Map<String, String> args = getDefaultArgs("hierarchical", outputDir);
         args.put("numConsumers", "1");
-        args.put("fileList", Paths.get(getResourceAsUri("/testFileList.txt")).toString());
+        args.put("fileList", Paths
+                .get(getResourceAsUri("/testFileList.txt"))
+                .toString());
         args.put("recursiveParserWrapper", "true");
         args.put("basicHandlerType", "text");
-        BatchProcessTestExecutor ex =
-                new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml");
+        BatchProcessTestExecutor ex = new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml");
         ex.execute();
         Path test1 = outputDir.resolve("test1.xml.json");
         Path test2 = outputDir.resolve("sub1a/test2.xml.json");
@@ -323,11 +304,11 @@ public class BatchProcessTest extends FSBatchTestBase {
         args.put("recursiveParserWrapper", "true");
         args.put("basicHandlerType", "text");
 
-        BatchProcessTestExecutor ex =
-                new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml",
-                        "/log4j2-on.properties");
+        BatchProcessTestExecutor ex = new BatchProcessTestExecutor(args, "/tika-batch-config-MockConsumersBuilder.xml", "/log4j2-on.properties");
         StreamStrings ss = ex.execute();
-        assertFalse(ss.getOutString().contains("error writing xml stream for"));
+        assertFalse(ss
+                .getOutString()
+                .contains("error writing xml stream for"));
         assertContains("parse_ex resourceId=\"test0_bad_chars.xml\"", ss.getOutString());
     }
 
@@ -340,14 +321,35 @@ public class BatchProcessTest extends FSBatchTestBase {
         args.put("recursiveParserWrapper", "true");
         args.put("basicHandlerType", "text");
 
-        BatchProcessTestExecutor ex =
-                new BatchProcessTestExecutor(args, "/tika-batch-config-test-suffix-override.xml",
-                        "/log4j2-on.properties");
+        BatchProcessTestExecutor ex = new BatchProcessTestExecutor(args, "/tika-batch-config-test-suffix-override.xml", "/log4j2-on.properties");
         ex.execute();
         Path targ = outputDir.resolve("test0.xml.mysuffix");
         assertTrue(Files.isRegularFile(targ));
     }
 
+    private static class StreamStrings {
+        private final String outString;
+        private final String errString;
+
+        private StreamStrings(String outString, String errString) {
+            this.outString = outString;
+            this.errString = errString;
+        }
+
+        private String getOutString() {
+            return outString;
+        }
+
+        private String getErrString() {
+            return errString;
+        }
+
+        @Override
+        public String toString() {
+            return "OUT>>" + outString + "<<\n" + "ERR>>" + errString + "<<\n";
+        }
+    }
+
     private class BatchProcessTestExecutor {
         private final Map<String, String> args;
         private final String configPath;
@@ -363,8 +365,7 @@ public class BatchProcessTest extends FSBatchTestBase {
             this(args, configPath, "/log4j2_process.properties");
         }
 
-        public BatchProcessTestExecutor(Map<String, String> args, String configPath,
-                                        String loggerProps) {
+        public BatchProcessTestExecutor(Map<String, String> args, String configPath, String loggerProps) {
             this.args = args;
             this.configPath = configPath;
             this.loggerProps = loggerProps;
@@ -407,27 +408,4 @@ public class BatchProcessTest extends FSBatchTestBase {
         }
 
     }
-
-    private static class StreamStrings {
-        private final String outString;
-        private final String errString;
-
-        private StreamStrings(String outString, String errString) {
-            this.outString = outString;
-            this.errString = errString;
-        }
-
-        private String getOutString() {
-            return outString;
-        }
-
-        private String getErrString() {
-            return errString;
-        }
-
-        @Override
-        public String toString() {
-            return "OUT>>" + outString + "<<\n" + "ERR>>" + errString + "<<\n";
-        }
-    }
 }
diff --git a/tika-batch/src/test/java/org/apache/tika/batch/fs/FSBatchTestBase.java b/tika-batch/src/test/java/org/apache/tika/batch/fs/FSBatchTestBase.java
index 933f66da2..86b807000 100644
--- a/tika-batch/src/test/java/org/apache/tika/batch/fs/FSBatchTestBase.java
+++ b/tika-batch/src/test/java/org/apache/tika/batch/fs/FSBatchTestBase.java
@@ -97,7 +97,9 @@ public abstract class FSBatchTestBase extends TikaTest {
         try (BufferedReader r = Files.newBufferedReader(p, cs)) {
             String line = r.readLine();
             while (line != null) {
-                sb.append(line).append("\n");
+                sb
+                        .append(line)
+                        .append("\n");
                 line = r.readLine();
             }
         }
@@ -149,17 +151,20 @@ public abstract class FSBatchTestBase extends TikaTest {
         return args;
     }
 
-    public String[] getDefaultCommandLineArgsArr(String inputSubDir, Path outputDir,
-                                                 Map<String, String> commandLine) throws Exception {
+    public String[] getDefaultCommandLineArgsArr(String inputSubDir, Path outputDir, Map<String, String> commandLine) throws Exception {
         List<String> args = new ArrayList<>();
         //need to include "-" because these are going to the commandline!
         if (inputSubDir != null) {
             args.add("-inputDir");
-            args.add(getInputRoot(inputSubDir).toAbsolutePath().toString());
+            args.add(getInputRoot(inputSubDir)
+                    .toAbsolutePath()
+                    .toString());
         }
         if (outputDir != null) {
             args.add("-outputDir");
-            args.add(outputDir.toAbsolutePath().toString());
+            args.add(outputDir
+                    .toAbsolutePath()
+                    .toString());
         }
         if (commandLine != null) {
             for (Map.Entry<String, String> e : commandLine.entrySet()) {
@@ -171,8 +176,7 @@ public abstract class FSBatchTestBase extends TikaTest {
     }
 
     public Path getInputRoot(String subdir) throws Exception {
-        String path =
-                (subdir == null || subdir.length() == 0) ? "/test-input" : "/test-input/" + subdir;
+        String path = (subdir == null || subdir.length() == 0) ? "/test-input" : "/test-input/" + subdir;
         return Paths.get(getResourceAsUri(path));
     }
 
@@ -185,8 +189,7 @@ public abstract class FSBatchTestBase extends TikaTest {
         return runner;
     }
 
-    public ProcessBuilder getNewBatchRunnerProcess(String testConfig, String loggerProps,
-                                                   Map<String, String> args) {
+    public ProcessBuilder getNewBatchRunnerProcess(String testConfig, String loggerProps, Map<String, String> args) {
         List<String> argList = new ArrayList<>();
 
         for (Map.Entry<String, String> e : args.entrySet()) {
@@ -194,8 +197,7 @@ public abstract class FSBatchTestBase extends TikaTest {
             argList.add(e.getValue());
         }
 
-        String[] fullCommandLine =
-                commandLine(testConfig, loggerProps, argList.toArray(new String[0]));
+        String[] fullCommandLine = commandLine(testConfig, loggerProps, argList.toArray(new String[0]));
         return new ProcessBuilder(fullCommandLine);
     }
 
@@ -214,7 +216,10 @@ public abstract class FSBatchTestBase extends TikaTest {
 
         String configFile = null;
         try {
-            configFile = Paths.get(getResourceAsUri(testConfig)).toAbsolutePath().toString();
+            configFile = Paths
+                    .get(getResourceAsUri(testConfig))
+                    .toAbsolutePath()
+                    .toString();
         } catch (URISyntaxException e) {
             e.printStackTrace();
         }
@@ -240,15 +245,17 @@ public abstract class FSBatchTestBase extends TikaTest {
         commandLine.add(cp);
         commandLine.add("org.apache.tika.batch.fs.FSBatchProcessCLI");
 
-        String configFile = Paths.get(getResourceAsUri(testConfig)).toAbsolutePath().toString();
+        String configFile = Paths
+                .get(getResourceAsUri(testConfig))
+                .toAbsolutePath()
+                .toString();
         commandLine.add("-bc");
 
         commandLine.add(configFile);
 
         commandLine.addAll(Arrays.asList(args));
 
-        BatchProcessDriverCLI driver =
-                new BatchProcessDriverCLI(commandLine.toArray(new String[0]));
+        BatchProcessDriverCLI driver = new BatchProcessDriverCLI(commandLine.toArray(new String[0]));
         driver.setRedirectForkedProcessToStdOut(false);
         return driver;
     }
diff --git a/tika-batch/src/test/java/org/apache/tika/batch/fs/FSFileResourceTest.java b/tika-batch/src/test/java/org/apache/tika/batch/fs/FSFileResourceTest.java
index 2bb795481..4df046202 100644
--- a/tika-batch/src/test/java/org/apache/tika/batch/fs/FSFileResourceTest.java
+++ b/tika-batch/src/test/java/org/apache/tika/batch/fs/FSFileResourceTest.java
@@ -29,8 +29,14 @@ public class FSFileResourceTest {
     @Test
     public void testRelativization() throws Exception {
         //test assertion error if alleged child is not actually child
-        Path root = Paths.get("root/abc/def").toAbsolutePath();
-        Path allegedChild = Paths.get(root.getParent().getParent().toAbsolutePath().toString());
+        Path root = Paths
+                .get("root/abc/def")
+                .toAbsolutePath();
+        Path allegedChild = Paths.get(root
+                .getParent()
+                .getParent()
+                .toAbsolutePath()
+                .toString());
         try {
             FSFileResource r = new FSFileResource(root, allegedChild);
             fail("should have had assertion error: alleged child not actually child of root");
diff --git a/tika-batch/src/test/java/org/apache/tika/batch/mock/MockConsumersBuilder.java b/tika-batch/src/test/java/org/apache/tika/batch/mock/MockConsumersBuilder.java
index 208d91d01..050d47cd8 100644
--- a/tika-batch/src/test/java/org/apache/tika/batch/mock/MockConsumersBuilder.java
+++ b/tika-batch/src/test/java/org/apache/tika/batch/mock/MockConsumersBuilder.java
@@ -28,8 +28,7 @@ import org.apache.tika.batch.fs.builders.BasicTikaFSConsumersBuilder;
 public class MockConsumersBuilder extends BasicTikaFSConsumersBuilder {
 
     @Override
-    public ConsumersManager build(Node node, Map<String, String> runtimeAttributes,
-                                  ArrayBlockingQueue<FileResource> queue) {
+    public ConsumersManager build(Node node, Map<String, String> runtimeAttributes, ArrayBlockingQueue<FileResource> queue) {
         ConsumersManager manager = super.build(node, runtimeAttributes, queue);
 
         boolean hangOnInit = runtimeAttributes.containsKey("hangOnInit");
diff --git a/tika-core/src/main/java/org/apache/tika/metadata/filter/CompositeMetadataFilter.java b/tika-core/src/main/java/org/apache/tika/metadata/filter/CompositeMetadataFilter.java
index 2c7d97661..95b12d520 100644
--- a/tika-core/src/main/java/org/apache/tika/metadata/filter/CompositeMetadataFilter.java
+++ b/tika-core/src/main/java/org/apache/tika/metadata/filter/CompositeMetadataFilter.java
@@ -35,4 +35,9 @@ public class CompositeMetadataFilter extends MetadataFilter {
             filter.filter(metadata);
         }
     }
+
+    @Override
+    public String toString() {
+        return "CompositeMetadataFilter{" + "filters=" + filters + '}';
+    }
 }
diff --git a/tika-core/src/main/java/org/apache/tika/metadata/filter/FieldNameMappingFilter.java b/tika-core/src/main/java/org/apache/tika/metadata/filter/FieldNameMappingFilter.java
index db16f5dff..00c60d8f5 100644
--- a/tika-core/src/main/java/org/apache/tika/metadata/filter/FieldNameMappingFilter.java
+++ b/tika-core/src/main/java/org/apache/tika/metadata/filter/FieldNameMappingFilter.java
@@ -75,4 +75,9 @@ public class FieldNameMappingFilter extends MetadataFilter {
             this.mappings.put(e.getKey(), e.getValue());
         }
     }
+
+    @Override
+    public String toString() {
+        return "FieldNameMappingFilter{" + "mappings=" + mappings + ", excludeUnmapped=" + excludeUnmapped + '}';
+    }
 }
diff --git a/tika-core/src/main/java/org/apache/tika/parser/ParseContext.java b/tika-core/src/main/java/org/apache/tika/parser/ParseContext.java
index 691bf7f9f..25256a77f 100644
--- a/tika-core/src/main/java/org/apache/tika/parser/ParseContext.java
+++ b/tika-core/src/main/java/org/apache/tika/parser/ParseContext.java
@@ -17,8 +17,10 @@
 package org.apache.tika.parser;
 
 import java.io.Serializable;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Set;
 
 /**
  * Parse context. Used to pass context information to Tika parsers.
@@ -82,4 +84,33 @@ public class ParseContext implements Serializable {
             return defaultValue;
         }
     }
+
+    public boolean isEmpty() {
+        return context.size() == 0;
+    }
+
+    //this should really only be used for serialization
+    public Set<String> keySet() {
+        return Collections
+                .unmodifiableSet(context.keySet());
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) {
+            return true;
+        }
+        if (o == null || getClass() != o.getClass()) {
+            return false;
+        }
+
+        ParseContext that = (ParseContext) o;
+        return context.equals(that.context);
+    }
+
+    @Override
+    public int hashCode() {
+        return context.hashCode();
+    }
+
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/FetchEmitTuple.java b/tika-core/src/main/java/org/apache/tika/pipes/FetchEmitTuple.java
index 0c0334fd4..a0f40901b 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/FetchEmitTuple.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/FetchEmitTuple.java
@@ -20,6 +20,7 @@ import java.io.Serializable;
 import java.util.Objects;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
 import org.apache.tika.pipes.fetcher.FetchKey;
@@ -36,41 +37,30 @@ public class FetchEmitTuple implements Serializable {
     private final FetchKey fetchKey;
     private EmitKey emitKey;
     private final Metadata metadata;
+    private final ParseContext parseContext;
     private final ON_PARSE_EXCEPTION onParseException;
-    private HandlerConfig handlerConfig;
 
     private EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig;
 
     public FetchEmitTuple(String id, FetchKey fetchKey, EmitKey emitKey) {
-        this(id, fetchKey, emitKey, new Metadata(), HandlerConfig.DEFAULT_HANDLER_CONFIG,
-                DEFAULT_ON_PARSE_EXCEPTION);
+        this(id, fetchKey, emitKey, new Metadata());
     }
-    public FetchEmitTuple(String id, FetchKey fetchKey, EmitKey emitKey, ON_PARSE_EXCEPTION onParseException) {
-        this(id, fetchKey, emitKey, new Metadata(), HandlerConfig.DEFAULT_HANDLER_CONFIG,
-                onParseException);
-    }
-
     public FetchEmitTuple(String id, FetchKey fetchKey, EmitKey emitKey, Metadata metadata) {
-        this(id, fetchKey, emitKey, metadata, HandlerConfig.DEFAULT_HANDLER_CONFIG,
-                DEFAULT_ON_PARSE_EXCEPTION);
+        this(id, fetchKey, emitKey, metadata, new ParseContext());
     }
 
-    public FetchEmitTuple(String id, FetchKey fetchKey, EmitKey emitKey, Metadata metadata,
-                          HandlerConfig handlerConfig, ON_PARSE_EXCEPTION onParseException) {
-        this(id, fetchKey, emitKey, metadata, handlerConfig, onParseException,
-                EmbeddedDocumentBytesConfig.SKIP);
+    public FetchEmitTuple(String id, FetchKey fetchKey, EmitKey emitKey, Metadata metadata, ParseContext parseContext) {
+        this(id, fetchKey, emitKey, metadata, parseContext, ON_PARSE_EXCEPTION.EMIT);
     }
 
-    public FetchEmitTuple(String id, FetchKey fetchKey, EmitKey emitKey, Metadata metadata,
-                          HandlerConfig handlerConfig, ON_PARSE_EXCEPTION onParseException,
-                          EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig) {
+    public FetchEmitTuple(String id, FetchKey fetchKey, EmitKey emitKey, Metadata metadata, ParseContext parseContext,
+                          ON_PARSE_EXCEPTION onParseException) {
         this.id = id;
         this.fetchKey = fetchKey;
         this.emitKey = emitKey;
         this.metadata = metadata;
-        this.handlerConfig = handlerConfig;
+        this.parseContext = parseContext;
         this.onParseException = onParseException;
-        this.embeddedDocumentBytesConfig = embeddedDocumentBytesConfig;
     }
 
     public String getId() {
@@ -88,24 +78,15 @@ public class FetchEmitTuple implements Serializable {
         return metadata;
     }
 
-    public ON_PARSE_EXCEPTION getOnParseException() {
-        return onParseException;
+    public ParseContext getParseContext() {
+        return parseContext;
     }
-
     public void setEmitKey(EmitKey emitKey) {
         this.emitKey = emitKey;
     }
 
-    public void setHandlerConfig(HandlerConfig handlerConfig) {
-        this.handlerConfig = handlerConfig;
-    }
-
-    public HandlerConfig getHandlerConfig() {
-        return handlerConfig == null ? HandlerConfig.DEFAULT_HANDLER_CONFIG : handlerConfig;
-    }
-
-    public EmbeddedDocumentBytesConfig getEmbeddedDocumentBytesConfig() {
-        return embeddedDocumentBytesConfig;
+    public ON_PARSE_EXCEPTION getOnParseException() {
+        return onParseException;
     }
 
     @Override
@@ -118,46 +99,28 @@ public class FetchEmitTuple implements Serializable {
         }
 
         FetchEmitTuple that = (FetchEmitTuple) o;
-
-        if (!Objects.equals(id, that.id)) {
-            return false;
-        }
-        if (!Objects.equals(fetchKey, that.fetchKey)) {
-            return false;
-        }
-        if (!Objects.equals(emitKey, that.emitKey)) {
-            return false;
-        }
-        if (!Objects.equals(metadata, that.metadata)) {
-            return false;
-        }
-        if (onParseException != that.onParseException) {
-            return false;
-        }
-        if (!Objects.equals(handlerConfig, that.handlerConfig)) {
-            return false;
-        }
-        return Objects.equals(embeddedDocumentBytesConfig, that.embeddedDocumentBytesConfig);
+        return Objects.equals(id, that.id) && Objects.equals(fetchKey, that.fetchKey) && Objects.equals(emitKey, that.emitKey)
+                && Objects.equals(metadata, that.metadata) &&
+                Objects.equals(parseContext, that.parseContext) && onParseException == that.onParseException &&
+                Objects.equals(embeddedDocumentBytesConfig, that.embeddedDocumentBytesConfig);
     }
 
     @Override
     public int hashCode() {
-        int result = id != null ? id.hashCode() : 0;
-        result = 31 * result + (fetchKey != null ? fetchKey.hashCode() : 0);
-        result = 31 * result + (emitKey != null ? emitKey.hashCode() : 0);
-        result = 31 * result + (metadata != null ? metadata.hashCode() : 0);
-        result = 31 * result + (onParseException != null ? onParseException.hashCode() : 0);
-        result = 31 * result + (handlerConfig != null ? handlerConfig.hashCode() : 0);
-        result = 31 * result +
-                (embeddedDocumentBytesConfig != null ? embeddedDocumentBytesConfig.hashCode() : 0);
+        int result = Objects.hashCode(id);
+        result = 31 * result + Objects.hashCode(fetchKey);
+        result = 31 * result + Objects.hashCode(emitKey);
+        result = 31 * result + Objects.hashCode(metadata);
+        result = 31 * result + Objects.hashCode(parseContext);
+        result = 31 * result + Objects.hashCode(onParseException);
+        result = 31 * result + Objects.hashCode(embeddedDocumentBytesConfig);
         return result;
     }
 
     @Override
     public String toString() {
-        return "FetchEmitTuple{" + "id='" + id + '\'' + ", fetchKey=" + fetchKey + ", emitKey=" +
-                emitKey + ", metadata=" + metadata + ", onParseException=" + onParseException +
-                ", handlerConfig=" + handlerConfig + ", embeddedDocumentBytesConfig=" +
-                embeddedDocumentBytesConfig + '}';
+        return "FetchEmitTuple{" + "id='" + id + '\'' + ", fetchKey=" + fetchKey + ", emitKey=" + emitKey +
+                ", metadata=" + metadata + ", parseContext=" + parseContext +
+                ", onParseException=" + onParseException + ", embeddedDocumentBytesConfig=" + embeddedDocumentBytesConfig + '}';
     }
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/HandlerConfig.java b/tika-core/src/main/java/org/apache/tika/pipes/HandlerConfig.java
index d128dcb3d..8cb3edfa8 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/HandlerConfig.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/HandlerConfig.java
@@ -78,6 +78,10 @@ public class HandlerConfig implements Serializable {
     PARSE_MODE parseMode = PARSE_MODE.RMETA;
 
 
+    public HandlerConfig() {
+
+    }
+
     public HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE type, PARSE_MODE parseMode,
                          int writeLimit,
                          int maxEmbeddedResources, boolean throwOnWriteLimitReached) {
@@ -92,22 +96,56 @@ public class HandlerConfig implements Serializable {
         return type;
     }
 
+    public void setType(BasicContentHandlerFactory.HANDLER_TYPE type) {
+        this.type = type;
+    }
+
+    public void setType(String typeString) {
+        setType(BasicContentHandlerFactory.HANDLER_TYPE.valueOf(typeString));
+    }
+
     public int getWriteLimit() {
         return writeLimit;
     }
 
+    public void setWriteLimit(int writeLimit) {
+        this.writeLimit = writeLimit;
+    }
+
     public int getMaxEmbeddedResources() {
         return maxEmbeddedResources;
     }
 
-    public PARSE_MODE getParseMode() {
-        return parseMode;
+    public void setMaxEmbeddedResources(int maxEmbeddedResources) {
+        this.maxEmbeddedResources = maxEmbeddedResources;
     }
 
     public boolean isThrowOnWriteLimitReached() {
         return throwOnWriteLimitReached;
     }
 
+    public void setThrowOnWriteLimitReached(boolean throwOnWriteLimitReached) {
+        this.throwOnWriteLimitReached = throwOnWriteLimitReached;
+    }
+
+    public PARSE_MODE getParseMode() {
+        return parseMode;
+    }
+
+    public void setParseMode(PARSE_MODE parseMode) {
+        this.parseMode = parseMode;
+    }
+
+    public void setParseMode(String parseMode) {
+        this.parseMode = PARSE_MODE.parseMode(parseMode);
+    }
+
+    @Override
+    public String toString() {
+        return "HandlerConfig{" + "type=" + type + ", writeLimit=" + writeLimit + ", maxEmbeddedResources=" + maxEmbeddedResources +
+                ", throwOnWriteLimitReached=" + throwOnWriteLimitReached + ", parseMode=" + parseMode + '}';
+    }
+
     @Override
     public boolean equals(Object o) {
         if (this == o) {
@@ -116,22 +154,19 @@ public class HandlerConfig implements Serializable {
         if (o == null || getClass() != o.getClass()) {
             return false;
         }
+
         HandlerConfig that = (HandlerConfig) o;
-        return writeLimit == that.writeLimit && maxEmbeddedResources == that.maxEmbeddedResources &&
-                throwOnWriteLimitReached == that.throwOnWriteLimitReached && type == that.type &&
-                parseMode == that.parseMode;
+        return writeLimit == that.writeLimit && maxEmbeddedResources == that.maxEmbeddedResources && throwOnWriteLimitReached == that.throwOnWriteLimitReached &&
+                type == that.type && parseMode == that.parseMode;
     }
 
     @Override
     public int hashCode() {
-        return Objects.hash(type, writeLimit, maxEmbeddedResources, throwOnWriteLimitReached,
-                parseMode);
-    }
-
-    @Override
-    public String toString() {
-        return "HandlerConfig{" + "type=" + type + ", writeLimit=" + writeLimit +
-                ", maxEmbeddedResources=" + maxEmbeddedResources + ", throwOnWriteLimitReached=" +
-                throwOnWriteLimitReached + ", parseMode=" + parseMode + '}';
+        int result = Objects.hashCode(type);
+        result = 31 * result + writeLimit;
+        result = 31 * result + maxEmbeddedResources;
+        result = 31 * result + Boolean.hashCode(throwOnWriteLimitReached);
+        result = 31 * result + Objects.hashCode(parseMode);
+        return result;
     }
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/PipesClient.java b/tika-core/src/main/java/org/apache/tika/pipes/PipesClient.java
index 52e72df85..129067136 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/PipesClient.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/PipesClient.java
@@ -48,6 +48,7 @@ import org.slf4j.LoggerFactory;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.EmitData;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.utils.ProcessUtils;
@@ -325,7 +326,7 @@ public class PipesClient implements Closeable {
             case INTERMEDIATE_RESULT:
                 LOG.debug("pipesClientId={} intermediate success: {} in {} ms", pipesClientId,
                         t.getId(), millis);
-                return deserializeIntermediateResult(t.getEmitKey());
+                return deserializeIntermediateResult(t.getEmitKey(), t.getParseContext());
             case PARSE_SUCCESS:
                 //there may have been a parse exception, but the parse didn't crash
                 LOG.debug("pipesClientId={} parse success: {} in {} ms", pipesClientId, t.getId(),
@@ -383,7 +384,7 @@ public class PipesClient implements Closeable {
         }
     }
 
-    private PipesResult deserializeIntermediateResult(EmitKey emitKey) throws IOException {
+    private PipesResult deserializeIntermediateResult(EmitKey emitKey, ParseContext parseContext) throws IOException {
 
         int length = input.readInt();
         byte[] bytes = new byte[length];
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/PipesServer.java b/tika-core/src/main/java/org/apache/tika/pipes/PipesServer.java
index 991694f88..95bb0e9ed 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/PipesServer.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/PipesServer.java
@@ -69,11 +69,10 @@ import org.apache.tika.pipes.emitter.Emitter;
 import org.apache.tika.pipes.emitter.EmitterManager;
 import org.apache.tika.pipes.emitter.StreamEmitter;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
+import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
 import org.apache.tika.pipes.extractor.EmittingEmbeddedDocumentBytesHandler;
-import org.apache.tika.pipes.fetcher.FetchKey;
 import org.apache.tika.pipes.fetcher.Fetcher;
 import org.apache.tika.pipes.fetcher.FetcherManager;
-import org.apache.tika.pipes.fetcher.RangeFetcher;
 import org.apache.tika.sax.BasicContentHandlerFactory;
 import org.apache.tika.sax.ContentHandlerFactory;
 import org.apache.tika.sax.RecursiveParserWrapperHandler;
@@ -281,7 +280,7 @@ public class PipesServer implements Runnable {
 
     private void emit(String taskId, EmitKey emitKey,
                       boolean isExtractEmbeddedBytes, MetadataListAndEmbeddedBytes parseData,
-                      String parseExceptionStack) {
+                      String parseExceptionStack, ParseContext parseContext) {
         Emitter emitter = null;
 
         try {
@@ -297,7 +296,7 @@ public class PipesServer implements Runnable {
                     parseData.toBePackagedForStreamEmitter()) {
                 emitContentsAndBytes(emitter, emitKey, parseData);
             } else {
-                emitter.emit(emitKey.getEmitKey(), parseData.getMetadataList());
+                emitter.emit(emitKey.getEmitKey(), parseData.getMetadataList(), parseContext);
             }
         } catch (IOException | TikaEmitterException e) {
             LOG.warn("emit exception", e);
@@ -401,8 +400,11 @@ public class PipesServer implements Runnable {
         String stack = getContainerStacktrace(t, parseData.getMetadataList());
         //we need to apply this after we pull out the stacktrace
         filterMetadata(parseData.getMetadataList());
+        ParseContext parseContext = t.getParseContext();
+        FetchEmitTuple.ON_PARSE_EXCEPTION onParseException = t.getOnParseException();
+        EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig = parseContext.get(EmbeddedDocumentBytesConfig.class);
         if (StringUtils.isBlank(stack) ||
-                t.getOnParseException() == FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT) {
+                onParseException == FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT) {
             injectUserMetadata(t.getMetadata(), parseData.getMetadataList());
             EmitKey emitKey = t.getEmitKey();
             if (StringUtils.isBlank(emitKey.getEmitKey())) {
@@ -410,14 +412,14 @@ public class PipesServer implements Runnable {
                 t.setEmitKey(emitKey);
             }
             EmitData emitData = new EmitData(t.getEmitKey(), parseData.getMetadataList(), stack);
-            if (t.getEmbeddedDocumentBytesConfig().isExtractEmbeddedDocumentBytes() &&
+            if (embeddedDocumentBytesConfig.isExtractEmbeddedDocumentBytes() &&
                     parseData.toBePackagedForStreamEmitter()) {
-                emit(t.getId(), emitKey, t.getEmbeddedDocumentBytesConfig().isExtractEmbeddedDocumentBytes(),
-                        parseData, stack);
+                emit(t.getId(), emitKey, embeddedDocumentBytesConfig.isExtractEmbeddedDocumentBytes(),
+                        parseData, stack, parseContext);
             } else if (maxForEmitBatchBytes >= 0 &&
                     emitData.getEstimatedSizeBytes() >= maxForEmitBatchBytes) {
-                emit(t.getId(), emitKey, t.getEmbeddedDocumentBytesConfig().isExtractEmbeddedDocumentBytes(),
-                        parseData, stack);
+                emit(t.getId(), emitKey, embeddedDocumentBytesConfig.isExtractEmbeddedDocumentBytes(),
+                        parseData, stack, parseContext);
             } else {
                 //send back to the client
                 write(emitData);
@@ -456,35 +458,18 @@ public class PipesServer implements Runnable {
     }
 
     protected MetadataListAndEmbeddedBytes parseFromTuple(FetchEmitTuple t, Fetcher fetcher) {
-        FetchKey fetchKey = t.getFetchKey();
-        if (fetchKey.hasRange()) {
-            if (!(fetcher instanceof RangeFetcher)) {
-                throw new IllegalArgumentException(
-                        "fetch key has a range, but the fetcher is not a range fetcher");
-            }
-            Metadata metadata = new Metadata();
-            try (InputStream stream = ((RangeFetcher) fetcher).fetch(fetchKey.getFetchKey(),
-                    fetchKey.getRangeStart(), fetchKey.getRangeEnd(), metadata)) {
-                return parseWithStream(t, stream, metadata);
-            } catch (SecurityException e) {
-                LOG.error("security exception " + t.getId(), e);
-                throw e;
-            } catch (TikaException | IOException e) {
-                LOG.warn("fetch exception " + t.getId(), e);
-                write(STATUS.FETCH_EXCEPTION, ExceptionUtils.getStackTrace(e));
-            }
-        } else {
-            Metadata metadata = new Metadata();
-            try (InputStream stream = fetcher.fetch(t.getFetchKey().getFetchKey(), metadata)) {
-                return parseWithStream(t, stream, metadata);
-            } catch (SecurityException e) {
-                LOG.error("security exception " + t.getId(), e);
-                throw e;
-            } catch (TikaException | IOException e) {
-                LOG.warn("fetch exception " + t.getId(), e);
-                write(STATUS.FETCH_EXCEPTION, ExceptionUtils.getStackTrace(e));
-            }
+
+        Metadata metadata = new Metadata();
+        try (InputStream stream = fetcher.fetch(t.getFetchKey().getFetchKey(), metadata, t.getParseContext())) {
+            return parseWithStream(t, stream, metadata);
+        } catch (SecurityException e) {
+            LOG.error("security exception " + t.getId(), e);
+            throw e;
+        } catch (TikaException | IOException e) {
+            LOG.warn("fetch exception " + t.getId(), e);
+            write(STATUS.FETCH_EXCEPTION, ExceptionUtils.getStackTrace(e));
         }
+
         return null;
     }
 
@@ -528,10 +513,11 @@ public class PipesServer implements Runnable {
     private MetadataListAndEmbeddedBytes parseWithStream(FetchEmitTuple fetchEmitTuple,
                                                          InputStream stream, Metadata metadata)
             throws TikaConfigException {
-        HandlerConfig handlerConfig = fetchEmitTuple.getHandlerConfig();
+
         List<Metadata> metadataList;
         //this adds the EmbeddedDocumentByteStore to the parsecontext
-        ParseContext parseContext = createParseContext(fetchEmitTuple);
+        ParseContext parseContext = setupParseContext(fetchEmitTuple);
+        HandlerConfig handlerConfig = parseContext.get(HandlerConfig.class);
         if (handlerConfig.getParseMode() == HandlerConfig.PARSE_MODE.RMETA) {
             metadataList =
                     parseRecursive(fetchEmitTuple, handlerConfig, stream, metadata, parseContext);
@@ -544,10 +530,16 @@ public class PipesServer implements Runnable {
                 parseContext.get(EmbeddedDocumentBytesHandler.class));
     }
 
-    private ParseContext createParseContext(FetchEmitTuple fetchEmitTuple)
+    private ParseContext setupParseContext(FetchEmitTuple fetchEmitTuple)
             throws TikaConfigException {
-        ParseContext parseContext = new ParseContext();
-        if (! fetchEmitTuple.getEmbeddedDocumentBytesConfig().isExtractEmbeddedDocumentBytes()) {
+        ParseContext parseContext = fetchEmitTuple.getParseContext();
+        if (parseContext.get(HandlerConfig.class) == null) {
+            parseContext.set(HandlerConfig.class, HandlerConfig.DEFAULT_HANDLER_CONFIG);
+        }
+        EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig = parseContext.get(EmbeddedDocumentBytesConfig.class);
+        if (embeddedDocumentBytesConfig == null) {
+            //make sure there's one here -- or do we make this default in fetchemit tuple?
+            parseContext.set(EmbeddedDocumentBytesConfig.class, EmbeddedDocumentBytesConfig.SKIP);
             return parseContext;
         }
         EmbeddedDocumentExtractorFactory factory = ((AutoDetectParser)autoDetectParser)
@@ -561,18 +553,17 @@ public class PipesServer implements Runnable {
                         "instance of EmbeddedDocumentByteStoreExtractorFactory if you want" +
                         "to extract embedded bytes! I see this embedded doc factory: " +
                         factory.getClass() + "and a request: " +
-                        fetchEmitTuple.getEmbeddedDocumentBytesConfig());
+                        embeddedDocumentBytesConfig);
             }
         }
         //TODO: especially clean this up.
-        if (!StringUtils.isBlank(fetchEmitTuple.getEmbeddedDocumentBytesConfig().getEmitter())) {
+        if (!StringUtils.isBlank(embeddedDocumentBytesConfig.getEmitter())) {
             parseContext.set(EmbeddedDocumentBytesHandler.class,
-                    new EmittingEmbeddedDocumentBytesHandler(fetchEmitTuple.getEmitKey(),
-                            fetchEmitTuple.getEmbeddedDocumentBytesConfig(), emitterManager));
+                    new EmittingEmbeddedDocumentBytesHandler(fetchEmitTuple, emitterManager));
         } else {
             parseContext.set(EmbeddedDocumentBytesHandler.class,
                     new BasicEmbeddedDocumentBytesHandler(
-                    fetchEmitTuple.getEmbeddedDocumentBytesConfig()));
+                    embeddedDocumentBytesConfig));
         }
         return parseContext;
     }
@@ -693,11 +684,10 @@ public class PipesServer implements Runnable {
         } catch (IOException e) {
             LOG.warn("problem detecting: " + t.getId(), e);
         }
-
-        if (t.getEmbeddedDocumentBytesConfig() != null &&
-                t.getEmbeddedDocumentBytesConfig().isIncludeOriginal()) {
-            EmbeddedDocumentBytesHandler embeddedDocumentByteStore =
-                    parseContext.get(EmbeddedDocumentBytesHandler.class);
+        EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig = parseContext.get(EmbeddedDocumentBytesConfig.class);
+        if (embeddedDocumentBytesConfig != null &&
+                embeddedDocumentBytesConfig.isIncludeOriginal()) {
+            EmbeddedDocumentBytesHandler embeddedDocumentByteStore = parseContext.get(EmbeddedDocumentBytesHandler.class);
             try (InputStream is = Files.newInputStream(tis.getPath())) {
                 embeddedDocumentByteStore.add(0, metadata, is);
             } catch (IOException e) {
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/async/AsyncEmitter.java b/tika-core/src/main/java/org/apache/tika/pipes/async/AsyncEmitter.java
index fce65c540..e0fe605f3 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/async/AsyncEmitter.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/async/AsyncEmitter.java
@@ -42,7 +42,7 @@ import org.apache.tika.utils.ExceptionUtils;
  */
 public class AsyncEmitter implements Callable<Integer> {
 
-    static final EmitData EMIT_DATA_STOP_SEMAPHORE = new EmitData(null, null);
+    static final EmitData EMIT_DATA_STOP_SEMAPHORE = new EmitData(null, null, null);
     static final int EMITTER_FUTURE_CODE = 2;
 
     private static final Logger LOG = LoggerFactory.getLogger(AsyncEmitter.class);
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/emitter/AbstractEmitter.java b/tika-core/src/main/java/org/apache/tika/pipes/emitter/AbstractEmitter.java
index 648e0949d..4629f9263 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/emitter/AbstractEmitter.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/emitter/AbstractEmitter.java
@@ -19,6 +19,8 @@ package org.apache.tika.pipes.emitter;
 import java.io.IOException;
 import java.util.List;
 
+import org.apache.tika.parser.ParseContext;
+
 public abstract class AbstractEmitter implements Emitter {
 
     private String name;
@@ -33,7 +35,7 @@ public abstract class AbstractEmitter implements Emitter {
     }
 
     /**
-     * The default behavior is to call {@link #emit(String, List)} on each item.
+     * The default behavior is to call {@link #emit(String, List, ParseContext)} on each item.
      * Some implementations, e.g. Solr/ES/vespa, can benefit from subclassing this and
      * emitting a bunch of docs at once.
      *
@@ -44,7 +46,7 @@ public abstract class AbstractEmitter implements Emitter {
     @Override
     public void emit(List<? extends EmitData> emitData) throws IOException, TikaEmitterException {
         for (EmitData d : emitData) {
-            emit(d.getEmitKey().getEmitKey(), d.getMetadataList());
+            emit(d.getEmitKey().getEmitKey(), d.getMetadataList(), d.getParseContext());
         }
     }
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitData.java b/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitData.java
index 95376a9fa..09d448adf 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitData.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitData.java
@@ -20,6 +20,7 @@ import java.io.Serializable;
 import java.util.List;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.utils.StringUtils;
 
 public class EmitData implements Serializable {
@@ -30,18 +31,23 @@ public class EmitData implements Serializable {
 
     private final EmitKey emitKey;
     private final List<Metadata> metadataList;
-
     private final String containerStackTrace;
+    private ParseContext parseContext = null;
 
     public EmitData(EmitKey emitKey, List<Metadata> metadataList) {
         this(emitKey, metadataList, StringUtils.EMPTY);
     }
 
     public EmitData(EmitKey emitKey, List<Metadata> metadataList, String containerStackTrace) {
+        this(emitKey, metadataList, containerStackTrace, new ParseContext());
+    }
+
+    public EmitData(EmitKey emitKey, List<Metadata> metadataList, String containerStackTrace, ParseContext parseContext) {
         this.emitKey = emitKey;
         this.metadataList = metadataList;
         this.containerStackTrace = (containerStackTrace == null) ? StringUtils.EMPTY :
                 containerStackTrace;
+        this.parseContext = parseContext;
     }
 
     public EmitKey getEmitKey() {
@@ -60,6 +66,14 @@ public class EmitData implements Serializable {
         return estimateSizeInBytes(getEmitKey().getEmitKey(), getMetadataList(), containerStackTrace);
     }
 
+    public void setParseContext(ParseContext parseContext) {
+        this.parseContext = parseContext;
+    }
+
+    public ParseContext getParseContext() {
+        return parseContext;
+    }
+
     private static long estimateSizeInBytes(String id, List<Metadata> metadataList,
                                             String containerStackTrace) {
         long sz = 36 + id.length() * 2;
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitKey.java b/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitKey.java
index e57006480..0f62ee06a 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitKey.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmitKey.java
@@ -21,6 +21,8 @@ import java.util.Objects;
 
 public class EmitKey implements Serializable {
 
+    public static EmitKey NO_EMIT = new EmitKey(null, null);
+
     /**
      * Serial version UID
      */
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/emitter/Emitter.java b/tika-core/src/main/java/org/apache/tika/pipes/emitter/Emitter.java
index f60ef3b77..c748541af 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/emitter/Emitter.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/emitter/Emitter.java
@@ -20,12 +20,13 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 public interface Emitter {
 
     String getName();
 
-    void emit(String emitKey, List<Metadata> metadataList) throws IOException, TikaEmitterException;
+    void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext) throws IOException, TikaEmitterException;
 
     void emit(List<? extends EmitData> emitData) throws IOException, TikaEmitterException;
     //TODO -- add this later for xhtml?
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmptyEmitter.java b/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmptyEmitter.java
index b77107ba0..ef7adbef5 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmptyEmitter.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/emitter/EmptyEmitter.java
@@ -20,6 +20,7 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 public class EmptyEmitter implements Emitter {
 
@@ -29,7 +30,7 @@ public class EmptyEmitter implements Emitter {
     }
 
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext)
             throws IOException, TikaEmitterException {
 
     }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/emitter/StreamEmitter.java b/tika-core/src/main/java/org/apache/tika/pipes/emitter/StreamEmitter.java
index 10526eb0e..30249b877 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/emitter/StreamEmitter.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/emitter/StreamEmitter.java
@@ -20,8 +20,9 @@ import java.io.IOException;
 import java.io.InputStream;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 public interface StreamEmitter extends Emitter {
-    void emit(String emitKey, InputStream inputStream, Metadata userMetadata)
+    void emit(String emitKey, InputStream inputStream, Metadata userMetadata, ParseContext parseContext)
             throws IOException, TikaEmitterException;
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmbeddedDocumentBytesConfig.java b/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmbeddedDocumentBytesConfig.java
index 071de05c4..c667fd766 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmbeddedDocumentBytesConfig.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmbeddedDocumentBytesConfig.java
@@ -96,13 +96,16 @@ public class EmbeddedDocumentBytesConfig implements Serializable {
         return includeOriginal;
     }
 
-    public void setZeroPadNameLength(int zeroPadName) {
+    public void setZeroPadName(int zeroPadName) {
         this.zeroPadName = zeroPadName;
     }
 
     public void setSuffixStrategy(SUFFIX_STRATEGY suffixStrategy) {
         this.suffixStrategy = suffixStrategy;
     }
+    public void setSuffixStrategy(String suffixStrategy) {
+        setSuffixStrategy(SUFFIX_STRATEGY.valueOf(suffixStrategy));
+    }
 
     public void setEmbeddedIdPrefix(String embeddedIdPrefix) {
         this.embeddedIdPrefix = embeddedIdPrefix;
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmittingEmbeddedDocumentBytesHandler.java b/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmittingEmbeddedDocumentBytesHandler.java
index 1132a4bc6..9c73578f0 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmittingEmbeddedDocumentBytesHandler.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/extractor/EmittingEmbeddedDocumentBytesHandler.java
@@ -25,6 +25,8 @@ import org.apache.commons.io.IOExceptionWithCause;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.extractor.AbstractEmbeddedDocumentBytesHandler;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
+import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.emitter.Emitter;
 import org.apache.tika.pipes.emitter.EmitterManager;
@@ -37,11 +39,16 @@ public class EmittingEmbeddedDocumentBytesHandler extends AbstractEmbeddedDocume
     private final StreamEmitter emitter;
 
     private static final Metadata METADATA = new Metadata();
-    public EmittingEmbeddedDocumentBytesHandler(EmitKey containerEmitKey,
-                                                EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig,
+    private static final ParseContext PARSE_CONTEXT = new ParseContext();
+
+    public EmittingEmbeddedDocumentBytesHandler(FetchEmitTuple fetchEmitTuple,
                                                 EmitterManager emitterManager) throws TikaConfigException {
-        this.containerEmitKey = containerEmitKey;
-        this.embeddedDocumentBytesConfig = embeddedDocumentBytesConfig;
+
+        this.containerEmitKey = fetchEmitTuple.getEmitKey();
+        this.embeddedDocumentBytesConfig = fetchEmitTuple.getParseContext().get(EmbeddedDocumentBytesConfig.class);
+        if (this.embeddedDocumentBytesConfig == null) {
+            throw new TikaConfigException("EmbeddedDocumentBytesConfig must not be null!");
+        }
         Emitter tmpEmitter =
                 emitterManager.getEmitter(embeddedDocumentBytesConfig.getEmitter());
         if (! (tmpEmitter instanceof StreamEmitter)) {
@@ -58,7 +65,7 @@ public class EmittingEmbeddedDocumentBytesHandler extends AbstractEmbeddedDocume
         String emitKey = getEmitKey(containerEmitKey.getEmitKey(),
                 id, embeddedDocumentBytesConfig, metadata);
         try {
-            emitter.emit(emitKey, inputStream, METADATA);
+            emitter.emit(emitKey, inputStream, METADATA, PARSE_CONTEXT);
         } catch (TikaEmitterException e) {
             throw new IOExceptionWithCause(e);
         }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/EmptyFetcher.java b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/EmptyFetcher.java
index 022d00a8c..d64f81524 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/EmptyFetcher.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/EmptyFetcher.java
@@ -21,6 +21,7 @@ import java.io.InputStream;
 
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 public class EmptyFetcher implements Fetcher {
 
@@ -30,7 +31,7 @@ public class EmptyFetcher implements Fetcher {
     }
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws TikaException, IOException {
         return null;
     }
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/Fetcher.java b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/Fetcher.java
index 1b3fa2a24..8f7a186fd 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/Fetcher.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/Fetcher.java
@@ -21,6 +21,7 @@ import java.io.InputStream;
 
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 /**
  * Interface for an object that will fetch an InputStream given
@@ -33,5 +34,5 @@ public interface Fetcher {
 
     String getName();
 
-    InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException;
+    InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws TikaException, IOException;
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/RangeFetcher.java b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/RangeFetcher.java
index 0a3ceae7f..246165c98 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/RangeFetcher.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/RangeFetcher.java
@@ -21,6 +21,7 @@ import java.io.InputStream;
 
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 /**
  * This class extracts a range of bytes from a given fetch key.
@@ -28,7 +29,12 @@ import org.apache.tika.metadata.Metadata;
 public interface RangeFetcher extends Fetcher {
     //At some point, Tika 3.x?, we may want to add optional ranges to the fetchKey?
 
-    InputStream fetch(String fetchKey, long startOffset, long endOffset, Metadata metadata)
+    default InputStream fetch(String fetchKey, long startOffset, long endOffset, Metadata metadata)
+            throws TikaException, IOException {
+        return fetch(fetchKey, startOffset, endOffset, metadata, new ParseContext());
+    }
+
+    InputStream fetch(String fetchKey, long startOffset, long endOffset, Metadata metadata, ParseContext parseContext)
             throws TikaException, IOException;
 
 }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/fs/FileSystemFetcher.java b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/fs/FileSystemFetcher.java
index d926e3ca6..718899976 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/fs/FileSystemFetcher.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/fs/FileSystemFetcher.java
@@ -41,6 +41,7 @@ import org.apache.tika.metadata.FileSystem;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.Property;
 import org.apache.tika.metadata.TikaCoreProperties;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.AbstractFetcher;
 
 public class FileSystemFetcher extends AbstractFetcher implements Initializable {
@@ -58,10 +59,9 @@ public class FileSystemFetcher extends AbstractFetcher implements Initializable
     }
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws IOException, TikaException {
-
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws IOException, TikaException {
         if (fetchKey.contains("\u0000")) {
-            throw new IllegalArgumentException("Path must not contain \u0000. " +
+            throw new IllegalArgumentException("Path must not contain 'u0000'. " +
                     "Please review the life decisions that led you to requesting " +
                     "a file name with this character in it.");
         }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/url/UrlFetcher.java b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/url/UrlFetcher.java
index f415a3560..7692516cd 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/fetcher/url/UrlFetcher.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/fetcher/url/UrlFetcher.java
@@ -24,6 +24,7 @@ import java.util.Locale;
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.AbstractFetcher;
 
 /**
@@ -35,7 +36,7 @@ import org.apache.tika.pipes.fetcher.AbstractFetcher;
 public class UrlFetcher extends AbstractFetcher {
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws IOException, TikaException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws IOException, TikaException {
         if (fetchKey.contains("\u0000")) {
             throw new IllegalArgumentException("URL must not contain \u0000. " +
                     "Please review the life decisions that led you to requesting " +
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/filelist/FileListPipesIterator.java b/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/filelist/FileListPipesIterator.java
index 90cabe881..75cb8390c 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/filelist/FileListPipesIterator.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/filelist/FileListPipesIterator.java
@@ -30,7 +30,9 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.TikaConfig;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
+import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.fetcher.FetchKey;
 import org.apache.tika.pipes.pipesiterator.PipesIterator;
@@ -67,8 +69,10 @@ public class FileListPipesIterator extends PipesIterator implements Initializabl
                 if (! line.startsWith("#") && !StringUtils.isBlank(line)) {
                     FetchKey fetchKey = new FetchKey(getFetcherName(), line);
                     EmitKey emitKey = new EmitKey(getEmitterName(), line);
+                    ParseContext parseContext = new ParseContext();
+                    parseContext.set(HandlerConfig.class, getHandlerConfig());
                     tryToAdd(new FetchEmitTuple(line, fetchKey, emitKey,
-                            new Metadata(), getHandlerConfig(), getOnParseException()));
+                            new Metadata(), parseContext, getOnParseException()));
                 }
                 line = reader.readLine();
             }
diff --git a/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/fs/FileSystemPipesIterator.java b/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/fs/FileSystemPipesIterator.java
index 9e903fd8b..793ac27fb 100644
--- a/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/fs/FileSystemPipesIterator.java
+++ b/tika-core/src/main/java/org/apache/tika/pipes/pipesiterator/fs/FileSystemPipesIterator.java
@@ -38,7 +38,9 @@ import org.apache.tika.config.Param;
 import org.apache.tika.config.TikaConfig;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
+import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.async.AsyncProcessor;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.fetcher.FetchKey;
@@ -152,8 +154,10 @@ public class FileSystemPipesIterator extends PipesIterator
             String relPath = basePath.relativize(file).toString();
 
             try {
+                ParseContext parseContext = new ParseContext();
+                parseContext.set(HandlerConfig.class, getHandlerConfig());
                 tryToAdd(new FetchEmitTuple(relPath, new FetchKey(fetcherName, relPath),
-                        new EmitKey(emitterName, relPath), new Metadata(), getHandlerConfig(),
+                        new EmitKey(emitterName, relPath), new Metadata(), parseContext,
                         getOnParseException()));
             } catch (TimeoutException e) {
                 throw new IOException(e);
diff --git a/tika-core/src/test/java/org/apache/tika/pipes/PipesServerTest.java b/tika-core/src/test/java/org/apache/tika/pipes/PipesServerTest.java
index 4aca5207e..dd8ef4ff4 100644
--- a/tika-core/src/test/java/org/apache/tika/pipes/PipesServerTest.java
+++ b/tika-core/src/test/java/org/apache/tika/pipes/PipesServerTest.java
@@ -33,6 +33,7 @@ import org.junit.jupiter.api.io.TempDir;
 import org.apache.tika.TikaTest;
 import org.apache.tika.extractor.BasicEmbeddedDocumentBytesHandler;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
 import org.apache.tika.pipes.fetcher.FetchKey;
@@ -104,12 +105,12 @@ public class PipesServerTest extends TikaTest {
         EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig =
                 new EmbeddedDocumentBytesConfig(true);
         embeddedDocumentBytesConfig.setIncludeOriginal(true);
-
+        ParseContext parseContext = new ParseContext();
+        parseContext.set(HandlerConfig.class, HandlerConfig.DEFAULT_HANDLER_CONFIG);
+        parseContext.set(EmbeddedDocumentBytesConfig.class, embeddedDocumentBytesConfig);
         FetchEmitTuple fetchEmitTuple = new FetchEmitTuple("id",
                 new FetchKey("fs", "mock.xml"),
-                new EmitKey("", ""), new Metadata(),
-                HandlerConfig.DEFAULT_HANDLER_CONFIG, FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT,
-                embeddedDocumentBytesConfig);
+                new EmitKey("", ""), new Metadata(), parseContext);
         Fetcher fetcher = FetcherManager.load(tikaConfig).getFetcher();
         PipesServer.MetadataListAndEmbeddedBytes
                 parseData = pipesServer.parseFromTuple(fetchEmitTuple, fetcher);
@@ -160,12 +161,13 @@ public class PipesServerTest extends TikaTest {
         EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig =
                 new EmbeddedDocumentBytesConfig(true);
         embeddedDocumentBytesConfig.setIncludeOriginal(true);
-
+        ParseContext parseContext = new ParseContext();
+        parseContext.set(HandlerConfig.class, HandlerConfig.DEFAULT_HANDLER_CONFIG);
+        parseContext.set(EmbeddedDocumentBytesConfig.class, embeddedDocumentBytesConfig);
         FetchEmitTuple fetchEmitTuple = new FetchEmitTuple("id",
                 new FetchKey("fs", "mock.xml"),
-                new EmitKey("", ""), new Metadata(),
-                HandlerConfig.DEFAULT_HANDLER_CONFIG, FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT,
-                embeddedDocumentBytesConfig);
+                new EmitKey("", ""), new Metadata(), parseContext);
+
         Fetcher fetcher = FetcherManager.load(tikaConfig).getFetcher();
         PipesServer.MetadataListAndEmbeddedBytes
                 parseData = pipesServer.parseFromTuple(fetchEmitTuple, fetcher);
diff --git a/tika-core/src/test/java/org/apache/tika/pipes/async/MockEmitter.java b/tika-core/src/test/java/org/apache/tika/pipes/async/MockEmitter.java
index 2374c1474..b940ec7be 100644
--- a/tika-core/src/test/java/org/apache/tika/pipes/async/MockEmitter.java
+++ b/tika-core/src/test/java/org/apache/tika/pipes/async/MockEmitter.java
@@ -23,6 +23,7 @@ import java.util.List;
 import java.util.concurrent.ArrayBlockingQueue;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.EmitData;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -40,11 +41,11 @@ public class MockEmitter extends AbstractEmitter {
     }
 
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext)
             throws IOException, TikaEmitterException {
         emit(
                 Collections.singletonList(new EmitData(new EmitKey(getName(), emitKey),
-                 metadataList)));
+                 metadataList, null, parseContext)));
     }
 
     @Override
diff --git a/tika-core/src/test/java/org/apache/tika/pipes/async/MockFetcher.java b/tika-core/src/test/java/org/apache/tika/pipes/async/MockFetcher.java
index 10af275e3..acb533ece 100644
--- a/tika-core/src/test/java/org/apache/tika/pipes/async/MockFetcher.java
+++ b/tika-core/src/test/java/org/apache/tika/pipes/async/MockFetcher.java
@@ -23,6 +23,7 @@ import java.nio.charset.StandardCharsets;
 
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.Fetcher;
 
 public class MockFetcher implements Fetcher {
@@ -37,7 +38,7 @@ public class MockFetcher implements Fetcher {
     }
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws TikaException, IOException {
         return new ByteArrayInputStream(BYTES);
     }
 }
diff --git a/tika-core/src/test/java/org/apache/tika/pipes/emitter/MockEmitter.java b/tika-core/src/test/java/org/apache/tika/pipes/emitter/MockEmitter.java
index 036a95965..89c3c2ce4 100644
--- a/tika-core/src/test/java/org/apache/tika/pipes/emitter/MockEmitter.java
+++ b/tika-core/src/test/java/org/apache/tika/pipes/emitter/MockEmitter.java
@@ -26,6 +26,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 public class MockEmitter extends AbstractEmitter implements Initializable {
 
@@ -52,7 +53,7 @@ public class MockEmitter extends AbstractEmitter implements Initializable {
     }
 
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext)
             throws IOException, TikaEmitterException {
 
     }
diff --git a/tika-core/src/test/java/org/apache/tika/pipes/fetcher/MockFetcher.java b/tika-core/src/test/java/org/apache/tika/pipes/fetcher/MockFetcher.java
index 060432724..e9104e0a8 100644
--- a/tika-core/src/test/java/org/apache/tika/pipes/fetcher/MockFetcher.java
+++ b/tika-core/src/test/java/org/apache/tika/pipes/fetcher/MockFetcher.java
@@ -29,6 +29,7 @@ import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 
 public class MockFetcher extends AbstractFetcher implements Initializable {
 
@@ -64,7 +65,7 @@ public class MockFetcher extends AbstractFetcher implements Initializable {
 
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws TikaException, IOException {
         return byteString == null ? new ByteArrayInputStream(new byte[0]) :
                 new ByteArrayInputStream(byteString.getBytes(StandardCharsets.UTF_8));
     }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/AbstractProfiler.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/AbstractProfiler.java
index 0cd609d3b..71f722403 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/AbstractProfiler.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/AbstractProfiler.java
@@ -80,15 +80,13 @@ import org.apache.tika.utils.StringUtils;
 
 public abstract class AbstractProfiler extends FileResourceConsumer {
 
-    //Container exception key from the 1.x branch
-    private static final Property CONTAINER_EXCEPTION_1X = Property.externalText("X-TIKA" +
-            ":EXCEPTION:runtime");
-
     public static final String TRUE = Boolean.toString(true);
     public static final String FALSE = Boolean.toString(false);
     protected static final AtomicInteger ID = new AtomicInteger();
     static final long NON_EXISTENT_FILE_LENGTH = -1l;
     final static int FILE_PATH_MAX_LEN = 1024;//max len for varchar for file_path
+    //Container exception key from the 1.x branch
+    private static final Property CONTAINER_EXCEPTION_1X = Property.externalText("X-TIKA" + ":EXCEPTION:runtime");
     private static final Logger LOG = LoggerFactory.getLogger(AbstractProfiler.class);
     private static final String[] EXTRACT_EXTENSIONS = {".json", ".txt", ""};
     private static final String[] COMPRESSION_EXTENSIONS = {"", ".bz2", ".gzip", ".zip",};
@@ -97,24 +95,16 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
     //make this configurable
     private static final String DIGEST_KEY = "X-TIKA:digest:MD5";
     private static final Map<String, Cols> UC_TAGS_OF_INTEREST = initTags();
-    private final static Pattern ACCESS_PERMISSION_EXCEPTION =
-            Pattern.compile("org\\.apache\\.tika\\.exception\\.AccessPermissionException");
-    private final static Pattern ENCRYPTION_EXCEPTION =
-            Pattern.compile("org\\.apache\\.tika.exception\\.EncryptedDocumentException");
-    public static TableInfo REF_EXTRACT_EXCEPTION_TYPES =
-            new TableInfo("ref_extract_exception_types",
-                    new ColInfo(Cols.EXTRACT_EXCEPTION_ID, Types.INTEGER),
-                    new ColInfo(Cols.EXTRACT_EXCEPTION_DESCRIPTION, Types.VARCHAR, 128));
+    private final static Pattern ACCESS_PERMISSION_EXCEPTION = Pattern.compile("org\\.apache\\.tika\\.exception\\.AccessPermissionException");
+    private final static Pattern ENCRYPTION_EXCEPTION = Pattern.compile("org\\.apache\\.tika.exception\\.EncryptedDocumentException");
+    public static TableInfo REF_EXTRACT_EXCEPTION_TYPES = new TableInfo("ref_extract_exception_types", new ColInfo(Cols.EXTRACT_EXCEPTION_ID, Types.INTEGER),
+            new ColInfo(Cols.EXTRACT_EXCEPTION_DESCRIPTION, Types.VARCHAR, 128));
     public static TableInfo REF_PARSE_ERROR_TYPES =
-            new TableInfo("ref_parse_error_types", new ColInfo(Cols.PARSE_ERROR_ID, Types.INTEGER),
-                    new ColInfo(Cols.PARSE_ERROR_DESCRIPTION, Types.VARCHAR, 128));
-    public static TableInfo REF_PARSE_EXCEPTION_TYPES = new TableInfo("ref_parse_exception_types",
-            new ColInfo(Cols.PARSE_EXCEPTION_ID, Types.INTEGER),
-            new ColInfo(Cols.PARSE_EXCEPTION_DESCRIPTION, Types.VARCHAR, 128));
-    public static TableInfo MIME_TABLE =
-            new TableInfo("mimes", new ColInfo(Cols.MIME_ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.MIME_STRING, Types.VARCHAR, 256),
-                    new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 12));
+            new TableInfo("ref_parse_error_types", new ColInfo(Cols.PARSE_ERROR_ID, Types.INTEGER), new ColInfo(Cols.PARSE_ERROR_DESCRIPTION, Types.VARCHAR, 128));
+    public static TableInfo REF_PARSE_EXCEPTION_TYPES =
+            new TableInfo("ref_parse_exception_types", new ColInfo(Cols.PARSE_EXCEPTION_ID, Types.INTEGER), new ColInfo(Cols.PARSE_EXCEPTION_DESCRIPTION, Types.VARCHAR, 128));
+    public static TableInfo MIME_TABLE = new TableInfo("mimes", new ColInfo(Cols.MIME_ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.MIME_STRING, Types.VARCHAR, 256),
+            new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 12));
     private static CommonTokenCountManager COMMON_TOKEN_COUNT_MANAGER;
     private static Pattern FILE_NAME_CLEANER = Pattern.compile("\\.(json|txt)(\\.(bz2|gz|zip))?$");
     private static LanguageIDWrapper LANG_ID = new LanguageIDWrapper();
@@ -195,8 +185,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
      * @param data
      * @return
      */
-    protected static String truncateContent(ContentTags contentTags, int maxLength,
-                                            Map<Cols, String> data) {
+    protected static String truncateContent(ContentTags contentTags, int maxLength, Map<Cols, String> data) {
         data.put(Cols.CONTENT_TRUNCATED_AT_MAX_LEN, "FALSE");
         if (contentTags == null) {
             return "";
@@ -231,7 +220,9 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
 
         Map<String, Integer> counts = new HashMap<>();
         for (int i = 1; i < list.size(); i++) {
-            String path = list.get(i).get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH);
+            String path = list
+                    .get(i)
+                    .get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH);
             if (path == null) {
                 //shouldn't ever happen
                 continue;
@@ -253,7 +244,9 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         }
 
         for (int i = 1; i < list.size(); i++) {
-            Integer count = counts.get(list.get(i).get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH));
+            Integer count = counts.get(list
+                    .get(i)
+                    .get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH));
             if (count == null) {
                 count = 0;
             }
@@ -264,8 +257,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
 
     }
 
-    private static void join(String delimiter, StringBuilder sb, String[] parts, int start,
-                             int end) {
+    private static void join(String delimiter, StringBuilder sb, String[] parts, int start, int end) {
         for (int i = start; i <= end; i++) {
             sb.append(delimiter);
             sb.append(parts[i]);
@@ -279,33 +271,44 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         }
 
         String handlerClass = metadata.get(TikaCoreProperties.TIKA_CONTENT_HANDLER);
-        if (evalFilePaths.getExtractFile().getFileName().toString().toLowerCase(Locale.ENGLISH)
+        if (evalFilePaths
+                .getExtractFile()
+                .getFileName()
+                .toString()
+                .toLowerCase(Locale.ENGLISH)
                 .endsWith(".html")) {
             try {
                 return ContentTagParser.parseHTML(s, UC_TAGS_OF_INTEREST.keySet());
             } catch (IOException | SAXException e) {
-                LOG.warn("Problem parsing html in {}; backing off to treat string as text",
-                        evalFilePaths.getExtractFile().toAbsolutePath().toString(), e);
+                LOG.warn("Problem parsing html in {}; backing off to treat string as text", evalFilePaths
+                        .getExtractFile()
+                        .toAbsolutePath()
+                        .toString(), e);
 
                 return new ContentTags(s, true);
             }
-        } else if (
-                evalFilePaths.getExtractFile().getFileName().toString().toLowerCase(Locale.ENGLISH)
-                        .endsWith(".xhtml") || (handlerClass != null &&
-                        handlerClass.equals(ToXMLContentHandler.class.getSimpleName()))) {
+        } else if (evalFilePaths
+                .getExtractFile()
+                .getFileName()
+                .toString()
+                .toLowerCase(Locale.ENGLISH)
+                .endsWith(".xhtml") || (handlerClass != null && handlerClass.equals(ToXMLContentHandler.class.getSimpleName()))) {
             try {
                 return ContentTagParser.parseXML(s, UC_TAGS_OF_INTEREST.keySet());
             } catch (TikaException | IOException | SAXException e) {
-                LOG.warn("Problem parsing xhtml in {}; backing off to html parser",
-                        evalFilePaths.getExtractFile().toAbsolutePath().toString(), e);
+                LOG.warn("Problem parsing xhtml in {}; backing off to html parser", evalFilePaths
+                        .getExtractFile()
+                        .toAbsolutePath()
+                        .toString(), e);
                 try {
-                    ContentTags contentTags =
-                            ContentTagParser.parseHTML(s, UC_TAGS_OF_INTEREST.keySet());
+                    ContentTags contentTags = ContentTagParser.parseHTML(s, UC_TAGS_OF_INTEREST.keySet());
                     contentTags.setParseException(true);
                     return contentTags;
                 } catch (IOException | SAXException e2) {
-                    LOG.warn("Problem parsing html in {}; backing off to treat string as text",
-                            evalFilePaths.getExtractFile().toAbsolutePath().toString(), e2);
+                    LOG.warn("Problem parsing html in {}; backing off to treat string as text", evalFilePaths
+                            .getExtractFile()
+                            .toAbsolutePath()
+                            .toString(), e2);
                 }
                 return new ContentTags(s, true);
             }
@@ -313,8 +316,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         return new ContentTags(s);
     }
 
-    private CompositeTextStatsCalculator initAnalyzersAndTokenCounter(int maxTokens,
-                                                                      LanguageIDWrapper langIder) {
+    private CompositeTextStatsCalculator initAnalyzersAndTokenCounter(int maxTokens, LanguageIDWrapper langIder) {
         analyzerManager = AnalyzerManager.newInstance(maxTokens);
         List<TextStatsCalculator> calculators = new ArrayList<>();
         calculators.add(new CommonTokens(COMMON_TOKEN_COUNT_MANAGER));
@@ -325,8 +327,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         calculators.add(new ContentLengthCalculator());
         calculators.add(new UnicodeBlockCounter(maxContentLengthForLangId));
 
-        return new CompositeTextStatsCalculator(calculators, analyzerManager.getGeneralAnalyzer(),
-                langIder);
+        return new CompositeTextStatsCalculator(calculators, analyzerManager.getGeneralAnalyzer(), langIder);
     }
 
     /**
@@ -358,9 +359,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         initAnalyzersAndTokenCounter(maxTokens, new LanguageIDWrapper());
     }
 
-    protected void writeExtractException(TableInfo extractExceptionTable, String containerId,
-                                         String filePath, ExtractReaderException.TYPE type)
-            throws IOException {
+    protected void writeExtractException(TableInfo extractExceptionTable, String containerId, String filePath, ExtractReaderException.TYPE type) throws IOException {
         Map<Cols, String> data = new HashMap<>();
         data.put(Cols.CONTAINER_ID, containerId);
         data.put(Cols.FILE_PATH, filePath);
@@ -369,8 +368,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
 
     }
 
-    protected void writeProfileData(EvalFilePaths fps, int i, ContentTags contentTags, Metadata m,
-                                    String fileId, String containerId, List<Integer> numAttachments,
+    protected void writeProfileData(EvalFilePaths fps, int i, ContentTags contentTags, Metadata m, String fileId, String containerId, List<Integer> numAttachments,
                                     TableInfo profileTable) {
 
         Map<Cols, String> data = new HashMap<>();
@@ -396,12 +394,15 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         //if the outer wrapper document
         if (i == 0) {
             data.put(Cols.IS_EMBEDDED, FALSE);
-            data.put(Cols.FILE_NAME, fps.getRelativeSourceFilePath().getFileName().toString());
+            data.put(Cols.FILE_NAME, fps
+                    .getRelativeSourceFilePath()
+                    .getFileName()
+                    .toString());
             data.put(Cols.EMBEDDED_DEPTH, "0");
         } else {
             data.put(Cols.IS_EMBEDDED, TRUE);
             String embeddedFilePath = m.get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH);
-            if (! StringUtils.isBlank(embeddedFilePath)) {
+            if (!StringUtils.isBlank(embeddedFilePath)) {
                 data.put(Cols.FILE_NAME, getFileName(m.get(embeddedFilePath)));
                 data.put(Cols.EMBEDDED_FILE_PATH, embeddedFilePath);
             }
@@ -427,7 +428,9 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         data.put(Cols.ELAPSED_TIME_MILLIS, getTime(m));
 
         String content = contentTags.getContent();
-        if (content == null || content.trim().length() == 0) {
+        if (content == null || content
+                .trim()
+                .length() == 0) {
             data.put(Cols.HAS_CONTENT, FALSE);
         } else {
             data.put(Cols.HAS_CONTENT, TRUE);
@@ -443,7 +446,9 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
     protected void writeExceptionData(String fileId, Metadata m, TableInfo exceptionTable) {
         Map<Cols, String> data = new HashMap<>();
         getExceptionStrings(m, data);
-        if (data.keySet().size() > 0) {
+        if (data
+                .keySet()
+                .size() > 0) {
             try {
                 data.put(Cols.ID, fileId);
                 writer.writeRow(exceptionTable, data);
@@ -459,7 +464,9 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         }*/
         Map<Cols, String> data = new HashMap<>();
         String content = truncateContent(contentTags, maxContentLength, data);
-        if (content == null || content.trim().length() == 0) {
+        if (content == null || content
+                .trim()
+                .length() == 0) {
             content = "";
         }
         return compositeTextStatsCalculator.calculate(content);
@@ -474,8 +481,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
      * @param textStats
      * @param contentsTable
      */
-    protected void writeContentData(String fileId, Map<Class, Object> textStats,
-                                    TableInfo contentsTable) throws IOException {
+    protected void writeContentData(String fileId, Map<Class, Object> textStats, TableInfo contentsTable) throws IOException {
         Map<Cols, String> data = new HashMap<>();
         data.put(Cols.ID, fileId);
         if (textStats.containsKey(ContentLengthCalculator.class)) {
@@ -491,13 +497,10 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
         CommonTokenResult commonTokenResult = (CommonTokenResult) textStats.get(CommonTokens.class);
         if (commonTokenResult != null) {
             data.put(Cols.COMMON_TOKENS_LANG, commonTokenResult.getLangCode());
-            data.put(Cols.NUM_UNIQUE_COMMON_TOKENS,
-                    Integer.toString(commonTokenResult.getUniqueCommonTokens()));
+            data.put(Cols.NUM_UNIQUE_COMMON_TOKENS, Integer.toString(commonTokenResult.getUniqueCommonTokens()));
             data.put(Cols.NUM_COMMON_TOKENS, Integer.toString(commonTokenResult.getCommonTokens()));
-            data.put(Cols.NUM_UNIQUE_ALPHABETIC_TOKENS,
-                    Integer.toString(commonTokenResult.getUniqueAlphabeticTokens()));
-            data.put(Cols.NUM_ALPHABETIC_TOKENS,
-                    Integer.toString(commonTokenResult.getAlphabeticTokens()));
+            data.put(Cols.NUM_UNIQUE_ALPHABETIC_TOKENS, Integer.toString(commonTokenResult.getUniqueAlphabeticTokens()));
+            data.put(Cols.NUM_ALPHABETIC_TOKENS, Integer.toString(commonTokenResult.getAlphabeticTokens()));
             double oov = commonTokenResult.getAlphabeticTokens() > 0 ? commonTokenResult.getOOV() : -1.0;
             data.put(Cols.OOV, Double.toString(oov));
         }
@@ -508,8 +511,7 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
             data.put(Cols.NUM_TOKENS, Integer.toString(tokenCounts.getTotalTokens()));
         }
         if (textStats.get(TokenEntropy.class) != null) {
-            data.put(Cols.TOKEN_ENTROPY_RATE,
-                    Double.toString((Double) textStats.get(TokenEntropy.class)));
+            data.put(Cols.TOKEN_ENTROPY_RATE, Double.toString((Double) textStats.get(TokenEntropy.class)));
         }
 
 
@@ -597,14 +599,12 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
 
             Matcher matcher = ACCESS_PERMISSION_EXCEPTION.matcher(fullTrace);
             if (matcher.find()) {
-                data.put(Cols.PARSE_EXCEPTION_ID,
-                        Integer.toString(EXCEPTION_TYPE.ACCESS_PERMISSION.ordinal()));
+                data.put(Cols.PARSE_EXCEPTION_ID, Integer.toString(EXCEPTION_TYPE.ACCESS_PERMISSION.ordinal()));
                 return;
             }
             matcher = ENCRYPTION_EXCEPTION.matcher(fullTrace);
             if (matcher.find()) {
-                data.put(Cols.PARSE_EXCEPTION_ID,
-                        Integer.toString(EXCEPTION_TYPE.ENCRYPTION.ordinal()));
+                data.put(Cols.PARSE_EXCEPTION_ID, Integer.toString(EXCEPTION_TYPE.ENCRYPTION.ordinal()));
                 return;
             }
 
@@ -623,35 +623,52 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
 
     void unicodeBlocks(Map<Class, Object> tokenStats, Map<Cols, String> data) {
 
-        Map<String, MutableInt> blocks =
-                (Map<String, MutableInt>) tokenStats.get(UnicodeBlockCounter.class);
+        Map<String, MutableInt> blocks = (Map<String, MutableInt>) tokenStats.get(UnicodeBlockCounter.class);
         List<Pair<String, Integer>> pairs = new ArrayList<>();
         for (Map.Entry<String, MutableInt> e : blocks.entrySet()) {
-            pairs.add(Pair.of(e.getKey(), e.getValue().intValue()));
+            pairs.add(Pair.of(e.getKey(), e
+                    .getValue()
+                    .intValue()));
         }
-        pairs.sort((o1, o2) -> o2.getValue().compareTo(o1.getValue()));
+        pairs.sort((o1, o2) -> o2
+                .getValue()
+                .compareTo(o1.getValue()));
         StringBuilder sb = new StringBuilder();
 
         for (int i = 0; i < 20 && i < pairs.size(); i++) {
             if (i > 0) {
                 sb.append(" | ");
             }
-            sb.append(pairs.get(i).getKey()).append(": ").append(pairs.get(i).getValue());
+            sb
+                    .append(pairs
+                            .get(i)
+                            .getKey())
+                    .append(": ")
+                    .append(pairs
+                            .get(i)
+                            .getValue());
         }
         data.put(Cols.UNICODE_CHAR_BLOCKS, sb.toString());
     }
 
     void langid(Map<Class, Object> stats, Map<Cols, String> data) {
-        List<LanguageResult> probabilities =
-                (List<LanguageResult>) stats.get(LanguageIDWrapper.class);
+        List<LanguageResult> probabilities = (List<LanguageResult>) stats.get(LanguageIDWrapper.class);
 
         if (probabilities.size() > 0) {
-            data.put(Cols.LANG_ID_1, probabilities.get(0).getLanguage());
-            data.put(Cols.LANG_ID_PROB_1, Double.toString(probabilities.get(0).getRawScore()));
+            data.put(Cols.LANG_ID_1, probabilities
+                    .get(0)
+                    .getLanguage());
+            data.put(Cols.LANG_ID_PROB_1, Double.toString(probabilities
+                    .get(0)
+                    .getRawScore()));
         }
         if (probabilities.size() > 1) {
-            data.put(Cols.LANG_ID_2, probabilities.get(1).getLanguage());
-            data.put(Cols.LANG_ID_PROB_2, Double.toString(probabilities.get(1).getRawScore()));
+            data.put(Cols.LANG_ID_2, probabilities
+                    .get(1)
+                    .getLanguage());
+            data.put(Cols.LANG_ID_PROB_2, Double.toString(probabilities
+                    .get(1)
+                    .getRawScore()));
         }
     }
 
@@ -675,7 +692,10 @@ public abstract class AbstractProfiler extends FileResourceConsumer {
             if (i++ > 0) {
                 sb.append(" | ");
             }
-            sb.append(t.getToken()).append(": ").append(t.getValue());
+            sb
+                    .append(t.getToken())
+                    .append(": ")
+                    .append(t.getValue());
         }
 
         data.put(Cols.TOP_N_TOKENS, sb.toString());
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/EvalFilePaths.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/EvalFilePaths.java
index a70badd5a..958c7e0be 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/EvalFilePaths.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/EvalFilePaths.java
@@ -108,8 +108,7 @@ class EvalFilePaths {
 
     @Override
     public String toString() {
-        return "EvalFilePaths{" + "relativeSourceFilePath=" + relativeSourceFilePath +
-                ", extractFile=" + extractFile + ", sourceFileLength=" + sourceFileLength +
+        return "EvalFilePaths{" + "relativeSourceFilePath=" + relativeSourceFilePath + ", extractFile=" + extractFile + ", sourceFileLength=" + sourceFileLength +
                 ", extractFileLength=" + extractFileLength + '}';
     }
 }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractComparer.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractComparer.java
index 62723d7e6..1a57ac9e8 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractComparer.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractComparer.java
@@ -50,52 +50,30 @@ import org.apache.tika.metadata.TikaCoreProperties;
 
 public class ExtractComparer extends AbstractProfiler {
 
-    private static final String DIGEST_KEY_PREFIX = TikaCoreProperties.TIKA_META_PREFIX + "digest" +
-            TikaCoreProperties.NAMESPACE_PREFIX_DELIMITER;
+    private static final String DIGEST_KEY_PREFIX = TikaCoreProperties.TIKA_META_PREFIX + "digest" + TikaCoreProperties.NAMESPACE_PREFIX_DELIMITER;
     private final static String FIELD_A = "fa";
     private final static String FIELD_B = "fb";
-    public static TableInfo REF_PAIR_NAMES =
-            new TableInfo("pair_names", new ColInfo(Cols.DIR_NAME_A, Types.VARCHAR, 128),
-                    new ColInfo(Cols.DIR_NAME_B, Types.VARCHAR, 128));
-    public static TableInfo COMPARISON_CONTAINERS = new TableInfo("containers",
-            new ColInfo(Cols.CONTAINER_ID, Types.INTEGER, "PRIMARY KEY"),
-            new ColInfo(Cols.FILE_PATH, Types.VARCHAR, FILE_PATH_MAX_LEN),
-            new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 12),
-            new ColInfo(Cols.LENGTH, Types.BIGINT),
-            new ColInfo(Cols.EXTRACT_FILE_LENGTH_A, Types.BIGINT),
-            new ColInfo(Cols.EXTRACT_FILE_LENGTH_B, Types.BIGINT));
+    public static TableInfo REF_PAIR_NAMES = new TableInfo("pair_names", new ColInfo(Cols.DIR_NAME_A, Types.VARCHAR, 128), new ColInfo(Cols.DIR_NAME_B, Types.VARCHAR, 128));
+    public static TableInfo COMPARISON_CONTAINERS =
+            new TableInfo("containers", new ColInfo(Cols.CONTAINER_ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.FILE_PATH, Types.VARCHAR, FILE_PATH_MAX_LEN),
+                    new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 12), new ColInfo(Cols.LENGTH, Types.BIGINT), new ColInfo(Cols.EXTRACT_FILE_LENGTH_A, Types.BIGINT),
+                    new ColInfo(Cols.EXTRACT_FILE_LENGTH_B, Types.BIGINT));
     public static TableInfo CONTENT_COMPARISONS =
-            new TableInfo("content_comparisons", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.TOP_10_UNIQUE_TOKEN_DIFFS_A, Types.VARCHAR, 1024),
-                    new ColInfo(Cols.TOP_10_UNIQUE_TOKEN_DIFFS_B, Types.VARCHAR, 1024),
-                    new ColInfo(Cols.TOP_10_MORE_IN_A, Types.VARCHAR, 1024),
-                    new ColInfo(Cols.TOP_10_MORE_IN_B, Types.VARCHAR, 1024),
-                    new ColInfo(Cols.DICE_COEFFICIENT, Types.FLOAT),
-                    new ColInfo(Cols.OVERLAP, Types.FLOAT));
-    public static TableInfo PROFILES_A =
-            new TableInfo("profiles_a", ExtractProfiler.PROFILE_TABLE.getColInfos());
-    public static TableInfo PROFILES_B =
-            new TableInfo("profiles_b", ExtractProfiler.PROFILE_TABLE.getColInfos());
-    public static TableInfo EMBEDDED_FILE_PATH_TABLE_A =
-            new TableInfo("emb_path_a", ExtractProfiler.EMBEDDED_FILE_PATH_TABLE.getColInfos());
-    public static TableInfo EMBEDDED_FILE_PATH_TABLE_B =
-            new TableInfo("emb_path_b", ExtractProfiler.EMBEDDED_FILE_PATH_TABLE.getColInfos());
-    public static TableInfo CONTENTS_TABLE_A =
-            new TableInfo("contents_a", ExtractProfiler.CONTENTS_TABLE.getColInfos());
-    public static TableInfo CONTENTS_TABLE_B =
-            new TableInfo("contents_b", ExtractProfiler.CONTENTS_TABLE.getColInfos());
-    public static TableInfo TAGS_TABLE_A =
-            new TableInfo("tags_a", ExtractProfiler.TAGS_TABLE.getColInfos());
-    public static TableInfo TAGS_TABLE_B =
-            new TableInfo("tags_b", ExtractProfiler.TAGS_TABLE.getColInfos());
-    public static TableInfo EXCEPTION_TABLE_A =
-            new TableInfo("exceptions_a", ExtractProfiler.EXCEPTION_TABLE.getColInfos());
-    public static TableInfo EXCEPTION_TABLE_B =
-            new TableInfo("exceptions_b", ExtractProfiler.EXCEPTION_TABLE.getColInfos());
-    public static TableInfo EXTRACT_EXCEPTION_TABLE_A = new TableInfo("extract_exceptions_a",
-            ExtractProfiler.EXTRACT_EXCEPTION_TABLE.getColInfos());
-    public static TableInfo EXTRACT_EXCEPTION_TABLE_B = new TableInfo("extract_exceptions_b",
-            ExtractProfiler.EXTRACT_EXCEPTION_TABLE.getColInfos());
+            new TableInfo("content_comparisons", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.TOP_10_UNIQUE_TOKEN_DIFFS_A, Types.VARCHAR, 1024),
+                    new ColInfo(Cols.TOP_10_UNIQUE_TOKEN_DIFFS_B, Types.VARCHAR, 1024), new ColInfo(Cols.TOP_10_MORE_IN_A, Types.VARCHAR, 1024),
+                    new ColInfo(Cols.TOP_10_MORE_IN_B, Types.VARCHAR, 1024), new ColInfo(Cols.DICE_COEFFICIENT, Types.FLOAT), new ColInfo(Cols.OVERLAP, Types.FLOAT));
+    public static TableInfo PROFILES_A = new TableInfo("profiles_a", ExtractProfiler.PROFILE_TABLE.getColInfos());
+    public static TableInfo PROFILES_B = new TableInfo("profiles_b", ExtractProfiler.PROFILE_TABLE.getColInfos());
+    public static TableInfo EMBEDDED_FILE_PATH_TABLE_A = new TableInfo("emb_path_a", ExtractProfiler.EMBEDDED_FILE_PATH_TABLE.getColInfos());
+    public static TableInfo EMBEDDED_FILE_PATH_TABLE_B = new TableInfo("emb_path_b", ExtractProfiler.EMBEDDED_FILE_PATH_TABLE.getColInfos());
+    public static TableInfo CONTENTS_TABLE_A = new TableInfo("contents_a", ExtractProfiler.CONTENTS_TABLE.getColInfos());
+    public static TableInfo CONTENTS_TABLE_B = new TableInfo("contents_b", ExtractProfiler.CONTENTS_TABLE.getColInfos());
+    public static TableInfo TAGS_TABLE_A = new TableInfo("tags_a", ExtractProfiler.TAGS_TABLE.getColInfos());
+    public static TableInfo TAGS_TABLE_B = new TableInfo("tags_b", ExtractProfiler.TAGS_TABLE.getColInfos());
+    public static TableInfo EXCEPTION_TABLE_A = new TableInfo("exceptions_a", ExtractProfiler.EXCEPTION_TABLE.getColInfos());
+    public static TableInfo EXCEPTION_TABLE_B = new TableInfo("exceptions_b", ExtractProfiler.EXCEPTION_TABLE.getColInfos());
+    public static TableInfo EXTRACT_EXCEPTION_TABLE_A = new TableInfo("extract_exceptions_a", ExtractProfiler.EXTRACT_EXCEPTION_TABLE.getColInfos());
+    public static TableInfo EXTRACT_EXCEPTION_TABLE_B = new TableInfo("extract_exceptions_b", ExtractProfiler.EXTRACT_EXCEPTION_TABLE.getColInfos());
     static Options OPTIONS;
 
     static {
@@ -106,34 +84,31 @@ public class ExtractComparer extends AbstractProfiler {
         extractsB.setRequired(true);
 
         Option inputDir = new Option("inputDir", true,
-                "optional: directory of original binary input files if it exists " +
-                        "or can be the same as -extractsA or -extractsB. If not specified, -inputDir=-extractsA");
+                "optional: directory of original binary input files if it exists " + "or can be the same as -extractsA or -extractsB. If not specified, -inputDir=-extractsA");
 
 
-        OPTIONS = new Options().addOption(extractsA).addOption(extractsB).addOption(inputDir)
+        OPTIONS = new Options()
+                .addOption(extractsA)
+                .addOption(extractsB)
+                .addOption(inputDir)
                 .addOption("bc", "optional: tika-batch config file")
-                .addOption("numConsumers", true, "optional: number of consumer threads").addOption(
-                        new Option("alterExtract", true, "for json-formatted extract files, " +
-                                "process full metadata list ('as_is'=default), " +
-                                "take just the first/container document ('first_only'), " +
+                .addOption("numConsumers", true, "optional: number of consumer threads")
+                .addOption(new Option("alterExtract", true,
+                        "for json-formatted extract files, " + "process full metadata list ('as_is'=default), " + "take just the first/container document ('first_only'), " +
                                 "concatenate all content into the first metadata item ('concatenate_content')"))
                 .addOption("minExtractLength", true, "minimum extract length to process (in bytes)")
                 .addOption("maxExtractLength", true, "maximum extract length to process (in bytes)")
-                .addOption("db", true, "db file to which to write results").addOption("jdbc", true,
-                        "EXPERT: full jdbc connection string. Must specify this or -db <h2db>")
+                .addOption("db", true, "db file to which to write results")
+                .addOption("jdbc", true, "EXPERT: full jdbc connection string. Must specify this or -db <h2db>")
                 .addOption("jdbcDriver", true, "EXPERT: jdbc driver, or specify via -Djdbc.driver")
                 .addOption("tablePrefixA", true, "EXPERT: optional prefix for table names for A")
                 .addOption("tablePrefixB", true, "EXPERT: optional prefix for table names for B")
                 .addOption("drop", false, "drop tables if they exist")
                 .addOption("maxFilesToAdd", true, "maximum number of files to add to the crawler")
                 .addOption("maxTokens", true, "maximum tokens to process, default=200000")
-                .addOption("maxContentLength", true,
-                        "truncate content beyond this length for calculating 'contents' stats, default=1000000")
-                .addOption("maxContentLengthForLangId", true,
-                        "truncate content beyond this length for language id, default=50000")
-                .addOption("defaultLangCode", true,
-                        "which language to use for common words if no 'common words' " +
-                                "file exists for the langid result");
+                .addOption("maxContentLength", true, "truncate content beyond this length for calculating 'contents' stats, default=1000000")
+                .addOption("maxContentLengthForLangId", true, "truncate content beyond this length for language id, default=50000")
+                .addOption("defaultLangCode", true, "which language to use for common words if no 'common words' " + "file exists for the langid result");
     }
 
     //need to parameterize?
@@ -143,8 +118,7 @@ public class ExtractComparer extends AbstractProfiler {
     private final TokenContraster tokenContraster = new TokenContraster();
     private final ExtractReader extractReader;
 
-    public ExtractComparer(ArrayBlockingQueue<FileResource> queue, Path inputDir, Path extractsA,
-                           Path extractsB, ExtractReader extractReader, IDBWriter writer) {
+    public ExtractComparer(ArrayBlockingQueue<FileResource> queue, Path inputDir, Path extractsA, Path extractsB, ExtractReader extractReader, IDBWriter writer) {
         super(queue, writer);
         this.inputDir = inputDir;
         this.extractsA = extractsA;
@@ -154,9 +128,7 @@ public class ExtractComparer extends AbstractProfiler {
 
     public static void USAGE() {
         HelpFormatter helpFormatter = new HelpFormatter();
-        helpFormatter.printHelp(80,
-                "java -jar tika-eval-x.y.jar Compare -extractsA extractsA -extractsB extractsB -db mydb",
-                "Tool: Compare", ExtractComparer.OPTIONS,
+        helpFormatter.printHelp(80, "java -jar tika-eval-x.y.jar Compare -extractsA extractsA -extractsB extractsB -db mydb", "Tool: Compare", ExtractComparer.OPTIONS,
                 "Note: for the default h2 db, do not include the .mv.db at the end of the db name.");
     }
 
@@ -180,8 +152,7 @@ public class ExtractComparer extends AbstractProfiler {
             compareFiles(fpsA, fpsB);
         } catch (Throwable e) {
             //this should be cataclysmic...
-            throw new RuntimeException(
-                    "Exception while working on: " + metadata.get(FSProperties.FS_REL_PATH), e);
+            throw new RuntimeException("Exception while working on: " + metadata.get(FSProperties.FS_REL_PATH), e);
         }
         return true;
     }
@@ -215,32 +186,33 @@ public class ExtractComparer extends AbstractProfiler {
         //container table
         Map<Cols, String> contData = new HashMap<>();
         contData.put(Cols.CONTAINER_ID, containerID);
-        contData.put(Cols.FILE_PATH, fpsA.getRelativeSourceFilePath().toString());
+        contData.put(Cols.FILE_PATH, fpsA
+                .getRelativeSourceFilePath()
+                .toString());
         long srcFileLength = getSourceFileLength(metadataListA, metadataListB);
-        contData.put(Cols.LENGTH,
-                srcFileLength > NON_EXISTENT_FILE_LENGTH ? Long.toString(srcFileLength) : "");
-        contData.put(Cols.FILE_EXTENSION, FilenameUtils
-                .getExtension(fpsA.getRelativeSourceFilePath().getFileName().toString()));
+        contData.put(Cols.LENGTH, srcFileLength > NON_EXISTENT_FILE_LENGTH ? Long.toString(srcFileLength) : "");
+        contData.put(Cols.FILE_EXTENSION, FilenameUtils.getExtension(fpsA
+                .getRelativeSourceFilePath()
+                .getFileName()
+                .toString()));
 
         long extractFileLengthA = getFileLength(fpsA.getExtractFile());
-        contData.put(Cols.EXTRACT_FILE_LENGTH_A,
-                extractFileLengthA > NON_EXISTENT_FILE_LENGTH ? Long.toString(extractFileLengthA) :
-                        "");
+        contData.put(Cols.EXTRACT_FILE_LENGTH_A, extractFileLengthA > NON_EXISTENT_FILE_LENGTH ? Long.toString(extractFileLengthA) : "");
 
         long extractFileLengthB = getFileLength(fpsB.getExtractFile());
-        contData.put(Cols.EXTRACT_FILE_LENGTH_B,
-                extractFileLengthB > NON_EXISTENT_FILE_LENGTH ? Long.toString(extractFileLengthB) :
-                        "");
+        contData.put(Cols.EXTRACT_FILE_LENGTH_B, extractFileLengthB > NON_EXISTENT_FILE_LENGTH ? Long.toString(extractFileLengthB) : "");
 
         writer.writeRow(COMPARISON_CONTAINERS, contData);
 
         if (extractExceptionA != null) {
-            writeExtractException(EXTRACT_EXCEPTION_TABLE_A, containerID,
-                    fpsA.getRelativeSourceFilePath().toString(), extractExceptionA);
+            writeExtractException(EXTRACT_EXCEPTION_TABLE_A, containerID, fpsA
+                    .getRelativeSourceFilePath()
+                    .toString(), extractExceptionA);
         }
         if (extractExceptionB != null) {
-            writeExtractException(EXTRACT_EXCEPTION_TABLE_B, containerID,
-                    fpsB.getRelativeSourceFilePath().toString(), extractExceptionB);
+            writeExtractException(EXTRACT_EXCEPTION_TABLE_B, containerID, fpsB
+                    .getRelativeSourceFilePath()
+                    .toString(), extractExceptionB);
         }
 
         if (metadataListA == null && metadataListB == null) {
@@ -265,11 +237,9 @@ public class ExtractComparer extends AbstractProfiler {
                 //TODO: shouldn't be fileA!!!!
                 writeTagData(fileId, contentTagsA, TAGS_TABLE_A);
 
-                writeProfileData(fpsA, i, contentTagsA, metadataA, fileId, containerID,
-                        numAttachmentsA, PROFILES_A);
+                writeProfileData(fpsA, i, contentTagsA, metadataA, fileId, containerID, numAttachmentsA, PROFILES_A);
                 writeExceptionData(fileId, metadataA, EXCEPTION_TABLE_A);
-                int matchIndex =
-                        getMatch(i, sharedDigestKey, handledB, metadataListA, metadataListB);
+                int matchIndex = getMatch(i, sharedDigestKey, handledB, metadataListA, metadataListB);
 
                 if (matchIndex > -1 && !handledB.contains(matchIndex)) {
                     metadataB = metadataListB.get(matchIndex);
@@ -278,8 +248,7 @@ public class ExtractComparer extends AbstractProfiler {
                 if (metadataB != null) {
                     contentTagsB = getContent(fpsB, metadataB);
                     writeTagData(fileId, contentTagsB, TAGS_TABLE_B);
-                    writeProfileData(fpsB, i, contentTagsB, metadataB, fileId, containerID,
-                            numAttachmentsB, PROFILES_B);
+                    writeProfileData(fpsB, i, contentTagsB, metadataB, fileId, containerID, numAttachmentsB, PROFILES_B);
                     writeExceptionData(fileId, metadataB, EXCEPTION_TABLE_B);
                 }
                 writeEmbeddedFilePathData(i, fileId, metadataA, metadataB);
@@ -295,10 +264,8 @@ public class ExtractComparer extends AbstractProfiler {
                     throw new RuntimeException(e);
                 }
                 if (metadataB != null) {
-                    TokenCounts tokenCountsA =
-                            (TokenCounts) tokenStatsA.get(BasicTokenCountStatsCalculator.class);
-                    TokenCounts tokenCountsB =
-                            (TokenCounts) tokenStatsB.get(BasicTokenCountStatsCalculator.class);
+                    TokenCounts tokenCountsA = (TokenCounts) tokenStatsA.get(BasicTokenCountStatsCalculator.class);
+                    TokenCounts tokenCountsB = (TokenCounts) tokenStatsB.get(BasicTokenCountStatsCalculator.class);
                     //arbitrary decision...only run the comparisons if there are > 10 tokens total
                     //We may want to bump that value a bit higher?
                     //now run comparisons
@@ -306,8 +273,7 @@ public class ExtractComparer extends AbstractProfiler {
                         Map<Cols, String> data = new HashMap<>();
                         data.put(Cols.ID, fileId);
 
-                        ContrastStatistics contrastStatistics = tokenContraster
-                                .calculateContrastStatistics(tokenCountsA, tokenCountsB);
+                        ContrastStatistics contrastStatistics = tokenContraster.calculateContrastStatistics(tokenCountsA, tokenCountsB);
 
                         writeContrasts(data, contrastStatistics);
                         writer.writeRow(CONTENT_COMPARISONS, data);
@@ -327,8 +293,7 @@ public class ExtractComparer extends AbstractProfiler {
                 //the first file should have the same id as the container id
                 String fileId = (i == 0) ? containerID : Integer.toString(ID.getAndIncrement());
                 writeTagData(fileId, contentTagsB, TAGS_TABLE_B);
-                writeProfileData(fpsB, i, contentTagsB, metadataB, fileId, containerID,
-                        numAttachmentsB, PROFILES_B);
+                writeProfileData(fpsB, i, contentTagsB, metadataB, fileId, containerID, numAttachmentsB, PROFILES_B);
                 writeEmbeddedFilePathData(i, fileId, null, metadataB);
                 writeExceptionData(fileId, metadataB, EXCEPTION_TABLE_B);
 
@@ -357,7 +322,9 @@ public class ExtractComparer extends AbstractProfiler {
         }
         Set<String> digestA = new HashSet<>();
         if (metadataListA != null && metadataListA.size() > 0) {
-            for (String n : metadataListA.get(0).names()) {
+            for (String n : metadataListA
+                    .get(0)
+                    .names()) {
                 if (n.startsWith(DIGEST_KEY_PREFIX)) {
                     digestA.add(n);
                 }
@@ -425,8 +392,7 @@ public class ExtractComparer extends AbstractProfiler {
      * @param metadataListB
      * @return
      */
-    private int getMatch(int aIndex, String sharedDigestKey, Set<Integer> handledB,
-                         List<Metadata> metadataListA, List<Metadata> metadataListB) {
+    private int getMatch(int aIndex, String sharedDigestKey, Set<Integer> handledB, List<Metadata> metadataListA, List<Metadata> metadataListB) {
         //TODO: could make this more robust
         if (metadataListB == null || metadataListB.size() == 0) {
             return -1;
@@ -439,8 +405,7 @@ public class ExtractComparer extends AbstractProfiler {
         if (sharedDigestKey != null) {
             //first try to find matching digests
             //this does not elegantly handle multiple matching digests
-            return findMatchingDigests(sharedDigestKey, handledB, metadataListA.get(aIndex),
-                    metadataListB);
+            return findMatchingDigests(sharedDigestKey, handledB, metadataListA.get(aIndex), metadataListB);
         }
 
         //assume same embedded resource path.  Not always true!
@@ -448,8 +413,9 @@ public class ExtractComparer extends AbstractProfiler {
         String embeddedPath = thisMetadata.get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH);
         if (embeddedPath != null) {
             for (int j = 0; j < metadataListB.size(); j++) {
-                String thatEmbeddedPath =
-                        metadataListB.get(j).get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH);
+                String thatEmbeddedPath = metadataListB
+                        .get(j)
+                        .get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH);
                 if (embeddedPath.equals(thatEmbeddedPath)) {
                     return j;
                 }
@@ -464,8 +430,7 @@ public class ExtractComparer extends AbstractProfiler {
         return -1;
     }
 
-    private int findMatchingDigests(String sharedDigestKey, Set<Integer> handledB,
-                                    Metadata metadata, List<Metadata> metadataListB) {
+    private int findMatchingDigests(String sharedDigestKey, Set<Integer> handledB, Metadata metadata, List<Metadata> metadataListB) {
         String digestA = metadata.get(sharedDigestKey);
         if (digestA == null) {
             return -1;
@@ -492,17 +457,14 @@ public class ExtractComparer extends AbstractProfiler {
     private void writeContrasts(Map<Cols, String> data, ContrastStatistics contrastStatistics) {
         writeContrastString(data, Cols.TOP_10_MORE_IN_A, contrastStatistics.getTopNMoreA());
         writeContrastString(data, Cols.TOP_10_MORE_IN_B, contrastStatistics.getTopNMoreB());
-        writeContrastString(data, Cols.TOP_10_UNIQUE_TOKEN_DIFFS_A,
-                contrastStatistics.getTopNUniqueA());
-        writeContrastString(data, Cols.TOP_10_UNIQUE_TOKEN_DIFFS_B,
-                contrastStatistics.getTopNUniqueB());
+        writeContrastString(data, Cols.TOP_10_UNIQUE_TOKEN_DIFFS_A, contrastStatistics.getTopNUniqueA());
+        writeContrastString(data, Cols.TOP_10_UNIQUE_TOKEN_DIFFS_B, contrastStatistics.getTopNUniqueB());
         data.put(Cols.OVERLAP, Double.toString(contrastStatistics.getOverlap()));
         data.put(Cols.DICE_COEFFICIENT, Double.toString(contrastStatistics.getDiceCoefficient()));
 
     }
 
-    private void writeContrastString(Map<Cols, String> data, Cols col,
-                                     TokenIntPair[] tokenIntPairs) {
+    private void writeContrastString(Map<Cols, String> data, Cols col, TokenIntPair[] tokenIntPairs) {
 
         int i = 0;
         StringBuilder sb = new StringBuilder();
@@ -510,7 +472,10 @@ public class ExtractComparer extends AbstractProfiler {
             if (i++ > 0) {
                 sb.append(" | ");
             }
-            sb.append(p.getToken()).append(": ").append(p.getValue());
+            sb
+                    .append(p.getToken())
+                    .append(": ")
+                    .append(p.getValue());
         }
         data.put(col, sb.toString());
     }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractProfiler.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractProfiler.java
index 4e7d45088..22889d73b 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractProfiler.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/ExtractProfiler.java
@@ -43,78 +43,36 @@ public class ExtractProfiler extends AbstractProfiler {
 
     private final static String FIELD = "f";
     public static TableInfo EXTRACT_EXCEPTION_TABLE =
-            new TableInfo("extract_exceptions", new ColInfo(Cols.CONTAINER_ID, Types.INTEGER),
-                    new ColInfo(Cols.FILE_PATH, Types.VARCHAR, FILE_PATH_MAX_LEN),
-                    new ColInfo(Cols.EXTRACT_EXCEPTION_ID, Types.INTEGER),
-                    new ColInfo(Cols.PARSE_ERROR_ID, Types.INTEGER));
+            new TableInfo("extract_exceptions", new ColInfo(Cols.CONTAINER_ID, Types.INTEGER), new ColInfo(Cols.FILE_PATH, Types.VARCHAR, FILE_PATH_MAX_LEN),
+                    new ColInfo(Cols.EXTRACT_EXCEPTION_ID, Types.INTEGER), new ColInfo(Cols.PARSE_ERROR_ID, Types.INTEGER));
     public static TableInfo EXCEPTION_TABLE =
-            new TableInfo("parse_exceptions", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.ORIG_STACK_TRACE, Types.VARCHAR, 8192),
-                    new ColInfo(Cols.SORT_STACK_TRACE, Types.VARCHAR, 8192),
-                    new ColInfo(Cols.PARSE_EXCEPTION_ID, Types.INTEGER));
-    public static TableInfo CONTAINER_TABLE = new TableInfo("containers",
-            new ColInfo(Cols.CONTAINER_ID, Types.INTEGER, "PRIMARY KEY"),
-            new ColInfo(Cols.FILE_PATH, Types.VARCHAR, FILE_PATH_MAX_LEN),
-            new ColInfo(Cols.LENGTH, Types.BIGINT),
-            new ColInfo(Cols.EXTRACT_FILE_LENGTH, Types.BIGINT));
-    public static TableInfo PROFILE_TABLE =
-            new TableInfo("profiles", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.CONTAINER_ID, Types.INTEGER),
-                    new ColInfo(Cols.FILE_NAME, Types.VARCHAR, 256),
-                    new ColInfo(Cols.MD5, Types.CHAR, 32), new ColInfo(Cols.LENGTH, Types.BIGINT),
-                    new ColInfo(Cols.IS_EMBEDDED, Types.BOOLEAN),
-                    new ColInfo(Cols.EMBEDDED_DEPTH, Types.INTEGER),
-                    new ColInfo(Cols.EMBEDDED_FILE_PATH, Types.VARCHAR, 1024),
-                    new ColInfo(Cols.ATTACHMENT_TYPE, Types.VARCHAR, 32),
-                    new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 12),
-                    new ColInfo(Cols.MIME_ID, Types.INTEGER),
-                    new ColInfo(Cols.ELAPSED_TIME_MILLIS, Types.INTEGER),
-                    new ColInfo(Cols.NUM_ATTACHMENTS, Types.INTEGER),
-                    new ColInfo(Cols.NUM_METADATA_VALUES, Types.INTEGER),
-                    new ColInfo(Cols.NUM_PAGES, Types.INTEGER),
-                    new ColInfo(Cols.NUM_OCR_PAGES, Types.INTEGER),
-                    new ColInfo(Cols.HAS_CONTENT, Types.BOOLEAN));
+            new TableInfo("parse_exceptions", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.ORIG_STACK_TRACE, Types.VARCHAR, 8192),
+                    new ColInfo(Cols.SORT_STACK_TRACE, Types.VARCHAR, 8192), new ColInfo(Cols.PARSE_EXCEPTION_ID, Types.INTEGER));
+    public static TableInfo CONTAINER_TABLE =
+            new TableInfo("containers", new ColInfo(Cols.CONTAINER_ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.FILE_PATH, Types.VARCHAR, FILE_PATH_MAX_LEN),
+                    new ColInfo(Cols.LENGTH, Types.BIGINT), new ColInfo(Cols.EXTRACT_FILE_LENGTH, Types.BIGINT));
+    public static TableInfo PROFILE_TABLE = new TableInfo("profiles", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.CONTAINER_ID, Types.INTEGER),
+            new ColInfo(Cols.FILE_NAME, Types.VARCHAR, 256), new ColInfo(Cols.MD5, Types.CHAR, 32), new ColInfo(Cols.LENGTH, Types.BIGINT),
+            new ColInfo(Cols.IS_EMBEDDED, Types.BOOLEAN), new ColInfo(Cols.EMBEDDED_DEPTH, Types.INTEGER), new ColInfo(Cols.EMBEDDED_FILE_PATH, Types.VARCHAR, 1024),
+            new ColInfo(Cols.ATTACHMENT_TYPE, Types.VARCHAR, 32), new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 12), new ColInfo(Cols.MIME_ID, Types.INTEGER),
+            new ColInfo(Cols.ELAPSED_TIME_MILLIS, Types.INTEGER), new ColInfo(Cols.NUM_ATTACHMENTS, Types.INTEGER), new ColInfo(Cols.NUM_METADATA_VALUES, Types.INTEGER),
+            new ColInfo(Cols.NUM_PAGES, Types.INTEGER), new ColInfo(Cols.NUM_OCR_PAGES, Types.INTEGER), new ColInfo(Cols.HAS_CONTENT, Types.BOOLEAN));
     public static TableInfo EMBEDDED_FILE_PATH_TABLE =
-            new TableInfo("emb_file_names", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.EMBEDDED_FILE_PATH, Types.VARCHAR, 1024));
-    public static TableInfo CONTENTS_TABLE =
-            new TableInfo("contents", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.CONTENT_LENGTH, Types.INTEGER),
-                    new ColInfo(Cols.NUM_UNIQUE_TOKENS, Types.INTEGER),
-                    new ColInfo(Cols.NUM_TOKENS, Types.INTEGER),
-                    new ColInfo(Cols.COMMON_TOKENS_LANG, Types.VARCHAR, 12),
-                    new ColInfo(Cols.NUM_UNIQUE_COMMON_TOKENS, Types.INTEGER),
-                    new ColInfo(Cols.NUM_COMMON_TOKENS, Types.INTEGER),
-                    new ColInfo(Cols.NUM_UNIQUE_ALPHABETIC_TOKENS, Types.INTEGER),
-                    new ColInfo(Cols.NUM_ALPHABETIC_TOKENS, Types.INTEGER),
-                    new ColInfo(Cols.OOV, Types.DOUBLE),
-                    new ColInfo(Cols.TOP_N_TOKENS, Types.VARCHAR, 1024),
-                    new ColInfo(Cols.LANG_ID_1, Types.VARCHAR, 12),
-                    new ColInfo(Cols.LANG_ID_PROB_1, Types.FLOAT),
-                    new ColInfo(Cols.LANG_ID_2, Types.VARCHAR, 12),
-                    new ColInfo(Cols.LANG_ID_PROB_2, Types.FLOAT),
-                    new ColInfo(Cols.UNICODE_CHAR_BLOCKS, Types.VARCHAR, 1024),
-                    new ColInfo(Cols.TOKEN_ENTROPY_RATE, Types.FLOAT),
-                    new ColInfo(Cols.TOKEN_LENGTH_SUM, Types.INTEGER),
-                    new ColInfo(Cols.TOKEN_LENGTH_MEAN, Types.FLOAT),
-                    new ColInfo(Cols.TOKEN_LENGTH_STD_DEV, Types.FLOAT),
-                    new ColInfo(Cols.CONTENT_TRUNCATED_AT_MAX_LEN, Types.BOOLEAN));
+            new TableInfo("emb_file_names", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.EMBEDDED_FILE_PATH, Types.VARCHAR, 1024));
+    public static TableInfo CONTENTS_TABLE = new TableInfo("contents", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.CONTENT_LENGTH, Types.INTEGER),
+            new ColInfo(Cols.NUM_UNIQUE_TOKENS, Types.INTEGER), new ColInfo(Cols.NUM_TOKENS, Types.INTEGER), new ColInfo(Cols.COMMON_TOKENS_LANG, Types.VARCHAR, 12),
+            new ColInfo(Cols.NUM_UNIQUE_COMMON_TOKENS, Types.INTEGER), new ColInfo(Cols.NUM_COMMON_TOKENS, Types.INTEGER),
+            new ColInfo(Cols.NUM_UNIQUE_ALPHABETIC_TOKENS, Types.INTEGER), new ColInfo(Cols.NUM_ALPHABETIC_TOKENS, Types.INTEGER), new ColInfo(Cols.OOV, Types.DOUBLE),
+            new ColInfo(Cols.TOP_N_TOKENS, Types.VARCHAR, 1024), new ColInfo(Cols.LANG_ID_1, Types.VARCHAR, 12), new ColInfo(Cols.LANG_ID_PROB_1, Types.FLOAT),
+            new ColInfo(Cols.LANG_ID_2, Types.VARCHAR, 12), new ColInfo(Cols.LANG_ID_PROB_2, Types.FLOAT), new ColInfo(Cols.UNICODE_CHAR_BLOCKS, Types.VARCHAR, 1024),
+            new ColInfo(Cols.TOKEN_ENTROPY_RATE, Types.FLOAT), new ColInfo(Cols.TOKEN_LENGTH_SUM, Types.INTEGER), new ColInfo(Cols.TOKEN_LENGTH_MEAN, Types.FLOAT),
+            new ColInfo(Cols.TOKEN_LENGTH_STD_DEV, Types.FLOAT), new ColInfo(Cols.CONTENT_TRUNCATED_AT_MAX_LEN, Types.BOOLEAN));
     public static TableInfo TAGS_TABLE =
-            new TableInfo("tags", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.TAGS_A, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_B, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_DIV, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_I, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_IMG, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_LI, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_OL, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_P, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_TABLE, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_TD, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_TITLE, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_TR, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_U, Types.INTEGER),
-                    new ColInfo(Cols.TAGS_UL, Types.INTEGER),
+            new TableInfo("tags", new ColInfo(Cols.ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.TAGS_A, Types.INTEGER), new ColInfo(Cols.TAGS_B, Types.INTEGER),
+                    new ColInfo(Cols.TAGS_DIV, Types.INTEGER), new ColInfo(Cols.TAGS_I, Types.INTEGER), new ColInfo(Cols.TAGS_IMG, Types.INTEGER),
+                    new ColInfo(Cols.TAGS_LI, Types.INTEGER), new ColInfo(Cols.TAGS_OL, Types.INTEGER), new ColInfo(Cols.TAGS_P, Types.INTEGER),
+                    new ColInfo(Cols.TAGS_TABLE, Types.INTEGER), new ColInfo(Cols.TAGS_TD, Types.INTEGER), new ColInfo(Cols.TAGS_TITLE, Types.INTEGER),
+                    new ColInfo(Cols.TAGS_TR, Types.INTEGER), new ColInfo(Cols.TAGS_U, Types.INTEGER), new ColInfo(Cols.TAGS_UL, Types.INTEGER),
                     new ColInfo(Cols.TAGS_PARSE_EXCEPTION, Types.BOOLEAN));
     static Options OPTIONS;
 
@@ -123,32 +81,28 @@ public class ExtractProfiler extends AbstractProfiler {
         Option extracts = new Option("extracts", true, "directory for extract files");
         extracts.setRequired(true);
 
-        Option inputDir = new Option("inputDir", true,
-                "optional: directory for original binary input documents." +
-                        " If not specified, -extracts is crawled as is.");
+        Option inputDir = new Option("inputDir", true, "optional: directory for original binary input documents." + " If not specified, -extracts is crawled as is.");
 
-        OPTIONS = new Options().addOption(extracts).addOption(inputDir)
+        OPTIONS = new Options()
+                .addOption(extracts)
+                .addOption(inputDir)
                 .addOption("bc", "optional: tika-batch config file")
-                .addOption("numConsumers", true, "optional: number of consumer threads").addOption(
-                        new Option("alterExtract", true, "for json-formatted extract files, " +
-                                "process full metadata list ('as_is'=default), " +
-                                "take just the first/container document ('first_only'), " +
+                .addOption("numConsumers", true, "optional: number of consumer threads")
+                .addOption(new Option("alterExtract", true,
+                        "for json-formatted extract files, " + "process full metadata list ('as_is'=default), " + "take just the first/container document ('first_only'), " +
                                 "concatenate all content into the first metadata item ('concatenate_content')"))
                 .addOption("minExtractLength", true, "minimum extract length to process (in bytes)")
                 .addOption("maxExtractLength", true, "maximum extract length to process (in bytes)")
-                .addOption("db", true, "db file to which to write results").addOption("jdbc", true,
-                        "EXPERT: full jdbc connection string. Must specify this or -db <h2db>")
+                .addOption("db", true, "db file to which to write results")
+                .addOption("jdbc", true, "EXPERT: full jdbc connection string. Must specify this or -db <h2db>")
                 .addOption("jdbcDriver", true, "EXPERT: jdbc driver, or specify via -Djdbc.driver")
                 .addOption("tablePrefix", true, "EXPERT: optional prefix for table names")
                 .addOption("drop", false, "drop tables if they exist")
                 .addOption("maxFilesToAdd", true, "maximum number of files to add to the crawler")
                 .addOption("maxTokens", true, "maximum tokens to process, default=200000")
-                .addOption("maxContentLength", true,
-                        "truncate content beyond this length for calculating 'contents' stats, default=1000000")
-                .addOption("maxContentLengthForLangId", true,
-                        "truncate content beyond this length for language id, default=50000")
-                .addOption("defaultLangCode", true,
-                        "which language to use for common words if no 'common words' file exists for the langid result")
+                .addOption("maxContentLength", true, "truncate content beyond this length for calculating 'contents' stats, default=1000000")
+                .addOption("maxContentLengthForLangId", true, "truncate content beyond this length for language id, default=50000")
+                .addOption("defaultLangCode", true, "which language to use for common words if no 'common words' file exists for the langid result")
 
         ;
 
@@ -158,8 +112,7 @@ public class ExtractProfiler extends AbstractProfiler {
     private final Path extracts;
     private final ExtractReader extractReader;
 
-    public ExtractProfiler(ArrayBlockingQueue<FileResource> queue, Path inputDir, Path extracts,
-                           ExtractReader extractReader, IDBWriter dbWriter) {
+    public ExtractProfiler(ArrayBlockingQueue<FileResource> queue, Path inputDir, Path extracts, ExtractReader extractReader, IDBWriter dbWriter) {
         super(queue, dbWriter);
         this.inputDir = inputDir;
         this.extracts = extracts;
@@ -168,9 +121,7 @@ public class ExtractProfiler extends AbstractProfiler {
 
     public static void USAGE() {
         HelpFormatter helpFormatter = new HelpFormatter();
-        helpFormatter.printHelp(80,
-                "java -jar tika-eval-x.y.jar Profile -extracts extracts -db mydb [-inputDir input]",
-                "Tool: Profile", ExtractProfiler.OPTIONS,
+        helpFormatter.printHelp(80, "java -jar tika-eval-x.y.jar Profile -extracts extracts -db mydb [-inputDir input]", "Tool: Profile", ExtractProfiler.OPTIONS,
                 "Note: for the default h2 db, do not include the .mv.db at the end of the db name.");
     }
 
@@ -199,14 +150,14 @@ public class ExtractProfiler extends AbstractProfiler {
 
         Map<Cols, String> contOutput = new HashMap<>();
         long srcFileLen = getSourceFileLength(fps, metadataList);
-        contOutput.put(Cols.LENGTH,
-                srcFileLen > NON_EXISTENT_FILE_LENGTH ? Long.toString(srcFileLen) : "");
+        contOutput.put(Cols.LENGTH, srcFileLen > NON_EXISTENT_FILE_LENGTH ? Long.toString(srcFileLen) : "");
         contOutput.put(Cols.CONTAINER_ID, containerIdString);
-        contOutput.put(Cols.FILE_PATH, fps.getRelativeSourceFilePath().toString());
+        contOutput.put(Cols.FILE_PATH, fps
+                .getRelativeSourceFilePath()
+                .toString());
 
         if (fps.getExtractFileLength() > 0) {
-            contOutput.put(Cols.EXTRACT_FILE_LENGTH,
-                    (fps.getExtractFile() == null) ? "" : Long.toString(fps.getExtractFileLength()));
+            contOutput.put(Cols.EXTRACT_FILE_LENGTH, (fps.getExtractFile() == null) ? "" : Long.toString(fps.getExtractFileLength()));
         }
         try {
             writer.writeRow(CONTAINER_TABLE, contOutput);
@@ -217,8 +168,9 @@ public class ExtractProfiler extends AbstractProfiler {
 
         if (extractExceptionType != null) {
             try {
-                writeExtractException(EXTRACT_EXCEPTION_TABLE, containerIdString,
-                        fps.getRelativeSourceFilePath().toString(), extractExceptionType);
+                writeExtractException(EXTRACT_EXCEPTION_TABLE, containerIdString, fps
+                        .getRelativeSourceFilePath()
+                        .toString(), extractExceptionType);
             } catch (IOException e) {
                 throw new RuntimeException(e);
             }
@@ -232,8 +184,7 @@ public class ExtractProfiler extends AbstractProfiler {
             //the first file should have the same id as the container id
             String fileId = (i == 0) ? containerIdString : Integer.toString(ID.incrementAndGet());
             writeTagData(fileId, contentTags, TAGS_TABLE);
-            writeProfileData(fps, i, contentTags, m, fileId, containerIdString, numAttachments,
-                    PROFILE_TABLE);
+            writeProfileData(fps, i, contentTags, m, fileId, containerIdString, numAttachments, PROFILE_TABLE);
             writeEmbeddedPathData(i, fileId, m, EMBEDDED_FILE_PATH_TABLE);
             writeExceptionData(fileId, m, EXCEPTION_TABLE);
             try {
@@ -248,8 +199,7 @@ public class ExtractProfiler extends AbstractProfiler {
     }
 
 
-    private void writeEmbeddedPathData(int i, String fileId, Metadata m,
-                                       TableInfo embeddedFilePathTable) {
+    private void writeEmbeddedPathData(int i, String fileId, Metadata m, TableInfo embeddedFilePathTable) {
         if (i == 0) {
             return;
         }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/FileProfiler.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/FileProfiler.java
index 28023f2f6..925452094 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/FileProfiler.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/FileProfiler.java
@@ -61,39 +61,30 @@ public class FileProfiler extends AbstractProfiler {
     private static final Logger LOG = LoggerFactory.getLogger(FileProfiler.class);
     private static final Tika TIKA = new Tika();
     private static final FileCommandDetector FILE_COMMAND_DETECTOR = new FileCommandDetector();
-    public static TableInfo FILE_PROFILES = HAS_FILE ? new TableInfo("file_profiles",
-            new ColInfo(Cols.FILE_PATH, Types.VARCHAR, 2048, "PRIMARY KEY"),
-            new ColInfo(Cols.FILE_NAME, Types.VARCHAR, 2048),
-            new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 24),
-            new ColInfo(Cols.LENGTH, Types.BIGINT), new ColInfo(Cols.SHA256, Types.VARCHAR, 64),
-            new ColInfo(Cols.TIKA_MIME_ID, Types.INTEGER),
-            new ColInfo(Cols.FILE_MIME_ID, Types.INTEGER)) :
-            new TableInfo("file_profiles",
-                    new ColInfo(Cols.FILE_PATH, Types.VARCHAR, 2048, "PRIMARY KEY"),
-                    new ColInfo(Cols.FILE_NAME, Types.VARCHAR, 2048),
-                    new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 24),
-                    new ColInfo(Cols.LENGTH, Types.BIGINT), new ColInfo(Cols.SHA256, Types.VARCHAR,
-                    64),
+    public static TableInfo FILE_PROFILES = HAS_FILE ?
+            new TableInfo("file_profiles", new ColInfo(Cols.FILE_PATH, Types.VARCHAR, 2048, "PRIMARY KEY"), new ColInfo(Cols.FILE_NAME, Types.VARCHAR, 2048),
+                    new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 24), new ColInfo(Cols.LENGTH, Types.BIGINT), new ColInfo(Cols.SHA256, Types.VARCHAR, 64),
+                    new ColInfo(Cols.TIKA_MIME_ID, Types.INTEGER), new ColInfo(Cols.FILE_MIME_ID, Types.INTEGER)) :
+            new TableInfo("file_profiles", new ColInfo(Cols.FILE_PATH, Types.VARCHAR, 2048, "PRIMARY KEY"), new ColInfo(Cols.FILE_NAME, Types.VARCHAR, 2048),
+                    new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 24), new ColInfo(Cols.LENGTH, Types.BIGINT), new ColInfo(Cols.SHA256, Types.VARCHAR, 64),
                     new ColInfo(Cols.TIKA_MIME_ID, Types.INTEGER));
 
 
     public static TableInfo FILE_MIME_TABLE =
-            new TableInfo("file_mimes", new ColInfo(Cols.MIME_ID, Types.INTEGER, "PRIMARY KEY"),
-                    new ColInfo(Cols.MIME_STRING, Types.VARCHAR, 256),
+            new TableInfo("file_mimes", new ColInfo(Cols.MIME_ID, Types.INTEGER, "PRIMARY KEY"), new ColInfo(Cols.MIME_STRING, Types.VARCHAR, 256),
                     new ColInfo(Cols.FILE_EXTENSION, Types.VARCHAR, 12));
     static Options OPTIONS;
 
     static {
 
-        Option inputDir = new Option("inputDir", true,
-                "optional: directory for original binary input documents." +
-                        " If not specified, -extracts is crawled as is.");
+        Option inputDir = new Option("inputDir", true, "optional: directory for original binary input documents." + " If not specified, -extracts is crawled as is.");
 
-        OPTIONS = new Options().addOption(inputDir)
+        OPTIONS = new Options()
+                .addOption(inputDir)
                 .addOption("bc", "optional: tika-batch config file")
                 .addOption("numConsumers", true, "optional: number of consumer threads")
-                .addOption("db", true, "db file to which to write results").addOption("jdbc", true,
-                        "EXPERT: full jdbc connection string. Must specify this or -db <h2db>")
+                .addOption("db", true, "db file to which to write results")
+                .addOption("jdbc", true, "EXPERT: full jdbc connection string. Must specify this or -db <h2db>")
                 .addOption("jdbcDriver", true, "EXPERT: jdbc driver, or specify via -Djdbc.driver")
                 .addOption("tablePrefix", true, "EXPERT: optional prefix for table names")
                 .addOption("drop", false, "drop tables if they exist")
@@ -105,23 +96,22 @@ public class FileProfiler extends AbstractProfiler {
 
     private final Path inputDir;
 
-    public FileProfiler(ArrayBlockingQueue<FileResource> fileQueue, Path inputDir,
-                        IDBWriter dbWriter) {
+    public FileProfiler(ArrayBlockingQueue<FileResource> fileQueue, Path inputDir, IDBWriter dbWriter) {
         super(fileQueue, dbWriter);
         this.inputDir = inputDir;
     }
 
     public static void USAGE() {
         HelpFormatter helpFormatter = new HelpFormatter();
-        helpFormatter.printHelp(80,
-                "java -jar tika-eval-x.y.jar FileProfiler -inputDir docs -db mydb [-inputDir input]",
-                "Tool: Profile", FileProfiler.OPTIONS,
+        helpFormatter.printHelp(80, "java -jar tika-eval-x.y.jar FileProfiler -inputDir docs -db mydb [-inputDir input]", "Tool: Profile", FileProfiler.OPTIONS,
                 "Note: for the default h2 db, do not include the .mv.db at the end of the db name.");
     }
 
     @Override
     public boolean processFileResource(FileResource fileResource) {
-        String relPath = fileResource.getMetadata().get(FSProperties.FS_REL_PATH);
+        String relPath = fileResource
+                .getMetadata()
+                .get(FSProperties.FS_REL_PATH);
         try (InputStream is = fileResource.openInputStream()) {
             try (TikaInputStream tis = TikaInputStream.get(is)) {
                 Path path = tis.getPath();
@@ -169,7 +159,9 @@ public class FileProfiler extends AbstractProfiler {
 
     private String detectFile(TikaInputStream tis) {
         try {
-            return FILE_COMMAND_DETECTOR.detect(tis, new Metadata()).toString();
+            return FILE_COMMAND_DETECTOR
+                    .detect(tis, new Metadata())
+                    .toString();
         } catch (IOException e) {
             return DETECT_EXCEPTION;
         }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/TikaEvalCLI.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/TikaEvalCLI.java
index d2679099a..a897461ee 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/TikaEvalCLI.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/TikaEvalCLI.java
@@ -40,7 +40,9 @@ public class TikaEvalCLI {
         StringBuilder sb = new StringBuilder();
         sb.append("Must specify one of the following tools in the first parameter:\n");
         for (String s : tools) {
-            sb.append(s).append("\n");
+            sb
+                    .append(s)
+                    .append("\n");
         }
         return sb.toString();
 
@@ -98,12 +100,15 @@ public class TikaEvalCLI {
         try {
             tmpBCConfig = Files.createTempFile("tika-eval-profiler", ".xml");
             if (!containsBC) {
-                try (InputStream is = this.getClass()
+                try (InputStream is = this
+                        .getClass()
                         .getResourceAsStream("/tika-eval-file-profiler-config.xml")) {
                     Files.copy(is, tmpBCConfig, StandardCopyOption.REPLACE_EXISTING);
                 }
                 argList.add("-bc");
-                argList.add(tmpBCConfig.toAbsolutePath().toString());
+                argList.add(tmpBCConfig
+                        .toAbsolutePath()
+                        .toString());
             }
 
             String[] updatedArgs = argList.toArray(new String[0]);
@@ -111,8 +116,7 @@ public class TikaEvalCLI {
             try {
                 CommandLine commandLine = defaultCLIParser.parse(FileProfiler.OPTIONS, updatedArgs);
                 if (commandLine.hasOption("db") && commandLine.hasOption("jdbc")) {
-                    System.out.println(
-                            "Please specify either the default -db or the full -jdbc, not both");
+                    System.out.println("Please specify either the default -db or the full -jdbc, not both");
                     ExtractProfiler.USAGE();
                     return;
                 }
@@ -124,7 +128,9 @@ public class TikaEvalCLI {
 
             // lazy delete because main() calls System.exit()
             if (tmpBCConfig != null && Files.isRegularFile(tmpBCConfig)) {
-                tmpBCConfig.toFile().deleteOnExit();
+                tmpBCConfig
+                        .toFile()
+                        .deleteOnExit();
             }
             FSBatchProcessCLI.main(updatedArgs);
         } finally {
@@ -181,8 +187,7 @@ public class TikaEvalCLI {
                     break;
                 case "-alterExtract":
                     if (i + 1 >= argList.size()) {
-                        System.err.println("Must specify type 'as_is', 'first_only' or " +
-                                           "'concatenate_content' after -alterExtract");
+                        System.err.println("Must specify type 'as_is', 'first_only' or " + "'concatenate_content' after -alterExtract");
                         ExtractComparer.USAGE();
                         return;
                     }
@@ -192,10 +197,8 @@ public class TikaEvalCLI {
             }
         }
 
-        if (alterExtract != null && !alterExtract.equals("as_is") &&
-                !alterExtract.equals("concatenate_content") && !alterExtract.equals("first_only")) {
-            System.out.println("Sorry, I don't understand:" + alterExtract +
-                    ". The values must be one of: as_is, first_only, concatenate_content");
+        if (alterExtract != null && !alterExtract.equals("as_is") && !alterExtract.equals("concatenate_content") && !alterExtract.equals("first_only")) {
+            System.out.println("Sorry, I don't understand:" + alterExtract + ". The values must be one of: as_is, first_only, concatenate_content");
             ExtractProfiler.USAGE();
             return;
         }
@@ -216,22 +219,23 @@ public class TikaEvalCLI {
         try {
             tmpBCConfig = Files.createTempFile("tika-eval-profiler", ".xml");
             if (!containsBC) {
-                try (InputStream is = this.getClass()
+                try (InputStream is = this
+                        .getClass()
                         .getResourceAsStream("/tika-eval-profiler-config.xml")) {
                     Files.copy(is, tmpBCConfig, StandardCopyOption.REPLACE_EXISTING);
                 }
                 argList.add("-bc");
-                argList.add(tmpBCConfig.toAbsolutePath().toString());
+                argList.add(tmpBCConfig
+                        .toAbsolutePath()
+                        .toString());
             }
 
             String[] updatedArgs = argList.toArray(new String[0]);
             DefaultParser defaultCLIParser = new DefaultParser();
             try {
-                CommandLine commandLine =
-                        defaultCLIParser.parse(ExtractProfiler.OPTIONS, updatedArgs);
+                CommandLine commandLine = defaultCLIParser.parse(ExtractProfiler.OPTIONS, updatedArgs);
                 if (commandLine.hasOption("db") && commandLine.hasOption("jdbc")) {
-                    System.out.println(
-                            "Please specify either the default -db or the full -jdbc, not both");
+                    System.out.println("Please specify either the default -db or the full -jdbc, not both");
                     ExtractProfiler.USAGE();
                     return;
                 }
@@ -243,7 +247,9 @@ public class TikaEvalCLI {
 
             // lazy delete because main() calls System.exit()
             if (tmpBCConfig != null && Files.isRegularFile(tmpBCConfig)) {
-                tmpBCConfig.toFile().deleteOnExit();
+                tmpBCConfig
+                        .toFile()
+                        .deleteOnExit();
             }
             FSBatchProcessCLI.main(updatedArgs);
         } finally {
@@ -287,8 +293,7 @@ public class TikaEvalCLI {
                     break;
                 case "-alterExtract":
                     if (i + 1 >= argList.size()) {
-                        System.err.println("Must specify type 'as_is', 'first_only' or " +
-                                           "'concatenate_content' after -alterExtract");
+                        System.err.println("Must specify type 'as_is', 'first_only' or " + "'concatenate_content' after -alterExtract");
                         ExtractComparer.USAGE();
                         return;
                     }
@@ -297,10 +302,8 @@ public class TikaEvalCLI {
                     break;
             }
         }
-        if (alterExtract != null && !alterExtract.equals("as_is") &&
-                !alterExtract.equals("concatenate_content") && !alterExtract.equals("first_only")) {
-            System.out.println("Sorry, I don't understand:" + alterExtract +
-                    ". The values must be one of: as_is, first_only, concatenate_content");
+        if (alterExtract != null && !alterExtract.equals("as_is") && !alterExtract.equals("concatenate_content") && !alterExtract.equals("first_only")) {
+            System.out.println("Sorry, I don't understand:" + alterExtract + ". The values must be one of: as_is, first_only, concatenate_content");
             ExtractComparer.USAGE();
             return;
         }
@@ -318,22 +321,23 @@ public class TikaEvalCLI {
         try {
             tmpBCConfig = Files.createTempFile("tika-eval", ".xml");
             if (!containsBC) {
-                try (InputStream is = this.getClass()
+                try (InputStream is = this
+                        .getClass()
                         .getResourceAsStream("/tika-eval-comparison-config.xml")) {
                     Files.copy(is, tmpBCConfig, StandardCopyOption.REPLACE_EXISTING);
                 }
                 argList.add("-bc");
-                argList.add(tmpBCConfig.toAbsolutePath().toString());
+                argList.add(tmpBCConfig
+                        .toAbsolutePath()
+                        .toString());
 
             }
             String[] updatedArgs = argList.toArray(new String[0]);
             DefaultParser defaultCLIParser = new DefaultParser();
             try {
-                CommandLine commandLine =
-                        defaultCLIParser.parse(ExtractComparer.OPTIONS, updatedArgs);
+                CommandLine commandLine = defaultCLIParser.parse(ExtractComparer.OPTIONS, updatedArgs);
                 if (commandLine.hasOption("db") && commandLine.hasOption("jdbc")) {
-                    System.out.println(
-                            "Please specify either the default -db or the full -jdbc, not both");
+                    System.out.println("Please specify either the default -db or the full -jdbc, not both");
                     ExtractComparer.USAGE();
                     return;
                 }
@@ -345,7 +349,9 @@ public class TikaEvalCLI {
 
             // lazy delete because main() calls System.exit()
             if (tmpBCConfig != null && Files.isRegularFile(tmpBCConfig)) {
-                tmpBCConfig.toFile().deleteOnExit();
+                tmpBCConfig
+                        .toFile()
+                        .deleteOnExit();
             }
             FSBatchProcessCLI.main(updatedArgs);
         } finally {
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/XMLErrorLogUpdater.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/XMLErrorLogUpdater.java
index 1df0a070c..81ae5f2be 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/XMLErrorLogUpdater.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/XMLErrorLogUpdater.java
@@ -65,15 +65,15 @@ public class XMLErrorLogUpdater {
         connection.commit();
     }
 
-    public void update(Connection connection, TableInfo tableInfo, Path xmlLogFile)
-            throws Exception {
+    public void update(Connection connection, TableInfo tableInfo, Path xmlLogFile) throws Exception {
         statement = connection.createStatement();
         XMLLogReader reader = new XMLLogReader();
         try (InputStream is = Files.newInputStream(xmlLogFile)) {
             reader.read(is, new ErrorMsgUpdater(tableInfo.getName()));
         } catch (IOException e) {
-            throw new RuntimeException(
-                    "Problem reading: " + xmlLogFile.toAbsolutePath().toString());
+            throw new RuntimeException("Problem reading: " + xmlLogFile
+                    .toAbsolutePath()
+                    .toString());
         } finally {
             try {
                 connection.commit();
@@ -98,7 +98,9 @@ public class XMLErrorLogUpdater {
             }
             XMLStreamReader reader = null;
             try {
-                reader = XMLInputFactory.newInstance().createXMLStreamReader(new StringReader(xml));
+                reader = XMLInputFactory
+                        .newInstance()
+                        .createXMLStreamReader(new StringReader(xml));
             } catch (XMLStreamException e) {
                 throw new IOException(e);
             }
@@ -111,13 +113,11 @@ public class XMLErrorLogUpdater {
                         case XMLStreamConstants.START_ELEMENT:
                             if ("timed_out".equals(reader.getLocalName())) {
                                 resourceId = reader.getAttributeValue("", "resourceId");
-                                update(errorTablename, resourceId,
-                                        AbstractProfiler.PARSE_ERROR_TYPE.TIMEOUT);
+                                update(errorTablename, resourceId, AbstractProfiler.PARSE_ERROR_TYPE.TIMEOUT);
 
                             } else if ("oom".equals(reader.getLocalName())) {
                                 resourceId = reader.getAttributeValue("", "resourceId");
-                                update(errorTablename, resourceId,
-                                        AbstractProfiler.PARSE_ERROR_TYPE.OOM);
+                                update(errorTablename, resourceId, AbstractProfiler.PARSE_ERROR_TYPE.OOM);
                             }
                             break;
                     }
@@ -128,11 +128,9 @@ public class XMLErrorLogUpdater {
             }
         }
 
-        private void update(String errorTableName, String filePath,
-                            AbstractProfiler.PARSE_ERROR_TYPE type) throws SQLException {
+        private void update(String errorTableName, String filePath, AbstractProfiler.PARSE_ERROR_TYPE type) throws SQLException {
             int containerId = getContainerId(filePath);
-            String sql = "SELECT count(1) from " + errorTableName + " where " + Cols.CONTAINER_ID +
-                    " = " + containerId + " or " + Cols.FILE_PATH + "='" + filePath + "'";
+            String sql = "SELECT count(1) from " + errorTableName + " where " + Cols.CONTAINER_ID + " = " + containerId + " or " + Cols.FILE_PATH + "='" + filePath + "'";
             int hitCount;
             try (ResultSet rs = statement.executeQuery(sql)) {
                 //now try to figure out if that file already exists
@@ -145,23 +143,18 @@ public class XMLErrorLogUpdater {
 
             //if it does, update all records matching that path or container id
             if (hitCount > 0) {
-                sql = "UPDATE " + errorTableName + " SET " + Cols.PARSE_ERROR_ID + " = " +
-                        type.ordinal() + "," + Cols.FILE_PATH + "='" + filePath + "'" + " where " +
-                        Cols.CONTAINER_ID + "=" + containerId + " or " + Cols.FILE_PATH + "='" +
-                        filePath + "'";
+                sql = "UPDATE " + errorTableName + " SET " + Cols.PARSE_ERROR_ID + " = " + type.ordinal() + "," + Cols.FILE_PATH + "='" + filePath + "'" + " where " +
+                        Cols.CONTAINER_ID + "=" + containerId + " or " + Cols.FILE_PATH + "='" + filePath + "'";
 
             } else {
                 //if not and container id > -1
                 //insert full record
                 if (containerId > -1) {
-                    sql = "INSERT INTO " + errorTableName + " (" + Cols.CONTAINER_ID + "," +
-                            Cols.FILE_PATH + "," + Cols.PARSE_ERROR_ID + ")" + " values (" +
-                            containerId + ", '" + filePath + "'," + type.ordinal() + ");";
+                    sql = "INSERT INTO " + errorTableName + " (" + Cols.CONTAINER_ID + "," + Cols.FILE_PATH + "," + Cols.PARSE_ERROR_ID + ")" + " values (" + containerId + ", '" +
+                            filePath + "'," + type.ordinal() + ");";
                 } else {
                     //if container id == -1, insert only file path and parse error type id
-                    sql = "INSERT INTO " + errorTableName + " (" + Cols.FILE_PATH.name() + "," +
-                            Cols.PARSE_ERROR_ID + ")" + "values ('" + filePath + "'," +
-                            type.ordinal() + ");";
+                    sql = "INSERT INTO " + errorTableName + " (" + Cols.FILE_PATH.name() + "," + Cols.PARSE_ERROR_ID + ")" + "values ('" + filePath + "'," + type.ordinal() + ");";
                 }
 
             }
@@ -176,9 +169,7 @@ public class XMLErrorLogUpdater {
 
         private int getContainerId(String resourceId) throws SQLException {
             int containerId = -1;
-            String sql = "SELECT " + Cols.CONTAINER_ID.name() + " from " +
-                    ExtractProfiler.CONTAINER_TABLE.getName() + " where " + Cols.FILE_PATH + " ='" +
-                    resourceId + "'";
+            String sql = "SELECT " + Cols.CONTAINER_ID.name() + " from " + ExtractProfiler.CONTAINER_TABLE.getName() + " where " + Cols.FILE_PATH + " ='" + resourceId + "'";
             int resultCount;
             try (ResultSet rs = statement.executeQuery(sql)) {
                 resultCount = 0;
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/DBConsumersManager.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/DBConsumersManager.java
index d654027e9..aba35416f 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/DBConsumersManager.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/DBConsumersManager.java
@@ -38,8 +38,7 @@ public class DBConsumersManager extends ConsumersManager {
     private final MimeBuffer mimeBuffer;
     private final List<LogTablePair> errorLogs = new ArrayList<>();
 
-    public DBConsumersManager(JDBCUtil dbUtil, MimeBuffer mimeBuffer,
-                              List<FileResourceConsumer> consumers) throws SQLException {
+    public DBConsumersManager(JDBCUtil dbUtil, MimeBuffer mimeBuffer, List<FileResourceConsumer> consumers) throws SQLException {
         super(consumers);
         this.conn = dbUtil.getConnection();
         this.mimeBuffer = mimeBuffer;
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumerBuilder.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumerBuilder.java
index d8d790bc8..872fef530 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumerBuilder.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumerBuilder.java
@@ -54,8 +54,7 @@ public abstract class EvalConsumerBuilder {
     AtomicInteger initialized = new AtomicInteger(0);
     private MimeBuffer mimeBuffer;
 
-    public MimeBuffer init(ArrayBlockingQueue<FileResource> queue, Map<String, String> localAttrs,
-                           JDBCUtil dbUtil, boolean forceDrop) throws IOException, SQLException {
+    public MimeBuffer init(ArrayBlockingQueue<FileResource> queue, Map<String, String> localAttrs, JDBCUtil dbUtil, boolean forceDrop) throws IOException, SQLException {
         if (initialized.getAndIncrement() > 0) {
             throw new RuntimeException("Can only init a consumer builder once!");
         }
@@ -66,20 +65,16 @@ public abstract class EvalConsumerBuilder {
         //step 1. update the table names with prefixes
         updateTableInfosWithPrefixes(localAttrs);
 
-        JDBCUtil.CREATE_TABLE createRegularTable =
-                (forceDrop) ? JDBCUtil.CREATE_TABLE.DROP_IF_EXISTS :
-                        JDBCUtil.CREATE_TABLE.THROW_EX_IF_EXISTS;
+        JDBCUtil.CREATE_TABLE createRegularTable = (forceDrop) ? JDBCUtil.CREATE_TABLE.DROP_IF_EXISTS : JDBCUtil.CREATE_TABLE.THROW_EX_IF_EXISTS;
 
-        JDBCUtil.CREATE_TABLE createRefTable = (forceDrop) ? JDBCUtil.CREATE_TABLE.DROP_IF_EXISTS :
-                JDBCUtil.CREATE_TABLE.SKIP_IF_EXISTS;
+        JDBCUtil.CREATE_TABLE createRefTable = (forceDrop) ? JDBCUtil.CREATE_TABLE.DROP_IF_EXISTS : JDBCUtil.CREATE_TABLE.SKIP_IF_EXISTS;
 
         //step 2. create the tables
         dbUtil.createTables(getNonRefTableInfos(), createRegularTable);
         dbUtil.createTables(getRefTableInfos(), createRefTable);
 
         //step 3. create mime buffer
-        this.mimeBuffer = new MimeBuffer(dbUtil.getConnection(), getMimeTable(),
-                TikaConfig.getDefaultConfig());
+        this.mimeBuffer = new MimeBuffer(dbUtil.getConnection(), getMimeTable(), TikaConfig.getDefaultConfig());
 
         //step 4. populate the reference tables
         populateRefTables();
@@ -111,7 +106,8 @@ public abstract class EvalConsumerBuilder {
             Connection connection = dbUtil.getConnection();
             for (TableInfo tableInfo : getRefTableInfos()) {
                 int rows = 0;
-                try (ResultSet rs = connection.createStatement()
+                try (ResultSet rs = connection
+                        .createStatement()
                         .executeQuery("select * from " + tableInfo.getName())) {
                     while (rs.next()) {
                         rows++;
@@ -172,9 +168,7 @@ public abstract class EvalConsumerBuilder {
         } else if (alterExtractString.equalsIgnoreCase("concatenate_content")) {
             alterExtractList = ExtractReader.ALTER_METADATA_LIST.CONCATENATE_CONTENT_INTO_FIRST;
         } else {
-            throw new RuntimeException(
-                    "options for alterExtract: as_is, first_only, concatenate_content." +
-                            " I don't understand:" + alterExtractString);
+            throw new RuntimeException("options for alterExtract: as_is, first_only, concatenate_content." + " I don't understand:" + alterExtractString);
         }
         return alterExtractList;
     }
@@ -194,8 +188,7 @@ public abstract class EvalConsumerBuilder {
             abstractProfiler.setMaxContentLength(maxContentLength);
         }
 
-        int maxContentLengthForLangId =
-                PropsUtil.getInt(localAttrs.get("maxContentLengthForLangId"), -2);
+        int maxContentLengthForLangId = PropsUtil.getInt(localAttrs.get("maxContentLengthForLangId"), -2);
         if (maxContentLengthForLangId > -2) {
             abstractProfiler.setMaxContentLengthForLangId(maxContentLengthForLangId);
         }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumersBuilder.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumersBuilder.java
index cebaa8efe..520baaad5 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumersBuilder.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/EvalConsumersBuilder.java
@@ -44,8 +44,7 @@ import org.apache.tika.util.XMLDOMUtil;
 public class EvalConsumersBuilder extends AbstractConsumersBuilder {
 
     @Override
-    public ConsumersManager build(Node node, Map<String, String> runtimeAttributes,
-                                  ArrayBlockingQueue<FileResource> queue) {
+    public ConsumersManager build(Node node, Map<String, String> runtimeAttributes, ArrayBlockingQueue<FileResource> queue) {
 
         List<FileResourceConsumer> consumers = new LinkedList<>();
         int numConsumers = BatchProcessBuilder.getNumConsumers(runtimeAttributes);
@@ -77,8 +76,7 @@ public class EvalConsumersBuilder extends AbstractConsumersBuilder {
         } else {
             throw new RuntimeException("Must specify: -db or -jdbc");
         }
-        EvalConsumerBuilder consumerBuilder = ClassLoaderUtil.buildClass(EvalConsumerBuilder.class,
-                PropsUtil.getString(localAttrs.get("consumerBuilderClass"), null));
+        EvalConsumerBuilder consumerBuilder = ClassLoaderUtil.buildClass(EvalConsumerBuilder.class, PropsUtil.getString(localAttrs.get("consumerBuilderClass"), null));
         if (consumerBuilder == null) {
             throw new RuntimeException("Must specify consumerBuilderClass in config file");
         }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractComparerBuilder.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractComparerBuilder.java
index 85dfeb457..6788de49e 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractComparerBuilder.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractComparerBuilder.java
@@ -92,8 +92,7 @@ public class ExtractComparerBuilder extends EvalConsumerBuilder {
             throw new RuntimeException("Must specify an -inputDir");
         }
 
-        return parameterizeProfiler(new ExtractComparer(queue, inputRootDir, extractsA, extractsB,
-                buildExtractReader(localAttrs), getDBWriter(getNonRefTableInfos())));
+        return parameterizeProfiler(new ExtractComparer(queue, inputRootDir, extractsA, extractsB, buildExtractReader(localAttrs), getDBWriter(getNonRefTableInfos())));
     }
 
 
@@ -103,10 +102,8 @@ public class ExtractComparerBuilder extends EvalConsumerBuilder {
 
         String tablePrefixB = localAttrs.get(TABLE_PREFIX_B_KEY);
 
-        tablePrefixA = (tablePrefixA == null || tablePrefixA.endsWith("_")) ? tablePrefixA :
-                tablePrefixA + "_";
-        tablePrefixB = (tablePrefixB == null || tablePrefixB.endsWith("_")) ? tablePrefixB :
-                tablePrefixB + "_";
+        tablePrefixA = (tablePrefixA == null || tablePrefixA.endsWith("_")) ? tablePrefixA : tablePrefixA + "_";
+        tablePrefixB = (tablePrefixB == null || tablePrefixB.endsWith("_")) ? tablePrefixB : tablePrefixB + "_";
 
         if (tablePrefixA != null) {
             for (TableInfo tableInfo : tableInfosA) {
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractProfilerBuilder.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractProfilerBuilder.java
index 099de6e2d..c59d53016 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractProfilerBuilder.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/batch/ExtractProfilerBuilder.java
@@ -65,8 +65,7 @@ public class ExtractProfilerBuilder extends EvalConsumerBuilder {
             throw new RuntimeException("Must specify \"extracts\" -- directory to crawl");
         }
         if (!Files.isDirectory(extracts)) {
-            throw new RuntimeException(
-                    "ROOT DIRECTORY DOES NOT EXIST: " + extracts.toAbsolutePath());
+            throw new RuntimeException("ROOT DIRECTORY DOES NOT EXIST: " + extracts.toAbsolutePath());
         }
 
         Path inputDir = PropsUtil.getPath(localAttrs.get("inputDir"), null);
@@ -80,9 +79,7 @@ public class ExtractProfilerBuilder extends EvalConsumerBuilder {
         if (extracts == null && inputDir != null) {
             extracts = inputDir;
         }
-        return parameterizeProfiler(
-                new ExtractProfiler(queue, inputDir, extracts, buildExtractReader(localAttrs),
-                        getDBWriter(tableInfos)));
+        return parameterizeProfiler(new ExtractProfiler(queue, inputDir, extracts, buildExtractReader(localAttrs), getDBWriter(tableInfos)));
     }
 
 
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/Cols.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/Cols.java
index 35d70b430..5ab3d8665 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/Cols.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/Cols.java
@@ -24,26 +24,18 @@ public enum Cols {
     EXTRACT_FILE_LENGTH_B,
 
     //profile table
-    ID, LENGTH, FILE_NAME, FILE_EXTENSION, ELAPSED_TIME_MILLIS, NUM_METADATA_VALUES, IS_EMBEDDED,
-    EMBEDDED_FILE_PATH, MIME_ID, TIKA_MIME_ID, FILE_MIME_ID, SHA256, MD5, NUM_ATTACHMENTS,
-    ATTACHMENT_TYPE,
-    EMBEDDED_DEPTH,
-    HAS_CONTENT,
+    ID, LENGTH, FILE_NAME, FILE_EXTENSION, ELAPSED_TIME_MILLIS, NUM_METADATA_VALUES, IS_EMBEDDED, EMBEDDED_FILE_PATH, MIME_ID, TIKA_MIME_ID, FILE_MIME_ID, SHA256, MD5,
+    NUM_ATTACHMENTS, ATTACHMENT_TYPE, EMBEDDED_DEPTH, HAS_CONTENT,
 
     //content
-    CONTENT_LENGTH, NUM_UNIQUE_TOKENS, NUM_TOKENS, NUM_UNIQUE_ALPHABETIC_TOKENS,
-    NUM_ALPHABETIC_TOKENS, //alphabetic or ideographic tokens
+    CONTENT_LENGTH, NUM_UNIQUE_TOKENS, NUM_TOKENS, NUM_UNIQUE_ALPHABETIC_TOKENS, NUM_ALPHABETIC_TOKENS, //alphabetic or ideographic tokens
     COMMON_TOKENS_LANG, //which language was used for the common tokens metric?
-    NUM_UNIQUE_COMMON_TOKENS, NUM_COMMON_TOKENS, TOP_N_TOKENS, LANG_ID_1, LANG_ID_PROB_1, LANG_ID_2,
-    OOV,
-    LANG_ID_PROB_2, TOKEN_ENTROPY_RATE, TOKEN_LENGTH_SUM, TOKEN_LENGTH_MEAN, TOKEN_LENGTH_STD_DEV,
-    UNICODE_CHAR_BLOCKS, NUM_PAGES, //number of pages a document alleges it has
-    NUM_OCR_PAGES,
-    CONTENT_TRUNCATED_AT_MAX_LEN, // was the string truncated at AbstractProfiler.MAX_STRING_LENGTH
+    NUM_UNIQUE_COMMON_TOKENS, NUM_COMMON_TOKENS, TOP_N_TOKENS, LANG_ID_1, LANG_ID_PROB_1, LANG_ID_2, OOV, LANG_ID_PROB_2, TOKEN_ENTROPY_RATE, TOKEN_LENGTH_SUM, TOKEN_LENGTH_MEAN,
+    TOKEN_LENGTH_STD_DEV, UNICODE_CHAR_BLOCKS, NUM_PAGES, //number of pages a document alleges it has
+    NUM_OCR_PAGES, CONTENT_TRUNCATED_AT_MAX_LEN, // was the string truncated at AbstractProfiler.MAX_STRING_LENGTH
 
     //content comparisons
-    TOP_10_UNIQUE_TOKEN_DIFFS_A, TOP_10_UNIQUE_TOKEN_DIFFS_B, TOP_10_MORE_IN_A, TOP_10_MORE_IN_B,
-    OVERLAP, DICE_COEFFICIENT,
+    TOP_10_UNIQUE_TOKEN_DIFFS_A, TOP_10_UNIQUE_TOKEN_DIFFS_B, TOP_10_MORE_IN_A, TOP_10_MORE_IN_B, OVERLAP, DICE_COEFFICIENT,
 
     //errors
     PARSE_ERROR_ID,
@@ -63,8 +55,7 @@ public enum Cols {
     DIR_NAME_B,
 
     //structure tags
-    TAGS_A, TAGS_B, TAGS_DIV, TAGS_I, TAGS_IMG, TAGS_LI, TAGS_P, TAGS_OL, TAGS_TABLE, TAGS_TD,
-    TAGS_TITLE, TAGS_TR, TAGS_UL, TAGS_U,
+    TAGS_A, TAGS_B, TAGS_DIV, TAGS_I, TAGS_IMG, TAGS_LI, TAGS_P, TAGS_OL, TAGS_TABLE, TAGS_TD, TAGS_TITLE, TAGS_TR, TAGS_UL, TAGS_U,
     TAGS_PARSE_EXCEPTION, //if there was a SAX|IO|TikaException while parsing the html or xhtml
 
 }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/DBBuffer.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/DBBuffer.java
index 62fd10ff0..36f65a44d 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/DBBuffer.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/DBBuffer.java
@@ -25,11 +25,8 @@ public class DBBuffer extends AbstractDBBuffer {
 
     private final PreparedStatement st;
 
-    public DBBuffer(Connection connection, String tableName, String idColumnName,
-                    String valueColumnName) throws SQLException {
-        st = connection.prepareStatement(
-                "insert into " + tableName + "( " + idColumnName + ", " + valueColumnName +
-                        ") values (?,?);");
+    public DBBuffer(Connection connection, String tableName, String idColumnName, String valueColumnName) throws SQLException {
+        st = connection.prepareStatement("insert into " + tableName + "( " + idColumnName + ", " + valueColumnName + ") values (?,?);");
     }
 
     @Override
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/H2Util.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/H2Util.java
index 19e0407d5..2547d802d 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/H2Util.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/H2Util.java
@@ -49,7 +49,9 @@ public class H2Util extends JDBCUtil {
     }
 
     private static String getConnectionString(Path db, boolean createDBIfItDoesntExist) {
-        String s = "jdbc:h2:" + FilenameUtils.separatorsToUnix(db.toAbsolutePath().toString());
+        String s = "jdbc:h2:" + FilenameUtils.separatorsToUnix(db
+                .toAbsolutePath()
+                .toString());
         if (!createDBIfItDoesntExist) {
             s += ";IFEXISTS=TRUE";
         }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/JDBCUtil.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/JDBCUtil.java
index 64bc0824e..3e14c35f3 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/JDBCUtil.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/JDBCUtil.java
@@ -45,6 +45,7 @@ public class JDBCUtil {
     private final String connectionString;
     private String driverClass;
     private Connection connection = null;
+
     public JDBCUtil(String connectionString, String driverClass) {
         this.connectionString = connectionString;
         this.driverClass = driverClass;
@@ -58,7 +59,9 @@ public class JDBCUtil {
                     Properties properties = new Properties();
                     properties.load(is);
                     for (String k : properties.stringPropertyNames()) {
-                        Matcher m = Pattern.compile("(?i)jdbc:" + k).matcher(connectionString);
+                        Matcher m = Pattern
+                                .compile("(?i)jdbc:" + k)
+                                .matcher(connectionString);
                         if (m.find()) {
                             this.driverClass = properties.getProperty(k);
                         }
@@ -71,8 +74,7 @@ public class JDBCUtil {
         }
     }
 
-    public static void batchInsert(PreparedStatement insertStatement, TableInfo table,
-                                   Map<Cols, String> data) throws SQLException {
+    public static void batchInsert(PreparedStatement insertStatement, TableInfo table, Map<Cols, String> data) throws SQLException {
 
         try {
             int i = 1;
@@ -82,9 +84,7 @@ public class JDBCUtil {
             }
             for (Cols c : data.keySet()) {
                 if (!table.containsColumn(c)) {
-                    throw new IllegalArgumentException(
-                            "Can't add data to " + c + " because it doesn't exist in the table: " +
-                                    table.getName());
+                    throw new IllegalArgumentException("Can't add data to " + c + " because it doesn't exist in the table: " + table.getName());
                 }
             }
             insertStatement.addBatch();
@@ -93,8 +93,7 @@ public class JDBCUtil {
         }
     }
 
-    public static void updateInsertStatement(int dbColOffset, PreparedStatement st, ColInfo colInfo,
-                                             String value) throws SQLException {
+    public static void updateInsertStatement(int dbColOffset, PreparedStatement st, ColInfo colInfo, String value) throws SQLException {
         if (value == null) {
             st.setNull(dbColOffset, colInfo.getType());
             return;
@@ -131,8 +130,7 @@ public class JDBCUtil {
                     st.setBoolean(dbColOffset, Boolean.parseBoolean(value));
                     break;
                 default:
-                    throw new UnsupportedOperationException(
-                            "Don't yet support type: " + colInfo.getType());
+                    throw new UnsupportedOperationException("Don't yet support type: " + colInfo.getType());
             }
         } catch (NumberFormatException e) {
             if (!"".equals(value)) {
@@ -201,14 +199,15 @@ public class JDBCUtil {
 
         try (ResultSet rs = dbMeta.getTables(null, null, "%", null)) {
             while (rs.next()) {
-                tables.add(rs.getString(3).toLowerCase(Locale.US));
+                tables.add(rs
+                        .getString(3)
+                        .toLowerCase(Locale.US));
             }
         }
         return tables;
     }
 
-    public void createTables(List<TableInfo> tableInfos, CREATE_TABLE createTable)
-            throws SQLException, IOException {
+    public void createTables(List<TableInfo> tableInfos, CREATE_TABLE createTable) throws SQLException, IOException {
 
         Connection conn = getConnection();
         for (TableInfo tableInfo : tableInfos) {
@@ -250,7 +249,9 @@ public class JDBCUtil {
     //does not close the connection
     private void createTable(Connection conn, TableInfo tableInfo) throws SQLException {
         StringBuilder createSql = new StringBuilder();
-        createSql.append("CREATE TABLE ").append(tableInfo.getName());
+        createSql
+                .append("CREATE TABLE ")
+                .append(tableInfo.getName());
         createSql.append("(");
 
         int last = 0;
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/MimeBuffer.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/MimeBuffer.java
index 5a46ea311..cdf47fdab 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/MimeBuffer.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/db/MimeBuffer.java
@@ -35,12 +35,9 @@ public class MimeBuffer extends AbstractDBBuffer {
     private final Connection connection;
 
 
-    public MimeBuffer(Connection connection, TableInfo mimeTable, TikaConfig config)
-            throws SQLException {
+    public MimeBuffer(Connection connection, TableInfo mimeTable, TikaConfig config) throws SQLException {
         st = connection.prepareStatement(
-                "insert into " + mimeTable.getName() + "( " + Cols.MIME_ID.name() + ", " +
-                        Cols.MIME_STRING.name() + ", " + Cols.FILE_EXTENSION.name() +
-                        ") values (?,?,?)");
+                "insert into " + mimeTable.getName() + "( " + Cols.MIME_ID.name() + ", " + Cols.MIME_STRING.name() + ", " + Cols.FILE_EXTENSION.name() + ") values (?,?,?)");
         this.config = config;
         this.connection = connection;
     }
@@ -99,8 +96,7 @@ public class MimeBuffer extends AbstractDBBuffer {
          * @return extension or empty string
          * @throws MimeTypeException thrown if MimeTypes can't parse the contentType
          */
-        public static String getExtension(String contentType, TikaConfig config)
-                throws MimeTypeException {
+        public static String getExtension(String contentType, TikaConfig config) throws MimeTypeException {
             MimeTypes types = config.getMimeRepository();
             MimeType mime = types.forName(contentType);
             return getExtension(mime);
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/DBWriter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/DBWriter.java
index 7a3916a0a..a44ddfd83 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/DBWriter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/DBWriter.java
@@ -61,8 +61,7 @@ public class DBWriter implements IDBWriter {
     private final Map<String, PreparedStatement> inserts = new HashMap<>();
     private final Map<String, LastInsert> lastInsertMap = new HashMap<>();
 
-    public DBWriter(Connection connection, List<TableInfo> tableInfos, JDBCUtil dbUtil,
-                    MimeBuffer mimeBuffer) throws IOException, SQLException {
+    public DBWriter(Connection connection, List<TableInfo> tableInfos, JDBCUtil dbUtil, MimeBuffer mimeBuffer) throws IOException, SQLException {
 
         this.conn = connection;
         this.mimeBuffer = mimeBuffer;
@@ -84,7 +83,9 @@ public class DBWriter implements IDBWriter {
 
     private PreparedStatement createPreparedInsert(TableInfo tableInfo) throws SQLException {
         StringBuilder sb = new StringBuilder();
-        sb.append("INSERT INTO ").append(tableInfo.getName());
+        sb
+                .append("INSERT INTO ")
+                .append(tableInfo.getName());
         sb.append("(");
         int i = 0;
         for (ColInfo c : tableInfo.getColInfos()) {
@@ -114,8 +115,7 @@ public class DBWriter implements IDBWriter {
         try {
             PreparedStatement p = inserts.get(table.getName());
             if (p == null) {
-                throw new RuntimeException(
-                        "Failed to create prepared statement for: " + table.getName());
+                throw new RuntimeException("Failed to create prepared statement for: " + table.getName());
             }
             dbUtil.batchInsert(p, table, data);
             LastInsert lastInsert = lastInsertMap.get(table.getName());
@@ -124,8 +124,7 @@ public class DBWriter implements IDBWriter {
             if (
                 //elapsed > commitEveryXMS ||
                     lastInsert.rowCount % commitEveryXRows == 0) {
-                LOG.info("writer ({}) on table ({}) is committing after {} rows and {} ms", myId,
-                        table.getName(), lastInsert.rowCount, elapsed);
+                LOG.info("writer ({}) on table ({}) is committing after {} rows and {} ms", myId, table.getName(), lastInsert.rowCount, elapsed);
                 p.executeBatch();
                 conn.commit();
                 lastInsert.lastInsert = System.currentTimeMillis();
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReader.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReader.java
index a0ec4b5a4..ff23006ec 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReader.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReader.java
@@ -40,10 +40,10 @@ import org.slf4j.LoggerFactory;
 import org.apache.tika.config.TikaConfig;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
 import org.apache.tika.mime.MediaType;
 import org.apache.tika.sax.ToTextContentHandler;
 import org.apache.tika.sax.ToXMLContentHandler;
+import org.apache.tika.serialization.JsonMetadataList;
 
 
 public class ExtractReader {
@@ -53,6 +53,7 @@ public class ExtractReader {
     private final long minExtractLength;
     private final long maxExtractLength;
     private TikaConfig tikaConfig = TikaConfig.getDefaultConfig();
+
     /**
      * Reads full extract, no modification of metadata list, no min or max extract length checking
      */
@@ -64,15 +65,12 @@ public class ExtractReader {
         this(alterMetadataList, IGNORE_LENGTH, IGNORE_LENGTH);
     }
 
-    public ExtractReader(ALTER_METADATA_LIST alterMetadataList, long minExtractLength,
-                         long maxExtractLength) {
+    public ExtractReader(ALTER_METADATA_LIST alterMetadataList, long minExtractLength, long maxExtractLength) {
         this.alterMetadataList = alterMetadataList;
         this.minExtractLength = minExtractLength;
         this.maxExtractLength = maxExtractLength;
         if (maxExtractLength > IGNORE_LENGTH && minExtractLength >= maxExtractLength) {
-            throw new IllegalArgumentException(
-                    "minExtractLength(" + minExtractLength + ") must be < maxExtractLength(" +
-                            maxExtractLength + ")");
+            throw new IllegalArgumentException("minExtractLength(" + minExtractLength + ") must be < maxExtractLength(" + maxExtractLength + ")");
         }
     }
 
@@ -81,7 +79,8 @@ public class ExtractReader {
         if (fName == null) {
             return fileSuffixes;
         }
-        Matcher m = Pattern.compile("(?i)^(.*?)\\.(json|txt|x?html)(?:\\.(bz2|gz(?:ip)?|zip))?$")
+        Matcher m = Pattern
+                .compile("(?i)^(.*?)\\.(json|txt|x?html)(?:\\.(bz2|gz(?:ip)?|zip))?$")
                 .matcher(fName);
         if (m.find()) {
             fileSuffixes.originalFileName = m.group(1);
@@ -98,10 +97,11 @@ public class ExtractReader {
             throw new ExtractReaderException(ExtractReaderException.TYPE.NO_EXTRACT_FILE);
         }
 
-        FileSuffixes fileSuffixes = parseSuffixes(extractFile.getFileName().toString());
+        FileSuffixes fileSuffixes = parseSuffixes(extractFile
+                .getFileName()
+                .toString());
         if (fileSuffixes.format == null) {
-            throw new ExtractReaderException(
-                    ExtractReaderException.TYPE.INCORRECT_EXTRACT_FILE_SUFFIX);
+            throw new ExtractReaderException(ExtractReaderException.TYPE.INCORRECT_EXTRACT_FILE_SUFFIX);
         }
         if (!Files.isRegularFile(extractFile)) {
             throw new ExtractReaderException(ExtractReaderException.TYPE.NO_EXTRACT_FILE);
@@ -154,14 +154,11 @@ public class ExtractReader {
         try {
             if (fileSuffixes.format == FileSuffixes.FORMAT.JSON) {
                 metadataList = JsonMetadataList.fromJson(reader);
-                if (alterMetadataList.equals(ALTER_METADATA_LIST.FIRST_ONLY) &&
-                        metadataList.size() > 1) {
+                if (alterMetadataList.equals(ALTER_METADATA_LIST.FIRST_ONLY) && metadataList.size() > 1) {
                     while (metadataList.size() > 1) {
                         metadataList.remove(metadataList.size() - 1);
                     }
-                } else if (alterMetadataList
-                        .equals(ALTER_METADATA_LIST.AS_IS.CONCATENATE_CONTENT_INTO_FIRST) &&
-                        metadataList.size() > 1) {
+                } else if (alterMetadataList.equals(ALTER_METADATA_LIST.AS_IS.CONCATENATE_CONTENT_INTO_FIRST) && metadataList.size() > 1) {
                     StringBuilder sb = new StringBuilder();
                     Metadata containerMetadata = metadataList.get(0);
                     for (Metadata m : metadataList) {
@@ -188,25 +185,24 @@ public class ExtractReader {
         return metadataList;
     }
 
-    private List<Metadata> generateListFromTextFile(Reader reader, FileSuffixes fileSuffixes)
-            throws IOException {
+    private List<Metadata> generateListFromTextFile(Reader reader, FileSuffixes fileSuffixes) throws IOException {
         List<Metadata> metadataList = new ArrayList<>();
         String content = IOUtils.toString(reader);
         Metadata m = new Metadata();
         m.set(TikaCoreProperties.TIKA_CONTENT, content);
         if (fileSuffixes.format == FileSuffixes.FORMAT.HTML) {
-            m.set(TikaCoreProperties.TIKA_CONTENT_HANDLER,
-                    ToXMLContentHandler.class.getSimpleName());
+            m.set(TikaCoreProperties.TIKA_CONTENT_HANDLER, ToXMLContentHandler.class.getSimpleName());
         } else if (fileSuffixes.format == FileSuffixes.FORMAT.TXT) {
-            m.set(TikaCoreProperties.TIKA_CONTENT_HANDLER,
-                    ToTextContentHandler.class.getSimpleName());
+            m.set(TikaCoreProperties.TIKA_CONTENT_HANDLER, ToTextContentHandler.class.getSimpleName());
         }
         //Let's hope the file name has a suffix that can
         //be used to determine the mime.  Could be wrong or missing,
         //but better than nothing.
         m.set(TikaCoreProperties.RESOURCE_NAME_KEY, fileSuffixes.originalFileName);
 
-        MediaType mimeType = tikaConfig.getMimeRepository().detect(null, m);
+        MediaType mimeType = tikaConfig
+                .getMimeRepository()
+                .detect(null, m);
         if (mimeType != null) {
             m.set(Metadata.CONTENT_TYPE, mimeType.toString());
         }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReaderException.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReaderException.java
index 5f48a363a..eba256f64 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReaderException.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/ExtractReaderException.java
@@ -35,8 +35,7 @@ public class ExtractReaderException extends IOException {
 
     public enum TYPE {
         //what do you see when you look at the extract file
-        NO_EXTRACT_FILE, ZERO_BYTE_EXTRACT_FILE, IO_EXCEPTION, EXTRACT_PARSE_EXCEPTION,
-        EXTRACT_FILE_TOO_SHORT, EXTRACT_FILE_TOO_LONG,
+        NO_EXTRACT_FILE, ZERO_BYTE_EXTRACT_FILE, IO_EXCEPTION, EXTRACT_PARSE_EXCEPTION, EXTRACT_FILE_TOO_SHORT, EXTRACT_FILE_TOO_LONG,
         INCORRECT_EXTRACT_FILE_SUFFIX//extract file must have suffix of .json or .txt,
         // optionally followed by gzip, zip or bz2
     }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/XMLLogReader.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/XMLLogReader.java
index 972e3f7f0..2b8350894 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/XMLLogReader.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/io/XMLLogReader.java
@@ -81,8 +81,7 @@ public class XMLLogReader {
     static class LogXMLWrappingInputStream extends InputStream {
         //plagiarized from log4j's chainsaw
         private final static String HEADER =
-                "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>" + "<log4j:eventSet version=\"1.2\" " +
-                        "xmlns:log4j=\"http://jakarta.apache.org/log4j/\">";
+                "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>" + "<log4j:eventSet version=\"1.2\" " + "xmlns:log4j=\"http://jakarta.apache.org/log4j/\">";
         private static final String FOOTER = "</log4j:eventSet>";
         int currentStreamIndex = 0;
         private InputStream[] streams;
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/Report.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/Report.java
index 4c2274ac9..167176354 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/Report.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/Report.java
@@ -90,8 +90,7 @@ public class Report {
         }
     }
 
-    private void dumpReportToWorkbook(Statement st, SXSSFWorkbook wb)
-            throws SQLException {
+    private void dumpReportToWorkbook(Statement st, SXSSFWorkbook wb) throws SQLException {
         SXSSFSheet sheet;
         try (ResultSet rs = st.executeQuery(sql)) {
 
@@ -157,8 +156,7 @@ public class Report {
         }
     }
 
-    private void writeCell(ResultSetMetaData meta, int colIndex, ResultSet rs, Cell cell)
-            throws SQLException {
+    private void writeCell(ResultSetMetaData meta, int colIndex, ResultSet rs, Cell cell) throws SQLException {
 
         switch (meta.getColumnType(colIndex)) {
             //fall through on numerics
@@ -195,8 +193,7 @@ public class Report {
                 } else {
                     cell.setCellValue(rs.getString(colIndex));
                 }
-                LOG.warn("Couldn't find type for: {}. Defaulting to String",
-                        meta.getColumnType(colIndex));
+                LOG.warn("Couldn't find type for: {}. Defaulting to String", meta.getColumnType(colIndex));
         }
     }
 
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/ResultsReporter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/ResultsReporter.java
index a7cbb2abc..0abee4c35 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/ResultsReporter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/ResultsReporter.java
@@ -60,20 +60,15 @@ public class ResultsReporter {
 
     static {
         OPTIONS = new Options();
-        OPTIONS.addOption("rd", "reportsDir", true,
-                "directory for the reports. " + "If not specified, will write to 'reports'" +
-                        "BEWARE: Will overwrite existing reports without warning!")
+        OPTIONS
+                .addOption("rd", "reportsDir", true,
+                        "directory for the reports. " + "If not specified, will write to 'reports'" + "BEWARE: Will overwrite existing reports without warning!")
                 .addOption("rf", "reportsFile", true,
-                        "xml specifying sql to call for the reports." +
-                                "If not specified, will use default reports in resources/tika-eval-*-config.xml")
-                .addOption("db", true,
-                        "default database (in memory H2). Specify a file name for the H2 database.")
-                .addOption("jdbc", true,
-                        "EXPERT: full jdbc connection string. Specify this or use -db <h2db_name>")
-                .addOption("jdbcdriver", true,
-                        "EXPERT: specify the jdbc driver class if all else fails")
-                .addOption("tablePrefix", true,
-                        "EXPERT: if not using the default tables, specify your table name prefix");
+                        "xml specifying sql to call for the reports." + "If not specified, will use default reports in resources/tika-eval-*-config.xml")
+                .addOption("db", true, "default database (in memory H2). Specify a file name for the H2 database.")
+                .addOption("jdbc", true, "EXPERT: full jdbc connection string. Specify this or use -db <h2db_name>")
+                .addOption("jdbcdriver", true, "EXPERT: specify the jdbc driver class if all else fails")
+                .addOption("tablePrefix", true, "EXPERT: if not using the default tables, specify your table name prefix");
 
     }
 
@@ -83,9 +78,7 @@ public class ResultsReporter {
 
     public static void USAGE() {
         HelpFormatter helpFormatter = new HelpFormatter();
-        helpFormatter.printHelp(80,
-                "java -jar tika-eval-x.y.jar Report -db mydb [-rd myreports] [-rf myreports.xml]",
-                "Tool: Report", ResultsReporter.OPTIONS,
+        helpFormatter.printHelp(80, "java -jar tika-eval-x.y.jar Report -db mydb [-rd myreports] [-rf myreports.xml]", "Tool: Report", ResultsReporter.OPTIONS,
                 "Note: for h2 db, do not include the .mv.db at the end of the db name.");
 
     }
@@ -100,7 +93,9 @@ public class ResultsReporter {
             doc = docBuilder.parse(is);
         }
         Node docElement = doc.getDocumentElement();
-        assert (docElement.getNodeName().equals("reports"));
+        assert (docElement
+                .getNodeName()
+                .equals("reports"));
         NodeList children = docElement.getChildNodes();
         for (int i = 0; i < children.getLength(); i++) {
             Node n = children.item(i);
@@ -126,9 +121,15 @@ public class ResultsReporter {
         Report r = new Report();
         NamedNodeMap attrs = n.getAttributes();
 
-        r.includeSql = Boolean.parseBoolean(attrs.getNamedItem("includeSql").getNodeValue());
-        r.reportFilename = attrs.getNamedItem("reportFilename").getNodeValue();
-        r.reportName = attrs.getNamedItem("reportName").getNodeValue();
+        r.includeSql = Boolean.parseBoolean(attrs
+                .getNamedItem("includeSql")
+                .getNodeValue());
+        r.reportFilename = attrs
+                .getNamedItem("reportFilename")
+                .getNodeValue();
+        r.reportName = attrs
+                .getNamedItem("reportName")
+                .getNodeValue();
 
         for (int i = 0; i < children.getLength(); i++) {
             Node child = children.item(i);
@@ -137,8 +138,7 @@ public class ResultsReporter {
             }
             if ("sql".equals(child.getNodeName())) {
                 if (r.sql != null) {
-                    throw new IllegalArgumentException(
-                            "Can only have one sql statement per report");
+                    throw new IllegalArgumentException("Can only have one sql statement per report");
                 }
                 r.sql = child.getTextContent();
             } else if ("colformats".equals(child.getNodeName())) {
@@ -159,11 +159,17 @@ public class ResultsReporter {
                 continue;
             }
             NamedNodeMap attrs = child.getAttributes();
-            String columnName = attrs.getNamedItem("name").getNodeValue();
+            String columnName = attrs
+                    .getNamedItem("name")
+                    .getNodeValue();
             assert (!ret.containsKey(columnName));
-            String type = attrs.getNamedItem("type").getNodeValue();
+            String type = attrs
+                    .getNamedItem("type")
+                    .getNodeValue();
             if ("numberFormatter".equals(type)) {
-                String format = attrs.getNamedItem("format").getNodeValue();
+                String format = attrs
+                        .getNamedItem("format")
+                        .getNodeValue();
                 XSLXCellFormatter f = new XLSXNumFormatter(format);
                 ret.put(columnName, f);
             } else if ("urlLink".equals(type)) {
@@ -222,8 +228,7 @@ public class ResultsReporter {
             }
             Path db = Paths.get(dbString);
             if (!H2Util.databaseExists(db)) {
-                throw new RuntimeException(
-                        "I'm sorry, but I couldn't find this h2 database: " + db);
+                throw new RuntimeException("I'm sorry, but I couldn't find this h2 database: " + db);
             }
             dbUtil = new H2Util(db);
         } else if (commandLine.hasOption("jdbc")) {
@@ -233,8 +238,7 @@ public class ResultsReporter {
             }
             dbUtil = new JDBCUtil(commandLine.getOptionValue("jdbc"), driverClass);
         } else {
-            System.err.println("Must specify either -db for the default in-memory h2 database\n" +
-                    "or -jdbc for a full jdbc connection string");
+            System.err.println("Must specify either -db for the default in-memory h2 database\n" + "or -jdbc for a full jdbc connection string");
             USAGE();
             return;
         }
@@ -270,10 +274,14 @@ public class ResultsReporter {
         try (ResultSet rs = md.getTables(null, null, "%", null)) {
             while (rs.next()) {
                 String tName = rs.getString(3);
-                if (ExtractComparer.CONTENTS_TABLE_B.getName().equalsIgnoreCase(tName)) {
+                if (ExtractComparer.CONTENTS_TABLE_B
+                        .getName()
+                        .equalsIgnoreCase(tName)) {
                     internalPath = "/comparison-reports.xml";
                     break;
-                } else if (ExtractProfiler.PROFILE_TABLE.getName().equalsIgnoreCase(tName)) {
+                } else if (ExtractProfiler.PROFILE_TABLE
+                        .getName()
+                        .equalsIgnoreCase(tName)) {
                     internalPath = "/profile-reports.xml";
                     break;
                 }
@@ -281,12 +289,10 @@ public class ResultsReporter {
         }
 
         if (internalPath == null) {
-            throw new RuntimeException(
-                    "Couldn't determine if this database was a 'profiler' or 'comparison' db");
+            throw new RuntimeException("Couldn't determine if this database was a 'profiler' or 'comparison' db");
         }
         Path tmp = Files.createTempFile("tmp-tika-reports", ".xml");
-        Files.copy(ResultsReporter.class.getResourceAsStream(internalPath), tmp,
-                StandardCopyOption.REPLACE_EXISTING);
+        Files.copy(ResultsReporter.class.getResourceAsStream(internalPath), tmp, StandardCopyOption.REPLACE_EXISTING);
         return tmp;
     }
 
@@ -308,7 +314,7 @@ public class ResultsReporter {
                 long start = System.currentTimeMillis();
                 LOG.info("processing 'before': {}", sql);
                 st.execute(sql);
-                if (! c.getAutoCommit()) {
+                if (!c.getAutoCommit()) {
                     c.commit();
                     LOG.info("committing");
                 }
@@ -322,7 +328,7 @@ public class ResultsReporter {
                 LOG.info("processing 'after': {}", sql);
                 long start = System.currentTimeMillis();
                 st.execute(sql);
-                if (! c.getAutoCommit()) {
+                if (!c.getAutoCommit()) {
                     c.commit();
                 }
                 long elapsed = System.currentTimeMillis() - start;
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXHREFFormatter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXHREFFormatter.java
index 91165b31a..ab5dc886a 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXHREFFormatter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXHREFFormatter.java
@@ -59,16 +59,20 @@ public class XLSXHREFFormatter implements XSLXCellFormatter {
     }
 
     @Override
-    public void applyStyleAndValue(int dbColNum, ResultSet resultSet, Cell cell)
-            throws SQLException {
+    public void applyStyleAndValue(int dbColNum, ResultSet resultSet, Cell cell) throws SQLException {
         if (links < MAX_HYPERLINKS) {
-            Hyperlink hyperlink = workbook.getCreationHelper().createHyperlink(linkType);
+            Hyperlink hyperlink = workbook
+                    .getCreationHelper()
+                    .createHyperlink(linkType);
             String path = resultSet.getString(dbColNum);
             String address = urlBase + path;
             hyperlink.setAddress(address);
             cell.setHyperlink(hyperlink);
             cell.setCellStyle(style);
-            String fName = Paths.get(path).getFileName().toString();
+            String fName = Paths
+                    .get(path)
+                    .getFileName()
+                    .toString();
             cell.setCellValue(fName);
             links++;
         } else {
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXNumFormatter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXNumFormatter.java
index ee4d5304e..eba1c8472 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXNumFormatter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XLSXNumFormatter.java
@@ -37,13 +37,14 @@ class XLSXNumFormatter implements XSLXCellFormatter {
     @Override
     public void reset(XSSFWorkbook workbook) {
         style = workbook.createCellStyle();
-        style.setDataFormat(
-                workbook.getCreationHelper().createDataFormat().getFormat(formatString));
+        style.setDataFormat(workbook
+                .getCreationHelper()
+                .createDataFormat()
+                .getFormat(formatString));
     }
 
     @Override
-    public void applyStyleAndValue(int dbColNum, ResultSet resultSet, Cell cell)
-            throws SQLException {
+    public void applyStyleAndValue(int dbColNum, ResultSet resultSet, Cell cell) throws SQLException {
         double d = resultSet.getDouble(dbColNum);
         if (resultSet.wasNull()) {
 
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XSLXCellFormatter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XSLXCellFormatter.java
index b86fb35f8..2eb007140 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XSLXCellFormatter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/reports/XSLXCellFormatter.java
@@ -27,6 +27,5 @@ interface XSLXCellFormatter {
 
     public void reset(XSSFWorkbook workbook);
 
-    public void applyStyleAndValue(int dbColNum, ResultSet resultSet, Cell cell)
-            throws SQLException;
+    public void applyStyleAndValue(int dbColNum, ResultSet resultSet, Cell cell) throws SQLException;
 }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/BatchTopCommonTokenCounter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/BatchTopCommonTokenCounter.java
index fe92d0946..ea21112c4 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/BatchTopCommonTokenCounter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/BatchTopCommonTokenCounter.java
@@ -38,12 +38,21 @@ public class BatchTopCommonTokenCounter {
 
         for (Map.Entry<String, List<Path>> e : langFiles.entrySet()) {
 
-            String[] cmd = new String[e.getValue().size() + 1];
+            String[] cmd = new String[e
+                    .getValue()
+                    .size() + 1];
             Path commonTokensFile = commonTokensDir.resolve(e.getKey());
-            cmd[0] = ProcessUtils.escapeCommandLine(commonTokensFile.toAbsolutePath().toString());
-            for (int i = 0; i < e.getValue().size(); i++) {
-                cmd[i + 1] = ProcessUtils
-                        .escapeCommandLine(e.getValue().get(i).toAbsolutePath().toString());
+            cmd[0] = ProcessUtils.escapeCommandLine(commonTokensFile
+                    .toAbsolutePath()
+                    .toString());
+            for (int i = 0; i < e
+                    .getValue()
+                    .size(); i++) {
+                cmd[i + 1] = ProcessUtils.escapeCommandLine(e
+                        .getValue()
+                        .get(i)
+                        .toAbsolutePath()
+                        .toString());
             }
             TopCommonTokenCounter.main(cmd);
         }
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/CommonTokenOverlapCounter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/CommonTokenOverlapCounter.java
index d78a64a0f..ed1b09e3e 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/CommonTokenOverlapCounter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/CommonTokenOverlapCounter.java
@@ -37,7 +37,9 @@ public class CommonTokenOverlapCounter {
 
     private void execute(Path commonTokensDir) throws IOException {
         List<String> langs = new ArrayList<>();
-        for (File f : commonTokensDir.toFile().listFiles()) {
+        for (File f : commonTokensDir
+                .toFile()
+                .listFiles()) {
             langs.add(f.getName());
         }
         CommonTokenCountManager mgr = new CommonTokenCountManager(commonTokensDir, "");
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigHelper.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigHelper.java
index 4850a1a58..df42d414b 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigHelper.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigHelper.java
@@ -29,17 +29,26 @@ import java.util.regex.Pattern;
 public class LeipzigHelper {
 
     static Map<String, List<Path>> getFiles(Path leipzigDir) throws IOException {
-        Matcher tableMatcher = Pattern.compile("([a-z]+)_table(\\.txt)?(\\.gz)?$").matcher("");
-        Matcher leipzigMatcher =
-                Pattern.compile("([a-z]{3,3}(-(simp|trad|rom|zaw))?)[-_].*$").matcher("");
+        Matcher tableMatcher = Pattern
+                .compile("([a-z]+)_table(\\.txt)?(\\.gz)?$")
+                .matcher("");
+        Matcher leipzigMatcher = Pattern
+                .compile("([a-z]{3,3}(-(simp|trad|rom|zaw))?)[-_].*$")
+                .matcher("");
 
         Map<String, List<Path>> m = new TreeMap<>();
-        for (File f : leipzigDir.toFile().listFiles()) {
+        for (File f : leipzigDir
+                .toFile()
+                .listFiles()) {
             System.err.println(f);
             String lang = null;
-            if (tableMatcher.reset(f.getName()).find()) {
+            if (tableMatcher
+                    .reset(f.getName())
+                    .find()) {
                 lang = tableMatcher.group(1);
-            } else if (leipzigMatcher.reset(f.getName()).find()) {
+            } else if (leipzigMatcher
+                    .reset(f.getName())
+                    .find()) {
                 lang = leipzigMatcher.group(1);
             }
             if (lang == null) {
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigSampler.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigSampler.java
index e974314a8..30f44c7e3 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigSampler.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/LeipzigSampler.java
@@ -40,8 +40,7 @@ public class LeipzigSampler {
         }
     }
 
-    private void execute(Path leipzigDir, int sentsPerLang, BufferedWriter writer)
-            throws IOException {
+    private void execute(Path leipzigDir, int sentsPerLang, BufferedWriter writer) throws IOException {
         Map<String, List<Path>> fileMap = LeipzigHelper.getFiles(leipzigDir);
         for (Map.Entry<String, List<Path>> e : fileMap.entrySet()) {
             List<String> sentences = new ArrayList<>();
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/SlowCompositeReaderWrapper.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/SlowCompositeReaderWrapper.java
index 6dc15bbed..27393c528 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/SlowCompositeReaderWrapper.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/SlowCompositeReaderWrapper.java
@@ -88,12 +88,17 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
     SlowCompositeReaderWrapper(CompositeReader reader) throws IOException {
         in = reader;
         in.registerParentReader(this);
-        if (reader.leaves().isEmpty()) {
+        if (reader
+                .leaves()
+                .isEmpty()) {
             metaData = new LeafMetaData(Version.LATEST.major, Version.LATEST, null, false);
         } else {
             Version minVersion = Version.LATEST;
             for (LeafReaderContext leafReaderContext : reader.leaves()) {
-                Version leafVersion = leafReaderContext.reader().getMetaData().getMinVersion();
+                Version leafVersion = leafReaderContext
+                        .reader()
+                        .getMetaData()
+                        .getMinVersion();
                 if (leafVersion == null) {
                     minVersion = null;
                     break;
@@ -101,9 +106,12 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
                     minVersion = leafVersion;
                 }
             }
-            metaData = new LeafMetaData(
-                    reader.leaves().get(0).reader().getMetaData().getCreatedVersionMajor(),
-                    minVersion, null, false);
+            metaData = new LeafMetaData(reader
+                    .leaves()
+                    .get(0)
+                    .reader()
+                    .getMetaData()
+                    .getCreatedVersionMajor(), minVersion, null, false);
         }
         fieldInfos = FieldInfos.getMergedFieldInfos(in);
     }
@@ -152,7 +160,9 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
                 }
             });
         } catch (RuntimeException e) {
-            if (e.getMessage().equals("unwrapMe") && e.getCause() instanceof IOException) {
+            if (e
+                    .getMessage()
+                    .equals("unwrapMe") && e.getCause() instanceof IOException) {
                 throw (IOException) e.getCause();
             }
             throw e;
@@ -196,14 +206,20 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
                 return dv;
             }
         }
-        int size = in.leaves().size();
+        int size = in
+                .leaves()
+                .size();
         final SortedDocValues[] values = new SortedDocValues[size];
         final int[] starts = new int[size + 1];
         long totalCost = 0;
         for (int i = 0; i < size; i++) {
-            LeafReaderContext context = in.leaves().get(i);
+            LeafReaderContext context = in
+                    .leaves()
+                    .get(i);
             final LeafReader reader = context.reader();
-            final FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
+            final FieldInfo fieldInfo = reader
+                    .getFieldInfos()
+                    .fieldInfo(field);
             if (fieldInfo != null && fieldInfo.getDocValuesType() != DocValuesType.SORTED) {
                 return null;
             }
@@ -240,14 +256,20 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
         }
 
         assert map != null;
-        int size = in.leaves().size();
+        int size = in
+                .leaves()
+                .size();
         final SortedSetDocValues[] values = new SortedSetDocValues[size];
         final int[] starts = new int[size + 1];
         long cost = 0;
         for (int i = 0; i < size; i++) {
-            LeafReaderContext context = in.leaves().get(i);
+            LeafReaderContext context = in
+                    .leaves()
+                    .get(i);
             final LeafReader reader = context.reader();
-            final FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
+            final FieldInfo fieldInfo = reader
+                    .getFieldInfos()
+                    .fieldInfo(field);
             if (fieldInfo != null && fieldInfo.getDocValuesType() != DocValuesType.SORTED_SET) {
                 return null;
             }
@@ -282,21 +304,21 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
     }
 
     @Override
-    public void searchNearestVectors(String string, float[] floats, KnnCollector kc, Bits bits)
-            throws IOException {
+    public void searchNearestVectors(String string, float[] floats, KnnCollector kc, Bits bits) throws IOException {
         //TODO figure out how to implement this... if needed
     }
 
     @Override
-    public void searchNearestVectors(String string, byte[] bytes, KnnCollector kc, Bits bits)
-            throws IOException {
+    public void searchNearestVectors(String string, byte[] bytes, KnnCollector kc, Bits bits) throws IOException {
         //TODO figure out how to implement this... if needed
     }
 
     @Override
     public Fields getTermVectors(int docID) throws IOException {
         ensureOpen();
-        return in.termVectors().get(docID);
+        return in
+                .termVectors()
+                .get(docID);
     }
 
     @Override
@@ -319,7 +341,9 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
     @Override
     public void document(int docID, StoredFieldVisitor visitor) throws IOException {
         ensureOpen();
-        in.storedFields().document(docID, visitor);
+        in
+                .storedFields()
+                .document(docID, visitor);
     }
 
     @Override
@@ -354,7 +378,9 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
     public void checkIntegrity() throws IOException {
         ensureOpen();
         for (LeafReaderContext ctx : in.leaves()) {
-            ctx.reader().checkIntegrity();
+            ctx
+                    .reader()
+                    .checkIntegrity();
         }
     }
 
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TopCommonTokenCounter.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TopCommonTokenCounter.java
index e58074256..4cdcf4f19 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TopCommonTokenCounter.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TopCommonTokenCounter.java
@@ -68,30 +68,21 @@ public class TopCommonTokenCounter {
 
     private static final String FIELD = "f";
     //these should exist in every list
-    static Set<String> INCLUDE_LIST = new HashSet<>(Arrays.asList(
-            new String[]{URLEmailNormalizingFilterFactory.URL,
-                    URLEmailNormalizingFilterFactory.EMAIL}));
+    static Set<String> INCLUDE_LIST = new HashSet<>(Arrays.asList(new String[]{URLEmailNormalizingFilterFactory.URL, URLEmailNormalizingFilterFactory.EMAIL}));
     //words to ignore
     //these are common 4 letter html markup words that we do
     //not want to count in case of failed markup processing.
     //see: https://issues.apache.org/jira/browse/TIKA-2267?focusedCommentId=15872055&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15872055
     static Set<String> SKIP_LIST = new HashSet<>(
-            Arrays.asList("span", "table", "href", "head", "title", "body", "html", "tagname",
-                    "lang", "style", "script", "strong", "blockquote", "form", "iframe", "section",
+            Arrays.asList("span", "table", "href", "head", "title", "body", "html", "tagname", "lang", "style", "script", "strong", "blockquote", "form", "iframe", "section",
                     "colspan", "rowspan"));
     private static String LICENSE =
-            "# Licensed to the Apache Software Foundation (ASF) under one or more\n" +
-                    "# contributor license agreements.  See the NOTICE file distributed with\n" +
-                    "# this work for additional information regarding copyright ownership.\n" +
-                    "# The ASF licenses this file to You under the Apache License, Version 2.0\n" +
-                    "# (the \"License\"); you may not use this file except in compliance with\n" +
-                    "# the License.  You may obtain a copy of the License at\n" + "#\n" +
-                    "#     http://www.apache.org/licenses/LICENSE-2.0\n" + "#\n" +
-                    "# Unless required by applicable law or agreed to in writing, software\n" +
-                    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n" +
-                    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n" +
-                    "# See the License for the specific language governing permissions and\n" +
-                    "# limitations under the License.\n" + "#\n";
+            "# Licensed to the Apache Software Foundation (ASF) under one or more\n" + "# contributor license agreements.  See the NOTICE file distributed with\n" +
+                    "# this work for additional information regarding copyright ownership.\n" + "# The ASF licenses this file to You under the Apache License, Version 2.0\n" +
+                    "# (the \"License\"); you may not use this file except in compliance with\n" + "# the License.  You may obtain a copy of the License at\n" + "#\n" +
+                    "#     http://www.apache.org/licenses/LICENSE-2.0\n" + "#\n" + "# Unless required by applicable law or agreed to in writing, software\n" +
+                    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n" + "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n" +
+                    "# See the License for the specific language governing permissions and\n" + "# limitations under the License.\n" + "#\n";
     private static int TOP_N = 30000;
     private static int MIN_DOC_FREQ = 10;
 
@@ -103,16 +94,15 @@ public class TopCommonTokenCounter {
         }
         TopCommonTokenCounter counter = new TopCommonTokenCounter();
         if (Files.exists(commonTokensFile)) {
-            System.err.println(
-                    commonTokensFile.getFileName().toString() + " exists. I'm skipping this.");
+            System.err.println(commonTokensFile
+                    .getFileName()
+                    .toString() + " exists. I'm skipping this.");
             return;
         }
         counter.execute(commonTokensFile, inputFiles);
     }
 
-    private static void writeTopN(Path path, long totalDocs, long sumDocFreqs,
-                                  long sumTotalTermFreqs, long uniqueTerms,
-                                  AbstractTokenTFDFPriorityQueue queue) throws IOException {
+    private static void writeTopN(Path path, long totalDocs, long sumDocFreqs, long sumTotalTermFreqs, long uniqueTerms, AbstractTokenTFDFPriorityQueue queue) throws IOException {
         if (Files.isRegularFile(path)) {
             System.err.println("File " + path.getFileName() + " already exists. Skipping.");
             return;
@@ -142,8 +132,12 @@ public class TopCommonTokenCounter {
     private static String getRow(StringBuilder sb, TokenDFTF tp) {
         sb.setLength(0);
         sb.append(clean(tp.token));
-        sb.append("\t").append(tp.df);
-        sb.append("\t").append(tp.tf);
+        sb
+                .append("\t")
+                .append(tp.df);
+        sb
+                .append("\t")
+                .append(tp.tf);
         return sb.toString();
     }
 
@@ -151,7 +145,9 @@ public class TopCommonTokenCounter {
         if (s == null) {
             return "";
         }
-        return s.replaceAll("\\s+", " ").trim();
+        return s
+                .replaceAll("\\s+", " ")
+                .trim();
     }
 
     private void execute(Path commonTokensFile, List<Path> inputFiles) throws Exception {
@@ -173,7 +169,10 @@ public class TopCommonTokenCounter {
                 List<Document> docs = new ArrayList<>();
                 for (Path inputFile : inputFiles) {
                     //total hack
-                    boolean isLeipzig = inputFile.getFileName().toString().contains("-sentences.txt");
+                    boolean isLeipzig = inputFile
+                            .getFileName()
+                            .toString()
+                            .contains("-sentences.txt");
                     int lines = 0;
                     try (BufferedReader reader = getReader(inputFile)) {
                         String line = reader.readLine();
@@ -195,9 +194,7 @@ public class TopCommonTokenCounter {
                             }
                             line = reader.readLine();
                             if (++lines % 100000 == 0) {
-                                System.out.println(
-                                        "processed " + lines + " for " + inputFile.getFileName() +
-                                                " :: " + commonTokensFile.toAbsolutePath());
+                                System.out.println("processed " + lines + " for " + inputFile.getFileName() + " :: " + commonTokensFile.toAbsolutePath());
                             }
                         }
                     }
@@ -249,7 +246,9 @@ public class TopCommonTokenCounter {
 
     private BufferedReader getReader(Path inputFile) throws IOException {
         InputStream is = Files.newInputStream(inputFile);
-        if (inputFile.toString().endsWith(".gz")) {
+        if (inputFile
+                .toString()
+                .endsWith(".gz")) {
             is = new GzipCompressorInputStream(is);
         }
         return new BufferedReader(new InputStreamReader(is, StandardCharsets.UTF_8));
diff --git a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TrainTestSplit.java b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TrainTestSplit.java
index 3de468463..eec35cb42 100644
--- a/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TrainTestSplit.java
+++ b/tika-eval/tika-eval-app/src/main/java/org/apache/tika/eval/app/tools/TrainTestSplit.java
@@ -54,7 +54,9 @@ public class TrainTestSplit {
 
     private void execute(Path leipzigDir, Path outputDir) throws Exception {
         initOutDirs(outputDir);
-        for (File f : leipzigDir.toFile().listFiles()) {
+        for (File f : leipzigDir
+                .toFile()
+                .listFiles()) {
             if (f.isDirectory()) {
                 continue;
             }
@@ -80,11 +82,17 @@ public class TrainTestSplit {
             while (line != null) {
                 float r = random.nextFloat();
                 if (r <= trainingP) {
-                    writers.get(TRAINING).write(line + "\n");
+                    writers
+                            .get(TRAINING)
+                            .write(line + "\n");
                 } else if (r < trainingP + devTestP) {
-                    writers.get(DEVTEST).write(line + "\n");
+                    writers
+                            .get(DEVTEST)
+                            .write(line + "\n");
                 } else {
-                    writers.get(TESTING).write(line + "\n");
+                    writers
+                            .get(TESTING)
+                            .write(line + "\n");
                 }
                 line = reader.readLine();
             }
@@ -106,8 +114,9 @@ public class TrainTestSplit {
     }
 
     private BufferedWriter getWriter(Path outputDir, String which, File f) throws IOException {
-        OutputStream os = new GzipCompressorOutputStream(new BufferedOutputStream(
-                Files.newOutputStream(outputDir.resolve(which).resolve(f.getName() + ".gz"))));
+        OutputStream os = new GzipCompressorOutputStream(new BufferedOutputStream(Files.newOutputStream(outputDir
+                .resolve(which)
+                .resolve(f.getName() + ".gz"))));
         return new BufferedWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8));
     }
 }
diff --git a/tika-eval/tika-eval-app/src/main/resources/comparison-reports-pg.xml b/tika-eval/tika-eval-app/src/main/resources/comparison-reports-pg.xml
index 5940f4646..b4c3cf0a1 100644
--- a/tika-eval/tika-eval-app/src/main/resources/comparison-reports-pg.xml
+++ b/tika-eval/tika-eval-app/src/main/resources/comparison-reports-pg.xml
@@ -24,1732 +24,1732 @@
 <reports>
 
 
-    <before>
-
-        <sql>drop table if exists md5_multiples_tmp_a</sql>
-        <sql>create table md5_multiples_tmp_a (MD5, cnt)
-            as
-            select md5, count(1) as cnt
-            from profiles_a
-            where md5 is not null
-            group by md5
-            having count(1) &gt; 1
-            order by cnt desc
-        </sql>
-
-        <sql>drop table if exists md5_multiples_tmp_b</sql>
-        <sql>create table md5_multiples_tmp_b (MD5, cnt)
-            as
-            select md5, count(1) cnt
-            from profiles_b
-            where md5 is not null
-            group by md5
-            having count(1) &gt; 1
-            order by cnt desc
-        </sql>
-        <!-- build mime indexes -->
-
-        <sql>create index if not exists pa_m_idx
-            on profiles_a (mime_id);
-        </sql>
-
-        <sql>
-            create index if not exists pb_m_idx
-            on profiles_b (mime_id);
-        </sql>
-
-        <!-- build exceptions comparison table -->
-        <sql>drop table if exists exceptions_compared</sql>
-        <sql>
-            create table exceptions_compared (
-            mime_id_a integer,
-            mime_id_b integer,
-            total integer,
-            exc_cnt_a integer,
-            exc_cnt_b integer,
-            exc_prcnt_a float,
-            exc_prcnt_b float,
-            notes varchar(12)
-            );
-        </sql>
-        <sql>
-            insert into exceptions_compared (
-            select ma.mime_id, mb.mime_id, count(1) as total, 0, 0, 0.0, 0.0, ''
-            from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join mimes ma on pa.mime_id = ma.mime_id
-            join mimes mb on pb.mime_id = mb.mime_id
-            group by ma.mime_id, mb.mime_id
-            order by total desc );
-        </sql>
-
-        <sql>
-            update exceptions_compared ec set
-            exc_cnt_a = (
-            select count(1) as cnt
-            from exceptions_a ea
-            join profiles_a pa on ea.id=pa.id
-            join profiles_b pb on pb.id=pa.id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
-            group by ma.mime_id, mb.mime_id);
-        </sql>
-        <sql>
-            update exceptions_compared ec set
-            exc_cnt_b = (
-            select count(1) as cnt
-            from exceptions_b eb
-            join profiles_b pb on eb.id=pb.id
-            join profiles_a pa on pa.id=pb.id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
-            group by mb.mime_id, ma.mime_id);
-        </sql>
-        <sql>
-            update exceptions_compared
-            set exc_prcnt_a = cast(exc_cnt_a as decimal)/cast(total as decimal)
-            where total > 0;
-        </sql>
-        <sql>
-            update exceptions_compared
-            set exc_prcnt_b = cast(exc_cnt_b as decimal)/cast(total as decimal)
-            where total > 0;
-        </sql>
-
-        <sql>
-            update exceptions_compared
-            set notes = 'YAY!'
-            where total > 100 and (exc_prcnt_a-exc_prcnt_b) > 0.10;
-        </sql>
-        <sql>
-            update exceptions_compared
-            set notes = 'YIKES!'
-            where total > 100 and (exc_prcnt_b-exc_prcnt_a) > 0.10;
-        </sql>
-
-        <!-- build tmp common words table -->
-        <sql>drop table if exists token_counts_compared</sql>
-        <sql>
-            create table token_counts_compared
-            (mime_id_a integer,
-            mime_id_b integer,
-            num_tokens_a bigint default 0,
-            num_tokens_b bigint default 0,
-            num_alphabetic_tokens_a bigint default 0,
-            num_alphabetic_tokens_b bigint default 0,
-            num_common_tokens_a bigint default 0,
-            num_common_tokens_b bigint default 0
-            );
-        </sql>
-        <sql>
-            insert into token_counts_compared (mime_id_a, mime_id_b)
-            select ma.mime_id, mb.mime_id
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            group by ma.mime_id, mb.mime_id
-
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_tokens_a=(
-            select sum(num_tokens) as cnt from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join contents_a c on c.id = pa.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_tokens_b=(
-            select sum(num_tokens) as cnt from profiles_b pb
-            join profiles_a pa on pa.id=pb.id
-            join contents_b c on c.id = pb.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_alphabetic_tokens_a=(
-            select sum(num_alphabetic_tokens) as cnt from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join contents_a c on c.id = pa.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_alphabetic_tokens_b=(
-            select sum(num_alphabetic_tokens) as cnt from profiles_b pb
-            join profiles_a pa on pb.id=pa.id
-            join contents_b c on c.id = pb.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_common_tokens_a=(
-            select sum(num_common_tokens) as cnt from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join contents_a c on c.id = pa.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_common_tokens_b=(
-            select sum(num_common_tokens) as cnt from profiles_b pb
-            join profiles_a pa on pa.id=pb.id
-            join contents_b c on c.id = pb.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>drop table if exists tags_by_mime</sql>
-        <sql>create table tags_by_mime (
-                mime_id_a integer,
-                mime_id_b integer,
-                tags_a_a integer,
-                tags_b_a integer,
-                tags_div_a integer,
-                tags_i_a integer,
-                tags_img_a integer,
-                tags_li_a integer,
-                tags_ol_a integer,
-                tags_p_a integer,
-                tags_table_a integer,
-                tags_td_a integer,
-                tags_title_a integer,
-                tags_tr_a integer,
-                tags_u_a integer,
-                tags_ul_a integer,
-                tags_a_b integer,
-                tags_b_b integer,
-                tags_div_b integer,
-                tags_i_b integer,
-                tags_img_b integer,
-                tags_li_b integer,
-                tags_ol_b integer,
-                tags_p_b integer,
-                tags_table_b integer,
-                tags_td_b integer,
-                tags_title_b integer,
-                tags_tr_b integer,
-                tags_u_b integer,
-                tags_ul_b integer
-            );
-        </sql>
-        <sql>
-            insert into tags_by_mime (mime_id_a, mime_id_b)
-            select ma.mime_id, mb.mime_id
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            group by ma.mime_id, mb.mime_id
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_a_a=(
-            select sum(ta.tags_a) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_b_a=(
-            select sum(ta.tags_b) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_div_a=(
-            select sum(ta.tags_div) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_i_a=(
-            select sum(ta.tags_i) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_img_a=(
-            select sum(ta.tags_img) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_li_a=(
-            select sum(ta.tags_li) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ol_a=(
-            select sum(ta.tags_ol) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_p_a=(
-            select sum(ta.tags_p) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_table_a=(
-            select sum(ta.tags_table) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_td_a=(
-            select sum(ta.tags_td) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_title_a=(
-            select sum(ta.tags_title) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_tr_a=(
-            select sum(ta.tags_tr) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_u_a=(
-            select sum(ta.tags_u) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ul_a=(
-            select sum(ta.tags_ul) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <!-- now update tags_b counts -->
-        <sql>
-            update tags_by_mime tbm set tags_a_b=(
-            select sum(tb.tags_a) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_b_b=(
-            select sum(tb.tags_b) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_div_b=(
-            select sum(tb.tags_div) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_i_b=(
-            select sum(tb.tags_i) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_img_b=(
-            select sum(tb.tags_img) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_li_b=(
-            select sum(tb.tags_li) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ol_b=(
-            select sum(tb.tags_ol) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_p_b=(
-            select sum(tb.tags_p) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_table_b=(
-            select sum(tb.tags_table) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_td_b=(
-            select sum(tb.tags_td) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_title_b=(
-            select sum(tb.tags_title) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_tr_b=(
-            select sum(tb.tags_tr) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_u_b=(
-            select sum(tb.tags_u) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ul_b=(
-            select sum(tb.tags_ul) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>drop table if exists tag_exceptions_by_mime</sql>
-        <sql>create table tag_exceptions_by_mime (
-            mime_id_a integer,
-            mime_id_b integer,
-            tag_exceptions_a integer,
-            tag_exceptions_b integer)
-        </sql>
-        <sql>
-            insert into tag_exceptions_by_mime (mime_id_a, mime_id_b,
-                tag_exceptions_a, tag_exceptions_b)
-            select ma.mime_id, mb.mime_id,0,0
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            group by ma.mime_id, mb.mime_id
-        </sql>
-        <sql>
-            update tag_exceptions_by_mime tebm set tag_exceptions_a=(
-            select count(1) as cnt from tags_a ta
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tebm.mime_id_b
-            and pa.mime_id=tebm.mime_id_a
-            and ta.tags_parse_exception=true
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tag_exceptions_by_mime tebm set tag_exceptions_b=(
-            select count(1) as cnt from tags_b tb
-            join profiles_a pa on pa.id=tb.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tebm.mime_id_b
-            and pa.mime_id=tebm.mime_id_a
-            and tb.tags_parse_exception=true
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            drop table if exists parse_time_compared;
-        </sql>
-        <sql>
-            create table parse_time_compared (
-            mime_id_a integer,
-            mime_id_b integer,
-            total_a bigint,
-            total_b bigint,
-            prcnt_increase double precision
-            );
-        </sql>
-            <sql>
-                insert into parse_time_compared (mime_id_a, mime_id_b,
-                total_a, total_b, prcnt_increase)
-                select ma.mime_id, mb.mime_id,0,0,0.0
-                from profiles_a a
-                join profiles_b b on a.id=b.id
-                join mimes ma on ma.mime_id=a.mime_id
-                join mimes mb on mb.mime_id=b.mime_id
-                group by ma.mime_id, mb.mime_id
-            </sql>
-        <sql>
-            update parse_time_compared ptc set total_a=(
-            select sum(pa.elapsed_time_millis) as total_a from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            where pa.mime_id= ptc.mime_id_a
-            and pb.mime_id=ptc.mime_id_b
-            group by mime_id_a, mime_id_b)
-        </sql>
-        <sql>
-            update parse_time_compared ptc set total_b=(
-            select sum(pb.elapsed_time_millis) as total_b from profiles_b pb
-            join profiles_a pa on pa.id=pb.id
-            where pa.mime_id= ptc.mime_id_a
-            and pb.mime_id=ptc.mime_id_b
-            group by mime_id_a, mime_id_b)
-        </sql>
-        <sql>
-            update parse_time_compared ptc set prcnt_increase=(100.0 *
-            cast(total_b as decimal)/cast(total_a as decimal))
-            where total_a > 0;
-        </sql>
-    </before>
-
-    <!-- MIMES -->
-    <report reportName="All Mimes In A"
-            reportFilename="mimes/all_mimes_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_a p
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="All Mimes In B"
-            reportFilename="mimes/all_mimes_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_b p
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container Mimes In A"
-            reportFilename="mimes/container_mimes_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_a p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="Container Mimes In B"
-            reportFilename="mimes/container_mimes_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_b p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Embedded Mimes In A"
-            reportFilename="mimes/embedded_mimes_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_a p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="Embedded Mimes In B"
-            reportFilename="mimes/embedded_mimes_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_b p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Mime Differences A -> B"
-            reportFilename="mimes/mime_diffs_A_to_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
-            MIME_A_TO_MIME_B, count(1) as COUNT
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            where a.mime_id &lt;&gt; b.mime_id
-            group by MIME_A_TO_MIME_B
-            order by COUNT DESC
-        </sql>
-    </report>
-
-    <report reportName="Mime Differences A -> B Details"
-            reportFilename="mimes/mime_diffs_A_to_B_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
-            MIME_A_TO_MIME_B,
-            file_path,
-            c.length as CONTAINER_LENGTH,
-            a.file_name
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            join containers c on a.container_id=c.container_id
-            where a.mime_id &lt;&gt; b.mime_id
-            order by MIME_A_TO_MIME_B
-        </sql>
-    </report>
-
-
-    <!-- Exceptions -->
-    <report reportName="AllExceptionsByMimeA"
-            reportFilename="exceptions/exceptions_by_mime_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_a e
-            join profiles_a p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="AllExceptionsByMimeB"
-            reportFilename="exceptions/exceptions_by_mime_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_b e
-            join profiles_b p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="ContainerExceptionsByMimeA"
-            reportFilename="exceptions/container_exceptions_by_mime_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_a e
-            join profiles_a p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            and parse_exception_id=0
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="ContainerExceptionsByMimeB"
-            reportFilename="exceptions/container_exceptions_by_mime_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_b e
-            join profiles_b p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            and parse_exception_id=0
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="AllExceptionsByMimeByTypeA"
-            reportFilename="exceptions/exceptions_by_mime_by_type_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string as MIME_TYPE,
-            parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
-            from exceptions_a e
-            join profiles_a p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            join ref_parse_exception_types r on
-            r.parse_exception_id=e.parse_exception_id
-            group by m.mime_string, parse_exception_description
-            order by MIME_TYPE, EXCEPTION_TYPE
-        </sql>
-    </report>
-
-    <report reportName="AllExceptionsByMimeByTypeB"
-            reportFilename="exceptions/exceptions_by_mime_by_type_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string as MIME_TYPE,
-            parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
-            from exceptions_b e
-            join profiles_b p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            join ref_parse_exception_types r on
-            r.parse_exception_id=e.parse_exception_id
-            group by m.mime_string, parse_exception_description
-            order by MIME_TYPE, EXCEPTION_TYPE
-        </sql>
-    </report>
-
-    <report reportName="TextLostFromACausedByNewExceptionsInB"
-            reportFilename="exceptions/text_lost_from_A_caused_by_new_exceptions_in_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path as FILE_PATH,
-            c.length as CONTAINER_LENGTH,
-            ca.NUM_TOKENS as NUM_TOKENS_A,
-            cb.NUM_TOKENS as NUM_TOKENS_B,
-            ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A, cb.NUM_UNIQUE_TOKENS
-            as NUM_UNIQUE_TOKENS_B,
-            ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
-            ca.num_common_tokens as NUM_COMMON_TOKENS_A,
-            cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
-            cb.num_common_tokens as NUM_COMMON_TOKENS_B,
-            ca.top_n_tokens as TOP_N_TOKENS_A, cb.top_n_tokens as TOP_N_TOKENS_B,
-            eb.ORIG_STACK_TRACE as ORIG_STACK_TRACE_B
-            from contents_a ca
-            join profiles_a pa on ca.id = pa.id
-            join containers c on pa.container_id=c.container_id
-            left join contents_b cb on ca.id=cb.id
-            left join exceptions_b eb on ca.id = eb.id
-            left join exceptions_a ea on ca.id = ea.id
-            where eb.orig_stack_trace is not null
-            and ea.orig_stack_trace is null
-            order by ca.num_common_tokens - coalesce(cb.num_common_tokens,0) desc
-        </sql>
-    </report>
-
-    <report reportName="FixedExceptionsInBByMimeType"
-            reportFilename="exceptions/fixed_exceptions_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            count(1) as COUNT
-            from exceptions_a ea
-            left join exceptions_b eb on ea.id = eb.id
-            join profiles_a pa on pa.id=ea.id
-            join profiles_b pb on pa.id=pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where eb.id is null
-            and ea.parse_exception_id=0
-            group by mime_type_a, mime_type_b
-        </sql>
-    </report>
-
-    <report reportName="FixedExceptionsInByDetails"
-            reportFilename="exceptions/fixed_exceptions_in_B_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select
-            file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            pa.file_name, pa.is_embedded
-            from exceptions_a ea
-            left join exceptions_b eb on ea.id = eb.id
-            join profiles_a pa on pa.id=ea.id
-            join profiles_b pb on pb.id=pa.id --this ensures that files were actually processed in both runs
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where eb.id is null
-            and ea.parse_exception_id=0
-            order by mime_type_a, mime_type_b
-        </sql>
-    </report>
-    <report reportName="ContentsOfFixedExceptionsInB"
-            reportFilename="exceptions/contents_of_fixed_exceptions_in_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            CONTENT_LENGTH,
-            NUM_TOKENS, NUM_UNIQUE_TOKENS,
-            TOP_N_TOKENS, LANG_ID_1,TOKEN_LENGTH_MEAN, TOKEN_LENGTH_STD_DEV
-            from exceptions_a ea
-            left join exceptions_b eb on ea.id = eb.id
-            join profiles_a pa on pa.id=ea.id
-            join profiles_b pb on pa.id=pb.id
-            join contents_b cb on cb.id=ea.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where eb.id is null
-            and ea.parse_exception_id=0
-        </sql>
-    </report>
-
-    <report reportName="NewExceptionsByMimeType"
-            reportFilename="exceptions/new_exceptions_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select ma.mime_string as MIME_TYPE_A, mb.mime_string as MIME_TYPE_B, count(1) as COUNT
-            from exceptions_b eb
-            left join exceptions_a ea on ea.id = eb.id
-            join profiles_a pa on pa.id=eb.id
-            join profiles_b pb on pb.id=pa.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where ea.id is null
-            and eb.parse_exception_id=0
-            group by ma.mime_string, mb.mime_string
-            order by COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="NewExceptionsInBByMimeTypeByStackTrace"
-            reportFilename="exceptions/new_exceptions_in_B_by_mime_by_stack_trace.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select
-            ma.MIME_STRING as MIME_TYPE_A,
-            mb.MIME_STRING as MIME_TYPE_B,
-            eb.sort_stack_trace, count(1) as
-            COUNT
-            from exceptions_b eb
-            left join exceptions_a ea on ea.id = eb.id
-            join profiles_a pa on pa.id=eb.id
-            join profiles_b pb on pb.id=eb.id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where ea.id is null
-            and eb.parse_exception_id=0
-            group by MIME_TYPE_A, MIME_TYPE_B, eb.sort_stack_trace
-            order by MIME_TYPE_A asc, MIME_TYPE_B asc, COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="NewExceptionsInBDetails"
-            reportFilename="exceptions/new_exceptions_in_B_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            eb.orig_stack_trace, eb.sort_stack_trace
-            from exceptions_b eb
-            left join exceptions_a ea on ea.id = eb.id
-            join profiles_a pa on pa.id=eb.id
-            join profiles_b pb on pb.id=eb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where ea.id is null
-            and eb.parse_exception_id=0
-            order by MIME_TYPE_A asc, MIME_TYPE_B asc, eb.ORIG_STACK_TRACE
-        </sql>
-    </report>
-
-    <report reportName="StackTracesByMimeInA"
-            reportFilename="exceptions/stack_traces_by_mime_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
-            COUNT
-            from exceptions_a e
-            join profiles_a p on p.id=e.id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            group by MIME_TYPE, e.sort_stack_trace
-            order by MIME_TYPE asc, COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="AllStackTracesInA"
-            reportFilename="exceptions/stack_traces_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            mime_string as MIME_TYPE,
-            orig_stack_trace, sort_stack_trace
-            from exceptions_a e
-            join profiles_a p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
-            CONTAINER_LENGTH asc
-        </sql>
-    </report>
-    <report reportName="AllStackTracesInB"
-            reportFilename="exceptions/stack_traces_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            mime_string as MIME_TYPE,
-            orig_stack_trace, sort_stack_trace
-            from exceptions_b e
-            join profiles_b p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
-            CONTAINER_LENGTH asc
-        </sql>
-    </report>
-
-    <report reportName="StackTracesByMimeInB"
-            reportFilename="exceptions/stack_traces_by_mime_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
-            COUNT
-            from exceptions_b e
-            join profiles_b p on p.id=e.id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            group by MIME_TYPE, e.sort_stack_trace
-            order by MIME_TYPE asc, COUNT desc
-        </sql>
-    </report>
-    <report reportName="extractExceptionsA"
-            reportFilename="exceptions/extract_exceptions_a.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, extract_exception_description
-            from extract_exceptions_a e
-            join ref_extract_exception_types t
-            on e.extract_exception_id=t.extract_exception_id
-        </sql>
-    </report>
-    <report reportName="extractExceptionsB"
-            reportFilename="exceptions/extract_exceptions_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, extract_exception_description
-            from extract_exceptions_b e
-            join ref_extract_exception_types t
-            on e.extract_exception_id=t.extract_exception_id
-        </sql>
-    </report>
-    <report reportName="parseExceptionTypesA"
-            reportFilename="exceptions/overall_exception_types_a.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select parse_exception_description, count(1)
-            from exceptions_a e
-            join ref_parse_exception_types t on
-            t.parse_exception_id=e.parse_exception_id
-            group by t.parse_exception_description
-        </sql>
-    </report>
-    <report reportName="parseExceptionTypesB"
-            reportFilename="exceptions/overall_exception_types_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select parse_exception_description, count(1)
-            from exceptions_b e
-            join ref_parse_exception_types t on
-            t.parse_exception_id=e.parse_exception_id
-            group by t.parse_exception_description
-        </sql>
-    </report>
-
-    <report reportName="contentDiffsWExceptions"
-            reportFilename="content/content_diffs_with_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            ca.num_unique_tokens as NUM_UNIQUE_TOKENS_A,
-            cb.num_unique_tokens as NUM_UNIQUE_TOKENS_B,
-            ca.num_tokens as NUM_TOKENS_A,
-            cb.num_tokens as NUM_TOKENS_B,
-            ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
-            ca.num_common_tokens as NUM_COMMON_TOKENS_A,
-            cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
-            cb.num_common_tokens as NUM_COMMON_TOKENS_B,
-            coalesce(cb.num_common_tokens,0)-
-            coalesce(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
-            ca.top_n_tokens as TOP_N_TOKENS_A,
-            cb.top_n_tokens as TOP_N_TOKENS_B,
-            ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
-            cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
-            top_10_unique_token_diffs_a,
-            top_10_unique_token_diffs_b,
-            top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap,
-            ref_ea.parse_exception_description as EXCEPTION_A,
-            ref_eb.parse_exception_description as EXCEPTION_B
-            from content_comparisons cc
-            join contents_a ca on ca.id=cc.id
-            left join contents_b cb on cb.id=cc.id
-            join profiles_a pa on pa.id = cc.id
-            join profiles_b pb on pb.id=cc.id
-            join containers c on c.container_id=pa.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            left join exceptions_a ea on ea.id=cc.id
-            left join exceptions_b eb on eb.id=cc.id
-            left join ref_parse_exception_types ref_ea on ref_ea.parse_exception_id=ea.parse_exception_id
-            left join ref_parse_exception_types ref_eb on ref_eb.parse_exception_id=eb.parse_exception_id
-            where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
-            and (ea.parse_exception_id is null or
-            ea.parse_exception_id &lt;&gt; 2)
-            and (eb.parse_exception_id is null or
-            eb.parse_exception_id &lt;&gt; 2)
-            order by ma.mime_string, overlap asc
-            limit 100000
-        </sql>
-    </report>
-    <report reportName="contentDiffsNoExceptions"
-            reportFilename="content/content_diffs_no_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A,
-            cb.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_B,
-            ca.NUM_TOKENS as NUM_TOKENS_A,
-            cb.NUM_TOKENS as NUM_TOKENS_B,
-            ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
-            ca.num_common_tokens as NUM_COMMON_TOKENS_A,
-            cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
-            cb.num_common_tokens as NUM_COMMON_TOKENS_B,
-            coalesce(cb.num_common_tokens,0)-
-            coalesce(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
-            ca.top_n_tokens as TOP_N_TOKENS_A,
-            cb.top_n_tokens as TOP_N_TOKENS_B,
-            ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
-            cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
-            top_10_unique_token_diffs_a,
-            top_10_unique_token_diffs_b,
-            top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap
-            from content_comparisons cc
-            join contents_a ca on ca.id=cc.id
-            join contents_b cb on cb.id=cc.id
-            join profiles_a pa on pa.id = cc.id
-            join profiles_b pb on pb.id=cc.id
-            join containers c on c.container_id=pa.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            left join exceptions_a ea on ea.id=cc.id
-            left join exceptions_b eb on eb.id=cc.id
-            where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
-            and (ea.parse_exception_id is null)
-            and (eb.parse_exception_id is null)
-            order by ma.mime_string, overlap asc
-            limit 100000
-        </sql>
-    </report>
-
-    <report reportName="CommonTokenComparisonsByMimeType"
-            reportFilename="content/common_token_comparisons_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select ma.mime_string as MIME_STRING_A, mb.mime_string as MIME_STRING_B,
-            num_tokens_a, num_tokens_b,
-            num_alphabetic_tokens_a, num_alphabetic_tokens_b,
-            num_common_tokens_a, num_common_tokens_b,
-            coalesce(num_common_tokens_b, 0)-coalesce(num_common_tokens_a, 0) as change_in_common_tokens_b
-            from token_counts_compared tcc
-            join mimes ma on tcc.mime_id_a = ma.mime_id
-            join mimes mb on tcc.mime_id_b = mb.mime_id
-            order by change_in_common_tokens_b desc
-        </sql>
-    </report>
-    <report reportName="PageCountDiffs"
-            reportFilename="content/page_count_diffs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.num_pages as NUM_PAGES_A,
-            pb.num_pages as NUM_PAGES_B,
-            (pb.num_pages-pa.num_pages) as DIFF_NUM_PAGES_IN_B
-            from profiles_a pa
-            join profiles_b pb on pa.id = pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where pa.num_pages is not null
-            and pb.num_pages is not null
-            and pa.num_pages &lt;&gt; pb.num_pages
-            order by DIFF_NUM_PAGES_IN_B asc
-            limit 10000;
-        </sql>
-    </report>
-
-
-    <report reportName="ExceptionComparisonsByMimeType"
-            reportFilename="exceptions/exceptions_compared_by_mime_type.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select ma.mime_string as mime_string_a, mb.mime_string as mime_string_b,
-            total, exc_cnt_a,
-            exc_cnt_b,
-            exc_prcnt_a,
-            exc_prcnt_b, notes
-
-            from exceptions_compared e
-            join mimes ma on ma.mime_id=e.mime_id_a
-            join mimes mb on mb.mime_id=e.mime_id_b
-            order by (exc_prcnt_b-exc_prcnt_a) desc, total desc;
-        </sql>
-    </report>
-    <!--    <report reportName="MD5 Duplicate Counts A"
-                reportFilename="md5/md5_duplicate_counts_A.xlsx"
-                format="xlsx"
-                            includeSql="true">
-            <sql>
-                select md5, count(1) cnt
-                from profiles_a
-                group by md5
-                having cnt > 2
-                order by cnt desc
-            </sql>
-        </report>
-
-        <report reportName="MD5 Duplicate Counts B"
-                reportFilename="md5/md5_duplicate_counts_B.xlsx"
-                format="xlsx"
-                            includeSql="true">
-
-            <sql>
-                select md5, count(1) cnt
-                from profiles_b
-                group by md5
-                having cnt > 2
-                order by cnt desc
-            </sql>
-        </report>
-
-        <report reportName="MD5 Duplicates A"
-                reportFilename="md5/md5_duplicates_A.xlsx"
-                format="xlsx"
-                            includeSql="true">
-
-            <sql>
-                select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
-                from md5_multiples_tmp_a t
-                join profiles_a p on p.md5 = t.md5
-                join containers c on p.container_id = c.container_id
-                join contents_a cb on p.id=cb.id
-                order by t.cnt desc
-            </sql>
-        </report>
-
-        <report reportName="MD5 Duplicates B"
-                reportFilename="md5/md5_duplicates_B.xlsx"
-                format="xlsx"
-                            includeSql="true">
-
-            <sql>
-                select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
-                from md5_multiples_tmp_b t
-                join profiles_b p on p.md5 = t.md5
-                join containers c on p.container_id = c.container_id
-                join contents_b cb on p.id=cb.id
-                order by t.cnt desc
-            </sql>
-        </report>
-    -->
-
-    <report reportName="Attachment Diffs no Exceptions"
-            reportFilename="attachments/attachment_diffs_no_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.num_attachments as NUM_ATTACHMENTS_A,
-            pb.num_attachments as NUM_ATTACHMENTS_B,
-            pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B
-            from profiles_a pa
-            join profiles_b pb on pa.id= pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            left join exceptions_a ea on ea.id=pa.id
-            left join exceptions_b eb on eb.id=pb.id
-            where pa.is_embedded=false and
-            ea.parse_exception_id is null and
-            eb.parse_exception_id is null
-            and pa.num_attachments &lt;&gt; pb.num_attachments
-            order by ma.mime_string, pb.num_attachments-pa.num_attachments
-            limit 100000;
-        </sql>
-    </report>
-
-    <report reportName="Attachment Diffs with exceptions"
-            reportFilename="attachments/attachment_diffs_with_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.num_attachments as NUM_ATTACHMENTS_A,
-            pb.num_attachments as NUM_ATTACHMENTS_B,
-            pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B,
-            refea.parse_exception_description as PARSE_EXCEPTION_A,
-            refeb.parse_exception_description as PARSE_EXCEPTION_B
-            from profiles_a pa
-            join profiles_b pb on pa.id= pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            left join exceptions_a ea on ea.id=pa.id
-            left join exceptions_b eb on eb.id=pb.id
-            left join ref_parse_exception_types refea on ea.parse_exception_id=refea.parse_exception_id
-            left join ref_parse_exception_types refeb on eb.parse_exception_id=refeb.parse_exception_id
-            where pa.is_embedded=false
-            and pa.num_attachments &lt;&gt; pb.num_attachments
-            order by ma.mime_string, pb.num_attachments-pa.num_attachments
-            limit 100000;
-        </sql>
-    </report>
-
-    <report reportName="Files missing in B by Mime"
-            reportFilename="attachments/all_files_missing_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_a pa
-            left join profiles_b pb on pa.id=pb.id
-            join mimes m on pa.mime_id=m.mime_id
-            where pb.id is null
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container files missing in B by Mime"
-            reportFilename="attachments/container_files_missing_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_a pa
-            left join profiles_b pb on pa.id=pb.id
-            join mimes m on pa.mime_id=m.mime_id
-            where pb.id is null and pa.is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Embedded files missing in B by Mime"
-            reportFilename="attachments/embedded_files_missing_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_a pa
-            left join profiles_b pb on pa.id=pb.id
-            join mimes m on pa.mime_id=m.mime_id
-            where pb.id is null and pa.is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="All files missing in A by Mime"
-            reportFilename="attachments/all_files_missing_in_A_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_b pb
-            left join profiles_a pa on pb.id=pa.id
-            join mimes m on pb.mime_id=m.mime_id
-            where pa.id is null
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container files missing in A by Mime"
-            reportFilename="attachments/container_files_missing_in_A_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_b pb
-            left join profiles_a pa on pb.id=pa.id
-            join mimes m on pb.mime_id=m.mime_id
-            where pa.id is null and pb.is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Embedded files missing in A by Mime"
-            reportFilename="attachments/embedded_files_missing_in_A_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_b pb
-            left join profiles_a pa on pb.id=pa.id
-            join mimes m on pb.mime_id=m.mime_id
-            where pa.id is null and pb.is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <!-- metadata values -->
-    <report reportName="Metadata Value Diffs"
-            reportFilename="metadata/metadata_value_count_diffs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            ma.mime_string as mime_string_a,
-            mb.mime_string as mime_string_b,
-            pa.num_metadata_values as num_metadata_values_a,
-            pb.num_metadata_values as num_metadata_values_b,
-            ea.parse_exception_id as parse_ex_id_a,
-            eb.parse_exception_id as parse_ex_id_b
-            from profiles_a pa
-            join profiles_b pb on pa.id= pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            left join exceptions_a ea on ea.id=pa.id
-            left join exceptions_b eb on eb.id=pb.id
-            where
-            ea.parse_exception_id is null and
-            eb.parse_exception_id is null
-            and pa.num_metadata_values &lt;&gt; pb.num_metadata_values
-            order by ma.mime_string,
-            pb.num_metadata_values-pa.num_metadata_values
-            limit 100000
-        </sql>
-    </report>
-    <report reportName="Tag Count Diffs By Mime"
-            reportFilename="tags/tag_count_diffs_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select ma.mime_string as mime_string_a,
-            mb.mime_string as mime_string_b,
-            tags_a_a,
-            tags_a_b,
-            tags_b_a,
-            tags_b_b,
-            tags_div_a,
-            tags_div_b,
-            tags_i_a,
-            tags_i_b,
-            tags_li_a,
-            tags_li_b,
-            tags_ol_a,
-            tags_ol_b,
-            tags_p_a,
-            tags_p_b,
-            tags_table_a,
-            tags_table_b,
-            tags_td_a,
-            tags_td_b,
-            tags_title_a,
-            tags_title_b,
-            tags_tr_a,
-            tags_tr_b,
-            tags_u_a,
-            tags_u_b,
-            tags_ul_a,
-            tags_ul_b
-            from
-            tags_by_mime tbm
-            join mimes ma on tbm.mime_id_a=ma.mime_id
-            join mimes mb on tbm.mime_id_b=mb.mime_id
-            limit 100000
-        </sql>
-
-    </report>
-    <report reportName="Tag Exceptions By Mime"
-            reportFilename="tags/tag_exceptions_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select ma.mime_string as mime_string_a,
-            mb.mime_string as mime_string_b,
-            tag_exceptions_a,
-            tag_exceptions_b,
-            (tag_exceptions_b-tag_exceptions_a) as diff_tag_exceptions_in_b
-            from tag_exceptions_by_mime tebm
-            join mimes ma on tebm.mime_id_a=ma.mime_id
-            join mimes mb on tebm.mime_id_b=mb.mime_id
-            order by diff_tag_exceptions_in_b desc
-        </sql>
-    </report>
-    <report reportName="Tag Exceptions Details A"
-                         reportFilename="tags/tag_exceptions_details_a.xlsx"
-                         format="xlsx"
-                         includeSql="true">
-        <sql>
-            select c.file_path,pa.file_name,mime_string,is_embedded from
-            tags_a ta
-            join profiles_a pa on ta.id=pa.id
-            join containers c on pa.container_id=c.container_id
-            join mimes m on pa.mime_id=m.mime_id
-            where ta.tags_parse_exception=true
-            order by m.mime_string
-            limit 20000
-        </sql>
-    </report>
-    <report reportName="Tag Exceptions Details B"
-            reportFilename="tags/tag_exceptions_details_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select c.file_path,pb.file_name,mime_string,is_embedded from
-            tags_b tb
-            join profiles_b pb on tb.id=pb.id
-            join containers c on pb.container_id=c.container_id
-            join mimes m on pb.mime_id=m.mime_id
-            where tb.tags_parse_exception=true
-            order by m.mime_string
-            limit 20000
-        </sql>
-    </report>
-
-    <report reportName="Parse Time (Millis) Compared"
-            reportFilename="parse_times/parse_time_millis_by_mime_compared.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            total_a as TOTAL_MILLIS_A, total_b as TOTAL_MILLIS_B,
-            prcnt_increase as PERCENT_INCREASE
-            from parse_time_compared ptc
-            join mimes ma on ptc.mime_id_a=ma.mime_id
-            join mimes mb on ptc.mime_id_b=mb.mime_id
-            where TOTAL_A &gt; 1000 AND TOTAL_B &gt; 1000 -- only show comparisons if &gt; a second
-            order by prcnt_increase desc
-        </sql>
-    </report>
-    <report reportName="Parse Time (Millis) Details"
-            reportFilename="parse_times/parse_time_millis_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, c.length as CONTAINTER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.elapsed_time_millis as TOTAL_MILLIS_A,
-            pb.elapsed_time_millis as TOTAL_MILLIS_B,
-            (pb.elapsed_time_millis-pa.elapsed_time_millis) as DIFF_MILLIS
-            from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            join containers c on pa.container_id=c.container_id
-            order by DIFF_MILLIS desc
-            limit 20000;
-        </sql>
-    </report>
-    <after>
-        <sql>drop table if exists md5_multiples_tmp_a</sql>
-        <sql>drop table if exists md5_multiples_tmp_b</sql>
-    </after>
+  <before>
+
+    <sql>drop table if exists md5_multiples_tmp_a</sql>
+    <sql>create table md5_multiples_tmp_a (MD5, cnt)
+      as
+      select md5, count(1) as cnt
+      from profiles_a
+      where md5 is not null
+      group by md5
+      having count(1) &gt; 1
+      order by cnt desc
+    </sql>
+
+    <sql>drop table if exists md5_multiples_tmp_b</sql>
+    <sql>create table md5_multiples_tmp_b (MD5, cnt)
+      as
+      select md5, count(1) cnt
+      from profiles_b
+      where md5 is not null
+      group by md5
+      having count(1) &gt; 1
+      order by cnt desc
+    </sql>
+    <!-- build mime indexes -->
+
+    <sql>create index if not exists pa_m_idx
+      on profiles_a (mime_id);
+    </sql>
+
+    <sql>
+      create index if not exists pb_m_idx
+      on profiles_b (mime_id);
+    </sql>
+
+    <!-- build exceptions comparison table -->
+    <sql>drop table if exists exceptions_compared</sql>
+    <sql>
+      create table exceptions_compared (
+      mime_id_a integer,
+      mime_id_b integer,
+      total integer,
+      exc_cnt_a integer,
+      exc_cnt_b integer,
+      exc_prcnt_a float,
+      exc_prcnt_b float,
+      notes varchar(12)
+      );
+    </sql>
+    <sql>
+      insert into exceptions_compared (
+      select ma.mime_id, mb.mime_id, count(1) as total, 0, 0, 0.0, 0.0, ''
+      from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join mimes ma on pa.mime_id = ma.mime_id
+      join mimes mb on pb.mime_id = mb.mime_id
+      group by ma.mime_id, mb.mime_id
+      order by total desc );
+    </sql>
+
+    <sql>
+      update exceptions_compared ec set
+      exc_cnt_a = (
+      select count(1) as cnt
+      from exceptions_a ea
+      join profiles_a pa on ea.id=pa.id
+      join profiles_b pb on pb.id=pa.id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
+      group by ma.mime_id, mb.mime_id);
+    </sql>
+    <sql>
+      update exceptions_compared ec set
+      exc_cnt_b = (
+      select count(1) as cnt
+      from exceptions_b eb
+      join profiles_b pb on eb.id=pb.id
+      join profiles_a pa on pa.id=pb.id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
+      group by mb.mime_id, ma.mime_id);
+    </sql>
+    <sql>
+      update exceptions_compared
+      set exc_prcnt_a = cast(exc_cnt_a as decimal)/cast(total as decimal)
+      where total > 0;
+    </sql>
+    <sql>
+      update exceptions_compared
+      set exc_prcnt_b = cast(exc_cnt_b as decimal)/cast(total as decimal)
+      where total > 0;
+    </sql>
+
+    <sql>
+      update exceptions_compared
+      set notes = 'YAY!'
+      where total > 100 and (exc_prcnt_a-exc_prcnt_b) > 0.10;
+    </sql>
+    <sql>
+      update exceptions_compared
+      set notes = 'YIKES!'
+      where total > 100 and (exc_prcnt_b-exc_prcnt_a) > 0.10;
+    </sql>
+
+    <!-- build tmp common words table -->
+    <sql>drop table if exists token_counts_compared</sql>
+    <sql>
+      create table token_counts_compared
+      (mime_id_a integer,
+      mime_id_b integer,
+      num_tokens_a bigint default 0,
+      num_tokens_b bigint default 0,
+      num_alphabetic_tokens_a bigint default 0,
+      num_alphabetic_tokens_b bigint default 0,
+      num_common_tokens_a bigint default 0,
+      num_common_tokens_b bigint default 0
+      );
+    </sql>
+    <sql>
+      insert into token_counts_compared (mime_id_a, mime_id_b)
+      select ma.mime_id, mb.mime_id
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_tokens_a=(
+      select sum(num_tokens) as cnt from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join contents_a c on c.id = pa.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_tokens_b=(
+      select sum(num_tokens) as cnt from profiles_b pb
+      join profiles_a pa on pa.id=pb.id
+      join contents_b c on c.id = pb.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_alphabetic_tokens_a=(
+      select sum(num_alphabetic_tokens) as cnt from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join contents_a c on c.id = pa.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_alphabetic_tokens_b=(
+      select sum(num_alphabetic_tokens) as cnt from profiles_b pb
+      join profiles_a pa on pb.id=pa.id
+      join contents_b c on c.id = pb.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_common_tokens_a=(
+      select sum(num_common_tokens) as cnt from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join contents_a c on c.id = pa.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_common_tokens_b=(
+      select sum(num_common_tokens) as cnt from profiles_b pb
+      join profiles_a pa on pa.id=pb.id
+      join contents_b c on c.id = pb.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>drop table if exists tags_by_mime</sql>
+    <sql>create table tags_by_mime (
+      mime_id_a integer,
+      mime_id_b integer,
+      tags_a_a integer,
+      tags_b_a integer,
+      tags_div_a integer,
+      tags_i_a integer,
+      tags_img_a integer,
+      tags_li_a integer,
+      tags_ol_a integer,
+      tags_p_a integer,
+      tags_table_a integer,
+      tags_td_a integer,
+      tags_title_a integer,
+      tags_tr_a integer,
+      tags_u_a integer,
+      tags_ul_a integer,
+      tags_a_b integer,
+      tags_b_b integer,
+      tags_div_b integer,
+      tags_i_b integer,
+      tags_img_b integer,
+      tags_li_b integer,
+      tags_ol_b integer,
+      tags_p_b integer,
+      tags_table_b integer,
+      tags_td_b integer,
+      tags_title_b integer,
+      tags_tr_b integer,
+      tags_u_b integer,
+      tags_ul_b integer
+      );
+    </sql>
+    <sql>
+      insert into tags_by_mime (mime_id_a, mime_id_b)
+      select ma.mime_id, mb.mime_id
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_a_a=(
+      select sum(ta.tags_a) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_b_a=(
+      select sum(ta.tags_b) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_div_a=(
+      select sum(ta.tags_div) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_i_a=(
+      select sum(ta.tags_i) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_img_a=(
+      select sum(ta.tags_img) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_li_a=(
+      select sum(ta.tags_li) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ol_a=(
+      select sum(ta.tags_ol) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_p_a=(
+      select sum(ta.tags_p) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_table_a=(
+      select sum(ta.tags_table) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_td_a=(
+      select sum(ta.tags_td) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_title_a=(
+      select sum(ta.tags_title) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_tr_a=(
+      select sum(ta.tags_tr) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_u_a=(
+      select sum(ta.tags_u) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ul_a=(
+      select sum(ta.tags_ul) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <!-- now update tags_b counts -->
+    <sql>
+      update tags_by_mime tbm set tags_a_b=(
+      select sum(tb.tags_a) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_b_b=(
+      select sum(tb.tags_b) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_div_b=(
+      select sum(tb.tags_div) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_i_b=(
+      select sum(tb.tags_i) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_img_b=(
+      select sum(tb.tags_img) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_li_b=(
+      select sum(tb.tags_li) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ol_b=(
+      select sum(tb.tags_ol) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_p_b=(
+      select sum(tb.tags_p) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_table_b=(
+      select sum(tb.tags_table) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_td_b=(
+      select sum(tb.tags_td) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_title_b=(
+      select sum(tb.tags_title) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_tr_b=(
+      select sum(tb.tags_tr) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_u_b=(
+      select sum(tb.tags_u) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ul_b=(
+      select sum(tb.tags_ul) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>drop table if exists tag_exceptions_by_mime</sql>
+    <sql>create table tag_exceptions_by_mime (
+      mime_id_a integer,
+      mime_id_b integer,
+      tag_exceptions_a integer,
+      tag_exceptions_b integer)
+    </sql>
+    <sql>
+      insert into tag_exceptions_by_mime (mime_id_a, mime_id_b,
+      tag_exceptions_a, tag_exceptions_b)
+      select ma.mime_id, mb.mime_id,0,0
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+    </sql>
+    <sql>
+      update tag_exceptions_by_mime tebm set tag_exceptions_a=(
+      select count(1) as cnt from tags_a ta
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tebm.mime_id_b
+      and pa.mime_id=tebm.mime_id_a
+      and ta.tags_parse_exception=true
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tag_exceptions_by_mime tebm set tag_exceptions_b=(
+      select count(1) as cnt from tags_b tb
+      join profiles_a pa on pa.id=tb.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tebm.mime_id_b
+      and pa.mime_id=tebm.mime_id_a
+      and tb.tags_parse_exception=true
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      drop table if exists parse_time_compared;
+    </sql>
+    <sql>
+      create table parse_time_compared (
+      mime_id_a integer,
+      mime_id_b integer,
+      total_a bigint,
+      total_b bigint,
+      prcnt_increase double precision
+      );
+    </sql>
+    <sql>
+      insert into parse_time_compared (mime_id_a, mime_id_b,
+      total_a, total_b, prcnt_increase)
+      select ma.mime_id, mb.mime_id,0,0,0.0
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+    </sql>
+    <sql>
+      update parse_time_compared ptc set total_a=(
+      select sum(pa.elapsed_time_millis) as total_a from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      where pa.mime_id= ptc.mime_id_a
+      and pb.mime_id=ptc.mime_id_b
+      group by mime_id_a, mime_id_b)
+    </sql>
+    <sql>
+      update parse_time_compared ptc set total_b=(
+      select sum(pb.elapsed_time_millis) as total_b from profiles_b pb
+      join profiles_a pa on pa.id=pb.id
+      where pa.mime_id= ptc.mime_id_a
+      and pb.mime_id=ptc.mime_id_b
+      group by mime_id_a, mime_id_b)
+    </sql>
+    <sql>
+      update parse_time_compared ptc set prcnt_increase=(100.0 *
+      cast(total_b as decimal)/cast(total_a as decimal))
+      where total_a > 0;
+    </sql>
+  </before>
+
+  <!-- MIMES -->
+  <report reportName="All Mimes In A"
+          reportFilename="mimes/all_mimes_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_a p
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="All Mimes In B"
+          reportFilename="mimes/all_mimes_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_b p
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container Mimes In A"
+          reportFilename="mimes/container_mimes_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_a p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="Container Mimes In B"
+          reportFilename="mimes/container_mimes_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_b p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Embedded Mimes In A"
+          reportFilename="mimes/embedded_mimes_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_a p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="Embedded Mimes In B"
+          reportFilename="mimes/embedded_mimes_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_b p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Mime Differences A -> B"
+          reportFilename="mimes/mime_diffs_A_to_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
+      MIME_A_TO_MIME_B, count(1) as COUNT
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      where a.mime_id &lt;&gt; b.mime_id
+      group by MIME_A_TO_MIME_B
+      order by COUNT DESC
+    </sql>
+  </report>
+
+  <report reportName="Mime Differences A -> B Details"
+          reportFilename="mimes/mime_diffs_A_to_B_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
+      MIME_A_TO_MIME_B,
+      file_path,
+      c.length as CONTAINER_LENGTH,
+      a.file_name
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      join containers c on a.container_id=c.container_id
+      where a.mime_id &lt;&gt; b.mime_id
+      order by MIME_A_TO_MIME_B
+    </sql>
+  </report>
+
+
+  <!-- Exceptions -->
+  <report reportName="AllExceptionsByMimeA"
+          reportFilename="exceptions/exceptions_by_mime_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_a e
+      join profiles_a p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="AllExceptionsByMimeB"
+          reportFilename="exceptions/exceptions_by_mime_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_b e
+      join profiles_b p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="ContainerExceptionsByMimeA"
+          reportFilename="exceptions/container_exceptions_by_mime_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_a e
+      join profiles_a p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      and parse_exception_id=0
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="ContainerExceptionsByMimeB"
+          reportFilename="exceptions/container_exceptions_by_mime_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_b e
+      join profiles_b p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      and parse_exception_id=0
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="AllExceptionsByMimeByTypeA"
+          reportFilename="exceptions/exceptions_by_mime_by_type_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string as MIME_TYPE,
+      parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
+      from exceptions_a e
+      join profiles_a p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      join ref_parse_exception_types r on
+      r.parse_exception_id=e.parse_exception_id
+      group by m.mime_string, parse_exception_description
+      order by MIME_TYPE, EXCEPTION_TYPE
+    </sql>
+  </report>
+
+  <report reportName="AllExceptionsByMimeByTypeB"
+          reportFilename="exceptions/exceptions_by_mime_by_type_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string as MIME_TYPE,
+      parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
+      from exceptions_b e
+      join profiles_b p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      join ref_parse_exception_types r on
+      r.parse_exception_id=e.parse_exception_id
+      group by m.mime_string, parse_exception_description
+      order by MIME_TYPE, EXCEPTION_TYPE
+    </sql>
+  </report>
+
+  <report reportName="TextLostFromACausedByNewExceptionsInB"
+          reportFilename="exceptions/text_lost_from_A_caused_by_new_exceptions_in_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path as FILE_PATH,
+      c.length as CONTAINER_LENGTH,
+      ca.NUM_TOKENS as NUM_TOKENS_A,
+      cb.NUM_TOKENS as NUM_TOKENS_B,
+      ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A, cb.NUM_UNIQUE_TOKENS
+      as NUM_UNIQUE_TOKENS_B,
+      ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
+      ca.num_common_tokens as NUM_COMMON_TOKENS_A,
+      cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
+      cb.num_common_tokens as NUM_COMMON_TOKENS_B,
+      ca.top_n_tokens as TOP_N_TOKENS_A, cb.top_n_tokens as TOP_N_TOKENS_B,
+      eb.ORIG_STACK_TRACE as ORIG_STACK_TRACE_B
+      from contents_a ca
+      join profiles_a pa on ca.id = pa.id
+      join containers c on pa.container_id=c.container_id
+      left join contents_b cb on ca.id=cb.id
+      left join exceptions_b eb on ca.id = eb.id
+      left join exceptions_a ea on ca.id = ea.id
+      where eb.orig_stack_trace is not null
+      and ea.orig_stack_trace is null
+      order by ca.num_common_tokens - coalesce(cb.num_common_tokens,0) desc
+    </sql>
+  </report>
+
+  <report reportName="FixedExceptionsInBByMimeType"
+          reportFilename="exceptions/fixed_exceptions_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      count(1) as COUNT
+      from exceptions_a ea
+      left join exceptions_b eb on ea.id = eb.id
+      join profiles_a pa on pa.id=ea.id
+      join profiles_b pb on pa.id=pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where eb.id is null
+      and ea.parse_exception_id=0
+      group by mime_type_a, mime_type_b
+    </sql>
+  </report>
+
+  <report reportName="FixedExceptionsInByDetails"
+          reportFilename="exceptions/fixed_exceptions_in_B_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select
+      file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      pa.file_name, pa.is_embedded
+      from exceptions_a ea
+      left join exceptions_b eb on ea.id = eb.id
+      join profiles_a pa on pa.id=ea.id
+      join profiles_b pb on pb.id=pa.id --this ensures that files were actually processed in both runs
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where eb.id is null
+      and ea.parse_exception_id=0
+      order by mime_type_a, mime_type_b
+    </sql>
+  </report>
+  <report reportName="ContentsOfFixedExceptionsInB"
+          reportFilename="exceptions/contents_of_fixed_exceptions_in_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      CONTENT_LENGTH,
+      NUM_TOKENS, NUM_UNIQUE_TOKENS,
+      TOP_N_TOKENS, LANG_ID_1,TOKEN_LENGTH_MEAN, TOKEN_LENGTH_STD_DEV
+      from exceptions_a ea
+      left join exceptions_b eb on ea.id = eb.id
+      join profiles_a pa on pa.id=ea.id
+      join profiles_b pb on pa.id=pb.id
+      join contents_b cb on cb.id=ea.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where eb.id is null
+      and ea.parse_exception_id=0
+    </sql>
+  </report>
+
+  <report reportName="NewExceptionsByMimeType"
+          reportFilename="exceptions/new_exceptions_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select ma.mime_string as MIME_TYPE_A, mb.mime_string as MIME_TYPE_B, count(1) as COUNT
+      from exceptions_b eb
+      left join exceptions_a ea on ea.id = eb.id
+      join profiles_a pa on pa.id=eb.id
+      join profiles_b pb on pb.id=pa.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where ea.id is null
+      and eb.parse_exception_id=0
+      group by ma.mime_string, mb.mime_string
+      order by COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="NewExceptionsInBByMimeTypeByStackTrace"
+          reportFilename="exceptions/new_exceptions_in_B_by_mime_by_stack_trace.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select
+      ma.MIME_STRING as MIME_TYPE_A,
+      mb.MIME_STRING as MIME_TYPE_B,
+      eb.sort_stack_trace, count(1) as
+      COUNT
+      from exceptions_b eb
+      left join exceptions_a ea on ea.id = eb.id
+      join profiles_a pa on pa.id=eb.id
+      join profiles_b pb on pb.id=eb.id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where ea.id is null
+      and eb.parse_exception_id=0
+      group by MIME_TYPE_A, MIME_TYPE_B, eb.sort_stack_trace
+      order by MIME_TYPE_A asc, MIME_TYPE_B asc, COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="NewExceptionsInBDetails"
+          reportFilename="exceptions/new_exceptions_in_B_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      eb.orig_stack_trace, eb.sort_stack_trace
+      from exceptions_b eb
+      left join exceptions_a ea on ea.id = eb.id
+      join profiles_a pa on pa.id=eb.id
+      join profiles_b pb on pb.id=eb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where ea.id is null
+      and eb.parse_exception_id=0
+      order by MIME_TYPE_A asc, MIME_TYPE_B asc, eb.ORIG_STACK_TRACE
+    </sql>
+  </report>
+
+  <report reportName="StackTracesByMimeInA"
+          reportFilename="exceptions/stack_traces_by_mime_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
+      COUNT
+      from exceptions_a e
+      join profiles_a p on p.id=e.id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      group by MIME_TYPE, e.sort_stack_trace
+      order by MIME_TYPE asc, COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="AllStackTracesInA"
+          reportFilename="exceptions/stack_traces_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      mime_string as MIME_TYPE,
+      orig_stack_trace, sort_stack_trace
+      from exceptions_a e
+      join profiles_a p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
+      CONTAINER_LENGTH asc
+    </sql>
+  </report>
+  <report reportName="AllStackTracesInB"
+          reportFilename="exceptions/stack_traces_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      mime_string as MIME_TYPE,
+      orig_stack_trace, sort_stack_trace
+      from exceptions_b e
+      join profiles_b p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
+      CONTAINER_LENGTH asc
+    </sql>
+  </report>
+
+  <report reportName="StackTracesByMimeInB"
+          reportFilename="exceptions/stack_traces_by_mime_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
+      COUNT
+      from exceptions_b e
+      join profiles_b p on p.id=e.id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      group by MIME_TYPE, e.sort_stack_trace
+      order by MIME_TYPE asc, COUNT desc
+    </sql>
+  </report>
+  <report reportName="extractExceptionsA"
+          reportFilename="exceptions/extract_exceptions_a.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, extract_exception_description
+      from extract_exceptions_a e
+      join ref_extract_exception_types t
+      on e.extract_exception_id=t.extract_exception_id
+    </sql>
+  </report>
+  <report reportName="extractExceptionsB"
+          reportFilename="exceptions/extract_exceptions_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, extract_exception_description
+      from extract_exceptions_b e
+      join ref_extract_exception_types t
+      on e.extract_exception_id=t.extract_exception_id
+    </sql>
+  </report>
+  <report reportName="parseExceptionTypesA"
+          reportFilename="exceptions/overall_exception_types_a.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select parse_exception_description, count(1)
+      from exceptions_a e
+      join ref_parse_exception_types t on
+      t.parse_exception_id=e.parse_exception_id
+      group by t.parse_exception_description
+    </sql>
+  </report>
+  <report reportName="parseExceptionTypesB"
+          reportFilename="exceptions/overall_exception_types_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select parse_exception_description, count(1)
+      from exceptions_b e
+      join ref_parse_exception_types t on
+      t.parse_exception_id=e.parse_exception_id
+      group by t.parse_exception_description
+    </sql>
+  </report>
+
+  <report reportName="contentDiffsWExceptions"
+          reportFilename="content/content_diffs_with_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      ca.num_unique_tokens as NUM_UNIQUE_TOKENS_A,
+      cb.num_unique_tokens as NUM_UNIQUE_TOKENS_B,
+      ca.num_tokens as NUM_TOKENS_A,
+      cb.num_tokens as NUM_TOKENS_B,
+      ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
+      ca.num_common_tokens as NUM_COMMON_TOKENS_A,
+      cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
+      cb.num_common_tokens as NUM_COMMON_TOKENS_B,
+      coalesce(cb.num_common_tokens,0)-
+      coalesce(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
+      ca.top_n_tokens as TOP_N_TOKENS_A,
+      cb.top_n_tokens as TOP_N_TOKENS_B,
+      ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
+      cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
+      top_10_unique_token_diffs_a,
+      top_10_unique_token_diffs_b,
+      top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap,
+      ref_ea.parse_exception_description as EXCEPTION_A,
+      ref_eb.parse_exception_description as EXCEPTION_B
+      from content_comparisons cc
+      join contents_a ca on ca.id=cc.id
+      left join contents_b cb on cb.id=cc.id
+      join profiles_a pa on pa.id = cc.id
+      join profiles_b pb on pb.id=cc.id
+      join containers c on c.container_id=pa.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      left join exceptions_a ea on ea.id=cc.id
+      left join exceptions_b eb on eb.id=cc.id
+      left join ref_parse_exception_types ref_ea on ref_ea.parse_exception_id=ea.parse_exception_id
+      left join ref_parse_exception_types ref_eb on ref_eb.parse_exception_id=eb.parse_exception_id
+      where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
+      and (ea.parse_exception_id is null or
+      ea.parse_exception_id &lt;&gt; 2)
+      and (eb.parse_exception_id is null or
+      eb.parse_exception_id &lt;&gt; 2)
+      order by ma.mime_string, overlap asc
+      limit 100000
+    </sql>
+  </report>
+  <report reportName="contentDiffsNoExceptions"
+          reportFilename="content/content_diffs_no_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A,
+      cb.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_B,
+      ca.NUM_TOKENS as NUM_TOKENS_A,
+      cb.NUM_TOKENS as NUM_TOKENS_B,
+      ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
+      ca.num_common_tokens as NUM_COMMON_TOKENS_A,
+      cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
+      cb.num_common_tokens as NUM_COMMON_TOKENS_B,
+      coalesce(cb.num_common_tokens,0)-
+      coalesce(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
+      ca.top_n_tokens as TOP_N_TOKENS_A,
+      cb.top_n_tokens as TOP_N_TOKENS_B,
+      ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
+      cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
+      top_10_unique_token_diffs_a,
+      top_10_unique_token_diffs_b,
+      top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap
+      from content_comparisons cc
+      join contents_a ca on ca.id=cc.id
+      join contents_b cb on cb.id=cc.id
+      join profiles_a pa on pa.id = cc.id
+      join profiles_b pb on pb.id=cc.id
+      join containers c on c.container_id=pa.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      left join exceptions_a ea on ea.id=cc.id
+      left join exceptions_b eb on eb.id=cc.id
+      where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
+      and (ea.parse_exception_id is null)
+      and (eb.parse_exception_id is null)
+      order by ma.mime_string, overlap asc
+      limit 100000
+    </sql>
+  </report>
+
+  <report reportName="CommonTokenComparisonsByMimeType"
+          reportFilename="content/common_token_comparisons_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select ma.mime_string as MIME_STRING_A, mb.mime_string as MIME_STRING_B,
+      num_tokens_a, num_tokens_b,
+      num_alphabetic_tokens_a, num_alphabetic_tokens_b,
+      num_common_tokens_a, num_common_tokens_b,
+      coalesce(num_common_tokens_b, 0)-coalesce(num_common_tokens_a, 0) as change_in_common_tokens_b
+      from token_counts_compared tcc
+      join mimes ma on tcc.mime_id_a = ma.mime_id
+      join mimes mb on tcc.mime_id_b = mb.mime_id
+      order by change_in_common_tokens_b desc
+    </sql>
+  </report>
+  <report reportName="PageCountDiffs"
+          reportFilename="content/page_count_diffs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.num_pages as NUM_PAGES_A,
+      pb.num_pages as NUM_PAGES_B,
+      (pb.num_pages-pa.num_pages) as DIFF_NUM_PAGES_IN_B
+      from profiles_a pa
+      join profiles_b pb on pa.id = pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where pa.num_pages is not null
+      and pb.num_pages is not null
+      and pa.num_pages &lt;&gt; pb.num_pages
+      order by DIFF_NUM_PAGES_IN_B asc
+      limit 10000;
+    </sql>
+  </report>
+
+
+  <report reportName="ExceptionComparisonsByMimeType"
+          reportFilename="exceptions/exceptions_compared_by_mime_type.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select ma.mime_string as mime_string_a, mb.mime_string as mime_string_b,
+      total, exc_cnt_a,
+      exc_cnt_b,
+      exc_prcnt_a,
+      exc_prcnt_b, notes
+
+      from exceptions_compared e
+      join mimes ma on ma.mime_id=e.mime_id_a
+      join mimes mb on mb.mime_id=e.mime_id_b
+      order by (exc_prcnt_b-exc_prcnt_a) desc, total desc;
+    </sql>
+  </report>
+  <!--    <report reportName="MD5 Duplicate Counts A"
+              reportFilename="md5/md5_duplicate_counts_A.xlsx"
+              format="xlsx"
+                          includeSql="true">
+          <sql>
+              select md5, count(1) cnt
+              from profiles_a
+              group by md5
+              having cnt > 2
+              order by cnt desc
+          </sql>
+      </report>
+
+      <report reportName="MD5 Duplicate Counts B"
+              reportFilename="md5/md5_duplicate_counts_B.xlsx"
+              format="xlsx"
+                          includeSql="true">
+
+          <sql>
+              select md5, count(1) cnt
+              from profiles_b
+              group by md5
+              having cnt > 2
+              order by cnt desc
+          </sql>
+      </report>
+
+      <report reportName="MD5 Duplicates A"
+              reportFilename="md5/md5_duplicates_A.xlsx"
+              format="xlsx"
+                          includeSql="true">
+
+          <sql>
+              select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
+              from md5_multiples_tmp_a t
+              join profiles_a p on p.md5 = t.md5
+              join containers c on p.container_id = c.container_id
+              join contents_a cb on p.id=cb.id
+              order by t.cnt desc
+          </sql>
+      </report>
+
+      <report reportName="MD5 Duplicates B"
+              reportFilename="md5/md5_duplicates_B.xlsx"
+              format="xlsx"
+                          includeSql="true">
+
+          <sql>
+              select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
+              from md5_multiples_tmp_b t
+              join profiles_b p on p.md5 = t.md5
+              join containers c on p.container_id = c.container_id
+              join contents_b cb on p.id=cb.id
+              order by t.cnt desc
+          </sql>
+      </report>
+  -->
+
+  <report reportName="Attachment Diffs no Exceptions"
+          reportFilename="attachments/attachment_diffs_no_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.num_attachments as NUM_ATTACHMENTS_A,
+      pb.num_attachments as NUM_ATTACHMENTS_B,
+      pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B
+      from profiles_a pa
+      join profiles_b pb on pa.id= pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      left join exceptions_a ea on ea.id=pa.id
+      left join exceptions_b eb on eb.id=pb.id
+      where pa.is_embedded=false and
+      ea.parse_exception_id is null and
+      eb.parse_exception_id is null
+      and pa.num_attachments &lt;&gt; pb.num_attachments
+      order by ma.mime_string, pb.num_attachments-pa.num_attachments
+      limit 100000;
+    </sql>
+  </report>
+
+  <report reportName="Attachment Diffs with exceptions"
+          reportFilename="attachments/attachment_diffs_with_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.num_attachments as NUM_ATTACHMENTS_A,
+      pb.num_attachments as NUM_ATTACHMENTS_B,
+      pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B,
+      refea.parse_exception_description as PARSE_EXCEPTION_A,
+      refeb.parse_exception_description as PARSE_EXCEPTION_B
+      from profiles_a pa
+      join profiles_b pb on pa.id= pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      left join exceptions_a ea on ea.id=pa.id
+      left join exceptions_b eb on eb.id=pb.id
+      left join ref_parse_exception_types refea on ea.parse_exception_id=refea.parse_exception_id
+      left join ref_parse_exception_types refeb on eb.parse_exception_id=refeb.parse_exception_id
+      where pa.is_embedded=false
+      and pa.num_attachments &lt;&gt; pb.num_attachments
+      order by ma.mime_string, pb.num_attachments-pa.num_attachments
+      limit 100000;
+    </sql>
+  </report>
+
+  <report reportName="Files missing in B by Mime"
+          reportFilename="attachments/all_files_missing_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_a pa
+      left join profiles_b pb on pa.id=pb.id
+      join mimes m on pa.mime_id=m.mime_id
+      where pb.id is null
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container files missing in B by Mime"
+          reportFilename="attachments/container_files_missing_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_a pa
+      left join profiles_b pb on pa.id=pb.id
+      join mimes m on pa.mime_id=m.mime_id
+      where pb.id is null and pa.is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Embedded files missing in B by Mime"
+          reportFilename="attachments/embedded_files_missing_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_a pa
+      left join profiles_b pb on pa.id=pb.id
+      join mimes m on pa.mime_id=m.mime_id
+      where pb.id is null and pa.is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="All files missing in A by Mime"
+          reportFilename="attachments/all_files_missing_in_A_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_b pb
+      left join profiles_a pa on pb.id=pa.id
+      join mimes m on pb.mime_id=m.mime_id
+      where pa.id is null
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container files missing in A by Mime"
+          reportFilename="attachments/container_files_missing_in_A_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_b pb
+      left join profiles_a pa on pb.id=pa.id
+      join mimes m on pb.mime_id=m.mime_id
+      where pa.id is null and pb.is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Embedded files missing in A by Mime"
+          reportFilename="attachments/embedded_files_missing_in_A_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_b pb
+      left join profiles_a pa on pb.id=pa.id
+      join mimes m on pb.mime_id=m.mime_id
+      where pa.id is null and pb.is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <!-- metadata values -->
+  <report reportName="Metadata Value Diffs"
+          reportFilename="metadata/metadata_value_count_diffs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      ma.mime_string as mime_string_a,
+      mb.mime_string as mime_string_b,
+      pa.num_metadata_values as num_metadata_values_a,
+      pb.num_metadata_values as num_metadata_values_b,
+      ea.parse_exception_id as parse_ex_id_a,
+      eb.parse_exception_id as parse_ex_id_b
+      from profiles_a pa
+      join profiles_b pb on pa.id= pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      left join exceptions_a ea on ea.id=pa.id
+      left join exceptions_b eb on eb.id=pb.id
+      where
+      ea.parse_exception_id is null and
+      eb.parse_exception_id is null
+      and pa.num_metadata_values &lt;&gt; pb.num_metadata_values
+      order by ma.mime_string,
+      pb.num_metadata_values-pa.num_metadata_values
+      limit 100000
+    </sql>
+  </report>
+  <report reportName="Tag Count Diffs By Mime"
+          reportFilename="tags/tag_count_diffs_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select ma.mime_string as mime_string_a,
+      mb.mime_string as mime_string_b,
+      tags_a_a,
+      tags_a_b,
+      tags_b_a,
+      tags_b_b,
+      tags_div_a,
+      tags_div_b,
+      tags_i_a,
+      tags_i_b,
+      tags_li_a,
+      tags_li_b,
+      tags_ol_a,
+      tags_ol_b,
+      tags_p_a,
+      tags_p_b,
+      tags_table_a,
+      tags_table_b,
+      tags_td_a,
+      tags_td_b,
+      tags_title_a,
+      tags_title_b,
+      tags_tr_a,
+      tags_tr_b,
+      tags_u_a,
+      tags_u_b,
+      tags_ul_a,
+      tags_ul_b
+      from
+      tags_by_mime tbm
+      join mimes ma on tbm.mime_id_a=ma.mime_id
+      join mimes mb on tbm.mime_id_b=mb.mime_id
+      limit 100000
+    </sql>
+
+  </report>
+  <report reportName="Tag Exceptions By Mime"
+          reportFilename="tags/tag_exceptions_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select ma.mime_string as mime_string_a,
+      mb.mime_string as mime_string_b,
+      tag_exceptions_a,
+      tag_exceptions_b,
+      (tag_exceptions_b-tag_exceptions_a) as diff_tag_exceptions_in_b
+      from tag_exceptions_by_mime tebm
+      join mimes ma on tebm.mime_id_a=ma.mime_id
+      join mimes mb on tebm.mime_id_b=mb.mime_id
+      order by diff_tag_exceptions_in_b desc
+    </sql>
+  </report>
+  <report reportName="Tag Exceptions Details A"
+          reportFilename="tags/tag_exceptions_details_a.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select c.file_path,pa.file_name,mime_string,is_embedded from
+      tags_a ta
+      join profiles_a pa on ta.id=pa.id
+      join containers c on pa.container_id=c.container_id
+      join mimes m on pa.mime_id=m.mime_id
+      where ta.tags_parse_exception=true
+      order by m.mime_string
+      limit 20000
+    </sql>
+  </report>
+  <report reportName="Tag Exceptions Details B"
+          reportFilename="tags/tag_exceptions_details_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select c.file_path,pb.file_name,mime_string,is_embedded from
+      tags_b tb
+      join profiles_b pb on tb.id=pb.id
+      join containers c on pb.container_id=c.container_id
+      join mimes m on pb.mime_id=m.mime_id
+      where tb.tags_parse_exception=true
+      order by m.mime_string
+      limit 20000
+    </sql>
+  </report>
+
+  <report reportName="Parse Time (Millis) Compared"
+          reportFilename="parse_times/parse_time_millis_by_mime_compared.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      total_a as TOTAL_MILLIS_A, total_b as TOTAL_MILLIS_B,
+      prcnt_increase as PERCENT_INCREASE
+      from parse_time_compared ptc
+      join mimes ma on ptc.mime_id_a=ma.mime_id
+      join mimes mb on ptc.mime_id_b=mb.mime_id
+      where TOTAL_A &gt; 1000 AND TOTAL_B &gt; 1000 -- only show comparisons if &gt; a second
+      order by prcnt_increase desc
+    </sql>
+  </report>
+  <report reportName="Parse Time (Millis) Details"
+          reportFilename="parse_times/parse_time_millis_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, c.length as CONTAINTER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.elapsed_time_millis as TOTAL_MILLIS_A,
+      pb.elapsed_time_millis as TOTAL_MILLIS_B,
+      (pb.elapsed_time_millis-pa.elapsed_time_millis) as DIFF_MILLIS
+      from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      join containers c on pa.container_id=c.container_id
+      order by DIFF_MILLIS desc
+      limit 20000;
+    </sql>
+  </report>
+  <after>
+    <sql>drop table if exists md5_multiples_tmp_a</sql>
+    <sql>drop table if exists md5_multiples_tmp_b</sql>
+  </after>
 </reports>
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/main/resources/comparison-reports-tags.xml b/tika-eval/tika-eval-app/src/main/resources/comparison-reports-tags.xml
index dd53a30e4..6acb47590 100644
--- a/tika-eval/tika-eval-app/src/main/resources/comparison-reports-tags.xml
+++ b/tika-eval/tika-eval-app/src/main/resources/comparison-reports-tags.xml
@@ -22,1761 +22,1761 @@
 <reports>
 
 
-    <before>
-
-        <sql>drop table if exists md5_multiples_tmp_a</sql>
-        <sql>create table md5_multiples_tmp_a (MD5 char(32), cnt int)
-            as
-            select md5, count(1) cnt
-            from profiles_a
-            where md5 is not null
-            group by md5
-            having cnt &gt; 1
-            order by cnt desc
-        </sql>
-
-        <sql>drop table if exists md5_multiples_tmp_b</sql>
-        <sql>create table md5_multiples_tmp_b (MD5 char(32), cnt int)
-            as
-            select md5, count(1) cnt
-            from profiles_b
-            where md5 is not null
-            group by md5
-            having cnt &gt; 1
-            order by cnt desc
-        </sql>
-        <!-- build mime indexes -->
-
-        <sql>create index if not exists pa_m_idx
-            on profiles_a (mime_id);
-        </sql>
-
-        <sql>
-            create index if not exists pb_m_idx
-            on profiles_b (mime_id);
-        </sql>
-
-        <!-- build exceptions comparison table -->
-        <sql>drop table if exists exceptions_compared</sql>
-        <sql>
-            create table exceptions_compared (
-            mime_id_a integer,
-            mime_id_b integer,
-            total integer,
-            exc_cnt_a integer,
-            exc_cnt_b integer,
-            exc_prcnt_a float,
-            exc_prcnt_b float,
-            notes varchar(12)
-            );
-        </sql>
-        <sql>
-            insert into exceptions_compared (
-            select ma.mime_id, mb.mime_id, count(1) as total, 0, 0, 0.0, 0.0, ''
-            from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join mimes ma on pa.mime_id = ma.mime_id
-            join mimes mb on pb.mime_id = mb.mime_id
-            group by ma.mime_id, mb.mime_id
-            order by total desc );
-        </sql>
-
-        <sql>
-            update exceptions_compared ec set
-            exc_cnt_a = (
-            select count(1) as cnt
-            from exceptions_a ea
-            join profiles_a pa on ea.id=pa.id
-            join profiles_b pb on pb.id=pa.id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
-            group by ma.mime_id, mb.mime_id);
-        </sql>
-        <sql>
-            update exceptions_compared ec set
-            exc_cnt_b = (
-            select count(1) as cnt
-            from exceptions_b eb
-            join profiles_b pb on eb.id=pb.id
-            join profiles_a pa on pa.id=pb.id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
-            group by mb.mime_id, ma.mime_id);
-        </sql>
-        <sql>
-            update exceptions_compared
-            set exc_prcnt_a = cast(exc_cnt_a as double)/cast(total as double)
-            where total > 0;
-        </sql>
-        <sql>
-            update exceptions_compared
-            set exc_prcnt_b = cast(exc_cnt_b as double)/cast(total as double)
-            where total > 0;
-        </sql>
-
-        <sql>
-            update exceptions_compared
-            set notes = 'YAY!'
-            where total > 100 and (exc_prcnt_a-exc_prcnt_b) > 0.10;
-        </sql>
-        <sql>
-            update exceptions_compared
-            set notes = 'YIKES!'
-            where total > 100 and (exc_prcnt_b-exc_prcnt_a) > 0.10;
-        </sql>
-
-        <!-- build tmp common words table -->
-        <sql>drop table if exists token_counts_compared</sql>
-        <sql>
-            create table token_counts_compared
-            (mime_id_a integer,
-            mime_id_b integer,
-            num_tokens_a long default 0,
-            num_tokens_b long default 0,
-            num_alphabetic_tokens_a long default 0,
-            num_alphabetic_tokens_b long default 0,
-            num_common_tokens_a long default 0,
-            num_common_tokens_b long default 0
-            );
-        </sql>
-        <sql>
-            insert into token_counts_compared (mime_id_a, mime_id_b)
-            select ma.mime_id, mb.mime_id
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            group by ma.mime_id, mb.mime_id
-
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_tokens_a=(
-            select sum(num_tokens) as cnt from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join contents_a c on c.id = pa.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_tokens_b=(
-            select sum(num_tokens) as cnt from profiles_b pb
-            join profiles_a pa on pa.id=pb.id
-            join contents_b c on c.id = pb.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_alphabetic_tokens_a=(
-            select sum(num_alphabetic_tokens) as cnt from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join contents_a c on c.id = pa.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_alphabetic_tokens_b=(
-            select sum(num_alphabetic_tokens) as cnt from profiles_b pb
-            join profiles_a pa on pb.id=pa.id
-            join contents_b c on c.id = pb.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_common_tokens_a=(
-            select sum(num_common_tokens) as cnt from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join contents_a c on c.id = pa.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>
-            update token_counts_compared tcc set num_common_tokens_b=(
-            select sum(num_common_tokens) as cnt from profiles_b pb
-            join profiles_a pa on pa.id=pb.id
-            join contents_b c on c.id = pb.id
-            where pb.mime_id= tcc.mime_id_b
-            and pa.mime_id=tcc.mime_id_a
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-
-        <sql>drop table if exists tags_by_mime</sql>
-        <sql>create table tags_by_mime (
-                mime_id_a integer,
-                mime_id_b integer,
-                tags_a_a integer,
-                tags_b_a integer,
-                tags_div_a integer,
-                tags_i_a integer,
-                tags_img_a integer,
-                tags_li_a integer,
-                tags_ol_a integer,
-                tags_p_a integer,
-                tags_table_a integer,
-                tags_td_a integer,
-                tags_title_a integer,
-                tags_tr_a integer,
-                tags_u_a integer,
-                tags_ul_a integer,
-                tags_a_b integer,
-                tags_b_b integer,
-                tags_div_b integer,
-                tags_i_b integer,
-                tags_img_b integer,
-                tags_li_b integer,
-                tags_ol_b integer,
-                tags_p_b integer,
-                tags_table_b integer,
-                tags_td_b integer,
-                tags_title_b integer,
-                tags_tr_b integer,
-                tags_u_b integer,
-                tags_ul_b integer
-            );
-        </sql>
-        <sql>
-            insert into tags_by_mime (mime_id_a, mime_id_b)
-            select ma.mime_id, mb.mime_id
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            group by ma.mime_id, mb.mime_id
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_a_a=(
-            select sum(ta.tags_a) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_b_a=(
-            select sum(ta.tags_b) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_div_a=(
-            select sum(ta.tags_div) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_i_a=(
-            select sum(ta.tags_i) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_img_a=(
-            select sum(ta.tags_img) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_li_a=(
-            select sum(ta.tags_li) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ol_a=(
-            select sum(ta.tags_ol) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_p_a=(
-            select sum(ta.tags_p) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_table_a=(
-            select sum(ta.tags_table) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_td_a=(
-            select sum(ta.tags_td) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_title_a=(
-            select sum(ta.tags_title) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_tr_a=(
-            select sum(ta.tags_tr) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_u_a=(
-            select sum(ta.tags_u) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ul_a=(
-            select sum(ta.tags_ul) as cnt from tags_a ta
-            join tags_b tb on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <!-- now update tags_b counts -->
-        <sql>
-            update tags_by_mime tbm set tags_a_b=(
-            select sum(tb.tags_a) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_b_b=(
-            select sum(tb.tags_b) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_div_b=(
-            select sum(tb.tags_div) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_i_b=(
-            select sum(tb.tags_i) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_img_b=(
-            select sum(tb.tags_img) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_li_b=(
-            select sum(tb.tags_li) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ol_b=(
-            select sum(tb.tags_ol) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_p_b=(
-            select sum(tb.tags_p) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_table_b=(
-            select sum(tb.tags_table) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_td_b=(
-            select sum(tb.tags_td) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_title_b=(
-            select sum(tb.tags_title) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_tr_b=(
-            select sum(tb.tags_tr) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_u_b=(
-            select sum(tb.tags_u) as cnt from tags_b tb
-            join tags_a ta on tb.id=ta.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tags_by_mime tbm set tags_ul_b=(
-            select sum(tb.tags_ul) as cnt from tags_b tb
-            join tags_a ta on ta.id=tb.id
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tbm.mime_id_b
-            and pa.mime_id=tbm.mime_id_a
-            and ta.tags_parse_exception=false
-            and tb.tags_parse_exception=false
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>drop table if exists tag_exceptions_by_mime</sql>
-        <sql>create table tag_exceptions_by_mime (
-            mime_id_a integer,
-            mime_id_b integer,
-            tag_exceptions_a integer,
-            tag_exceptions_b integer)
-        </sql>
-        <sql>
-            insert into tag_exceptions_by_mime (mime_id_a, mime_id_b,
-                tag_exceptions_a, tag_exceptions_b)
-            select ma.mime_id, mb.mime_id,0,0
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            group by ma.mime_id, mb.mime_id
-        </sql>
-        <sql>
-            update tag_exceptions_by_mime tebm set tag_exceptions_a=(
-            select count(1) as cnt from tags_a ta
-            join profiles_a pa on pa.id=ta.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tebm.mime_id_b
-            and pa.mime_id=tebm.mime_id_a
-            and ta.tags_parse_exception=true
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            update tag_exceptions_by_mime tebm set tag_exceptions_b=(
-            select count(1) as cnt from tags_b tb
-            join profiles_a pa on pa.id=tb.id
-            join profiles_b pb on pa.id=pb.id
-            where pb.mime_id= tebm.mime_id_b
-            and pa.mime_id=tebm.mime_id_a
-            and tb.tags_parse_exception=true
-            group by mime_id_a, mime_id_b
-            );
-        </sql>
-        <sql>
-            drop table if exists parse_time_compared;
-        </sql>
-        <sql>
-            create table parse_time_compared (
-            mime_id_a integer,
-            mime_id_b integer,
-            total_a bigint,
-            total_b bigint,
-            prcnt_increase double
-            );
-        </sql>
-            <sql>
-                insert into parse_time_compared (mime_id_a, mime_id_b,
-                total_a, total_b, prcnt_increase)
-                select ma.mime_id, mb.mime_id,0,0,0.0
-                from profiles_a a
-                join profiles_b b on a.id=b.id
-                join mimes ma on ma.mime_id=a.mime_id
-                join mimes mb on mb.mime_id=b.mime_id
-                group by ma.mime_id, mb.mime_id
-            </sql>
-        <sql>
-            update parse_time_compared ptc set total_a=(
-            select sum(pa.elapsed_time_millis) as total_a from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            where pa.mime_id= ptc.mime_id_a
-            and pb.mime_id=ptc.mime_id_b
-            group by mime_id_a, mime_id_b)
-        </sql>
-        <sql>
-            update parse_time_compared ptc set total_b=(
-            select sum(pb.elapsed_time_millis) as total_b from profiles_b pb
-            join profiles_a pa on pa.id=pb.id
-            where pa.mime_id= ptc.mime_id_a
-            and pb.mime_id=ptc.mime_id_b
-            group by mime_id_a, mime_id_b)
-        </sql>
-        <sql>
-            update parse_time_compared ptc set prcnt_increase=(100.0 *
-            cast(total_b as float)/cast(total_a as float))
-            where total_a > 0;
-        </sql>
-    </before>
-
-    <!-- MIMES -->
-    <report reportName="All Mimes In A"
-            reportFilename="mimes/all_mimes_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_a p
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="All Mimes In B"
-            reportFilename="mimes/all_mimes_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_b p
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container Mimes In A"
-            reportFilename="mimes/container_mimes_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_a p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="Container Mimes In B"
-            reportFilename="mimes/container_mimes_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_b p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Embedded Mimes In A"
-            reportFilename="mimes/embedded_mimes_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_a p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="Embedded Mimes In B"
-            reportFilename="mimes/embedded_mimes_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles_b p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Mime Differences A -> B"
-            reportFilename="mimes/mime_diffs_A_to_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
-            MIME_A_TO_MIME_B, count(1) as COUNT
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            where a.mime_id &lt;&gt; b.mime_id
-            group by MIME_A_TO_MIME_B
-            order by COUNT DESC
-        </sql>
-    </report>
-
-    <report reportName="Mime Differences A -> B Details"
-            reportFilename="mimes/mime_diffs_A_to_B_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
-            MIME_A_TO_MIME_B,
-            file_path,
-            c.length as CONTAINER_LENGTH,
-            a.file_name
-            from profiles_a a
-            join profiles_b b on a.id=b.id
-            join mimes ma on ma.mime_id=a.mime_id
-            join mimes mb on mb.mime_id=b.mime_id
-            join containers c on a.container_id=c.container_id
-            where a.mime_id &lt;&gt; b.mime_id
-            order by MIME_A_TO_MIME_B
-        </sql>
-    </report>
-
-
-    <!-- Exceptions -->
-    <report reportName="AllExceptionsByMimeA"
-            reportFilename="exceptions/exceptions_by_mime_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_a e
-            join profiles_a p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="AllExceptionsByMimeB"
-            reportFilename="exceptions/exceptions_by_mime_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_b e
-            join profiles_b p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="ContainerExceptionsByMimeA"
-            reportFilename="exceptions/container_exceptions_by_mime_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_a e
-            join profiles_a p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            and parse_exception_id=0
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="ContainerExceptionsByMimeB"
-            reportFilename="exceptions/container_exceptions_by_mime_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            exceptions_b e
-            join profiles_b p on p.id=e.id
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            and parse_exception_id=0
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="AllExceptionsByMimeByTypeA"
-            reportFilename="exceptions/exceptions_by_mime_by_type_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string as MIME_TYPE,
-            parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
-            from exceptions_a e
-            join profiles_a p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            join ref_parse_exception_types r on
-            r.parse_exception_id=e.parse_exception_id
-            group by p.mime_id, parse_exception_description
-            order by MIME_TYPE, EXCEPTION_TYPE
-        </sql>
-    </report>
-
-    <report reportName="AllExceptionsByMimeByTypeB"
-            reportFilename="exceptions/exceptions_by_mime_by_type_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string as MIME_TYPE,
-            parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
-            from exceptions_b e
-            join profiles_b p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            join ref_parse_exception_types r on
-            r.parse_exception_id=e.parse_exception_id
-            group by p.mime_id, parse_exception_description
-            order by MIME_TYPE, EXCEPTION_TYPE
-        </sql>
-    </report>
-
-    <report reportName="TextLostFromACausedByNewExceptionsInB"
-            reportFilename="exceptions/text_lost_from_A_caused_by_new_exceptions_in_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path as FILE_PATH,
-            c.length as CONTAINER_LENGTH,
-            ca.NUM_TOKENS as NUM_TOKENS_A,
-            cb.NUM_TOKENS as NUM_TOKENS_B,
-            ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A, cb.NUM_UNIQUE_TOKENS
-            as NUM_UNIQUE_TOKENS_B,
-            ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
-            ca.num_common_tokens as NUM_COMMON_TOKENS_A,
-            cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
-            cb.num_common_tokens as NUM_COMMON_TOKENS_B,
-            ca.top_n_tokens as TOP_N_TOKENS_A, cb.top_n_tokens as TOP_N_TOKENS_B,
-            eb.ORIG_STACK_TRACE as ORIG_STACK_TRACE_B
-            from contents_a ca
-            join profiles_a pa on ca.id = pa.id
-            join containers c on pa.container_id=c.container_id
-            left join contents_b cb on ca.id=cb.id
-            left join exceptions_b eb on ca.id = eb.id
-            left join exceptions_a ea on ca.id = ea.id
-            where eb.orig_stack_trace is not null
-            and ea.orig_stack_trace is null
-            order by ca.num_common_tokens - ifnull(cb.num_common_tokens,0) desc
-        </sql>
-    </report>
-
-    <report reportName="FixedExceptionsInBByMimeType"
-            reportFilename="exceptions/fixed_exceptions_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            count(1) as COUNT
-            from exceptions_a ea
-            left join exceptions_b eb on ea.id = eb.id
-            join profiles_a pa on pa.id=ea.id
-            join profiles_b pb on pa.id=pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where eb.id is null
-            and ea.parse_exception_id=0
-            group by mime_type_a, mime_type_b
-        </sql>
-    </report>
-
-    <report reportName="FixedExceptionsInByDetails"
-            reportFilename="exceptions/fixed_exceptions_in_B_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select
-            file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            pa.file_name, pa.is_embedded
-            from exceptions_a ea
-            left join exceptions_b eb on ea.id = eb.id
-            join profiles_a pa on pa.id=ea.id
-            join profiles_b pb on pb.id=pa.id //this ensures that files were actually processed in both runs
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where eb.id is null
-            and ea.parse_exception_id=0
-            order by mime_type_a, mime_type_b
-        </sql>
-    </report>
-    <report reportName="ContentsOfFixedExceptionsInB"
-            reportFilename="exceptions/contents_of_fixed_exceptions_in_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            CONTENT_LENGTH,
-            NUM_TOKENS, NUM_UNIQUE_TOKENS,
-            TOP_N_TOKENS, LANG_ID_1,TOKEN_LENGTH_MEAN, TOKEN_LENGTH_STD_DEV
-            from exceptions_a ea
-            left join exceptions_b eb on ea.id = eb.id
-            join profiles_a pa on pa.id=ea.id
-            join profiles_b pb on pa.id=pb.id
-            join contents_b cb on cb.id=ea.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where eb.id is null
-            and ea.parse_exception_id=0
-        </sql>
-    </report>
-
-    <report reportName="NewExceptionsByMimeType"
-            reportFilename="exceptions/new_exceptions_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select ma.mime_string as MIME_TYPE_A, mb.mime_string as MIME_TYPE_B, count(1) as COUNT
-            from exceptions_b eb
-            left join exceptions_a ea on ea.id = eb.id
-            join profiles_a pa on pa.id=eb.id
-            join profiles_b pb on pb.id=pa.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where ea.id is null
-            and eb.parse_exception_id=0
-            group by ma.mime_string, mb.mime_string
-            order by COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="NewExceptionsInBByMimeTypeByStackTrace"
-            reportFilename="exceptions/new_exceptions_in_B_by_mime_by_stack_trace.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select
-            ma.MIME_STRING as MIME_TYPE_A,
-            mb.MIME_STRING as MIME_TYPE_B,
-            eb.sort_stack_trace, count(1) as
-            COUNT
-            from exceptions_b eb
-            left join exceptions_a ea on ea.id = eb.id
-            join profiles_a pa on pa.id=eb.id
-            join profiles_b pb on pb.id=eb.id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where ea.id is null
-            and eb.parse_exception_id=0
-            group by MIME_TYPE_A, MIME_TYPE_B, eb.sort_stack_trace
-            order by MIME_TYPE_A asc, MIME_TYPE_B asc, COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="NewExceptionsInBDetails"
-            reportFilename="exceptions/new_exceptions_in_B_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_TYPE_A,
-            mb.mime_string as MIME_TYPE_B,
-            eb.orig_stack_trace, eb.sort_stack_trace
-            from exceptions_b eb
-            left join exceptions_a ea on ea.id = eb.id
-            join profiles_a pa on pa.id=eb.id
-            join profiles_b pb on pb.id=eb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where ea.id is null
-            and eb.parse_exception_id=0
-            order by MIME_TYPE_A asc, MIME_TYPE_B asc, eb.ORIG_STACK_TRACE
-        </sql>
-    </report>
-
-    <report reportName="StackTracesByMimeInA"
-            reportFilename="exceptions/stack_traces_by_mime_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
-            COUNT
-            from exceptions_a e
-            join profiles_a p on p.id=e.id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            group by MIME_TYPE, e.sort_stack_trace
-            order by MIME_TYPE asc, COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="AllStackTracesInA"
-            reportFilename="exceptions/stack_traces_A.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            mime_string as MIME_TYPE,
-            orig_stack_trace, sort_stack_trace
-            from exceptions_a e
-            join profiles_a p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
-            CONTAINER_LENGTH asc
-        </sql>
-    </report>
-    <report reportName="AllStackTracesInB"
-            reportFilename="exceptions/stack_traces_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            mime_string as MIME_TYPE,
-            orig_stack_trace, sort_stack_trace
-            from exceptions_b e
-            join profiles_b p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
-            CONTAINER_LENGTH asc
-        </sql>
-    </report>
-
-    <report reportName="StackTracesByMimeInB"
-            reportFilename="exceptions/stack_traces_by_mime_B.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
-            COUNT
-            from exceptions_b e
-            join profiles_b p on p.id=e.id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            group by MIME_TYPE, e.sort_stack_trace
-            order by MIME_TYPE asc, COUNT desc
-        </sql>
-    </report>
-    <report reportName="extractExceptionsA"
-            reportFilename="exceptions/extract_exceptions_a.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, extract_exception_description
-            from extract_exceptions_a e
-            join ref_extract_exception_types t
-            on e.extract_exception_id=t.extract_exception_id
-        </sql>
-    </report>
-    <report reportName="extractExceptionsB"
-            reportFilename="exceptions/extract_exceptions_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, extract_exception_description
-            from extract_exceptions_b e
-            join ref_extract_exception_types t
-            on e.extract_exception_id=t.extract_exception_id
-        </sql>
-    </report>
-    <report reportName="fixedCatastrophicExtractExceptions"
-            reportFilename="exceptions/fixed_catastrophic_exceptions_in_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select exa.file_path, ra.extract_exception_description, rb.extract_exception_description
-            from extract_exceptions_a exa
-            left join extract_exceptions_b exb on exa.container_id=exb.container_id
-            join ref_extract_exception_types ra on exa.extract_exception_id = ra.extract_exception_id
-            left join ref_extract_exception_types rb on exb.extract_exception_id = rb.extract_exception_id
-            where exa.extract_exception_id &lt; 4
-            and (exb.extract_exception_id is null or exb.extract_exception_id &gt; 3)
-        </sql>
-    </report>
-    <report reportName="newCatastrophicExtractExceptions"
-            reportFilename="exceptions/new_catastrophic_exceptions_in_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select exb.file_path, rb.extract_exception_description, ra.extract_exception_description
-            from extract_exceptions_b exb
-            left join extract_exceptions_a exa on exb.container_id=exa.container_id
-            join ref_extract_exception_types rb on exb.extract_exception_id = rb.extract_exception_id
-            left join ref_extract_exception_types ra on exa.extract_exception_id = ra.extract_exception_id
-            where exb.extract_exception_id &lt; 4
-            and (exa.extract_exception_id is null or exa.extract_exception_id &gt; 3)
-        </sql>
-    </report>
-
-    <report reportName="parseExceptionTypesA"
-            reportFilename="exceptions/overall_exception_types_a.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select parse_exception_description, count(1)
-            from exceptions_a e
-            join ref_parse_exception_types t on
-            t.parse_exception_id=e.parse_exception_id
-            group by e.parse_exception_id
-        </sql>
-    </report>
-    <report reportName="parseExceptionTypesB"
-            reportFilename="exceptions/overall_exception_types_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select parse_exception_description, count(1)
-            from exceptions_b e
-            join ref_parse_exception_types t on
-            t.parse_exception_id=e.parse_exception_id
-            group by e.parse_exception_id
-        </sql>
-    </report>
-
-    <report reportName="contentDiffsWExceptions"
-            reportFilename="content/content_diffs_with_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            ca.num_unique_tokens as NUM_UNIQUE_TOKENS_A,
-            cb.num_unique_tokens as NUM_UNIQUE_TOKENS_B,
-            ca.num_tokens as NUM_TOKENS_A,
-            cb.num_tokens as NUM_TOKENS_B,
-            ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
-            ca.num_common_tokens as NUM_COMMON_TOKENS_A,
-            cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
-            cb.num_common_tokens as NUM_COMMON_TOKENS_B,
-            ifnull(cb.num_common_tokens,0)-
-            ifnull(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
-            ca.top_n_tokens as TOP_N_TOKENS_A,
-            cb.top_n_tokens as TOP_N_TOKENS_B,
-            ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
-            cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
-            top_10_unique_token_diffs_a,
-            top_10_unique_token_diffs_b,
-            top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap,
-            ref_ea.parse_exception_description as EXCEPTION_A,
-            ref_eb.parse_exception_description as EXCEPTION_B
-            from content_comparisons cc
-            join contents_a ca on ca.id=cc.id
-            left join contents_b cb on cb.id=cc.id
-            join profiles_a pa on pa.id = cc.id
-            join profiles_b pb on pb.id=cc.id
-            join containers c on c.container_id=pa.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            left join exceptions_a ea on ea.id=cc.id
-            left join exceptions_b eb on eb.id=cc.id
-            left join ref_parse_exception_types ref_ea on ref_ea.parse_exception_id=ea.parse_exception_id
-            left join ref_parse_exception_types ref_eb on ref_eb.parse_exception_id=eb.parse_exception_id
-            where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
-            and (ea.parse_exception_id is null or
-            ea.parse_exception_id &lt;&gt; 2)
-            and (eb.parse_exception_id is null or
-            eb.parse_exception_id &lt;&gt; 2)
-            order by ma.mime_string, overlap asc
-            limit 100000
-        </sql>
-    </report>
-    <report reportName="contentDiffsNoExceptions"
-            reportFilename="content/content_diffs_no_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A,
-            cb.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_B,
-            ca.NUM_TOKENS as NUM_TOKENS_A,
-            cb.NUM_TOKENS as NUM_TOKENS_B,
-            ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
-            ca.num_common_tokens as NUM_COMMON_TOKENS_A,
-            cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
-            cb.num_common_tokens as NUM_COMMON_TOKENS_B,
-            ifnull(cb.num_common_tokens,0)-
-            ifnull(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
-            ca.top_n_tokens as TOP_N_TOKENS_A,
-            cb.top_n_tokens as TOP_N_TOKENS_B,
-            ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
-            cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
-            top_10_unique_token_diffs_a,
-            top_10_unique_token_diffs_b,
-            top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap
-            from content_comparisons cc
-            join contents_a ca on ca.id=cc.id
-            join contents_b cb on cb.id=cc.id
-            join profiles_a pa on pa.id = cc.id
-            join profiles_b pb on pb.id=cc.id
-            join containers c on c.container_id=pa.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            left join exceptions_a ea on ea.id=cc.id
-            left join exceptions_b eb on eb.id=cc.id
-            where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
-            and (ea.parse_exception_id is null)
-            and (eb.parse_exception_id is null)
-            order by ma.mime_string, overlap asc
-            limit 100000
-        </sql>
-    </report>
-
-    <report reportName="CommonTokenComparisonsByMimeType"
-            reportFilename="content/common_token_comparisons_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select ma.mime_string as MIME_STRING_A, mb.mime_string as MIME_STRING_B,
-            num_tokens_a, num_tokens_b,
-            num_alphabetic_tokens_a, num_alphabetic_tokens_b,
-            num_common_tokens_a, num_common_tokens_b,
-            ifnull(num_common_tokens_b, 0)-ifnull(num_common_tokens_a, 0) as change_in_common_tokens_b
-            from token_counts_compared tcc
-            join mimes ma on tcc.mime_id_a = ma.mime_id
-            join mimes mb on tcc.mime_id_b = mb.mime_id
-            order by change_in_common_tokens_b desc
-        </sql>
-    </report>
-    <report reportName="PageCountDiffs"
-            reportFilename="content/page_count_diffs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.num_pages as NUM_PAGES_A,
-            pb.num_pages as NUM_PAGES_B,
-            (pb.num_pages-pa.num_pages) as DIFF_NUM_PAGES_IN_B
-            from profiles_a pa
-            join profiles_b pb on pa.id = pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            where pa.num_pages is not null
-            and pb.num_pages is not null
-            and pa.num_pages &lt;&gt; pb.num_pages
-            order by DIFF_NUM_PAGES_IN_B asc
-            limit 10000;
-        </sql>
-    </report>
-
-
-    <report reportName="ExceptionComparisonsByMimeType"
-            reportFilename="exceptions/exceptions_compared_by_mime_type.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select ma.mime_string as mime_string_a, mb.mime_string as mime_string_b,
-            total, exc_cnt_a,
-            exc_cnt_b,
-            exc_prcnt_a,
-            exc_prcnt_b, notes
-
-            from exceptions_compared e
-            join mimes ma on ma.mime_id=e.mime_id_a
-            join mimes mb on mb.mime_id=e.mime_id_b
-            order by (exc_prcnt_b-exc_prcnt_a) desc, total desc;
-        </sql>
-    </report>
-    <!--    <report reportName="MD5 Duplicate Counts A"
-                reportFilename="md5/md5_duplicate_counts_A.xlsx"
-                format="xlsx"
-                            includeSql="true">
-            <sql>
-                select md5, count(1) cnt
-                from profiles_a
-                group by md5
-                having cnt > 2
-                order by cnt desc
-            </sql>
-        </report>
-
-        <report reportName="MD5 Duplicate Counts B"
-                reportFilename="md5/md5_duplicate_counts_B.xlsx"
-                format="xlsx"
-                            includeSql="true">
-
-            <sql>
-                select md5, count(1) cnt
-                from profiles_b
-                group by md5
-                having cnt > 2
-                order by cnt desc
-            </sql>
-        </report>
-
-        <report reportName="MD5 Duplicates A"
-                reportFilename="md5/md5_duplicates_A.xlsx"
-                format="xlsx"
-                            includeSql="true">
-
-            <sql>
-                select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
-                from md5_multiples_tmp_a t
-                join profiles_a p on p.md5 = t.md5
-                join containers c on p.container_id = c.container_id
-                join contents_a cb on p.id=cb.id
-                order by t.cnt desc
-            </sql>
-        </report>
-
-        <report reportName="MD5 Duplicates B"
-                reportFilename="md5/md5_duplicates_B.xlsx"
-                format="xlsx"
-                            includeSql="true">
-
-            <sql>
-                select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
-                from md5_multiples_tmp_b t
-                join profiles_b p on p.md5 = t.md5
-                join containers c on p.container_id = c.container_id
-                join contents_b cb on p.id=cb.id
-                order by t.cnt desc
-            </sql>
-        </report>
-    -->
-
-    <report reportName="Attachment Diffs no Exceptions"
-            reportFilename="attachments/attachment_diffs_no_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.num_attachments as NUM_ATTACHMENTS_A,
-            pb.num_attachments as NUM_ATTACHMENTS_B,
-            pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B
-            from profiles_a pa
-            join profiles_b pb on pa.id= pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            left join exceptions_a ea on ea.id=pa.id
-            left join exceptions_b eb on eb.id=pb.id
-            where pa.is_embedded=false and
-            ea.parse_exception_id is null and
-            eb.parse_exception_id is null
-            and pa.num_attachments &lt;&gt; pb.num_attachments
-            order by ma.mime_string, pb.num_attachments-pa.num_attachments
-            limit 100000;
-        </sql>
-    </report>
-
-    <report reportName="Attachment Diffs with exceptions"
-            reportFilename="attachments/attachment_diffs_with_exceptions.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            c.length as CONTAINER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.num_attachments as NUM_ATTACHMENTS_A,
-            pb.num_attachments as NUM_ATTACHMENTS_B,
-            pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B,
-            refea.parse_exception_description as PARSE_EXCEPTION_A,
-            refeb.parse_exception_description as PARSE_EXCEPTION_B
-            from profiles_a pa
-            join profiles_b pb on pa.id= pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            left join exceptions_a ea on ea.id=pa.id
-            left join exceptions_b eb on eb.id=pb.id
-            left join ref_parse_exception_types refea on ea.parse_exception_id=refea.parse_exception_id
-            left join ref_parse_exception_types refeb on eb.parse_exception_id=refeb.parse_exception_id
-            where pa.is_embedded=false
-            and pa.num_attachments &lt;&gt; pb.num_attachments
-            order by ma.mime_string, pb.num_attachments-pa.num_attachments
-            limit 100000;
-        </sql>
-    </report>
-
-    <report reportName="Files missing in B by Mime"
-            reportFilename="attachments/all_files_missing_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_a pa
-            left join profiles_b pb on pa.id=pb.id
-            join mimes m on pa.mime_id=m.mime_id
-            where pb.id is null
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container files missing in B by Mime"
-            reportFilename="attachments/container_files_missing_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_a pa
-            left join profiles_b pb on pa.id=pb.id
-            join mimes m on pa.mime_id=m.mime_id
-            where pb.id is null and pa.is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Embedded files missing in B by Mime"
-            reportFilename="attachments/embedded_files_missing_in_B_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_a pa
-            left join profiles_b pb on pa.id=pb.id
-            join mimes m on pa.mime_id=m.mime_id
-            where pb.id is null and pa.is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="All files missing in A by Mime"
-            reportFilename="attachments/all_files_missing_in_A_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_b pb
-            left join profiles_a pa on pb.id=pa.id
-            join mimes m on pb.mime_id=m.mime_id
-            where pa.id is null
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container files missing in A by Mime"
-            reportFilename="attachments/container_files_missing_in_A_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_b pb
-            left join profiles_a pa on pb.id=pa.id
-            join mimes m on pb.mime_id=m.mime_id
-            where pa.id is null and pb.is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Embedded files missing in A by Mime"
-            reportFilename="attachments/embedded_files_missing_in_A_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as cnt
-            from profiles_b pb
-            left join profiles_a pa on pb.id=pa.id
-            join mimes m on pb.mime_id=m.mime_id
-            where pa.id is null and pb.is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <!-- metadata values -->
-    <report reportName="Metadata Value Diffs"
-            reportFilename="metadata/metadata_value_count_diffs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            ma.mime_string as mime_string_a,
-            mb.mime_string as mime_string_b,
-            pa.num_metadata_values as num_metadata_values_a,
-            pb.num_metadata_values as num_metadata_values_b,
-            ea.parse_exception_id as parse_ex_id_a,
-            eb.parse_exception_id as parse_ex_id_b
-            from profiles_a pa
-            join profiles_b pb on pa.id= pb.id
-            join containers c on pa.container_id=c.container_id
-            join mimes ma on pa.mime_id=ma.mime_id
-            join mimes mb on pb.mime_id=mb.mime_id
-            left join exceptions_a ea on ea.id=pa.id
-            left join exceptions_b eb on eb.id=pb.id
-            where
-            ea.parse_exception_id is null and
-            eb.parse_exception_id is null
-            and pa.num_metadata_values &lt;&gt; pb.num_metadata_values
-            order by ma.mime_string,
-            pb.num_metadata_values-pa.num_metadata_values
-            limit 100000
-        </sql>
-    </report>
-    <report reportName="Tag Count Diffs By Mime"
-            reportFilename="tags/tag_count_diffs_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select ma.mime_string as mime_string_a,
-            mb.mime_string as mime_string_b,
-            tags_a_a,
-            tags_a_b,
-            tags_b_a,
-            tags_b_b,
-            tags_div_a,
-            tags_div_b,
-            tags_i_a,
-            tags_i_b,
-            tags_li_a,
-            tags_li_b,
-            tags_ol_a,
-            tags_ol_b,
-            tags_p_a,
-            tags_p_b,
-            tags_table_a,
-            tags_table_b,
-            tags_td_a,
-            tags_td_b,
-            tags_title_a,
-            tags_title_b,
-            tags_tr_a,
-            tags_tr_b,
-            tags_u_a,
-            tags_u_b,
-            tags_ul_a,
-            tags_ul_b
-            from
-            tags_by_mime tbm
-            join mimes ma on tbm.mime_id_a=ma.mime_id
-            join mimes mb on tbm.mime_id_b=mb.mime_id
-            limit 100000
-        </sql>
-
-    </report>
-    <report reportName="Tag Exceptions By Mime"
-            reportFilename="tags/tag_exceptions_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select ma.mime_string as mime_string_a,
-            mb.mime_string as mime_string_b,
-            tag_exceptions_a,
-            tag_exceptions_b,
-            (tag_exceptions_b-tag_exceptions_a) as diff_tag_exceptions_in_b
-            from tag_exceptions_by_mime tebm
-            join mimes ma on tebm.mime_id_a=ma.mime_id
-            join mimes mb on tebm.mime_id_b=mb.mime_id
-            order by diff_tag_exceptions_in_b desc
-        </sql>
-    </report>
-    <report reportName="Tag Exceptions Details A"
-                         reportFilename="tags/tag_exceptions_details_a.xlsx"
-                         format="xlsx"
-                         includeSql="true">
-        <sql>
-            select c.file_path,pa.file_name,mime_string,is_embedded from
-            tags_a ta
-            join profiles_a pa on ta.id=pa.id
-            join containers c on pa.container_id=c.container_id
-            join mimes m on pa.mime_id=m.mime_id
-            where ta.tags_parse_exception=true
-            order by m.mime_string
-            limit 20000
-        </sql>
-    </report>
-    <report reportName="Tag Exceptions Details B"
-            reportFilename="tags/tag_exceptions_details_b.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select c.file_path,pb.file_name,mime_string,is_embedded from
-            tags_b tb
-            join profiles_b pb on tb.id=pb.id
-            join containers c on pb.container_id=c.container_id
-            join mimes m on pb.mime_id=m.mime_id
-            where tb.tags_parse_exception=true
-            order by m.mime_string
-            limit 20000
-        </sql>
-    </report>
-
-    <report reportName="Parse Time (Millis) Compared"
-            reportFilename="parse_times/parse_time_millis_by_mime_compared.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            total_a as TOTAL_MILLIS_A, total_b as TOTAL_MILLIS_B,
-            prcnt_increase as PERCENT_INCREASE
-            from parse_time_compared ptc
-            join mimes ma on ptc.mime_id_a=ma.mime_id
-            join mimes mb on ptc.mime_id_b=mb.mime_id
-            where TOTAL_A &gt; 1000 AND TOTAL_B &gt; 1000 -- only show comparisons if &gt; a second
-            order by prcnt_increase desc
-        </sql>
-    </report>
-    <report reportName="Parse Time (Millis) Details"
-            reportFilename="parse_times/parse_time_millis_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, c.length as CONTAINTER_LENGTH,
-            ma.mime_string as MIME_STRING_A,
-            mb.mime_string as MIME_STRING_B,
-            pa.elapsed_time_millis as TOTAL_MILLIS_A,
-            pb.elapsed_time_millis as TOTAL_MILLIS_B,
-            (pb.elapsed_time_millis-pa.elapsed_time_millis) as DIFF_MILLIS
-            from profiles_a pa
-            join profiles_b pb on pa.id=pb.id
-            join mimes ma on ma.mime_id=pa.mime_id
-            join mimes mb on mb.mime_id=pb.mime_id
-            join containers c on pa.container_id=c.container_id
-            order by DIFF_MILLIS desc
-            limit 20000;
-        </sql>
-    </report>
-    <after>
-        <sql>drop table if exists md5_multiples_tmp_a</sql>
-        <sql>drop table if exists md5_multiples_tmp_b</sql>
-    </after>
+  <before>
+
+    <sql>drop table if exists md5_multiples_tmp_a</sql>
+    <sql>create table md5_multiples_tmp_a (MD5 char(32), cnt int)
+      as
+      select md5, count(1) cnt
+      from profiles_a
+      where md5 is not null
+      group by md5
+      having cnt &gt; 1
+      order by cnt desc
+    </sql>
+
+    <sql>drop table if exists md5_multiples_tmp_b</sql>
+    <sql>create table md5_multiples_tmp_b (MD5 char(32), cnt int)
+      as
+      select md5, count(1) cnt
+      from profiles_b
+      where md5 is not null
+      group by md5
+      having cnt &gt; 1
+      order by cnt desc
+    </sql>
+    <!-- build mime indexes -->
+
+    <sql>create index if not exists pa_m_idx
+      on profiles_a (mime_id);
+    </sql>
+
+    <sql>
+      create index if not exists pb_m_idx
+      on profiles_b (mime_id);
+    </sql>
+
+    <!-- build exceptions comparison table -->
+    <sql>drop table if exists exceptions_compared</sql>
+    <sql>
+      create table exceptions_compared (
+      mime_id_a integer,
+      mime_id_b integer,
+      total integer,
+      exc_cnt_a integer,
+      exc_cnt_b integer,
+      exc_prcnt_a float,
+      exc_prcnt_b float,
+      notes varchar(12)
+      );
+    </sql>
+    <sql>
+      insert into exceptions_compared (
+      select ma.mime_id, mb.mime_id, count(1) as total, 0, 0, 0.0, 0.0, ''
+      from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join mimes ma on pa.mime_id = ma.mime_id
+      join mimes mb on pb.mime_id = mb.mime_id
+      group by ma.mime_id, mb.mime_id
+      order by total desc );
+    </sql>
+
+    <sql>
+      update exceptions_compared ec set
+      exc_cnt_a = (
+      select count(1) as cnt
+      from exceptions_a ea
+      join profiles_a pa on ea.id=pa.id
+      join profiles_b pb on pb.id=pa.id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
+      group by ma.mime_id, mb.mime_id);
+    </sql>
+    <sql>
+      update exceptions_compared ec set
+      exc_cnt_b = (
+      select count(1) as cnt
+      from exceptions_b eb
+      join profiles_b pb on eb.id=pb.id
+      join profiles_a pa on pa.id=pb.id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      where ma.mime_id= ec.mime_id_a and mb.mime_id=ec.mime_id_b
+      group by mb.mime_id, ma.mime_id);
+    </sql>
+    <sql>
+      update exceptions_compared
+      set exc_prcnt_a = cast(exc_cnt_a as double)/cast(total as double)
+      where total > 0;
+    </sql>
+    <sql>
+      update exceptions_compared
+      set exc_prcnt_b = cast(exc_cnt_b as double)/cast(total as double)
+      where total > 0;
+    </sql>
+
+    <sql>
+      update exceptions_compared
+      set notes = 'YAY!'
+      where total > 100 and (exc_prcnt_a-exc_prcnt_b) > 0.10;
+    </sql>
+    <sql>
+      update exceptions_compared
+      set notes = 'YIKES!'
+      where total > 100 and (exc_prcnt_b-exc_prcnt_a) > 0.10;
+    </sql>
+
+    <!-- build tmp common words table -->
+    <sql>drop table if exists token_counts_compared</sql>
+    <sql>
+      create table token_counts_compared
+      (mime_id_a integer,
+      mime_id_b integer,
+      num_tokens_a long default 0,
+      num_tokens_b long default 0,
+      num_alphabetic_tokens_a long default 0,
+      num_alphabetic_tokens_b long default 0,
+      num_common_tokens_a long default 0,
+      num_common_tokens_b long default 0
+      );
+    </sql>
+    <sql>
+      insert into token_counts_compared (mime_id_a, mime_id_b)
+      select ma.mime_id, mb.mime_id
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_tokens_a=(
+      select sum(num_tokens) as cnt from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join contents_a c on c.id = pa.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_tokens_b=(
+      select sum(num_tokens) as cnt from profiles_b pb
+      join profiles_a pa on pa.id=pb.id
+      join contents_b c on c.id = pb.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_alphabetic_tokens_a=(
+      select sum(num_alphabetic_tokens) as cnt from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join contents_a c on c.id = pa.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_alphabetic_tokens_b=(
+      select sum(num_alphabetic_tokens) as cnt from profiles_b pb
+      join profiles_a pa on pb.id=pa.id
+      join contents_b c on c.id = pb.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_common_tokens_a=(
+      select sum(num_common_tokens) as cnt from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join contents_a c on c.id = pa.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>
+      update token_counts_compared tcc set num_common_tokens_b=(
+      select sum(num_common_tokens) as cnt from profiles_b pb
+      join profiles_a pa on pa.id=pb.id
+      join contents_b c on c.id = pb.id
+      where pb.mime_id= tcc.mime_id_b
+      and pa.mime_id=tcc.mime_id_a
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+
+    <sql>drop table if exists tags_by_mime</sql>
+    <sql>create table tags_by_mime (
+      mime_id_a integer,
+      mime_id_b integer,
+      tags_a_a integer,
+      tags_b_a integer,
+      tags_div_a integer,
+      tags_i_a integer,
+      tags_img_a integer,
+      tags_li_a integer,
+      tags_ol_a integer,
+      tags_p_a integer,
+      tags_table_a integer,
+      tags_td_a integer,
+      tags_title_a integer,
+      tags_tr_a integer,
+      tags_u_a integer,
+      tags_ul_a integer,
+      tags_a_b integer,
+      tags_b_b integer,
+      tags_div_b integer,
+      tags_i_b integer,
+      tags_img_b integer,
+      tags_li_b integer,
+      tags_ol_b integer,
+      tags_p_b integer,
+      tags_table_b integer,
+      tags_td_b integer,
+      tags_title_b integer,
+      tags_tr_b integer,
+      tags_u_b integer,
+      tags_ul_b integer
+      );
+    </sql>
+    <sql>
+      insert into tags_by_mime (mime_id_a, mime_id_b)
+      select ma.mime_id, mb.mime_id
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_a_a=(
+      select sum(ta.tags_a) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_b_a=(
+      select sum(ta.tags_b) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_div_a=(
+      select sum(ta.tags_div) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_i_a=(
+      select sum(ta.tags_i) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_img_a=(
+      select sum(ta.tags_img) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_li_a=(
+      select sum(ta.tags_li) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ol_a=(
+      select sum(ta.tags_ol) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_p_a=(
+      select sum(ta.tags_p) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_table_a=(
+      select sum(ta.tags_table) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_td_a=(
+      select sum(ta.tags_td) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_title_a=(
+      select sum(ta.tags_title) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_tr_a=(
+      select sum(ta.tags_tr) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_u_a=(
+      select sum(ta.tags_u) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ul_a=(
+      select sum(ta.tags_ul) as cnt from tags_a ta
+      join tags_b tb on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <!-- now update tags_b counts -->
+    <sql>
+      update tags_by_mime tbm set tags_a_b=(
+      select sum(tb.tags_a) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_b_b=(
+      select sum(tb.tags_b) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_div_b=(
+      select sum(tb.tags_div) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_i_b=(
+      select sum(tb.tags_i) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_img_b=(
+      select sum(tb.tags_img) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_li_b=(
+      select sum(tb.tags_li) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ol_b=(
+      select sum(tb.tags_ol) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_p_b=(
+      select sum(tb.tags_p) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_table_b=(
+      select sum(tb.tags_table) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_td_b=(
+      select sum(tb.tags_td) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_title_b=(
+      select sum(tb.tags_title) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_tr_b=(
+      select sum(tb.tags_tr) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_u_b=(
+      select sum(tb.tags_u) as cnt from tags_b tb
+      join tags_a ta on tb.id=ta.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tags_by_mime tbm set tags_ul_b=(
+      select sum(tb.tags_ul) as cnt from tags_b tb
+      join tags_a ta on ta.id=tb.id
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tbm.mime_id_b
+      and pa.mime_id=tbm.mime_id_a
+      and ta.tags_parse_exception=false
+      and tb.tags_parse_exception=false
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>drop table if exists tag_exceptions_by_mime</sql>
+    <sql>create table tag_exceptions_by_mime (
+      mime_id_a integer,
+      mime_id_b integer,
+      tag_exceptions_a integer,
+      tag_exceptions_b integer)
+    </sql>
+    <sql>
+      insert into tag_exceptions_by_mime (mime_id_a, mime_id_b,
+      tag_exceptions_a, tag_exceptions_b)
+      select ma.mime_id, mb.mime_id,0,0
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+    </sql>
+    <sql>
+      update tag_exceptions_by_mime tebm set tag_exceptions_a=(
+      select count(1) as cnt from tags_a ta
+      join profiles_a pa on pa.id=ta.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tebm.mime_id_b
+      and pa.mime_id=tebm.mime_id_a
+      and ta.tags_parse_exception=true
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      update tag_exceptions_by_mime tebm set tag_exceptions_b=(
+      select count(1) as cnt from tags_b tb
+      join profiles_a pa on pa.id=tb.id
+      join profiles_b pb on pa.id=pb.id
+      where pb.mime_id= tebm.mime_id_b
+      and pa.mime_id=tebm.mime_id_a
+      and tb.tags_parse_exception=true
+      group by mime_id_a, mime_id_b
+      );
+    </sql>
+    <sql>
+      drop table if exists parse_time_compared;
+    </sql>
+    <sql>
+      create table parse_time_compared (
+      mime_id_a integer,
+      mime_id_b integer,
+      total_a bigint,
+      total_b bigint,
+      prcnt_increase double
+      );
+    </sql>
+    <sql>
+      insert into parse_time_compared (mime_id_a, mime_id_b,
+      total_a, total_b, prcnt_increase)
+      select ma.mime_id, mb.mime_id,0,0,0.0
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      group by ma.mime_id, mb.mime_id
+    </sql>
+    <sql>
+      update parse_time_compared ptc set total_a=(
+      select sum(pa.elapsed_time_millis) as total_a from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      where pa.mime_id= ptc.mime_id_a
+      and pb.mime_id=ptc.mime_id_b
+      group by mime_id_a, mime_id_b)
+    </sql>
+    <sql>
+      update parse_time_compared ptc set total_b=(
+      select sum(pb.elapsed_time_millis) as total_b from profiles_b pb
+      join profiles_a pa on pa.id=pb.id
+      where pa.mime_id= ptc.mime_id_a
+      and pb.mime_id=ptc.mime_id_b
+      group by mime_id_a, mime_id_b)
+    </sql>
+    <sql>
+      update parse_time_compared ptc set prcnt_increase=(100.0 *
+      cast(total_b as float)/cast(total_a as float))
+      where total_a > 0;
+    </sql>
+  </before>
+
+  <!-- MIMES -->
+  <report reportName="All Mimes In A"
+          reportFilename="mimes/all_mimes_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_a p
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="All Mimes In B"
+          reportFilename="mimes/all_mimes_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_b p
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container Mimes In A"
+          reportFilename="mimes/container_mimes_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_a p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="Container Mimes In B"
+          reportFilename="mimes/container_mimes_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_b p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Embedded Mimes In A"
+          reportFilename="mimes/embedded_mimes_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_a p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="Embedded Mimes In B"
+          reportFilename="mimes/embedded_mimes_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles_b p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Mime Differences A -> B"
+          reportFilename="mimes/mime_diffs_A_to_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
+      MIME_A_TO_MIME_B, count(1) as COUNT
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      where a.mime_id &lt;&gt; b.mime_id
+      group by MIME_A_TO_MIME_B
+      order by COUNT DESC
+    </sql>
+  </report>
+
+  <report reportName="Mime Differences A -> B Details"
+          reportFilename="mimes/mime_diffs_A_to_B_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select concat(ma.mime_string, ' -&gt; ', mb.mime_string) as
+      MIME_A_TO_MIME_B,
+      file_path,
+      c.length as CONTAINER_LENGTH,
+      a.file_name
+      from profiles_a a
+      join profiles_b b on a.id=b.id
+      join mimes ma on ma.mime_id=a.mime_id
+      join mimes mb on mb.mime_id=b.mime_id
+      join containers c on a.container_id=c.container_id
+      where a.mime_id &lt;&gt; b.mime_id
+      order by MIME_A_TO_MIME_B
+    </sql>
+  </report>
+
+
+  <!-- Exceptions -->
+  <report reportName="AllExceptionsByMimeA"
+          reportFilename="exceptions/exceptions_by_mime_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_a e
+      join profiles_a p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="AllExceptionsByMimeB"
+          reportFilename="exceptions/exceptions_by_mime_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_b e
+      join profiles_b p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="ContainerExceptionsByMimeA"
+          reportFilename="exceptions/container_exceptions_by_mime_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_a e
+      join profiles_a p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      and parse_exception_id=0
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="ContainerExceptionsByMimeB"
+          reportFilename="exceptions/container_exceptions_by_mime_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      exceptions_b e
+      join profiles_b p on p.id=e.id
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      and parse_exception_id=0
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="AllExceptionsByMimeByTypeA"
+          reportFilename="exceptions/exceptions_by_mime_by_type_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string as MIME_TYPE,
+      parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
+      from exceptions_a e
+      join profiles_a p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      join ref_parse_exception_types r on
+      r.parse_exception_id=e.parse_exception_id
+      group by p.mime_id, parse_exception_description
+      order by MIME_TYPE, EXCEPTION_TYPE
+    </sql>
+  </report>
+
+  <report reportName="AllExceptionsByMimeByTypeB"
+          reportFilename="exceptions/exceptions_by_mime_by_type_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string as MIME_TYPE,
+      parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
+      from exceptions_b e
+      join profiles_b p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      join ref_parse_exception_types r on
+      r.parse_exception_id=e.parse_exception_id
+      group by p.mime_id, parse_exception_description
+      order by MIME_TYPE, EXCEPTION_TYPE
+    </sql>
+  </report>
+
+  <report reportName="TextLostFromACausedByNewExceptionsInB"
+          reportFilename="exceptions/text_lost_from_A_caused_by_new_exceptions_in_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path as FILE_PATH,
+      c.length as CONTAINER_LENGTH,
+      ca.NUM_TOKENS as NUM_TOKENS_A,
+      cb.NUM_TOKENS as NUM_TOKENS_B,
+      ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A, cb.NUM_UNIQUE_TOKENS
+      as NUM_UNIQUE_TOKENS_B,
+      ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
+      ca.num_common_tokens as NUM_COMMON_TOKENS_A,
+      cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
+      cb.num_common_tokens as NUM_COMMON_TOKENS_B,
+      ca.top_n_tokens as TOP_N_TOKENS_A, cb.top_n_tokens as TOP_N_TOKENS_B,
+      eb.ORIG_STACK_TRACE as ORIG_STACK_TRACE_B
+      from contents_a ca
+      join profiles_a pa on ca.id = pa.id
+      join containers c on pa.container_id=c.container_id
+      left join contents_b cb on ca.id=cb.id
+      left join exceptions_b eb on ca.id = eb.id
+      left join exceptions_a ea on ca.id = ea.id
+      where eb.orig_stack_trace is not null
+      and ea.orig_stack_trace is null
+      order by ca.num_common_tokens - ifnull(cb.num_common_tokens,0) desc
+    </sql>
+  </report>
+
+  <report reportName="FixedExceptionsInBByMimeType"
+          reportFilename="exceptions/fixed_exceptions_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      count(1) as COUNT
+      from exceptions_a ea
+      left join exceptions_b eb on ea.id = eb.id
+      join profiles_a pa on pa.id=ea.id
+      join profiles_b pb on pa.id=pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where eb.id is null
+      and ea.parse_exception_id=0
+      group by mime_type_a, mime_type_b
+    </sql>
+  </report>
+
+  <report reportName="FixedExceptionsInByDetails"
+          reportFilename="exceptions/fixed_exceptions_in_B_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select
+      file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      pa.file_name, pa.is_embedded
+      from exceptions_a ea
+      left join exceptions_b eb on ea.id = eb.id
+      join profiles_a pa on pa.id=ea.id
+      join profiles_b pb on pb.id=pa.id //this ensures that files were actually processed in both runs
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where eb.id is null
+      and ea.parse_exception_id=0
+      order by mime_type_a, mime_type_b
+    </sql>
+  </report>
+  <report reportName="ContentsOfFixedExceptionsInB"
+          reportFilename="exceptions/contents_of_fixed_exceptions_in_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      CONTENT_LENGTH,
+      NUM_TOKENS, NUM_UNIQUE_TOKENS,
+      TOP_N_TOKENS, LANG_ID_1,TOKEN_LENGTH_MEAN, TOKEN_LENGTH_STD_DEV
+      from exceptions_a ea
+      left join exceptions_b eb on ea.id = eb.id
+      join profiles_a pa on pa.id=ea.id
+      join profiles_b pb on pa.id=pb.id
+      join contents_b cb on cb.id=ea.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where eb.id is null
+      and ea.parse_exception_id=0
+    </sql>
+  </report>
+
+  <report reportName="NewExceptionsByMimeType"
+          reportFilename="exceptions/new_exceptions_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select ma.mime_string as MIME_TYPE_A, mb.mime_string as MIME_TYPE_B, count(1) as COUNT
+      from exceptions_b eb
+      left join exceptions_a ea on ea.id = eb.id
+      join profiles_a pa on pa.id=eb.id
+      join profiles_b pb on pb.id=pa.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where ea.id is null
+      and eb.parse_exception_id=0
+      group by ma.mime_string, mb.mime_string
+      order by COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="NewExceptionsInBByMimeTypeByStackTrace"
+          reportFilename="exceptions/new_exceptions_in_B_by_mime_by_stack_trace.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select
+      ma.MIME_STRING as MIME_TYPE_A,
+      mb.MIME_STRING as MIME_TYPE_B,
+      eb.sort_stack_trace, count(1) as
+      COUNT
+      from exceptions_b eb
+      left join exceptions_a ea on ea.id = eb.id
+      join profiles_a pa on pa.id=eb.id
+      join profiles_b pb on pb.id=eb.id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where ea.id is null
+      and eb.parse_exception_id=0
+      group by MIME_TYPE_A, MIME_TYPE_B, eb.sort_stack_trace
+      order by MIME_TYPE_A asc, MIME_TYPE_B asc, COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="NewExceptionsInBDetails"
+          reportFilename="exceptions/new_exceptions_in_B_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_TYPE_A,
+      mb.mime_string as MIME_TYPE_B,
+      eb.orig_stack_trace, eb.sort_stack_trace
+      from exceptions_b eb
+      left join exceptions_a ea on ea.id = eb.id
+      join profiles_a pa on pa.id=eb.id
+      join profiles_b pb on pb.id=eb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where ea.id is null
+      and eb.parse_exception_id=0
+      order by MIME_TYPE_A asc, MIME_TYPE_B asc, eb.ORIG_STACK_TRACE
+    </sql>
+  </report>
+
+  <report reportName="StackTracesByMimeInA"
+          reportFilename="exceptions/stack_traces_by_mime_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
+      COUNT
+      from exceptions_a e
+      join profiles_a p on p.id=e.id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      group by MIME_TYPE, e.sort_stack_trace
+      order by MIME_TYPE asc, COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="AllStackTracesInA"
+          reportFilename="exceptions/stack_traces_A.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      mime_string as MIME_TYPE,
+      orig_stack_trace, sort_stack_trace
+      from exceptions_a e
+      join profiles_a p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
+      CONTAINER_LENGTH asc
+    </sql>
+  </report>
+  <report reportName="AllStackTracesInB"
+          reportFilename="exceptions/stack_traces_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      mime_string as MIME_TYPE,
+      orig_stack_trace, sort_stack_trace
+      from exceptions_b e
+      join profiles_b p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
+      CONTAINER_LENGTH asc
+    </sql>
+  </report>
+
+  <report reportName="StackTracesByMimeInB"
+          reportFilename="exceptions/stack_traces_by_mime_B.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
+      COUNT
+      from exceptions_b e
+      join profiles_b p on p.id=e.id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      group by MIME_TYPE, e.sort_stack_trace
+      order by MIME_TYPE asc, COUNT desc
+    </sql>
+  </report>
+  <report reportName="extractExceptionsA"
+          reportFilename="exceptions/extract_exceptions_a.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, extract_exception_description
+      from extract_exceptions_a e
+      join ref_extract_exception_types t
+      on e.extract_exception_id=t.extract_exception_id
+    </sql>
+  </report>
+  <report reportName="extractExceptionsB"
+          reportFilename="exceptions/extract_exceptions_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, extract_exception_description
+      from extract_exceptions_b e
+      join ref_extract_exception_types t
+      on e.extract_exception_id=t.extract_exception_id
+    </sql>
+  </report>
+  <report reportName="fixedCatastrophicExtractExceptions"
+          reportFilename="exceptions/fixed_catastrophic_exceptions_in_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select exa.file_path, ra.extract_exception_description, rb.extract_exception_description
+      from extract_exceptions_a exa
+      left join extract_exceptions_b exb on exa.container_id=exb.container_id
+      join ref_extract_exception_types ra on exa.extract_exception_id = ra.extract_exception_id
+      left join ref_extract_exception_types rb on exb.extract_exception_id = rb.extract_exception_id
+      where exa.extract_exception_id &lt; 4
+      and (exb.extract_exception_id is null or exb.extract_exception_id &gt; 3)
+    </sql>
+  </report>
+  <report reportName="newCatastrophicExtractExceptions"
+          reportFilename="exceptions/new_catastrophic_exceptions_in_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select exb.file_path, rb.extract_exception_description, ra.extract_exception_description
+      from extract_exceptions_b exb
+      left join extract_exceptions_a exa on exb.container_id=exa.container_id
+      join ref_extract_exception_types rb on exb.extract_exception_id = rb.extract_exception_id
+      left join ref_extract_exception_types ra on exa.extract_exception_id = ra.extract_exception_id
+      where exb.extract_exception_id &lt; 4
+      and (exa.extract_exception_id is null or exa.extract_exception_id &gt; 3)
+    </sql>
+  </report>
+
+  <report reportName="parseExceptionTypesA"
+          reportFilename="exceptions/overall_exception_types_a.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select parse_exception_description, count(1)
+      from exceptions_a e
+      join ref_parse_exception_types t on
+      t.parse_exception_id=e.parse_exception_id
+      group by e.parse_exception_id
+    </sql>
+  </report>
+  <report reportName="parseExceptionTypesB"
+          reportFilename="exceptions/overall_exception_types_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select parse_exception_description, count(1)
+      from exceptions_b e
+      join ref_parse_exception_types t on
+      t.parse_exception_id=e.parse_exception_id
+      group by e.parse_exception_id
+    </sql>
+  </report>
+
+  <report reportName="contentDiffsWExceptions"
+          reportFilename="content/content_diffs_with_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      ca.num_unique_tokens as NUM_UNIQUE_TOKENS_A,
+      cb.num_unique_tokens as NUM_UNIQUE_TOKENS_B,
+      ca.num_tokens as NUM_TOKENS_A,
+      cb.num_tokens as NUM_TOKENS_B,
+      ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
+      ca.num_common_tokens as NUM_COMMON_TOKENS_A,
+      cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
+      cb.num_common_tokens as NUM_COMMON_TOKENS_B,
+      ifnull(cb.num_common_tokens,0)-
+      ifnull(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
+      ca.top_n_tokens as TOP_N_TOKENS_A,
+      cb.top_n_tokens as TOP_N_TOKENS_B,
+      ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
+      cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
+      top_10_unique_token_diffs_a,
+      top_10_unique_token_diffs_b,
+      top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap,
+      ref_ea.parse_exception_description as EXCEPTION_A,
+      ref_eb.parse_exception_description as EXCEPTION_B
+      from content_comparisons cc
+      join contents_a ca on ca.id=cc.id
+      left join contents_b cb on cb.id=cc.id
+      join profiles_a pa on pa.id = cc.id
+      join profiles_b pb on pb.id=cc.id
+      join containers c on c.container_id=pa.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      left join exceptions_a ea on ea.id=cc.id
+      left join exceptions_b eb on eb.id=cc.id
+      left join ref_parse_exception_types ref_ea on ref_ea.parse_exception_id=ea.parse_exception_id
+      left join ref_parse_exception_types ref_eb on ref_eb.parse_exception_id=eb.parse_exception_id
+      where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
+      and (ea.parse_exception_id is null or
+      ea.parse_exception_id &lt;&gt; 2)
+      and (eb.parse_exception_id is null or
+      eb.parse_exception_id &lt;&gt; 2)
+      order by ma.mime_string, overlap asc
+      limit 100000
+    </sql>
+  </report>
+  <report reportName="contentDiffsNoExceptions"
+          reportFilename="content/content_diffs_no_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      ca.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_A,
+      cb.NUM_UNIQUE_TOKENS as NUM_UNIQUE_TOKENS_B,
+      ca.NUM_TOKENS as NUM_TOKENS_A,
+      cb.NUM_TOKENS as NUM_TOKENS_B,
+      ca.common_tokens_lang as COMMON_TOKENS_LANG_A,
+      ca.num_common_tokens as NUM_COMMON_TOKENS_A,
+      cb.common_tokens_lang as COMMON_TOKENS_LANG_B,
+      cb.num_common_tokens as NUM_COMMON_TOKENS_B,
+      ifnull(cb.num_common_tokens,0)-
+      ifnull(ca.num_common_tokens, 0) as NUM_COMMON_TOKENS_DIFF_IN_B,
+      ca.top_n_tokens as TOP_N_TOKENS_A,
+      cb.top_n_tokens as TOP_N_TOKENS_B,
+      ca.unicode_char_blocks as UNICODE_CHAR_BLOCKS_A,
+      cb.unicode_char_blocks as UNICODE_CHAR_BLOCKS_B,
+      top_10_unique_token_diffs_a,
+      top_10_unique_token_diffs_b,
+      top_10_more_in_a, top_10_more_in_b, dice_coefficient, overlap
+      from content_comparisons cc
+      join contents_a ca on ca.id=cc.id
+      join contents_b cb on cb.id=cc.id
+      join profiles_a pa on pa.id = cc.id
+      join profiles_b pb on pb.id=cc.id
+      join containers c on c.container_id=pa.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      left join exceptions_a ea on ea.id=cc.id
+      left join exceptions_b eb on eb.id=cc.id
+      where (overlap &lt; 0.95 or abs(ca.NUM_TOKENS-cb.NUM_TOKENS) &gt;30)
+      and (ea.parse_exception_id is null)
+      and (eb.parse_exception_id is null)
+      order by ma.mime_string, overlap asc
+      limit 100000
+    </sql>
+  </report>
+
+  <report reportName="CommonTokenComparisonsByMimeType"
+          reportFilename="content/common_token_comparisons_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select ma.mime_string as MIME_STRING_A, mb.mime_string as MIME_STRING_B,
+      num_tokens_a, num_tokens_b,
+      num_alphabetic_tokens_a, num_alphabetic_tokens_b,
+      num_common_tokens_a, num_common_tokens_b,
+      ifnull(num_common_tokens_b, 0)-ifnull(num_common_tokens_a, 0) as change_in_common_tokens_b
+      from token_counts_compared tcc
+      join mimes ma on tcc.mime_id_a = ma.mime_id
+      join mimes mb on tcc.mime_id_b = mb.mime_id
+      order by change_in_common_tokens_b desc
+    </sql>
+  </report>
+  <report reportName="PageCountDiffs"
+          reportFilename="content/page_count_diffs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.num_pages as NUM_PAGES_A,
+      pb.num_pages as NUM_PAGES_B,
+      (pb.num_pages-pa.num_pages) as DIFF_NUM_PAGES_IN_B
+      from profiles_a pa
+      join profiles_b pb on pa.id = pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      where pa.num_pages is not null
+      and pb.num_pages is not null
+      and pa.num_pages &lt;&gt; pb.num_pages
+      order by DIFF_NUM_PAGES_IN_B asc
+      limit 10000;
+    </sql>
+  </report>
+
+
+  <report reportName="ExceptionComparisonsByMimeType"
+          reportFilename="exceptions/exceptions_compared_by_mime_type.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select ma.mime_string as mime_string_a, mb.mime_string as mime_string_b,
+      total, exc_cnt_a,
+      exc_cnt_b,
+      exc_prcnt_a,
+      exc_prcnt_b, notes
+
+      from exceptions_compared e
+      join mimes ma on ma.mime_id=e.mime_id_a
+      join mimes mb on mb.mime_id=e.mime_id_b
+      order by (exc_prcnt_b-exc_prcnt_a) desc, total desc;
+    </sql>
+  </report>
+  <!--    <report reportName="MD5 Duplicate Counts A"
+              reportFilename="md5/md5_duplicate_counts_A.xlsx"
+              format="xlsx"
+                          includeSql="true">
+          <sql>
+              select md5, count(1) cnt
+              from profiles_a
+              group by md5
+              having cnt > 2
+              order by cnt desc
+          </sql>
+      </report>
+
+      <report reportName="MD5 Duplicate Counts B"
+              reportFilename="md5/md5_duplicate_counts_B.xlsx"
+              format="xlsx"
+                          includeSql="true">
+
+          <sql>
+              select md5, count(1) cnt
+              from profiles_b
+              group by md5
+              having cnt > 2
+              order by cnt desc
+          </sql>
+      </report>
+
+      <report reportName="MD5 Duplicates A"
+              reportFilename="md5/md5_duplicates_A.xlsx"
+              format="xlsx"
+                          includeSql="true">
+
+          <sql>
+              select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
+              from md5_multiples_tmp_a t
+              join profiles_a p on p.md5 = t.md5
+              join containers c on p.container_id = c.container_id
+              join contents_a cb on p.id=cb.id
+              order by t.cnt desc
+          </sql>
+      </report>
+
+      <report reportName="MD5 Duplicates B"
+              reportFilename="md5/md5_duplicates_B.xlsx"
+              format="xlsx"
+                          includeSql="true">
+
+          <sql>
+              select file_path, file_name, is_embedded, content_length, NUM_TOKENS, p.md5
+              from md5_multiples_tmp_b t
+              join profiles_b p on p.md5 = t.md5
+              join containers c on p.container_id = c.container_id
+              join contents_b cb on p.id=cb.id
+              order by t.cnt desc
+          </sql>
+      </report>
+  -->
+
+  <report reportName="Attachment Diffs no Exceptions"
+          reportFilename="attachments/attachment_diffs_no_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.num_attachments as NUM_ATTACHMENTS_A,
+      pb.num_attachments as NUM_ATTACHMENTS_B,
+      pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B
+      from profiles_a pa
+      join profiles_b pb on pa.id= pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      left join exceptions_a ea on ea.id=pa.id
+      left join exceptions_b eb on eb.id=pb.id
+      where pa.is_embedded=false and
+      ea.parse_exception_id is null and
+      eb.parse_exception_id is null
+      and pa.num_attachments &lt;&gt; pb.num_attachments
+      order by ma.mime_string, pb.num_attachments-pa.num_attachments
+      limit 100000;
+    </sql>
+  </report>
+
+  <report reportName="Attachment Diffs with exceptions"
+          reportFilename="attachments/attachment_diffs_with_exceptions.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      c.length as CONTAINER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.num_attachments as NUM_ATTACHMENTS_A,
+      pb.num_attachments as NUM_ATTACHMENTS_B,
+      pb.num_attachments-pa.num_attachments as NUM_ATTACHMENTS_DIFF_IN_B,
+      refea.parse_exception_description as PARSE_EXCEPTION_A,
+      refeb.parse_exception_description as PARSE_EXCEPTION_B
+      from profiles_a pa
+      join profiles_b pb on pa.id= pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      left join exceptions_a ea on ea.id=pa.id
+      left join exceptions_b eb on eb.id=pb.id
+      left join ref_parse_exception_types refea on ea.parse_exception_id=refea.parse_exception_id
+      left join ref_parse_exception_types refeb on eb.parse_exception_id=refeb.parse_exception_id
+      where pa.is_embedded=false
+      and pa.num_attachments &lt;&gt; pb.num_attachments
+      order by ma.mime_string, pb.num_attachments-pa.num_attachments
+      limit 100000;
+    </sql>
+  </report>
+
+  <report reportName="Files missing in B by Mime"
+          reportFilename="attachments/all_files_missing_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_a pa
+      left join profiles_b pb on pa.id=pb.id
+      join mimes m on pa.mime_id=m.mime_id
+      where pb.id is null
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container files missing in B by Mime"
+          reportFilename="attachments/container_files_missing_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_a pa
+      left join profiles_b pb on pa.id=pb.id
+      join mimes m on pa.mime_id=m.mime_id
+      where pb.id is null and pa.is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Embedded files missing in B by Mime"
+          reportFilename="attachments/embedded_files_missing_in_B_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_a pa
+      left join profiles_b pb on pa.id=pb.id
+      join mimes m on pa.mime_id=m.mime_id
+      where pb.id is null and pa.is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="All files missing in A by Mime"
+          reportFilename="attachments/all_files_missing_in_A_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_b pb
+      left join profiles_a pa on pb.id=pa.id
+      join mimes m on pb.mime_id=m.mime_id
+      where pa.id is null
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container files missing in A by Mime"
+          reportFilename="attachments/container_files_missing_in_A_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_b pb
+      left join profiles_a pa on pb.id=pa.id
+      join mimes m on pb.mime_id=m.mime_id
+      where pa.id is null and pb.is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Embedded files missing in A by Mime"
+          reportFilename="attachments/embedded_files_missing_in_A_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as cnt
+      from profiles_b pb
+      left join profiles_a pa on pb.id=pa.id
+      join mimes m on pb.mime_id=m.mime_id
+      where pa.id is null and pb.is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <!-- metadata values -->
+  <report reportName="Metadata Value Diffs"
+          reportFilename="metadata/metadata_value_count_diffs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      ma.mime_string as mime_string_a,
+      mb.mime_string as mime_string_b,
+      pa.num_metadata_values as num_metadata_values_a,
+      pb.num_metadata_values as num_metadata_values_b,
+      ea.parse_exception_id as parse_ex_id_a,
+      eb.parse_exception_id as parse_ex_id_b
+      from profiles_a pa
+      join profiles_b pb on pa.id= pb.id
+      join containers c on pa.container_id=c.container_id
+      join mimes ma on pa.mime_id=ma.mime_id
+      join mimes mb on pb.mime_id=mb.mime_id
+      left join exceptions_a ea on ea.id=pa.id
+      left join exceptions_b eb on eb.id=pb.id
+      where
+      ea.parse_exception_id is null and
+      eb.parse_exception_id is null
+      and pa.num_metadata_values &lt;&gt; pb.num_metadata_values
+      order by ma.mime_string,
+      pb.num_metadata_values-pa.num_metadata_values
+      limit 100000
+    </sql>
+  </report>
+  <report reportName="Tag Count Diffs By Mime"
+          reportFilename="tags/tag_count_diffs_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select ma.mime_string as mime_string_a,
+      mb.mime_string as mime_string_b,
+      tags_a_a,
+      tags_a_b,
+      tags_b_a,
+      tags_b_b,
+      tags_div_a,
+      tags_div_b,
+      tags_i_a,
+      tags_i_b,
+      tags_li_a,
+      tags_li_b,
+      tags_ol_a,
+      tags_ol_b,
+      tags_p_a,
+      tags_p_b,
+      tags_table_a,
+      tags_table_b,
+      tags_td_a,
+      tags_td_b,
+      tags_title_a,
+      tags_title_b,
+      tags_tr_a,
+      tags_tr_b,
+      tags_u_a,
+      tags_u_b,
+      tags_ul_a,
+      tags_ul_b
+      from
+      tags_by_mime tbm
+      join mimes ma on tbm.mime_id_a=ma.mime_id
+      join mimes mb on tbm.mime_id_b=mb.mime_id
+      limit 100000
+    </sql>
+
+  </report>
+  <report reportName="Tag Exceptions By Mime"
+          reportFilename="tags/tag_exceptions_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select ma.mime_string as mime_string_a,
+      mb.mime_string as mime_string_b,
+      tag_exceptions_a,
+      tag_exceptions_b,
+      (tag_exceptions_b-tag_exceptions_a) as diff_tag_exceptions_in_b
+      from tag_exceptions_by_mime tebm
+      join mimes ma on tebm.mime_id_a=ma.mime_id
+      join mimes mb on tebm.mime_id_b=mb.mime_id
+      order by diff_tag_exceptions_in_b desc
+    </sql>
+  </report>
+  <report reportName="Tag Exceptions Details A"
+          reportFilename="tags/tag_exceptions_details_a.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select c.file_path,pa.file_name,mime_string,is_embedded from
+      tags_a ta
+      join profiles_a pa on ta.id=pa.id
+      join containers c on pa.container_id=c.container_id
+      join mimes m on pa.mime_id=m.mime_id
+      where ta.tags_parse_exception=true
+      order by m.mime_string
+      limit 20000
+    </sql>
+  </report>
+  <report reportName="Tag Exceptions Details B"
+          reportFilename="tags/tag_exceptions_details_b.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select c.file_path,pb.file_name,mime_string,is_embedded from
+      tags_b tb
+      join profiles_b pb on tb.id=pb.id
+      join containers c on pb.container_id=c.container_id
+      join mimes m on pb.mime_id=m.mime_id
+      where tb.tags_parse_exception=true
+      order by m.mime_string
+      limit 20000
+    </sql>
+  </report>
+
+  <report reportName="Parse Time (Millis) Compared"
+          reportFilename="parse_times/parse_time_millis_by_mime_compared.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      total_a as TOTAL_MILLIS_A, total_b as TOTAL_MILLIS_B,
+      prcnt_increase as PERCENT_INCREASE
+      from parse_time_compared ptc
+      join mimes ma on ptc.mime_id_a=ma.mime_id
+      join mimes mb on ptc.mime_id_b=mb.mime_id
+      where TOTAL_A &gt; 1000 AND TOTAL_B &gt; 1000 -- only show comparisons if &gt; a second
+      order by prcnt_increase desc
+    </sql>
+  </report>
+  <report reportName="Parse Time (Millis) Details"
+          reportFilename="parse_times/parse_time_millis_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, c.length as CONTAINTER_LENGTH,
+      ma.mime_string as MIME_STRING_A,
+      mb.mime_string as MIME_STRING_B,
+      pa.elapsed_time_millis as TOTAL_MILLIS_A,
+      pb.elapsed_time_millis as TOTAL_MILLIS_B,
+      (pb.elapsed_time_millis-pa.elapsed_time_millis) as DIFF_MILLIS
+      from profiles_a pa
+      join profiles_b pb on pa.id=pb.id
+      join mimes ma on ma.mime_id=pa.mime_id
+      join mimes mb on mb.mime_id=pb.mime_id
+      join containers c on pa.container_id=c.container_id
+      order by DIFF_MILLIS desc
+      limit 20000;
+    </sql>
+  </report>
+  <after>
+    <sql>drop table if exists md5_multiples_tmp_a</sql>
+    <sql>drop table if exists md5_multiples_tmp_b</sql>
+  </after>
 </reports>
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/main/resources/db.properties b/tika-eval/tika-eval-app/src/main/resources/db.properties
index a35e35a6a..6212d98d9 100644
--- a/tika-eval/tika-eval-app/src/main/resources/db.properties
+++ b/tika-eval/tika-eval-app/src/main/resources/db.properties
@@ -13,7 +13,7 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-postgresql  org.postgresql.Driver
+postgresql org.postgresql.Driver
 mysql com.mysql.jdbc.Driver
 oracle oracle.jdbc.driver.OracleDriver
 sqlite org.sqlite.JDBC
diff --git a/tika-eval/tika-eval-app/src/main/resources/profile-reports-tags.xml b/tika-eval/tika-eval-app/src/main/resources/profile-reports-tags.xml
index a6b0942ee..15d461692 100644
--- a/tika-eval/tika-eval-app/src/main/resources/profile-reports-tags.xml
+++ b/tika-eval/tika-eval-app/src/main/resources/profile-reports-tags.xml
@@ -21,309 +21,309 @@
 <reports>
 
 
-    <before>
-        <!-- <sql>create index on x</sql>-->
-    </before>
-
-
-    <!-- MIMES -->
-    <report reportName="All Mimes"
-            reportFilename="mimes/all_mimes.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles p
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container Mimes"
-            reportFilename="mimes/container_mimes.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="Embedded Mimes"
-            reportFilename="mimes/embedded_mimes.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <!-- content -->
-    <report reportName="Common Tokens by Lang"
-            reportFilename="content/common_tokens_by_lang.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select common_tokens_lang, sum(num_common_tokens) as cnt
-            from contents
-            group by common_tokens_lang
-            order by cnt desc;
-        </sql>
-    </report>
-
-    <report reportName="Detected Languages"
-            reportFilename="content/detected_langs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select LANG_ID_1 as DetectedLang, count(1) as cnt
-            from contents
-            group by LANG_ID_1
-            order by cnt desc
-        </sql>
-    </report>
-
-
-    <report reportName="Token Count by Detected Language"
-            reportFilename="content/num_tokens_by_detected_langs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select LANG_ID_1 as DetectedLang, sum(num_tokens) as cnt
-            from contents
-            group by LANG_ID_1
-            order by cnt desc;
-        </sql>
-    </report>
-
-    <report reportName="Common Tokens Divided by Alphabetic Tokens"
-            reportFilename="content/common_tokens_div_alphabetic_exclude_media_and_zips.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <!-- 0.50 is a complete heuristic -->
-        <sql>
-            select file_path, file_name, is_embedded,
-            mime_string, lang_id_1, common_tokens_lang,
-            num_tokens, num_alphabetic_tokens, num_common_tokens,
-            case
-                when num_alphabetic_tokens &gt; 0
-                then cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double)
-                else 0
-            end as common_div_alphabetic
-            from contents c
-            join profiles p on p.id=c.id
-            join containers ct on ct.container_id=p.container_id
-            join mimes m on p.mime_id=m.mime_id
-            where
-                (num_alphabetic_tokens = 0
-                    or cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double) &lt; 0.50
-                )
-            and mime_string not like 'image%'
-            and mime_string not like 'video%'
-            and mime_string not like 'audio%'
-            and mime_string not like 'application/zip'
-            order by common_div_alphabetic asc
-            limit 10000
-        </sql>
-    </report>
-
-
-    <!-- MSWord files do not usually store actual # of pages; rather, they store 1 or 0,
-         and the actual number is calculated dynamically by the
-         application when the file is loaded.  This will lead to some crazily high
-         tokens/page counts for MSWord files, but the focus of this query is on the low end.
+  <before>
+    <!-- <sql>create index on x</sql>-->
+  </before>
+
+
+  <!-- MIMES -->
+  <report reportName="All Mimes"
+          reportFilename="mimes/all_mimes.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles p
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container Mimes"
+          reportFilename="mimes/container_mimes.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="Embedded Mimes"
+          reportFilename="mimes/embedded_mimes.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <!-- content -->
+  <report reportName="Common Tokens by Lang"
+          reportFilename="content/common_tokens_by_lang.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select common_tokens_lang, sum(num_common_tokens) as cnt
+      from contents
+      group by common_tokens_lang
+      order by cnt desc;
+    </sql>
+  </report>
+
+  <report reportName="Detected Languages"
+          reportFilename="content/detected_langs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select LANG_ID_1 as DetectedLang, count(1) as cnt
+      from contents
+      group by LANG_ID_1
+      order by cnt desc
+    </sql>
+  </report>
+
+
+  <report reportName="Token Count by Detected Language"
+          reportFilename="content/num_tokens_by_detected_langs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select LANG_ID_1 as DetectedLang, sum(num_tokens) as cnt
+      from contents
+      group by LANG_ID_1
+      order by cnt desc;
+    </sql>
+  </report>
+
+  <report reportName="Common Tokens Divided by Alphabetic Tokens"
+          reportFilename="content/common_tokens_div_alphabetic_exclude_media_and_zips.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <!-- 0.50 is a complete heuristic -->
+    <sql>
+      select file_path, file_name, is_embedded,
+      mime_string, lang_id_1, common_tokens_lang,
+      num_tokens, num_alphabetic_tokens, num_common_tokens,
+      case
+      when num_alphabetic_tokens &gt; 0
+      then cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double)
+      else 0
+      end as common_div_alphabetic
+      from contents c
+      join profiles p on p.id=c.id
+      join containers ct on ct.container_id=p.container_id
+      join mimes m on p.mime_id=m.mime_id
+      where
+      (num_alphabetic_tokens = 0
+      or cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double) &lt; 0.50
+      )
+      and mime_string not like 'image%'
+      and mime_string not like 'video%'
+      and mime_string not like 'audio%'
+      and mime_string not like 'application/zip'
+      order by common_div_alphabetic asc
+      limit 10000
+    </sql>
+  </report>
+
+
+  <!-- MSWord files do not usually store actual # of pages; rather, they store 1 or 0,
+       and the actual number is calculated dynamically by the
+       application when the file is loaded.  This will lead to some crazily high
+       tokens/page counts for MSWord files, but the focus of this query is on the low end.
+  -->
+  <report reportName="Tokens Per Page"
+          reportFilename="content/tokens_per_page_in_container_files.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, mime_string, num_tokens,
+      num_pages,
+      case
+      when num_tokens = 0
+      then 0
+      else
+      cast(num_tokens as double)/cast(num_pages as double)
+      end as num_tokens_div_num_pages
+      from profiles p
+      left join contents c on p.id=c.id
+      join mimes m on p.mime_id = m.mime_id
+      join containers ct on p.container_id=ct.container_id
+      where num_pages is not null and num_pages &gt; 0
+      and is_embedded=false
+      order by num_tokens_div_num_pages asc
+      limit 1000
+    </sql>
+  </report>
+
+  <report reportName="Exceptions by Type"
+          reportFilename="exceptions/exceptions_by_type.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select parse_exception_description, count(1) cnt
+      from parse_exceptions e
+      join profiles p on p.id = e.id
+      join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
+      group by parse_exception_description
+      order by cnt desc;
+    </sql>
+  </report>
+
+
+  <report reportName="Embedded Exceptions by Type"
+          reportFilename="exceptions/exceptions_by_type_embedded.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select parse_exception_description, count(1) cnt
+      from parse_exceptions e
+      join profiles p on p.id = e.id
+      join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
+      where is_embedded=true
+      group by parse_exception_description
+      order by cnt desc;
+    </sql>
+  </report>
+
+  <report reportName="AllExceptionsByMimeByType"
+          reportFilename="exceptions/exceptions_by_mime_by_type.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string as MIME_TYPE,
+      parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
+      from parse_exceptions e
+      join profiles p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      join ref_parse_exception_types r on
+      r.parse_exception_id=e.parse_exception_id
+      group by p.mime_id, parse_exception_description
+      order by MIME_TYPE, EXCEPTION_TYPE
+    </sql>
+  </report>
+
+  <report reportName="StackTracesByMime"
+          reportFilename="exceptions/stack_traces_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
+      COUNT
+      from parse_exceptions e
+      join profiles p on p.id=e.id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      group by MIME_TYPE, e.sort_stack_trace
+      order by MIME_TYPE asc, COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="AllStackTraces"
+          reportFilename="exceptions/stack_traces_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      file_name, is_embedded,
+      c.length as CONTAINER_LENGTH,
+      mime_string as MIME_TYPE,
+      orig_stack_trace, sort_stack_trace
+      from parse_exceptions e
+      join profiles p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
+      CONTAINER_LENGTH asc
+    </sql>
+  </report>
+  <report reportName="TagExceptionsByMime"
+          reportFilename="tags/tag_exceptions_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) as CNT
+      from tags t
+      join profiles p on p.id=t.id
+      join mimes m on p.mime_id=m.mime_id
+      where tags_parse_exception=TRUE
+      group by mime_string
+      order by CNT desc
+    </sql>
+  </report>
+  <report reportName="Tag Exceptions Details"
+          reportFilename="tags/tag_exceptions_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select c.file_path,p.file_name,mime_string,is_embedded from
+      tags t
+      join profiles p on t.id=p.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on p.mime_id=m.mime_id
+      where t.tags_parse_exception=true
+      order by m.mime_string
+      limit 20000
+    </sql>
+  </report>
+  <report reportName="Tags by Mime"
+          reportFilename="tags/tags_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string,
+      sum(tags_a) as tags_a,
+      sum(tags_b) as tags_b,
+      sum(tags_div) as tags_div,
+      sum(tags_i) as tags_i,
+      sum(tags_img) as tags_img,
+      sum(tags_li) as tags_li,
+      sum(tags_ol) as tags_ol,
+      sum(tags_p) as tags_p,
+      sum(tags_table) as tags_table,
+      sum(tags_td) as tags_td,
+      sum(tags_title) as tags_title,
+      sum(tags_tr) as tags_tr,
+      sum(tags_u) as tags_u,
+      sum(tags_ul) as tags_ul
+
+      from tags t
+      join profiles p on t.id=p.id
+      join mimes m on p.mime_id=m.mime_id
+      where tags_parse_exception=false
+      group by m.mime_id
+    </sql>
+
+  </report>
+  <after>
+
+    <!--<sql>drop index on x</sql>
     -->
-    <report reportName="Tokens Per Page"
-            reportFilename="content/tokens_per_page_in_container_files.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, mime_string, num_tokens,
-            num_pages,
-            case
-                when num_tokens = 0
-                    then 0
-                else
-                    cast(num_tokens as double)/cast(num_pages as double)
-            end as num_tokens_div_num_pages
-            from profiles p
-            left join contents c on p.id=c.id
-            join mimes m on p.mime_id = m.mime_id
-            join containers ct on p.container_id=ct.container_id
-            where num_pages is not null and num_pages &gt; 0
-            and is_embedded=false
-            order by num_tokens_div_num_pages asc
-            limit 1000
-        </sql>
-    </report>
-
-    <report reportName="Exceptions by Type"
-            reportFilename="exceptions/exceptions_by_type.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select parse_exception_description, count(1) cnt
-            from parse_exceptions e
-            join profiles p on p.id = e.id
-            join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
-            group by parse_exception_description
-            order by cnt desc;
-        </sql>
-    </report>
-
-
-    <report reportName="Embedded Exceptions by Type"
-            reportFilename="exceptions/exceptions_by_type_embedded.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select parse_exception_description, count(1) cnt
-            from parse_exceptions e
-            join profiles p on p.id = e.id
-            join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
-            where is_embedded=true
-            group by parse_exception_description
-            order by cnt desc;
-        </sql>
-    </report>
-
-    <report reportName="AllExceptionsByMimeByType"
-            reportFilename="exceptions/exceptions_by_mime_by_type.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string as MIME_TYPE,
-            parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
-            from parse_exceptions e
-            join profiles p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            join ref_parse_exception_types r on
-            r.parse_exception_id=e.parse_exception_id
-            group by p.mime_id, parse_exception_description
-            order by MIME_TYPE, EXCEPTION_TYPE
-        </sql>
-    </report>
-
-    <report reportName="StackTracesByMime"
-            reportFilename="exceptions/stack_traces_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
-            COUNT
-            from parse_exceptions e
-            join profiles p on p.id=e.id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            group by MIME_TYPE, e.sort_stack_trace
-            order by MIME_TYPE asc, COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="AllStackTraces"
-            reportFilename="exceptions/stack_traces_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            file_name, is_embedded,
-            c.length as CONTAINER_LENGTH,
-            mime_string as MIME_TYPE,
-            orig_stack_trace, sort_stack_trace
-            from parse_exceptions e
-            join profiles p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
-            CONTAINER_LENGTH asc
-        </sql>
-    </report>
-    <report reportName="TagExceptionsByMime"
-        reportFilename="tags/tag_exceptions_by_mime.xlsx"
-        format="xlsx"
-        includeSql="true">
-
-        <sql>
-            select mime_string, count(1) as CNT
-            from tags t
-            join profiles p on p.id=t.id
-            join mimes m on p.mime_id=m.mime_id
-            where tags_parse_exception=TRUE
-            group by mime_string
-            order by CNT desc
-        </sql>
-    </report>
-    <report reportName="Tag Exceptions Details"
-            reportFilename="tags/tag_exceptions_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select c.file_path,p.file_name,mime_string,is_embedded from
-            tags t
-            join profiles p on t.id=p.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on p.mime_id=m.mime_id
-            where t.tags_parse_exception=true
-            order by m.mime_string
-            limit 20000
-        </sql>
-    </report>
-    <report reportName="Tags by Mime"
-            reportFilename="tags/tags_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string,
-            sum(tags_a) as tags_a,
-            sum(tags_b) as tags_b,
-            sum(tags_div) as tags_div,
-            sum(tags_i) as tags_i,
-            sum(tags_img) as tags_img,
-            sum(tags_li) as tags_li,
-            sum(tags_ol) as tags_ol,
-            sum(tags_p) as tags_p,
-            sum(tags_table) as tags_table,
-            sum(tags_td) as tags_td,
-            sum(tags_title) as tags_title,
-            sum(tags_tr) as tags_tr,
-            sum(tags_u) as tags_u,
-            sum(tags_ul) as tags_ul
-
-            from tags t
-            join profiles p on t.id=p.id
-            join mimes m on p.mime_id=m.mime_id
-            where tags_parse_exception=false
-            group by m.mime_id
-        </sql>
-
-    </report>
-    <after>
-
-        <!--<sql>drop index on x</sql>
-        -->
-    </after>
+  </after>
 </reports>
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/main/resources/profile-reports.xml b/tika-eval/tika-eval-app/src/main/resources/profile-reports.xml
index db744815a..d4636f466 100644
--- a/tika-eval/tika-eval-app/src/main/resources/profile-reports.xml
+++ b/tika-eval/tika-eval-app/src/main/resources/profile-reports.xml
@@ -21,248 +21,248 @@
 <reports>
 
 
-    <before>
-        <!-- <sql>create index on x</sql>-->
-    </before>
-
-
-    <!-- MIMES -->
-    <report reportName="All Mimes"
-            reportFilename="mimes/all_mimes.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles p
-            join mimes m on m.mime_id = p.mime_id
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-    <report reportName="Container Mimes"
-            reportFilename="mimes/container_mimes.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=false
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <report reportName="Embedded Mimes"
-            reportFilename="mimes/embedded_mimes.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string, count(1) cnt from
-            profiles p
-            join mimes m on m.mime_id = p.mime_id
-            where is_embedded=true
-            group by mime_string
-            order by cnt desc
-        </sql>
-    </report>
-
-    <!-- content -->
-    <report reportName="Common Tokens by Lang"
-            reportFilename="content/common_tokens_by_lang.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select common_tokens_lang, sum(num_common_tokens) as cnt
-            from contents
-            group by common_tokens_lang
-            order by cnt desc;
-        </sql>
-    </report>
-
-    <report reportName="Detected Languages"
-            reportFilename="content/detected_langs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select LANG_ID_1 as DetectedLang, count(1) as cnt
-            from contents
-            group by LANG_ID_1
-            order by cnt desc
-        </sql>
-    </report>
-
-
-    <report reportName="Token Count by Detected Language"
-            reportFilename="content/num_tokens_by_detected_langs.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select LANG_ID_1 as DetectedLang, sum(num_tokens) as cnt
-            from contents
-            group by LANG_ID_1
-            order by cnt desc;
-        </sql>
-    </report>
-
-    <report reportName="Common Tokens Divided by Alphabetic Tokens"
-            reportFilename="content/common_tokens_div_alphabetic_exclude_media_and_zips.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <!-- 0.50 is a complete heuristic -->
-        <sql>
-            select file_path, file_name, is_embedded,
-            mime_string, lang_id_1, common_tokens_lang,
-            num_tokens, num_alphabetic_tokens, num_common_tokens,
-            case
-                when num_alphabetic_tokens &gt; 0
-                then cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double)
-                else 0
-            end as common_div_alphabetic
-            from contents c
-            join profiles p on p.id=c.id
-            join containers ct on ct.container_id=p.container_id
-            join mimes m on p.mime_id=m.mime_id
-            where
-                (num_alphabetic_tokens = 0
-                    or cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double) &lt; 0.50
-                )
-            and mime_string not like 'image%'
-            and mime_string not like 'video%'
-            and mime_string not like 'audio%'
-            and mime_string not like 'application/zip'
-            order by common_div_alphabetic asc
-            limit 10000
-        </sql>
-    </report>
-
-
-    <!-- MSWord files do not usually store actual # of pages; rather, they store 1 or 0,
-         and the actual number is calculated dynamically by the
-         application when the file is loaded.  This will lead to some crazily high
-         tokens/page counts for MSWord files, but the focus of this query is on the low end.
+  <before>
+    <!-- <sql>create index on x</sql>-->
+  </before>
+
+
+  <!-- MIMES -->
+  <report reportName="All Mimes"
+          reportFilename="mimes/all_mimes.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles p
+      join mimes m on m.mime_id = p.mime_id
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+  <report reportName="Container Mimes"
+          reportFilename="mimes/container_mimes.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=false
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <report reportName="Embedded Mimes"
+          reportFilename="mimes/embedded_mimes.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string, count(1) cnt from
+      profiles p
+      join mimes m on m.mime_id = p.mime_id
+      where is_embedded=true
+      group by mime_string
+      order by cnt desc
+    </sql>
+  </report>
+
+  <!-- content -->
+  <report reportName="Common Tokens by Lang"
+          reportFilename="content/common_tokens_by_lang.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select common_tokens_lang, sum(num_common_tokens) as cnt
+      from contents
+      group by common_tokens_lang
+      order by cnt desc;
+    </sql>
+  </report>
+
+  <report reportName="Detected Languages"
+          reportFilename="content/detected_langs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select LANG_ID_1 as DetectedLang, count(1) as cnt
+      from contents
+      group by LANG_ID_1
+      order by cnt desc
+    </sql>
+  </report>
+
+
+  <report reportName="Token Count by Detected Language"
+          reportFilename="content/num_tokens_by_detected_langs.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select LANG_ID_1 as DetectedLang, sum(num_tokens) as cnt
+      from contents
+      group by LANG_ID_1
+      order by cnt desc;
+    </sql>
+  </report>
+
+  <report reportName="Common Tokens Divided by Alphabetic Tokens"
+          reportFilename="content/common_tokens_div_alphabetic_exclude_media_and_zips.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <!-- 0.50 is a complete heuristic -->
+    <sql>
+      select file_path, file_name, is_embedded,
+      mime_string, lang_id_1, common_tokens_lang,
+      num_tokens, num_alphabetic_tokens, num_common_tokens,
+      case
+      when num_alphabetic_tokens &gt; 0
+      then cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double)
+      else 0
+      end as common_div_alphabetic
+      from contents c
+      join profiles p on p.id=c.id
+      join containers ct on ct.container_id=p.container_id
+      join mimes m on p.mime_id=m.mime_id
+      where
+      (num_alphabetic_tokens = 0
+      or cast(num_common_tokens as double)/cast(num_alphabetic_tokens as double) &lt; 0.50
+      )
+      and mime_string not like 'image%'
+      and mime_string not like 'video%'
+      and mime_string not like 'audio%'
+      and mime_string not like 'application/zip'
+      order by common_div_alphabetic asc
+      limit 10000
+    </sql>
+  </report>
+
+
+  <!-- MSWord files do not usually store actual # of pages; rather, they store 1 or 0,
+       and the actual number is calculated dynamically by the
+       application when the file is loaded.  This will lead to some crazily high
+       tokens/page counts for MSWord files, but the focus of this query is on the low end.
+  -->
+  <report reportName="Tokens Per Page"
+          reportFilename="content/tokens_per_page_in_container_files.xlsx"
+          format="xlsx"
+          includeSql="true">
+    <sql>
+      select file_path, mime_string, num_tokens,
+      num_pages,
+      case
+      when num_tokens = 0
+      then 0
+      else
+      cast(num_tokens as double)/cast(num_pages as double)
+      end as num_tokens_div_num_pages
+      from profiles p
+      left join contents c on p.id=c.id
+      join mimes m on p.mime_id = m.mime_id
+      join containers ct on p.container_id=ct.container_id
+      where num_pages is not null and num_pages &gt; 0
+      and is_embedded=false
+      order by num_tokens_div_num_pages asc
+      limit 1000
+    </sql>
+  </report>
+
+  <report reportName="Exceptions by Type"
+          reportFilename="exceptions/exceptions_by_type.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select parse_exception_description, count(1) cnt
+      from parse_exceptions e
+      join profiles p on p.id = e.id
+      join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
+      group by parse_exception_description
+      order by cnt desc;
+    </sql>
+  </report>
+
+
+  <report reportName="Embedded Exceptions by Type"
+          reportFilename="exceptions/exceptions_by_type_embedded.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select parse_exception_description, count(1) cnt
+      from parse_exceptions e
+      join profiles p on p.id = e.id
+      join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
+      where is_embedded=true
+      group by parse_exception_description
+      order by cnt desc;
+    </sql>
+  </report>
+
+  <report reportName="AllExceptionsByMimeByType"
+          reportFilename="exceptions/exceptions_by_mime_by_type.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select mime_string as MIME_TYPE,
+      parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
+      from parse_exceptions e
+      join profiles p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      join ref_parse_exception_types r on
+      r.parse_exception_id=e.parse_exception_id
+      group by p.mime_id, parse_exception_description
+      order by MIME_TYPE, EXCEPTION_TYPE
+    </sql>
+  </report>
+
+  <report reportName="StackTracesByMime"
+          reportFilename="exceptions/stack_traces_by_mime.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
+      COUNT
+      from parse_exceptions e
+      join profiles p on p.id=e.id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      group by MIME_TYPE, e.sort_stack_trace
+      order by MIME_TYPE asc, COUNT desc
+    </sql>
+  </report>
+
+  <report reportName="AllStackTraces"
+          reportFilename="exceptions/stack_traces_details.xlsx"
+          format="xlsx"
+          includeSql="true">
+
+    <sql>
+      select file_path,
+      file_name, is_embedded,
+      c.length as CONTAINER_LENGTH,
+      mime_string as MIME_TYPE,
+      orig_stack_trace, sort_stack_trace
+      from parse_exceptions e
+      join profiles p on p.id=e.id
+      join containers c on p.container_id=c.container_id
+      join mimes m on m.mime_id=p.mime_id
+      and e.parse_exception_id=0
+      order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
+      CONTAINER_LENGTH asc
+    </sql>
+  </report>
+  <after>
+
+    <!--<sql>drop index on x</sql>
     -->
-    <report reportName="Tokens Per Page"
-            reportFilename="content/tokens_per_page_in_container_files.xlsx"
-            format="xlsx"
-            includeSql="true">
-        <sql>
-            select file_path, mime_string, num_tokens,
-            num_pages,
-            case
-                when num_tokens = 0
-                    then 0
-                else
-                    cast(num_tokens as double)/cast(num_pages as double)
-            end as num_tokens_div_num_pages
-            from profiles p
-            left join contents c on p.id=c.id
-            join mimes m on p.mime_id = m.mime_id
-            join containers ct on p.container_id=ct.container_id
-            where num_pages is not null and num_pages &gt; 0
-            and is_embedded=false
-            order by num_tokens_div_num_pages asc
-            limit 1000
-        </sql>
-    </report>
-
-    <report reportName="Exceptions by Type"
-            reportFilename="exceptions/exceptions_by_type.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select parse_exception_description, count(1) cnt
-            from parse_exceptions e
-            join profiles p on p.id = e.id
-            join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
-            group by parse_exception_description
-            order by cnt desc;
-        </sql>
-    </report>
-
-
-    <report reportName="Embedded Exceptions by Type"
-            reportFilename="exceptions/exceptions_by_type_embedded.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select parse_exception_description, count(1) cnt
-            from parse_exceptions e
-            join profiles p on p.id = e.id
-            join ref_parse_exception_types et on et.parse_exception_id=e.parse_exception_id
-            where is_embedded=true
-            group by parse_exception_description
-            order by cnt desc;
-        </sql>
-    </report>
-
-    <report reportName="AllExceptionsByMimeByType"
-            reportFilename="exceptions/exceptions_by_mime_by_type.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select mime_string as MIME_TYPE,
-            parse_exception_description as EXCEPTION_TYPE, count(1) as COUNT
-            from parse_exceptions e
-            join profiles p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            join ref_parse_exception_types r on
-            r.parse_exception_id=e.parse_exception_id
-            group by p.mime_id, parse_exception_description
-            order by MIME_TYPE, EXCEPTION_TYPE
-        </sql>
-    </report>
-
-    <report reportName="StackTracesByMime"
-            reportFilename="exceptions/stack_traces_by_mime.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select MIME_STRING as MIME_TYPE, e.sort_stack_trace, count(1) as
-            COUNT
-            from parse_exceptions e
-            join profiles p on p.id=e.id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            group by MIME_TYPE, e.sort_stack_trace
-            order by MIME_TYPE asc, COUNT desc
-        </sql>
-    </report>
-
-    <report reportName="AllStackTraces"
-            reportFilename="exceptions/stack_traces_details.xlsx"
-            format="xlsx"
-            includeSql="true">
-
-        <sql>
-            select file_path,
-            file_name, is_embedded,
-            c.length as CONTAINER_LENGTH,
-            mime_string as MIME_TYPE,
-            orig_stack_trace, sort_stack_trace
-            from parse_exceptions e
-            join profiles p on p.id=e.id
-            join containers c on p.container_id=c.container_id
-            join mimes m on m.mime_id=p.mime_id
-            and e.parse_exception_id=0
-            order by MIME_TYPE asc, sort_stack_trace, orig_stack_trace,
-            CONTAINER_LENGTH asc
-        </sql>
-    </report>
-    <after>
-
-        <!--<sql>drop index on x</sql>
-        -->
-    </after>
+  </after>
 </reports>
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/main/resources/tika-eval-comparison-config.xml b/tika-eval/tika-eval-app/src/main/resources/tika-eval-comparison-config.xml
index 046212b1d..c94c782eb 100644
--- a/tika-eval/tika-eval-app/src/main/resources/tika-eval-comparison-config.xml
+++ b/tika-eval/tika-eval-app/src/main/resources/tika-eval-comparison-config.xml
@@ -19,84 +19,84 @@
 -->
 
 <tika-batch-config
-        maxAliveTimeSeconds="-1"
-        pauseOnEarlyTerminationMillis="500"
-        timeoutCheckPulseMillis="1000"
-        maxQueueSize="10000"
-        numConsumers="5"
-        timeoutThresholdMillis="300000"
+    maxAliveTimeSeconds="-1"
+    pauseOnEarlyTerminationMillis="500"
+    timeoutCheckPulseMillis="1000"
+    maxQueueSize="10000"
+    numConsumers="5"
+    timeoutThresholdMillis="300000"
 >
 
-    <commandline>
-        <option opt="bc" longOpt="batch-config" hasArg="true"
-                description="xml batch config file" required="true"/>
-        <option opt="inputDir" hasArg="true"
-                description="dir to start crawling"/>
-        <option opt="numConsumers" hasArg="true"
-                description="number of fileConsumers threads"/>
-        <option opt="extractsA" hasArg="true"
-                description="this dir for analysis" required="false"/>
-        <option opt="extractsB" hasArg="true"
-                description="thatDir for analysis"/>
-        <option opt="db" hasArg="true"
-                description="name of db directory or file to which to write results"/>
-        <option opt="alterExtract" hasArg="true"
-                description="for json-formatted extract files
+  <commandline>
+    <option opt="bc" longOpt="batch-config" hasArg="true"
+            description="xml batch config file" required="true"/>
+    <option opt="inputDir" hasArg="true"
+            description="dir to start crawling"/>
+    <option opt="numConsumers" hasArg="true"
+            description="number of fileConsumers threads"/>
+    <option opt="extractsA" hasArg="true"
+            description="this dir for analysis" required="false"/>
+    <option opt="extractsB" hasArg="true"
+            description="thatDir for analysis"/>
+    <option opt="db" hasArg="true"
+            description="name of db directory or file to which to write results"/>
+    <option opt="alterExtract" hasArg="true"
+            description="for json-formatted extract files
                 process full metadata list ('as_is'=default),
                 take just the first/container document ('first_only'),
                 concatenate all content into the first metadata item ('concatenate_content')"/>
-        <option opt="minExtractLength" hasArg="true"
-                description="minimum extract length to process"/>
-        <option opt="maxExtractLength" hasArg="true"
-                description="maximum extract length to process"/>
-        <option opt="jdbc" hasArg="true"
-                description="full jdbc connection string"/>
-        <option opt="jdbcDriver" hasArg="true"
-                description="canonical class name for jdbc driver"/>
-        <option opt="tablePrefixA" hasArg="true"
-                description="EXPERT: prefix for table names for A"/>
-        <option opt="tablePrefixB" hasArg="true"
-                description="EXPERT: prefix for table names for B"/>
-        <option opt="drop" hasArg="false" description="drop tables if they exist"/>
-        <option opt="maxFilesToAdd" hasArg="true" description="maximum number of files to add to the crawler"/>
-        <option opt="maxTokens" hasArg="true" description="maximum tokens to process, default=200000"/>
-        <option opt="maxContentLength" hasArg="true"
-                description="truncate content beyond this length for calculating 'contents' stats, default=1000000"/>
-        <option opt="maxContentLengthForLangId" hasArg="true"
-                description="truncate content beyond this length for language id, default=50000"/>
-        <option opt="defaultLangCode" hasArg="true"
-                description="which language to use for common words if no 'common words' file exists for the langid result"/>
+    <option opt="minExtractLength" hasArg="true"
+            description="minimum extract length to process"/>
+    <option opt="maxExtractLength" hasArg="true"
+            description="maximum extract length to process"/>
+    <option opt="jdbc" hasArg="true"
+            description="full jdbc connection string"/>
+    <option opt="jdbcDriver" hasArg="true"
+            description="canonical class name for jdbc driver"/>
+    <option opt="tablePrefixA" hasArg="true"
+            description="EXPERT: prefix for table names for A"/>
+    <option opt="tablePrefixB" hasArg="true"
+            description="EXPERT: prefix for table names for B"/>
+    <option opt="drop" hasArg="false" description="drop tables if they exist"/>
+    <option opt="maxFilesToAdd" hasArg="true" description="maximum number of files to add to the crawler"/>
+    <option opt="maxTokens" hasArg="true" description="maximum tokens to process, default=200000"/>
+    <option opt="maxContentLength" hasArg="true"
+            description="truncate content beyond this length for calculating 'contents' stats, default=1000000"/>
+    <option opt="maxContentLengthForLangId" hasArg="true"
+            description="truncate content beyond this length for language id, default=50000"/>
+    <option opt="defaultLangCode" hasArg="true"
+            description="which language to use for common words if no 'common words' file exists for the langid result"/>
 
 
-    </commandline>
+  </commandline>
 
 
-    <!--
-        Can also add startDir: this tells the crawler to start indexing a
-        child directory of the srcDir directory.
-    -->
-    <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
-             crawlOrder="sorted"
-             maxConsecWaitMillis="30000"
-             maxFilesToAdd="-1"
-             maxFilesToConsider="-1"
-             includeFilePat=""
-             excludeFilePat=""
-    />
+  <!--
+      Can also add startDir: this tells the crawler to start indexing a
+      child directory of the srcDir directory.
+  -->
+  <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
+           crawlOrder="sorted"
+           maxConsecWaitMillis="30000"
+           maxFilesToAdd="-1"
+           maxFilesToConsider="-1"
+           includeFilePat=""
+           excludeFilePat=""
+  />
 
-    <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
-               consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractComparerBuilder"
-               dbAppend="false"
-               crawlingInputDir="false"
-               minExtractLength="-1"
-               maxExtractLength="2000000"
-               commonTokens="resources/common_tokens"
-    />
+  <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
+             consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractComparerBuilder"
+             dbAppend="false"
+             crawlingInputDir="false"
+             minExtractLength="-1"
+             maxExtractLength="2000000"
+             commonTokens="resources/common_tokens"
+  />
 
-    <!--  this is no longer implemented
-        langModelDir="resources/langmodels" -->
+  <!--  this is no longer implemented
+      langModelDir="resources/langmodels" -->
 
-    <!-- reporter and interrupter are optional -->
-    <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
-              staleThresholdMillis="500000"/>
+  <!-- reporter and interrupter are optional -->
+  <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
+            staleThresholdMillis="500000"/>
 </tika-batch-config>
diff --git a/tika-eval/tika-eval-app/src/main/resources/tika-eval-file-profiler-config.xml b/tika-eval/tika-eval-app/src/main/resources/tika-eval-file-profiler-config.xml
index 2016e2041..3ebee60a5 100644
--- a/tika-eval/tika-eval-app/src/main/resources/tika-eval-file-profiler-config.xml
+++ b/tika-eval/tika-eval-app/src/main/resources/tika-eval-file-profiler-config.xml
@@ -19,56 +19,56 @@
 -->
 
 <tika-batch-config
-        maxAliveTimeSeconds="-1"
-        pauseOnEarlyTerminationMillis="500"
-        timeoutCheckPulseMillis="1000"
-        maxQueueSize="10000"
-        numConsumers="5"
-        timeoutThresholdMillis="300000">
+    maxAliveTimeSeconds="-1"
+    pauseOnEarlyTerminationMillis="500"
+    timeoutCheckPulseMillis="1000"
+    maxQueueSize="10000"
+    numConsumers="5"
+    timeoutThresholdMillis="300000">
 
-    <commandline>
-        <option opt="bc" longOpt="batch-config" hasArg="true"
-                description="xml batch config file" required="true"/>
-        <option opt="inputDir" hasArg="true"
-                description="dir to start crawling"/>
-        <option opt="numConsumers" hasArg="true"
-                description="number of fileConsumers threads"/>
-        <option opt="extracts" hasArg="true"
-                description="this dir for analysis" required="false"/>
-        <option opt="db" hasArg="true"
-                description="name of db directory or file to which to write results"/>
-        <option opt="jdbc" hasArg="true"
-                description="full jdbc connection string"/>
-        <option opt="jdbcDriver" hasArg="true"
-                description="canonical class name for jdbc driver"/>
-        <option opt="tablePrefix" hasArg="true"
-                description="EXPERT: prefix for table names"/>
-        <option opt="drop" hasArg="false" description="drop tables if they exist"/>
-        <option opt="maxFilesToAdd" hasArg="true" description="maximum number of files to add to the crawler"/>
+  <commandline>
+    <option opt="bc" longOpt="batch-config" hasArg="true"
+            description="xml batch config file" required="true"/>
+    <option opt="inputDir" hasArg="true"
+            description="dir to start crawling"/>
+    <option opt="numConsumers" hasArg="true"
+            description="number of fileConsumers threads"/>
+    <option opt="extracts" hasArg="true"
+            description="this dir for analysis" required="false"/>
+    <option opt="db" hasArg="true"
+            description="name of db directory or file to which to write results"/>
+    <option opt="jdbc" hasArg="true"
+            description="full jdbc connection string"/>
+    <option opt="jdbcDriver" hasArg="true"
+            description="canonical class name for jdbc driver"/>
+    <option opt="tablePrefix" hasArg="true"
+            description="EXPERT: prefix for table names"/>
+    <option opt="drop" hasArg="false" description="drop tables if they exist"/>
+    <option opt="maxFilesToAdd" hasArg="true" description="maximum number of files to add to the crawler"/>
 
-    </commandline>
+  </commandline>
 
 
-    <!--
-        Can also add startDir: this tells the crawler to start indexing a
-        child directory of the inputDir directory.
-    -->
-    <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
+  <!--
+      Can also add startDir: this tells the crawler to start indexing a
+      child directory of the inputDir directory.
+  -->
+  <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
 
-             crawlOrder="sorted"
-             maxConsecWaitMillis="300000"
-             maxFilesToAdd="-1"
-             maxFilesToConsider="-1"
-             includeFilePat=""
-             excludeFilePat=""
-             maxFileSizeBytes="-1"
-    />
+           crawlOrder="sorted"
+           maxConsecWaitMillis="300000"
+           maxFilesToAdd="-1"
+           maxFilesToConsider="-1"
+           includeFilePat=""
+           excludeFilePat=""
+           maxFileSizeBytes="-1"
+  />
 
-    <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
-               consumerBuilderClass="org.apache.tika.eval.app.batch.FileProfilerBuilder"/>
+  <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
+             consumerBuilderClass="org.apache.tika.eval.app.batch.FileProfilerBuilder"/>
 
 
-    <!-- reporter and interrupter are optional -->
-    <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
-              staleThresholdMillis="500000"/>
+  <!-- reporter and interrupter are optional -->
+  <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
+            staleThresholdMillis="500000"/>
 </tika-batch-config>
diff --git a/tika-eval/tika-eval-app/src/main/resources/tika-eval-profiler-config.xml b/tika-eval/tika-eval-app/src/main/resources/tika-eval-profiler-config.xml
index dbf017da2..b6d81cc98 100644
--- a/tika-eval/tika-eval-app/src/main/resources/tika-eval-profiler-config.xml
+++ b/tika-eval/tika-eval-app/src/main/resources/tika-eval-profiler-config.xml
@@ -19,78 +19,76 @@
 -->
 
 <tika-batch-config
-        maxAliveTimeSeconds="-1"
-        pauseOnEarlyTerminationMillis="500"
-        timeoutCheckPulseMillis="1000"
-        maxQueueSize="10000"
-        numConsumers="5"
-        timeoutThresholdMillis="300000">
+    maxAliveTimeSeconds="-1"
+    pauseOnEarlyTerminationMillis="500"
+    timeoutCheckPulseMillis="1000"
+    maxQueueSize="10000"
+    numConsumers="5"
+    timeoutThresholdMillis="300000">
 
-    <commandline>
-        <option opt="bc" longOpt="batch-config" hasArg="true"
-                description="xml batch config file" required="true"/>
-        <option opt="inputDir" hasArg="true"
-                description="dir to start crawling"/>
-        <option opt="numConsumers" hasArg="true"
-                description="number of fileConsumers threads"/>
-        <option opt="extracts" hasArg="true"
-                description="this dir for analysis" required="false"/>
-        <option opt="db" hasArg="true"
-                description="name of db directory or file to which to write results"/>
-        <option opt="alterExtract" hasArg="true"
-                description="for json-formatted extract files
+  <commandline>
+    <option opt="bc" longOpt="batch-config" hasArg="true"
+            description="xml batch config file" required="true"/>
+    <option opt="inputDir" hasArg="true"
+            description="dir to start crawling"/>
+    <option opt="numConsumers" hasArg="true"
+            description="number of fileConsumers threads"/>
+    <option opt="extracts" hasArg="true"
+            description="this dir for analysis" required="false"/>
+    <option opt="db" hasArg="true"
+            description="name of db directory or file to which to write results"/>
+    <option opt="alterExtract" hasArg="true"
+            description="for json-formatted extract files
                 process full metadata list ('as_is'=default),
                 take just the first/container document ('first_only'),
                 concatenate all content into the first metadata item ('concatenate_content')"/>
-        <option opt="minExtractLength" hasArg="true"
-                description="minimum extract length to process"/>
-        <option opt="maxExtractLength" hasArg="true"
-                description="maximum extract length to process"/>
-        <option opt="jdbc" hasArg="true"
-                description="full jdbc connection string"/>
-        <option opt="jdbcDriver" hasArg="true"
-                description="canonical class name for jdbc driver"/>
-        <option opt="tablePrefix" hasArg="true"
-                description="EXPERT: prefix for table names"/>
-        <option opt="drop" hasArg="false" description="drop tables if they exist"/>
-        <option opt="maxFilesToAdd" hasArg="true" description="maximum number of files to add to the crawler"/>
-        <option opt="maxTokens" hasArg="true" description="maximum tokens to process, default=200000"/>
-        <option opt="maxContentLength" hasArg="true"
-                description="truncate content beyond this length for calculating 'contents' stats, default=1000000"/>
-        <option opt="maxContentLengthForLangId" hasArg="true"
-                description="truncate content beyond this length for language id, default=50000"/>
-        <option opt="defaultLangCode" hasArg="true"
-                description="which language to use for common words if no 'common words' file exists for the langid result"/>
+    <option opt="minExtractLength" hasArg="true"
+            description="minimum extract length to process"/>
+    <option opt="maxExtractLength" hasArg="true"
+            description="maximum extract length to process"/>
+    <option opt="jdbc" hasArg="true"
+            description="full jdbc connection string"/>
+    <option opt="jdbcDriver" hasArg="true"
+            description="canonical class name for jdbc driver"/>
+    <option opt="tablePrefix" hasArg="true"
+            description="EXPERT: prefix for table names"/>
+    <option opt="drop" hasArg="false" description="drop tables if they exist"/>
+    <option opt="maxFilesToAdd" hasArg="true" description="maximum number of files to add to the crawler"/>
+    <option opt="maxTokens" hasArg="true" description="maximum tokens to process, default=200000"/>
+    <option opt="maxContentLength" hasArg="true"
+            description="truncate content beyond this length for calculating 'contents' stats, default=1000000"/>
+    <option opt="maxContentLengthForLangId" hasArg="true"
+            description="truncate content beyond this length for language id, default=50000"/>
+    <option opt="defaultLangCode" hasArg="true"
+            description="which language to use for common words if no 'common words' file exists for the langid result"/>
 
 
+  </commandline>
 
 
-    </commandline>
+  <!--
+      Can also add startDir: this tells the crawler to start indexing a
+      child directory of the inputDir directory.
+  -->
+  <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
 
+           crawlOrder="sorted"
+           maxConsecWaitMillis="300000"
+           maxFilesToAdd="-1"
+           maxFilesToConsider="-1"
+           includeFilePat=""
+           excludeFilePat=""
+           maxFileSizeBytes="-1"
+  />
 
-    <!--
-        Can also add startDir: this tells the crawler to start indexing a
-        child directory of the inputDir directory.
-    -->
-    <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
+  <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
+             consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractProfilerBuilder"
+             minExtractLength="-1"
+             maxExtractLength="2000000"
+             commonTokens="resources/common_tokens"/>
 
-             crawlOrder="sorted"
-             maxConsecWaitMillis="300000"
-             maxFilesToAdd="-1"
-             maxFilesToConsider="-1"
-             includeFilePat=""
-             excludeFilePat=""
-             maxFileSizeBytes="-1"
-    />
 
-    <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
-               consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractProfilerBuilder"
-               minExtractLength="-1"
-               maxExtractLength="2000000"
-               commonTokens="resources/common_tokens"/>
-
-
-    <!-- reporter and interrupter are optional -->
-    <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
-              staleThresholdMillis="500000"/>
+  <!-- reporter and interrupter are optional -->
+  <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
+            staleThresholdMillis="500000"/>
 </tika-batch-config>
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/AnalyzerManagerTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/AnalyzerManagerTest.java
index c4202b56d..efa0e4610 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/AnalyzerManagerTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/AnalyzerManagerTest.java
@@ -66,8 +66,7 @@ public class AnalyzerManagerTest {
         Set<String> seen = new HashSet<>();
         while (ts.incrementToken()) {
             String t = termAtt.toString();
-            if (AlphaIdeographFilterFactory.isAlphabetic(t.toCharArray(), t.length()) &&
-                    t.contains("5")) {
+            if (AlphaIdeographFilterFactory.isAlphabetic(t.toCharArray(), t.length()) && t.contains("5")) {
                 fail("Shouldn't have found a numeric");
             }
             seen.add(termAtt.toString());
@@ -87,7 +86,9 @@ public class AnalyzerManagerTest {
         for (int i = 0; i < 1001000; i++) {
             sb.append("the ");
         }
-        TokenStream ts = analyzerManager.getGeneralAnalyzer().tokenStream("f", sb.toString());
+        TokenStream ts = analyzerManager
+                .getGeneralAnalyzer()
+                .tokenStream("f", sb.toString());
         ts.reset();
         CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);
         int tokens = 0;
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ComparerBatchTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ComparerBatchTest.java
index 8e0655b83..c37c4e813 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ComparerBatchTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ComparerBatchTest.java
@@ -47,8 +47,7 @@ import org.apache.tika.eval.app.db.Cols;
 @Disabled("move these tests to TikaEvalCLITest")
 public class ComparerBatchTest extends FSBatchTestBase {
 
-    public final static String COMPARER_PROCESS_CLASS =
-            "org.apache.tika.batch.fs.FSBatchProcessCLI";
+    public final static String COMPARER_PROCESS_CLASS = "org.apache.tika.batch.fs.FSBatchProcessCLI";
     private final static String compJoinCont = "";
 
     @TempDir
@@ -62,10 +61,14 @@ public class ComparerBatchTest extends FSBatchTestBase {
     @BeforeAll
     public static void setUp() throws Exception {
 
-        File inputRoot = new File(ComparerBatchTest.class.getResource("/test-dirs").toURI());
+        File inputRoot = new File(ComparerBatchTest.class
+                .getResource("/test-dirs")
+                .toURI());
         dbDir = Files.createTempDirectory(inputRoot.toPath(), "tika-test-db-dir-");
         Map<String, String> args = new HashMap<>();
-        Path db = FileSystems.getDefault().getPath(dbDir.toString(), "comparisons_test");
+        Path db = FileSystems
+                .getDefault()
+                .getPath(dbDir.toString(), "comparisons_test");
         args.put("-db", db.toString());
 
         //for debugging, you can use this to select only one file pair to load
@@ -90,14 +93,12 @@ public class ComparerBatchTest extends FSBatchTestBase {
     public void testSimpleDBWriteAndRead() throws Exception {
         Set<String> set = new HashSet<>();
         //filenames
-        List<String> list =
-                getColStrings(Cols.FILE_NAME.name(), ExtractComparer.PROFILES_A.getName(), "");
+        List<String> list = getColStrings(Cols.FILE_NAME.name(), ExtractComparer.PROFILES_A.getName(), "");
         assertEquals(7, list.size());
         assertTrue(list.contains("file1.pdf"));
 
         //container ids in comparisons table
-        list = getColStrings(Cols.CONTAINER_ID.name(),
-                ExtractComparer.COMPARISON_CONTAINERS.getName(), "");
+        list = getColStrings(Cols.CONTAINER_ID.name(), ExtractComparer.COMPARISON_CONTAINERS.getName(), "");
         assertEquals(10, list.size());
         set.clear();
         set.addAll(list);
@@ -378,8 +379,7 @@ public class ComparerBatchTest extends FSBatchTestBase {
         return getColStrings(colName, ExtractComparer.CONTENT_COMPARISONS.getName(), null);
     }
 
-    private List<String> getColStrings(String colName, String table, String where)
-            throws Exception {
+    private List<String> getColStrings(String colName, String table, String where) throws Exception {
         String sql = getSql(colName, table, where);
         List<String> results = new ArrayList<>();
         try (Statement st = conn.createStatement()) {
@@ -394,9 +394,15 @@ public class ComparerBatchTest extends FSBatchTestBase {
 
     private String getSql(String colName, String table, String where) {
         StringBuilder sb = new StringBuilder();
-        sb.append("select ").append(colName).append(" from ").append(table);
+        sb
+                .append("select ")
+                .append(colName)
+                .append(" from ")
+                .append(table);
         if (where != null && !where.equals("")) {
-            sb.append(" where ").append(where);
+            sb
+                    .append(" where ")
+                    .append(where);
         }
         return sb.toString();
     }
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ProfilerBatchTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ProfilerBatchTest.java
index 9f8a20011..be58e0ed2 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ProfilerBatchTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/ProfilerBatchTest.java
@@ -47,8 +47,7 @@ import org.apache.tika.eval.app.io.ExtractReaderException;
 @Disabled
 public class ProfilerBatchTest {
 
-    public final static String COMPARER_PROCESS_CLASS =
-            "org.apache.tika.batch.fs.FSBatchProcessCLI";
+    public final static String COMPARER_PROCESS_CLASS = "org.apache.tika.batch.fs.FSBatchProcessCLI";
     private final static String profileTable = ExtractProfiler.PROFILE_TABLE.getName();
     private final static String exTable = ExtractProfiler.EXCEPTION_TABLE.getName();
     private final static String fpCol = Cols.FILE_PATH.name();
@@ -58,8 +57,9 @@ public class ProfilerBatchTest {
     @BeforeAll
     public static void setUp() throws Exception {
 
-        Path inputRoot = Paths.get(
-                ComparerBatchTest.class.getResource("/test-dirs/extractsA").toURI());
+        Path inputRoot = Paths.get(ComparerBatchTest.class
+                .getResource("/test-dirs/extractsA")
+                .toURI());
         dbDir = Files.createTempDirectory(inputRoot, "tika-test-db-dir-");
         Map<String, String> args = new HashMap<>();
         Path db = dbDir.resolve("profiler_test");
@@ -128,9 +128,8 @@ public class ProfilerBatchTest {
 
     @Test
     public void testExtractErrors() throws Exception {
-        String sql = "select EXTRACT_EXCEPTION_ID from extract_exceptions e" +
-                " join containers c on c.container_id = e.container_id " +
-                " where c.file_path='file9_noextract.txt'";
+        String sql =
+                "select EXTRACT_EXCEPTION_ID from extract_exceptions e" + " join containers c on c.container_id = e.container_id " + " where c.file_path='file9_noextract.txt'";
 
         assertEquals("missing extract: file9_noextract.txt", "0", getSingleResult(sql));
         debugTable(ExtractProfiler.CONTAINER_TABLE);
@@ -139,14 +138,10 @@ public class ProfilerBatchTest {
         debugTable(ExtractProfiler.EXCEPTION_TABLE);
         debugTable(ExtractProfiler.EXTRACT_EXCEPTION_TABLE);
 
-        sql = "select EXTRACT_EXCEPTION_ID from errors e" +
-                " join containers c on c.container_id = e.container_id " +
-                " where c.file_path='file5_emptyA.pdf'";
+        sql = "select EXTRACT_EXCEPTION_ID from errors e" + " join containers c on c.container_id = e.container_id " + " where c.file_path='file5_emptyA.pdf'";
         assertEquals("empty extract: file5_emptyA.pdf", "1", getSingleResult(sql));
 
-        sql = "select EXTRACT_EXCEPTION_ID from errors e" +
-                " join containers c on c.container_id = e.container_id " +
-                " where c.file_path='file7_badJson.pdf'";
+        sql = "select EXTRACT_EXCEPTION_ID from errors e" + " join containers c on c.container_id = e.container_id " + " where c.file_path='file7_badJson.pdf'";
         assertEquals("extract error:file7_badJson.pdf", "2", getSingleResult(sql));
 
     }
@@ -157,14 +152,11 @@ public class ProfilerBatchTest {
         String sql = "select file_path from errors where container_id is null";
         assertEquals("file10_permahang.txt", getSingleResult(sql));
 
-        sql = "select extract_error_id from extract_exceptions " +
-                "where file_path='file11_oom.txt'";
-        assertEquals(Integer.toString(ExtractReaderException.TYPE.ZERO_BYTE_EXTRACT_FILE.ordinal()),
-                getSingleResult(sql));
+        sql = "select extract_error_id from extract_exceptions " + "where file_path='file11_oom.txt'";
+        assertEquals(Integer.toString(ExtractReaderException.TYPE.ZERO_BYTE_EXTRACT_FILE.ordinal()), getSingleResult(sql));
 
         sql = "select parse_error_id from extract_exceptions where file_path='file11_oom.txt'";
-        assertEquals(Integer.toString(AbstractProfiler.PARSE_ERROR_TYPE.OOM.ordinal()),
-                getSingleResult(sql));
+        assertEquals(Integer.toString(AbstractProfiler.PARSE_ERROR_TYPE.OOM.ordinal()), getSingleResult(sql));
 
     }
 
@@ -180,8 +172,9 @@ public class ProfilerBatchTest {
         int hits = 0;
         String val = "";
         while (rs.next()) {
-            assertEquals(1,
-                    rs.getMetaData().getColumnCount(), "must have only one column in result");
+            assertEquals(1, rs
+                    .getMetaData()
+                    .getColumnCount(), "must have only one column in result");
             val = rs.getString(1);
             hits++;
         }
@@ -197,13 +190,17 @@ public class ProfilerBatchTest {
             String sql = "select * from " + table.getName();
             st = conn.createStatement();
             ResultSet rs = st.executeQuery(sql);
-            int colCount = rs.getMetaData().getColumnCount();
+            int colCount = rs
+                    .getMetaData()
+                    .getColumnCount();
             System.out.println("TABLE: " + table.getName());
             for (int i = 1; i <= colCount; i++) {
                 if (i > 1) {
                     System.out.print(" | ");
                 }
-                System.out.print(rs.getMetaData().getColumnName(i));
+                System.out.print(rs
+                        .getMetaData()
+                        .getColumnName(i));
             }
             System.out.println("");
             int rowCount = 0;
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/SimpleComparerTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/SimpleComparerTest.java
index be9c71cdb..035869c9a 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/SimpleComparerTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/SimpleComparerTest.java
@@ -56,30 +56,29 @@ public class SimpleComparerTest extends TikaTest {
     @BeforeAll
     public static void staticSetUp() throws Exception {
         WRITER = new MockDBWriter();
-        AbstractProfiler.loadCommonTokens(
-                Paths.get(SimpleComparerTest.class.getResource("/common_tokens").toURI()), "en");
+        AbstractProfiler.loadCommonTokens(Paths.get(SimpleComparerTest.class
+                .getResource("/common_tokens")
+                .toURI()), "en");
     }
 
     @BeforeEach
     public void setUp() throws Exception {
         WRITER.clear();
         comparer = new ExtractComparer(null, null, Paths.get("extractsA"), Paths.get("extractsB"),
-                new ExtractReader(ExtractReader.ALTER_METADATA_LIST.AS_IS, IGNORE_LENGTH,
-                        IGNORE_LENGTH), WRITER);
+                new ExtractReader(ExtractReader.ALTER_METADATA_LIST.AS_IS, IGNORE_LENGTH, IGNORE_LENGTH), WRITER);
     }
 
     @Test
     public void testBasic() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsA/file1.pdf.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsB/file1.pdf.json").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf.json"), getResourceAsFile("/test-dirs/extractsA/file1.pdf.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf.json"), getResourceAsFile("/test-dirs/extractsB/file1.pdf.json").toPath());
 
         comparer.compareFiles(fpsA, fpsB);
 
         List<Map<Cols, String>> tableInfos = WRITER.getTable(ExtractComparer.CONTENT_COMPARISONS);
         Map<Cols, String> row = tableInfos.get(0);
-        assertTrue(row.get(Cols.TOP_10_UNIQUE_TOKEN_DIFFS_A)
+        assertTrue(row
+                .get(Cols.TOP_10_UNIQUE_TOKEN_DIFFS_A)
                 .startsWith("1,200: 1 | 120000: 1 | over: 1"));
 
         tableInfos = WRITER.getTable(ExtractComparer.CONTENTS_TABLE_A);
@@ -112,10 +111,8 @@ public class SimpleComparerTest extends TikaTest {
 
     @Test
     public void testBasicSpanish() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsA/file12_es.txt.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsB/file12_es.txt.json").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf.json"), getResourceAsFile("/test-dirs/extractsA/file12_es.txt.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf.json"), getResourceAsFile("/test-dirs/extractsB/file12_es.txt.json").toPath());
 
         comparer.compareFiles(fpsA, fpsB);
 
@@ -138,10 +135,8 @@ public class SimpleComparerTest extends TikaTest {
         //file names.  The test file contains MT'd Simplified Chinese with
         //known "common words" appended at end.
 
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file13_attachANotB.doc.json"),
-                getResourceAsFile("/test-dirs/extractsA/file13_attachANotB.doc.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("non-existent.json"),
-                Paths.get("/test-dirs/extractsB/non-existent.json"));
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file13_attachANotB.doc.json"), getResourceAsFile("/test-dirs/extractsA/file13_attachANotB.doc.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("non-existent.json"), Paths.get("/test-dirs/extractsB/non-existent.json"));
 
         comparer.compareFiles(fpsA, fpsB);
 
@@ -156,15 +151,12 @@ public class SimpleComparerTest extends TikaTest {
 
     @Test
     public void testEmpty() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf"),
-                getResourceAsFile("/test-dirs/extractsA/file1.pdf.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf"),
-                getResourceAsFile("/test-dirs/extractsB/file4_emptyB.pdf.json").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf"), getResourceAsFile("/test-dirs/extractsA/file1.pdf.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf"), getResourceAsFile("/test-dirs/extractsB/file4_emptyB.pdf.json").toPath());
         comparer.compareFiles(fpsA, fpsB);
         List<Map<Cols, String>> table = WRITER.getTable(ExtractComparer.EXTRACT_EXCEPTION_TABLE_B);
         Map<Cols, String> row = table.get(0);
-        assertEquals(Integer.toString(ExtractReaderException.TYPE.ZERO_BYTE_EXTRACT_FILE.ordinal()),
-                row.get(Cols.EXTRACT_EXCEPTION_ID));
+        assertEquals(Integer.toString(ExtractReaderException.TYPE.ZERO_BYTE_EXTRACT_FILE.ordinal()), row.get(Cols.EXTRACT_EXCEPTION_ID));
     }
 
 
@@ -193,19 +185,14 @@ public class SimpleComparerTest extends TikaTest {
 
     @Test
     public void testAccessException() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file6_accessEx.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsA/file6_accessEx.pdf.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file6_accessEx.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsB/file6_accessEx.pdf.json").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file6_accessEx.pdf.json"), getResourceAsFile("/test-dirs/extractsA/file6_accessEx.pdf.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file6_accessEx.pdf.json"), getResourceAsFile("/test-dirs/extractsB/file6_accessEx.pdf.json").toPath());
         comparer.compareFiles(fpsA, fpsB);
-        for (TableInfo t : new TableInfo[]{ExtractComparer.EXCEPTION_TABLE_A,
-                ExtractComparer.EXCEPTION_TABLE_B}) {
+        for (TableInfo t : new TableInfo[]{ExtractComparer.EXCEPTION_TABLE_A, ExtractComparer.EXCEPTION_TABLE_B}) {
             List<Map<Cols, String>> table = WRITER.getTable(t);
 
             Map<Cols, String> rowA = table.get(0);
-            assertEquals(
-                    Integer.toString(AbstractProfiler.EXCEPTION_TYPE.ACCESS_PERMISSION.ordinal()),
-                    rowA.get(Cols.PARSE_EXCEPTION_ID));
+            assertEquals(Integer.toString(AbstractProfiler.EXCEPTION_TYPE.ACCESS_PERMISSION.ordinal()), rowA.get(Cols.PARSE_EXCEPTION_ID));
             assertNull(rowA.get(Cols.ORIG_STACK_TRACE));
             assertNull(rowA.get(Cols.SORT_STACK_TRACE));
         }
@@ -215,8 +202,7 @@ public class SimpleComparerTest extends TikaTest {
     public void testAttachmentCounts() {
         List<Metadata> list = new ArrayList<>();
         Metadata m0 = new Metadata();
-        m0.set(TikaCoreProperties.EMBEDDED_RESOURCE_PATH,
-                "dir1/dir2/file.zip");//bad data should be ignored
+        m0.set(TikaCoreProperties.EMBEDDED_RESOURCE_PATH, "dir1/dir2/file.zip");//bad data should be ignored
         //in the first metadata object
         list.add(m0);
         Metadata m1 = new Metadata();
@@ -249,24 +235,22 @@ public class SimpleComparerTest extends TikaTest {
 
     @Test
     public void testDifferentlyOrderedAttachments() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file14_diffAttachOrder.json"),
-                getResourceAsFile("/test-dirs/extractsA/file14_diffAttachOrder.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file6_accessEx.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsB/file14_diffAttachOrder.json").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file14_diffAttachOrder.json"), getResourceAsFile("/test-dirs/extractsA/file14_diffAttachOrder.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file6_accessEx.pdf.json"), getResourceAsFile("/test-dirs/extractsB/file14_diffAttachOrder.json").toPath());
         comparer.compareFiles(fpsA, fpsB);
         List<Map<Cols, String>> tableInfos = WRITER.getTable(ExtractComparer.CONTENT_COMPARISONS);
         assertEquals(3, tableInfos.size());
         for (int i = 0; i < tableInfos.size(); i++) {
-            assertEquals("1.0", tableInfos.get(i).get(Cols.OVERLAP), "problem with " + i);
+            assertEquals("1.0", tableInfos
+                    .get(i)
+                    .get(Cols.OVERLAP), "problem with " + i);
         }
     }
 
     @Test
     public void testTags() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file15_tags.json"),
-                getResourceAsFile("/test-dirs/extractsA/file15_tags.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file15_tags.html"),
-                getResourceAsFile("/test-dirs/extractsB/file15_tags.html").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file15_tags.json"), getResourceAsFile("/test-dirs/extractsA/file15_tags.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file15_tags.html"), getResourceAsFile("/test-dirs/extractsB/file15_tags.html").toPath());
         comparer.compareFiles(fpsA, fpsB);
         List<Map<Cols, String>> tableInfosA = WRITER.getTable(ExtractComparer.TAGS_TABLE_A);
         assertEquals(1, tableInfosA.size());
@@ -284,10 +268,8 @@ public class SimpleComparerTest extends TikaTest {
 
     @Test
     public void testBadTags() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file16_badtags.json"),
-                getResourceAsFile("/test-dirs/extractsA/file16_badTags.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file16_badtags.html"),
-                getResourceAsFile("/test-dirs/extractsB/file16_badTags.html").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file16_badtags.json"), getResourceAsFile("/test-dirs/extractsA/file16_badTags.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file16_badtags.html"), getResourceAsFile("/test-dirs/extractsB/file16_badTags.html").toPath());
         comparer.compareFiles(fpsA, fpsB);
         List<Map<Cols, String>> tableInfosA = WRITER.getTable(ExtractComparer.TAGS_TABLE_A);
         assertEquals(1, tableInfosA.size());
@@ -304,10 +286,8 @@ public class SimpleComparerTest extends TikaTest {
 
     @Test
     public void testTagsOutOfOrder() throws Exception {
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file17_tagsOutOfOrder.json"),
-                getResourceAsFile("/test-dirs/extractsA/file17_tagsOutOfOrder.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file16_badTags.html"),
-                getResourceAsFile("/test-dirs/extractsB/file16_badTags.html").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file17_tagsOutOfOrder.json"), getResourceAsFile("/test-dirs/extractsA/file17_tagsOutOfOrder.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file16_badTags.html"), getResourceAsFile("/test-dirs/extractsB/file16_badTags.html").toPath());
         comparer.compareFiles(fpsA, fpsB);
         List<Map<Cols, String>> tableInfosA = WRITER.getTable(ExtractComparer.TAGS_TABLE_A);
         assertEquals(1, tableInfosA.size());
@@ -330,16 +310,11 @@ public class SimpleComparerTest extends TikaTest {
     public void testDebug() throws Exception {
         Path commonTokens = Paths.get(getResourceAsFile("/common_tokens_short.txt").toURI());
         AbstractProfiler.loadCommonTokens(commonTokens, "en");
-        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsA/file1.pdf.json").toPath());
-        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf.json"),
-                getResourceAsFile("/test-dirs/extractsB/file1.pdf.json").toPath());
+        EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf.json"), getResourceAsFile("/test-dirs/extractsA/file1.pdf.json").toPath());
+        EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf.json"), getResourceAsFile("/test-dirs/extractsB/file1.pdf.json").toPath());
         comparer.compareFiles(fpsA, fpsB);
-        for (TableInfo t : new TableInfo[]{ExtractComparer.COMPARISON_CONTAINERS,
-                ExtractComparer.EXTRACT_EXCEPTION_TABLE_A,
-                ExtractComparer.EXTRACT_EXCEPTION_TABLE_B, ExtractComparer.EXCEPTION_TABLE_A,
-                ExtractComparer.EXCEPTION_TABLE_B, ExtractComparer.PROFILES_A,
-                ExtractComparer.PROFILES_B, ExtractComparer.CONTENTS_TABLE_A,
+        for (TableInfo t : new TableInfo[]{ExtractComparer.COMPARISON_CONTAINERS, ExtractComparer.EXTRACT_EXCEPTION_TABLE_A, ExtractComparer.EXTRACT_EXCEPTION_TABLE_B,
+                ExtractComparer.EXCEPTION_TABLE_A, ExtractComparer.EXCEPTION_TABLE_B, ExtractComparer.PROFILES_A, ExtractComparer.PROFILES_B, ExtractComparer.CONTENTS_TABLE_A,
                 ExtractComparer.CONTENTS_TABLE_B, ExtractComparer.CONTENT_COMPARISONS}) {
             //debugPrintTable(t);
         }
@@ -378,11 +353,8 @@ public class SimpleComparerTest extends TikaTest {
         EvalFilePaths fpsA = new EvalFilePaths(Paths.get("file1.pdf.json"), p1);
         EvalFilePaths fpsB = new EvalFilePaths(Paths.get("file1.pdf.json"), p2);
         comparer.compareFiles(fpsA, fpsB);
-        for (TableInfo t : new TableInfo[]{ExtractComparer.COMPARISON_CONTAINERS,
-                ExtractComparer.EXTRACT_EXCEPTION_TABLE_A,
-                ExtractComparer.EXTRACT_EXCEPTION_TABLE_B, ExtractComparer.EXCEPTION_TABLE_A,
-                ExtractComparer.EXCEPTION_TABLE_B, ExtractComparer.PROFILES_A,
-                ExtractComparer.PROFILES_B, ExtractComparer.CONTENTS_TABLE_A,
+        for (TableInfo t : new TableInfo[]{ExtractComparer.COMPARISON_CONTAINERS, ExtractComparer.EXTRACT_EXCEPTION_TABLE_A, ExtractComparer.EXTRACT_EXCEPTION_TABLE_B,
+                ExtractComparer.EXCEPTION_TABLE_A, ExtractComparer.EXCEPTION_TABLE_B, ExtractComparer.PROFILES_A, ExtractComparer.PROFILES_B, ExtractComparer.CONTENTS_TABLE_A,
                 ExtractComparer.CONTENTS_TABLE_B, ExtractComparer.CONTENT_COMPARISONS}) {
             debugPrintTable(t);
         }
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/TikaEvalCLITest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/TikaEvalCLITest.java
index 0af1a2833..f3292065f 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/TikaEvalCLITest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/TikaEvalCLITest.java
@@ -69,11 +69,15 @@ public class TikaEvalCLITest extends TikaTest {
         List<String> args = new ArrayList<>();
         args.add("Compare");
         args.add("-extractsA");
-        args.add(ProcessUtils
-                .escapeCommandLine(extractsDir.resolve("extractsA").toAbsolutePath().toString()));
+        args.add(ProcessUtils.escapeCommandLine(extractsDir
+                .resolve("extractsA")
+                .toAbsolutePath()
+                .toString()));
         args.add("-extractsB");
-        args.add(ProcessUtils
-                .escapeCommandLine(extractsDir.resolve("extractsB").toAbsolutePath().toString()));
+        args.add(ProcessUtils.escapeCommandLine(extractsDir
+                .resolve("extractsB")
+                .toAbsolutePath()
+                .toString()));
         //add these just to confirm this info doesn't cause problems w cli
         args.add("-maxTokens");
         args.add("10000000");
@@ -83,8 +87,9 @@ public class TikaEvalCLITest extends TikaTest {
         args.add("100000");
 
         args.add("-db");
-        args.add(ProcessUtils
-                .escapeCommandLine(compareDBDir.toAbsolutePath().toString() + "/" + dbName));
+        args.add(ProcessUtils.escapeCommandLine(compareDBDir
+                .toAbsolutePath()
+                .toString() + "/" + dbName));
 
         execute(args, 60000);
 
@@ -94,8 +99,10 @@ public class TikaEvalCLITest extends TikaTest {
         List<String> args = new ArrayList<>();
         args.add("Profile");
         args.add("-extracts");
-        args.add(ProcessUtils
-                .escapeCommandLine(extractsDir.resolve("extractsA").toAbsolutePath().toString()));
+        args.add(ProcessUtils.escapeCommandLine(extractsDir
+                .resolve("extractsA")
+                .toAbsolutePath()
+                .toString()));
         //add these just to confirm this info doesn't cause problems w cli
         args.add("-maxTokens");
         args.add("10000000");
@@ -105,8 +112,9 @@ public class TikaEvalCLITest extends TikaTest {
         args.add("100000");
 
         args.add("-db");
-        args.add(ProcessUtils
-                .escapeCommandLine(profileDBDir.toAbsolutePath().toString() + "/" + dbName));
+        args.add(ProcessUtils.escapeCommandLine(profileDBDir
+                .toAbsolutePath()
+                .toString() + "/" + dbName));
         execute(args, 60000);
     }
 
@@ -114,10 +122,13 @@ public class TikaEvalCLITest extends TikaTest {
         List<String> args = new ArrayList<>();
         args.add("Report");
         args.add("-db");
-        args.add(ProcessUtils
-                .escapeCommandLine(profileDBDir.toAbsolutePath().toString() + "/" + dbName));
+        args.add(ProcessUtils.escapeCommandLine(profileDBDir
+                .toAbsolutePath()
+                .toString() + "/" + dbName));
         args.add("-rd");
-        args.add(ProcessUtils.escapeCommandLine(profileReportsDir.toAbsolutePath().toString()));
+        args.add(ProcessUtils.escapeCommandLine(profileReportsDir
+                .toAbsolutePath()
+                .toString()));
         execute(args, 60000);
     }
 
@@ -125,10 +136,13 @@ public class TikaEvalCLITest extends TikaTest {
         List<String> args = new ArrayList<>();
         args.add("Report");
         args.add("-db");
-        args.add(ProcessUtils
-                .escapeCommandLine(compareDBDir.toAbsolutePath().toString() + "/" + dbName));
+        args.add(ProcessUtils.escapeCommandLine(compareDBDir
+                .toAbsolutePath()
+                .toString() + "/" + dbName));
         args.add("-rd");
-        args.add(ProcessUtils.escapeCommandLine(compareReportsDir.toAbsolutePath().toString()));
+        args.add(ProcessUtils.escapeCommandLine(compareReportsDir
+                .toAbsolutePath()
+                .toString()));
         execute(args, 60000);
     }
 
@@ -159,15 +173,16 @@ public class TikaEvalCLITest extends TikaTest {
         }
         if (exitValue == Integer.MIN_VALUE) {
             process.destroy();
-            throw new RuntimeException("Process never exited within the allowed amount of time.\n" +
-                    "I needed to destroy it");
+            throw new RuntimeException("Process never exited within the allowed amount of time.\n" + "I needed to destroy it");
         }
     }
 
     @Test
     public void testBasicCompare() throws Exception {
         Set<String> fNames = new HashSet<>();
-        for (File f : compareDBDir.toFile().listFiles()) {
+        for (File f : compareDBDir
+                .toFile()
+                .listFiles()) {
             fNames.add(f.getName());
         }
         assertContains(dbName + ".mv.db", fNames);
@@ -176,7 +191,9 @@ public class TikaEvalCLITest extends TikaTest {
     @Test
     public void testBasicProfile() throws Exception {
         Set<String> fNames = new HashSet<>();
-        for (File f : profileDBDir.toFile().listFiles()) {
+        for (File f : profileDBDir
+                .toFile()
+                .listFiles()) {
             fNames.add(f.getName());
         }
         assertContains(dbName + ".mv.db", fNames);
@@ -189,7 +206,10 @@ public class TikaEvalCLITest extends TikaTest {
         int cnt = 0;
         for (Path report : v.getPaths()) {
 
-            if (report.getFileName().toString().endsWith(".xlsx")) {
+            if (report
+                    .getFileName()
+                    .toString()
+                    .endsWith(".xlsx")) {
                 cnt++;
             }
         }
@@ -202,7 +222,10 @@ public class TikaEvalCLITest extends TikaTest {
         Files.walkFileTree(compareReportsDir, v);
         int cnt = 0;
         for (Path report : v.getPaths()) {
-            if (report.getFileName().toString().endsWith(".xlsx")) {
+            if (report
+                    .getFileName()
+                    .toString()
+                    .endsWith(".xlsx")) {
                 cnt++;
             }
         }
@@ -216,14 +239,19 @@ public class TikaEvalCLITest extends TikaTest {
         List<String> args = new ArrayList<>();
         args.add("Compare");
         args.add("-extractsA");
-        args.add(ProcessUtils
-                .escapeCommandLine(extractsDir.resolve("extractsA").toAbsolutePath().toString()));
+        args.add(ProcessUtils.escapeCommandLine(extractsDir
+                .resolve("extractsA")
+                .toAbsolutePath()
+                .toString()));
         args.add("-extractsB");
-        args.add(ProcessUtils
-                .escapeCommandLine(extractsDir.resolve("extractsB").toAbsolutePath().toString()));
+        args.add(ProcessUtils.escapeCommandLine(extractsDir
+                .resolve("extractsB")
+                .toAbsolutePath()
+                .toString()));
         args.add("-db");
-        args.add(ProcessUtils
-                .escapeCommandLine(compareDBDir.toAbsolutePath().toString() + "/" + dbName));
+        args.add(ProcessUtils.escapeCommandLine(compareDBDir
+                .toAbsolutePath()
+                .toString() + "/" + dbName));
 
         execute(args, 60000);
         //      args.add("-drop");
@@ -236,8 +264,7 @@ public class TikaEvalCLITest extends TikaTest {
         Set<Path> paths = new HashSet<>();
 
         @Override
-        public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs)
-                throws IOException {
+        public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException {
             return FileVisitResult.CONTINUE;
         }
 
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/db/AbstractBufferTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/db/AbstractBufferTest.java
index cf4c7aad3..32c4e5240 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/db/AbstractBufferTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/db/AbstractBufferTest.java
@@ -47,8 +47,7 @@ public class AbstractBufferTest {
     @Timeout(30000)
     public void runTest() throws InterruptedException, ExecutionException {
         List<String> keys = new ArrayList<>();
-        Collections
-                .addAll(keys, new String[]{"a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k"});
+        Collections.addAll(keys, new String[]{"a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k"});
 
         int numGets = 100;
         int numTesters = 20;
@@ -67,8 +66,15 @@ public class AbstractBufferTest {
             Future<MyTestResult> futureResult = completionService.poll(1, TimeUnit.SECONDS);
             if (futureResult != null) {
                 results++;
-                assertEquals(keys.size(), futureResult.get().getMap().keySet().size());
-                for (Map.Entry<String, Integer> e : futureResult.get().getMap().entrySet()) {
+                assertEquals(keys.size(), futureResult
+                        .get()
+                        .getMap()
+                        .keySet()
+                        .size());
+                for (Map.Entry<String, Integer> e : futureResult
+                        .get()
+                        .getMap()
+                        .entrySet()) {
                     if (!combined.containsKey(e.getKey())) {
                         combined.put(e.getKey(), e.getValue());
                     } else {
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/ExtractReaderTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/ExtractReaderTest.java
index d3e9a01df..8c33d42ff 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/ExtractReaderTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/ExtractReaderTest.java
@@ -37,8 +37,7 @@ public class ExtractReaderTest extends TikaTest {
 
     @BeforeEach
     public void setUp() throws Exception {
-        testJsonFile =
-                getResourceAsFile("/test-dirs/extractsA/file2_attachANotB.doc.json").toPath();
+        testJsonFile = getResourceAsFile("/test-dirs/extractsA/file2_attachANotB.doc.json").toPath();
         testTxtFile = getResourceAsFile("/test-dirs/extractsB/file13_attachANotB.doc.txt").toPath();
     }
 
@@ -49,25 +48,44 @@ public class ExtractReaderTest extends TikaTest {
         List<Metadata> metadataList = extractReader.loadExtract(testJsonFile);
 
         assertEquals(2, metadataList.size());
-        assertEquals(1, metadataList.get(0).getValues(TikaCoreProperties.TIKA_CONTENT).length);
-        assertEquals(1, metadataList.get(1).getValues(TikaCoreProperties.TIKA_CONTENT).length);
-        assertContains("fox", metadataList.get(0).get(TikaCoreProperties.TIKA_CONTENT));
-        assertContains("attachment", metadataList.get(1).get(TikaCoreProperties.TIKA_CONTENT));
+        assertEquals(1, metadataList
+                .get(0)
+                .getValues(TikaCoreProperties.TIKA_CONTENT).length);
+        assertEquals(1, metadataList
+                .get(1)
+                .getValues(TikaCoreProperties.TIKA_CONTENT).length);
+        assertContains("fox", metadataList
+                .get(0)
+                .get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("attachment", metadataList
+                .get(1)
+                .get(TikaCoreProperties.TIKA_CONTENT));
 
         extractReader = new ExtractReader(ExtractReader.ALTER_METADATA_LIST.FIRST_ONLY);
         metadataList = extractReader.loadExtract(testJsonFile);
         assertEquals(1, metadataList.size());
-        assertEquals(1, metadataList.get(0).getValues(TikaCoreProperties.TIKA_CONTENT).length);
-        assertContains("fox", metadataList.get(0).get(TikaCoreProperties.TIKA_CONTENT));
-        assertNotContained("attachment", metadataList.get(0).get(TikaCoreProperties.TIKA_CONTENT));
-
-        extractReader =
-                new ExtractReader(ExtractReader.ALTER_METADATA_LIST.CONCATENATE_CONTENT_INTO_FIRST);
+        assertEquals(1, metadataList
+                .get(0)
+                .getValues(TikaCoreProperties.TIKA_CONTENT).length);
+        assertContains("fox", metadataList
+                .get(0)
+                .get(TikaCoreProperties.TIKA_CONTENT));
+        assertNotContained("attachment", metadataList
+                .get(0)
+                .get(TikaCoreProperties.TIKA_CONTENT));
+
+        extractReader = new ExtractReader(ExtractReader.ALTER_METADATA_LIST.CONCATENATE_CONTENT_INTO_FIRST);
         metadataList = extractReader.loadExtract(testJsonFile);
         assertEquals(1, metadataList.size());
-        assertEquals(1, metadataList.get(0).getValues(TikaCoreProperties.TIKA_CONTENT).length);
-        assertContains("fox", metadataList.get(0).get(TikaCoreProperties.TIKA_CONTENT));
-        assertContains("attachment", metadataList.get(0).get(TikaCoreProperties.TIKA_CONTENT));
+        assertEquals(1, metadataList
+                .get(0)
+                .getValues(TikaCoreProperties.TIKA_CONTENT).length);
+        assertContains("fox", metadataList
+                .get(0)
+                .get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("attachment", metadataList
+                .get(0)
+                .get(TikaCoreProperties.TIKA_CONTENT));
     }
 
     @Test
@@ -77,8 +95,7 @@ public class ExtractReaderTest extends TikaTest {
         assertEquals(1, metadataList.size());
         Metadata m = metadataList.get(0);
         assertEquals(1, m.getValues(TikaCoreProperties.TIKA_CONTENT).length);
-        assertContains("the quick brown fox fox fox jumped over the lazy lazy dog",
-                m.get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("the quick brown fox fox fox jumped over the lazy lazy dog", m.get(TikaCoreProperties.TIKA_CONTENT));
 
         //test that the mime is inferred from the file extension
         assertEquals("application/msword", m.get(Metadata.CONTENT_TYPE));
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/FatalExceptionReaderTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/FatalExceptionReaderTest.java
index aba84d436..2cd0d1363 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/FatalExceptionReaderTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/io/FatalExceptionReaderTest.java
@@ -25,7 +25,8 @@ import org.junit.jupiter.api.Test;
 public class FatalExceptionReaderTest {
     @Test
     public void testSimpleRead() throws Exception {
-        try (InputStream is = this.getClass()
+        try (InputStream is = this
+                .getClass()
                 .getResourceAsStream("/test-dirs/batch-logs/batch-process-fatal.xml")) {
             XMLLogReader reader = new XMLLogReader();
             //reader.read(is);
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/reports/ResultsReporterTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/reports/ResultsReporterTest.java
index e7d80ba73..a7f381fff 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/reports/ResultsReporterTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/reports/ResultsReporterTest.java
@@ -37,7 +37,10 @@ public class ResultsReporterTest {
 
     @BeforeEach
     public void setUp() throws Exception {
-        configFile = Paths.get(this.getClass().getResource("/reports.xml").toURI());
+        configFile = Paths.get(this
+                .getClass()
+                .getResource("/reports.xml")
+                .toURI());
         tmpDir = Files.createTempDirectory("tika-eval-report-test-");
 
         connection = new H2Util(tmpDir.resolve(dbName)).getConnection();
diff --git a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/tools/TopCommonTokenCounterTest.java b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/tools/TopCommonTokenCounterTest.java
index f2ea6e57b..e79fabc32 100644
--- a/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/tools/TopCommonTokenCounterTest.java
+++ b/tika-eval/tika-eval-app/src/test/java/org/apache/tika/eval/app/tools/TopCommonTokenCounterTest.java
@@ -43,12 +43,9 @@ public class TopCommonTokenCounterTest extends TikaTest {
     @BeforeAll
     public static void setUp() throws Exception {
         String[] docs =
-                new String[]{"th quick brown fox", "jumped over th brown lazy", "brown lazy fox",
-                        "\u666e\u6797\u65af\u987f\u5927\u5b66",
-                        "\u666e\u6797\u65af\u987f\u5927\u5b66"};
+                new String[]{"th quick brown fox", "jumped over th brown lazy", "brown lazy fox", "\u666e\u6797\u65af\u987f\u5927\u5b66", "\u666e\u6797\u65af\u987f\u5927\u5b66"};
 
-        try (BufferedWriter writer = Files
-                .newBufferedWriter(WORKING_DIR.resolve(INPUT_FILE), StandardCharsets.UTF_8)) {
+        try (BufferedWriter writer = Files.newBufferedWriter(WORKING_DIR.resolve(INPUT_FILE), StandardCharsets.UTF_8)) {
             //do this 10 times to bump the numbers above the TopCommonTokenCounter's MIN_DOC_FREQ
             for (int i = 0; i < 10; i++) {
                 for (String d : docs) {
@@ -58,17 +55,21 @@ public class TopCommonTokenCounterTest extends TikaTest {
             }
             writer.flush();
         }
-        TopCommonTokenCounter.main(new String[]{ProcessUtils.escapeCommandLine(
-                WORKING_DIR.resolve(COMMON_TOKENS_FILE).toAbsolutePath().toString()),
-                ProcessUtils.escapeCommandLine(
-                        WORKING_DIR.resolve(INPUT_FILE).toAbsolutePath().toString())});
+        TopCommonTokenCounter.main(new String[]{ProcessUtils.escapeCommandLine(WORKING_DIR
+                .resolve(COMMON_TOKENS_FILE)
+                .toAbsolutePath()
+                .toString()), ProcessUtils.escapeCommandLine(WORKING_DIR
+                .resolve(INPUT_FILE)
+                .toAbsolutePath()
+                .toString())});
     }
 
 
     @Test
     public void testSimple() throws Exception {
-        List<String> rows = FileUtils.readLines(WORKING_DIR.resolve(COMMON_TOKENS_FILE).toFile(),
-                StandardCharsets.UTF_8);
+        List<String> rows = FileUtils.readLines(WORKING_DIR
+                .resolve(COMMON_TOKENS_FILE)
+                .toFile(), StandardCharsets.UTF_8);
         List<String> tokens = new ArrayList<>();
         for (String row : rows) {
             if (!row.startsWith("#")) {
diff --git a/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-extract-config.xml b/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-extract-config.xml
index 5f33581f7..9ea67b224 100644
--- a/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-extract-config.xml
+++ b/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-extract-config.xml
@@ -19,58 +19,58 @@
 -->
 
 <tika-batch-config
-        maxAliveTimeSeconds="-1"
-        pauseOnEarlyTerminationMillis="500"
-        timeoutCheckPulseMillis="1000"
-        maxQueueSize="10000"
-        numConsumers="5"
-        timeoutThresholdMillis="300000">
+    maxAliveTimeSeconds="-1"
+    pauseOnEarlyTerminationMillis="500"
+    timeoutCheckPulseMillis="1000"
+    maxQueueSize="10000"
+    numConsumers="5"
+    timeoutThresholdMillis="300000">
 
-    <commandline>
-        <option opt="c" longOpt="tika-config" hasArg="true"
-                description="TikaConfig file"/>
+  <commandline>
+    <option opt="c" longOpt="tika-config" hasArg="true"
+            description="TikaConfig file"/>
 
-        <option opt="bc" longOpt="batch-config" hasArg="true"
-                description="xml batch config file" required="true"/>
-        <option opt="inputDir" hasArg="true"
-                description="dir to start crawling"/>
-        <option opt="numConsumers" hasArg="true"
-                description="number of fileConsumers threads"/>
-        <option opt="extracts" hasArg="true"
-                description="this dir contains the files containing extracted metadata/content" required="false"/>
-        <option opt="db" hasArg="true"
-                description="name of db directory or file to which to write results"/>
-        <option opt="minExtractLength" hasArg="true"
-                description="minimum extract length to process"/>
-        <option opt="maxExtractLength" hasArg="true"
-                description="maximum extract length to process"/>
-    </commandline>
+    <option opt="bc" longOpt="batch-config" hasArg="true"
+            description="xml batch config file" required="true"/>
+    <option opt="inputDir" hasArg="true"
+            description="dir to start crawling"/>
+    <option opt="numConsumers" hasArg="true"
+            description="number of fileConsumers threads"/>
+    <option opt="extracts" hasArg="true"
+            description="this dir contains the files containing extracted metadata/content" required="false"/>
+    <option opt="db" hasArg="true"
+            description="name of db directory or file to which to write results"/>
+    <option opt="minExtractLength" hasArg="true"
+            description="minimum extract length to process"/>
+    <option opt="maxExtractLength" hasArg="true"
+            description="maximum extract length to process"/>
+  </commandline>
 
 
-    <!--
-        Can also add startDir: this tells the crawler to start indexing a
-        child directory of the inputDir directory.
-    -->
-    <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
-             inputDir="src/test/resources/test-dirs/extractsA"
-             crawlOrder="sorted"
-             maxConsecWaitMillis="5000"
-             maxFilesToAdd="-1"
-             maxFilesToConsider="-1"
-             includeFilePat=""
-             excludeFilePat=""
-             maxFileSizeBytes="-1"
-    />
+  <!--
+      Can also add startDir: this tells the crawler to start indexing a
+      child directory of the inputDir directory.
+  -->
+  <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
+           inputDir="src/test/resources/test-dirs/extractsA"
+           crawlOrder="sorted"
+           maxConsecWaitMillis="5000"
+           maxFilesToAdd="-1"
+           maxFilesToConsider="-1"
+           includeFilePat=""
+           excludeFilePat=""
+           maxFileSizeBytes="-1"
+  />
 
-    <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
-               consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractProfilerBuilder"
-               errorLogFile="src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml"
-               extracts="src/test/resources/test-dirs/extractsA"
-               commonTokens="src/test/resources/common_tokens"/>
+  <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
+             consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractProfilerBuilder"
+             errorLogFile="src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml"
+             extracts="src/test/resources/test-dirs/extractsA"
+             commonTokens="src/test/resources/common_tokens"/>
 
 
-    <!-- reporter and interrupter are optional -->
-    <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
-              staleThresholdMillis="500000"/>
-    <interrupter builderClass="org.apache.tika.batch.builders.InterrupterBuilder"/>
+  <!-- reporter and interrupter are optional -->
+  <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
+            staleThresholdMillis="500000"/>
+  <interrupter builderClass="org.apache.tika.batch.builders.InterrupterBuilder"/>
 </tika-batch-config>
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-input-config.xml b/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-input-config.xml
index 81ecb3169..871851ffd 100644
--- a/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-input-config.xml
+++ b/tika-eval/tika-eval-app/src/test/resources/single-file-profiler-crawl-input-config.xml
@@ -19,59 +19,59 @@
 -->
 
 <tika-batch-config
-        maxAliveTimeSeconds="-1"
-        pauseOnEarlyTerminationMillis="500"
-        timeoutCheckPulseMillis="1000"
-        maxQueueSize="10000"
-        numConsumers="5"
-        timeoutThresholdMillis="300000">
+    maxAliveTimeSeconds="-1"
+    pauseOnEarlyTerminationMillis="500"
+    timeoutCheckPulseMillis="1000"
+    maxQueueSize="10000"
+    numConsumers="5"
+    timeoutThresholdMillis="300000">
 
-    <commandline>
-        <option opt="c" longOpt="tika-config" hasArg="true"
-                description="TikaConfig file"/>
+  <commandline>
+    <option opt="c" longOpt="tika-config" hasArg="true"
+            description="TikaConfig file"/>
 
-        <option opt="bc" longOpt="batch-config" hasArg="true"
-                description="xml batch config file" required="true"/>
-        <option opt="inputDir" hasArg="true"
-                description="dir to start crawling"/>
-        <option opt="numConsumers" hasArg="true"
-                description="number of fileConsumers threads"/>
-        <option opt="extracts" hasArg="true"
-                description="this dir contains the files containing extracted metadata/content" required="false"/>
-        <option opt="db" hasArg="true"
-                description="name of db directory or file to which to write results"/>
-        <option opt="minExtractLength" hasArg="true"
-                description="minimum extract length to process"/>
-        <option opt="maxExtractLength" hasArg="true"
-                description="maximum extract length to process"/>
-    </commandline>
+    <option opt="bc" longOpt="batch-config" hasArg="true"
+            description="xml batch config file" required="true"/>
+    <option opt="inputDir" hasArg="true"
+            description="dir to start crawling"/>
+    <option opt="numConsumers" hasArg="true"
+            description="number of fileConsumers threads"/>
+    <option opt="extracts" hasArg="true"
+            description="this dir contains the files containing extracted metadata/content" required="false"/>
+    <option opt="db" hasArg="true"
+            description="name of db directory or file to which to write results"/>
+    <option opt="minExtractLength" hasArg="true"
+            description="minimum extract length to process"/>
+    <option opt="maxExtractLength" hasArg="true"
+            description="maximum extract length to process"/>
+  </commandline>
 
 
-    <!--
-        Can also add startDir: this tells the crawler to start indexing a
-        child directory of the inputDir directory.
-    -->
-    <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
-             inputDir="src/test/resources/test-dirs/raw_input"
-             crawlOrder="sorted"
-             maxConsecWaitMillis="5000"
-             maxFilesToAdd="-1"
-             maxFilesToConsider="-1"
-             includeFilePat=""
-             excludeFilePat=""
-             maxFileSizeBytes="-1"
-    />
+  <!--
+      Can also add startDir: this tells the crawler to start indexing a
+      child directory of the inputDir directory.
+  -->
+  <crawler builderClass="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
+           inputDir="src/test/resources/test-dirs/raw_input"
+           crawlOrder="sorted"
+           maxConsecWaitMillis="5000"
+           maxFilesToAdd="-1"
+           maxFilesToConsider="-1"
+           includeFilePat=""
+           excludeFilePat=""
+           maxFileSizeBytes="-1"
+  />
 
-    <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
-               consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractProfilerBuilder"
-               errorLogFile="src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml"
-               extracts="src/test/resources/test-dirs/extractsA"
-               inputDir="src/test/resources/test-dirs/raw_input"
-               commonTokens="src/test/resources/common_tokens"/>
+  <consumers builderClass="org.apache.tika.eval.app.batch.EvalConsumersBuilder"
+             consumerBuilderClass="org.apache.tika.eval.app.batch.ExtractProfilerBuilder"
+             errorLogFile="src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml"
+             extracts="src/test/resources/test-dirs/extractsA"
+             inputDir="src/test/resources/test-dirs/raw_input"
+             commonTokens="src/test/resources/common_tokens"/>
 
 
-    <!-- reporter and interrupter are optional -->
-    <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
-              staleThresholdMillis="500000"/>
-    <interrupter builderClass="org.apache.tika.batch.builders.InterrupterBuilder"/>
+  <!-- reporter and interrupter are optional -->
+  <reporter builderClass="org.apache.tika.batch.builders.SimpleLogReporterBuilder" sleepMillis="1000"
+            staleThresholdMillis="500000"/>
+  <interrupter builderClass="org.apache.tika.batch.builders.InterrupterBuilder"/>
 </tika-batch-config>
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml b/tika-eval/tika-eval-app/src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml
index 520306b60..a688ab229 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/batch-logs/batch-process-fatal.xml
@@ -17,11 +17,14 @@
   under the License.
 -->
 
-<log4j:event logger="org.apache.tika.batch.FileResourceConsumer" timestamp="1436376775762" level="ERROR" thread="pool-2-thread-11">
-<log4j:message><![CDATA[<?xml version="1.0" ?><timed_out resourceId="file10_permahang.txt" elapsedMS="340302"></timed_out>]]></log4j:message>
+<log4j:event logger="org.apache.tika.batch.FileResourceConsumer" timestamp="1436376775762" level="ERROR"
+             thread="pool-2-thread-11">
+  <log4j:message>
+    <![CDATA[<?xml version="1.0" ?><timed_out resourceId="file10_permahang.txt" elapsedMS="340302"></timed_out>]]></log4j:message>
 </log4j:event>
 
-<log4j:event logger="org.apache.tika.batch.FileResourceConsumer" timestamp="1436376775758" level="ERROR" thread="pool-2-thread-10">
+<log4j:event logger="org.apache.tika.batch.FileResourceConsumer" timestamp="1436376775758" level="ERROR"
+             thread="pool-2-thread-10">
 <log4j:message><![CDATA[<?xml version="1.0" ?><oom resourceId="file11_oom.txt">java.lang.OutOfMemoryError: Java heap space
 	at java.io.ByteArrayOutputStream.&lt;init&gt;(ByteArrayOutputStream.java:77)
 	at org.apache.fontbox.ttf.MemoryTTFDataStream.&lt;init&gt;(MemoryTTFDataStream.java:45)
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file1.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file1.pdf.json
index 8e6ae43ff..99e4ecd57 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file1.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file1.pdf.json
@@ -1,5 +1,7 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox fox fox jumped over the lazy lazy dog 1,200 120000",
-  "xmpTPg:NPages":"2"
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox fox fox jumped over the lazy lazy dog 1,200 120000",
+    "xmpTPg:NPages": "2"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file12_es.txt.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file12_es.txt.json
index 0e2558bab..5ffd824a9 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file12_es.txt.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file12_es.txt.json
@@ -1,4 +1,6 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro"
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file13_attachANotB.doc.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file13_attachANotB.doc.json
index 048c8535c..15bc592a5 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file13_attachANotB.doc.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file13_attachANotB.doc.json
@@ -1,11 +1,12 @@
-[{
-  "Content-Type":"text/plain",
-  "_comment" : "simplified",
-  "X-TIKA:content":"调整每一个心脏和每个声音，投标每个护理提取;让大家一起欢乐，赞美老拿骚.调整每一个心脏和每个声音，投标每个护理提取;让大家一起欢乐，赞美老拿骚 狐狸狐狸狐狸 "
+[
+  {
+    "Content-Type": "text/plain",
+    "_comment": "simplified",
+    "X-TIKA:content": "调整每一个心脏和每个声音，投标每个护理提取;让大家一起欢乐，赞美老拿骚.调整每一个心脏和每个声音，投标每个护理提取;让大家一起欢乐，赞美老拿骚 狐狸狐狸狐狸 "
   },
   {
-    "Content-Type":"text/plain",
-    "X-TIKA:embedded_resource_path":"inner.txt",
-    "X-TIKA:content":"attachment contents"
+    "Content-Type": "text/plain",
+    "X-TIKA:embedded_resource_path": "inner.txt",
+    "X-TIKA:content": "attachment contents"
   }
 ]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file14_diffAttachOrder.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file14_diffAttachOrder.json
index 2888c2576..25f0db9a1 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file14_diffAttachOrder.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file14_diffAttachOrder.json
@@ -10,7 +10,6 @@
     "X-TIKA:content": "a b c d e f g h i j k l m n",
     "X-TIKA:digest:MD5": "471d98383e9f40444e5ecf821f2c8354",
     "X-TIKA:embedded_depth": "1"
-
   },
   {
     "Content-Type": "text/plain",
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file2_attachANotB.doc.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file2_attachANotB.doc.json
index 9120bb393..67f4f2cea 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file2_attachANotB.doc.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file2_attachANotB.doc.json
@@ -1,11 +1,12 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox fox fox jumped over the lazy lazy dog"
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox fox fox jumped over the lazy lazy dog"
   },
   {
-    "Content-Type":"text/plain",
-    "X-TIKA:embedded_resource_path":"inner.txt",
-    "X-TIKA:content":"attachment contents",
+    "Content-Type": "text/plain",
+    "X-TIKA:embedded_resource_path": "inner.txt",
+    "X-TIKA:content": "attachment contents",
     "X-TIKA:embedded_depth": "1"
   }
 ]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file3_attachBNotA.doc.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file3_attachBNotA.doc.json
index 18763d1ea..36a84bed7 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file3_attachBNotA.doc.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file3_attachBNotA.doc.json
@@ -1,4 +1,6 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox fox fox jumped over the lazy lazy dog"
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox fox fox jumped over the lazy lazy dog"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file4_emptyB.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file4_emptyB.pdf.json
index 18763d1ea..36a84bed7 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file4_emptyB.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file4_emptyB.pdf.json
@@ -1,4 +1,6 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox fox fox jumped over the lazy lazy dog"
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox fox fox jumped over the lazy lazy dog"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file6_accessEx.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file6_accessEx.pdf.json
index 04b321ead..d9bfad0f3 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file6_accessEx.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file6_accessEx.pdf.json
@@ -1 +1,23 @@
-[{"Content-Type":"application/pdf","X-Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"X-TIKA:EXCEPTION:container_exception":"org.apache.tika.exception.AccessPermissionException: Content extraction is not allowed.\n\tat org.apache.tika.parser.pdf.AccessChecker.check(AccessChecker.java:77)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:147)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:123)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:171)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:104)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:44)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n","access_permission:assemble_document":"false","access_permission:can_modify":"false","access_permission:can_print":"true","access_permission:can_print_degraded":"true","access_permission:extract_content":"false","access_permission:extract_for_accessibility":"true","access_permission:fill_in_form":"false","access_permission:modify_annotations":"false","pdf:encrypted":"true","resourceName":"file3_accessEx","tika:file_ext":"pdf","tika_batch_fs:relative_path":"file3_accessEx","xmpTPg:NPages":"4"}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "application/pdf",
+    "X-Parsed-By": [
+      "org.apache.tika.parser.DefaultParser",
+      "org.apache.tika.parser.pdf.PDFParser"
+    ],
+    "X-TIKA:EXCEPTION:container_exception": "org.apache.tika.exception.AccessPermissionException: Content extraction is not allowed.\n\tat org.apache.tika.parser.pdf.AccessChecker.check(AccessChecker.java:77)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:147)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:123)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:171)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:104)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:44)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n",
+    "access_permission:assemble_document": "false",
+    "access_permission:can_modify": "false",
+    "access_permission:can_print": "true",
+    "access_permission:can_print_degraded": "true",
+    "access_permission:extract_content": "false",
+    "access_permission:extract_for_accessibility": "true",
+    "access_permission:fill_in_form": "false",
+    "access_permission:modify_annotations": "false",
+    "pdf:encrypted": "true",
+    "resourceName": "file3_accessEx",
+    "tika:file_ext": "pdf",
+    "tika_batch_fs:relative_path": "file3_accessEx",
+    "xmpTPg:NPages": "4"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file7_badJson.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file7_badJson.pdf.json
index 8cf61dab5..7393ed8f2 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file7_badJson.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file7_badJson.pdf.json
@@ -1,4 +1,7 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":2,100
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": 2,
+    100
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file8_IOEx.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file8_IOEx.pdf.json
index 4ecf0e832..ccd8b88c8 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file8_IOEx.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsA/file8_IOEx.pdf.json
@@ -1 +1,22 @@
-[{"Content-Length":"479562","Content-Type":"application/pdf","X-Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"X-TIKA:EXCEPTION:runtime":"java.lang.RuntimeException: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:186)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.hasNext(PDFStreamParser.java:193)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:255)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:235)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:215)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:456)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:381)\n\tat org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:340)\n\tat org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:106)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:148)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.FileResourceConsumer.parse(FileResourceConsumer.java:410)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:106)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:182)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:115)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:49)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.cos.COSNumber.get(COSNumber.java:104)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.parseNextToken(PDFStreamParser.java:350)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.access$000(PDFStreamParser.java:46)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:181)\n\t... 24 more\n","access_permission:assemble_document":"true","access_permission:can_modify":"true","access_permission:can_print":"true","access_permission:can_print_degraded":"true","access_permission:extract_content":"true","access_permission:extract_for_accessibility":"true","access_permission:fill_in_form":"true","access_permission:modify_annotations":"true", "resourceName":"file8_IOEx.pdf","tika:file_ext":"pdf","tika_batch_fs:relative_path":"file8_IOEx.pdf"}]
\ No newline at end of file
+[
+  {
+    "Content-Length": "479562",
+    "Content-Type": "application/pdf",
+    "X-Parsed-By": [
+      "org.apache.tika.parser.DefaultParser",
+      "org.apache.tika.parser.pdf.PDFParser"
+    ],
+    "X-TIKA:EXCEPTION:runtime": "java.lang.RuntimeException: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:186)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.hasNext(PDFStreamParser.java:193)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:255)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:235)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:215)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:456)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:381)\n\tat org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:340)\n\tat org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:106)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:148)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.FileResourceConsumer.parse(FileResourceConsumer.java:410)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:106)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:182)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:115)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:49)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.cos.COSNumber.get(COSNumber.java:104)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.parseNextToken(PDFStreamParser.java:350)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.access$000(PDFStreamParser.java:46)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:181)\n\t... 24 more\n",
+    "access_permission:assemble_document": "true",
+    "access_permission:can_modify": "true",
+    "access_permission:can_print": "true",
+    "access_permission:can_print_degraded": "true",
+    "access_permission:extract_content": "true",
+    "access_permission:extract_for_accessibility": "true",
+    "access_permission:fill_in_form": "true",
+    "access_permission:modify_annotations": "true",
+    "resourceName": "file8_IOEx.pdf",
+    "tika:file_ext": "pdf",
+    "tika_batch_fs:relative_path": "file8_IOEx.pdf"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file1.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file1.pdf.json
index cbb51cfa9..ba474b2ef 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file1.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file1.pdf.json
@@ -1,2 +1,6 @@
-[{  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox jumped the lazy dog aardvark aardvark aardvark bear bear"}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox jumped the lazy dog aardvark aardvark aardvark bear bear"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file12_es.txt.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file12_es.txt.json
index 0e2558bab..5ffd824a9 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file12_es.txt.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file12_es.txt.json
@@ -1,4 +1,6 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro"
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro. El zorro marrón rápido saltó sobre el perro"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file14_diffAttachOrder.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file14_diffAttachOrder.json
index 9223769f7..76bc8a81e 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file14_diffAttachOrder.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file14_diffAttachOrder.json
@@ -10,7 +10,6 @@
     "X-TIKA:content": "o p q r s t u v w x y z",
     "X-TIKA:digest:MD5": "471d98383e9f40444e5ecf821f2c8353",
     "X-TIKA:embedded_depth": "1"
-
   },
   {
     "Content-Type": "text/plain",
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file15_tags.html b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file15_tags.html
index a08be4620..641ac07d5 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file15_tags.html
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file15_tags.html
@@ -1,31 +1,81 @@
 <html>
 <head>
-<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
-<style type="text/css">
-.txt { white-space:nowrap; }
-#f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }
+    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
+    <style type="text/css">
+        .txt { white-space:nowrap; }
+        #f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }
 
-</style>
+    </style>
 </head>
 <body>
-<img id="background" style="position:absolute; left:0px; top:0px;" width="595" height="842" src="page1.png">
-<div class="txt" style="position:absolute; left:18px; top:20px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika - Apache Tika</span></div>
-<div class="txt" style="position:absolute; left:449px; top:20px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">http://incubator.apache.org/tika/</span></div>
-<div class="txt" style="position:absolute; left:62px; top:77px;"><span id="f25" style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Tika - Content Analysis Toolkit</span></div>
-<div class="txt" style="position:absolute; left:57px; top:118px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is a toolkit for detecting and extracting metadata and structured text content</span></div>
-<div class="txt" style="position:absolute; left:57px; top:131px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">from various documents using existing parser libraries.</span></div>
-<div class="txt" style="position:absolute; left:57px; top:154px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is an effort undergoing </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">incubation </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">at </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">The Apache Software Foundation </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">(ASF),</span></div>
-<div class="txt" style="position:absolute; left:57px; top:167px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">sponsored by the </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">Apache Lucene PMC. </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Incubation is required of all newly accepted projects</span></div>
-<div class="txt" style="position:absolute; left:57px; top:180px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">until a further review indicates that the infrastructure, communications, and decision making</span></div>
-<div class="txt" style="position:absolute; left:57px; top:193px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">process have stabilized in a manner consistent with other successful ASF projects. While</span></div>
-<div class="txt" style="position:absolute; left:57px; top:206px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">incubation status is not necessarily a reflection of the completeness or stability of the code, it</span></div>
-<div class="txt" style="position:absolute; left:57px; top:219px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">does indicate that the project has yet to be fully endorsed by the ASF.</span></div>
-<div class="txt" style="position:absolute; left:57px; top:242px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">See the </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache Tika Incubation Status </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">page for the current incubation status.</span></div>
-<div class="txt" style="position:absolute; left:62px; top:289px;"><span id="f25" style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Latest News</span></div>
-<div class="txt" style="position:absolute; left:62px; top:333px;"><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(152,0,0,1);">March 22nd, 2007: Apache Tika project started</span></div>
-<div class="txt" style="position:absolute; left:92px; top:344px;"><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">The Apache Tika project was formally started when the </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">Tika proposal </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">was </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">accepted </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">by the </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache</span></div>
-<div class="txt" style="position:absolute; left:92px; top:355px;"><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Incubator PMC.</span></div>
-<div class="txt" style="position:absolute; left:18px; top:792px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">1 of 1</span></div>
-<div class="txt" style="position:absolute; left:510px; top:792px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">15.9.2007 11:02</span></div>
+<img height="842" id="background" src="page1.png" style="position:absolute; left:0px; top:0px;" width="595">
+<div class="txt" style="position:absolute; left:18px; top:20px;"><span id="f4"
+                                                                       style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika - Apache Tika</span>
+</div>
+<div class="txt" style="position:absolute; left:449px; top:20px;"><span id="f4"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">http://incubator.apache.org/tika/</span>
+</div>
+<div class="txt" style="position:absolute; left:62px; top:77px;"><span id="f25"
+                                                                       style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Tika - Content Analysis Toolkit</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:118px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is a toolkit for detecting and extracting metadata and structured text content</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:131px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">from various documents using existing parser libraries.</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:154px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is an effort undergoing </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">incubation </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">at </span><span id="f27"
+                                                                                                     style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">The Apache Software Foundation </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">(ASF),</span></div>
+<div class="txt" style="position:absolute; left:57px; top:167px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">sponsored by the </span><span
+        id="f27"
+        style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">Apache Lucene PMC. </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Incubation is required of all newly accepted projects</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:180px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">until a further review indicates that the infrastructure, communications, and decision making</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:193px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">process have stabilized in a manner consistent with other successful ASF projects. While</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:206px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">incubation status is not necessarily a reflection of the completeness or stability of the code, it</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:219px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">does indicate that the project has yet to be fully endorsed by the ASF.</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:242px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">See the </span><span
+        id="f27"
+        style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache Tika Incubation Status </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">page for the current incubation status.</span>
+</div>
+<div class="txt" style="position:absolute; left:62px; top:289px;"><span id="f25"
+                                                                        style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Latest News</span>
+</div>
+<div class="txt" style="position:absolute; left:62px; top:333px;"><span id="f27"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(152,0,0,1);">March 22nd, 2007: Apache Tika project started</span>
+</div>
+<div class="txt" style="position:absolute; left:92px; top:344px;"><span id="f27"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">The Apache Tika project was formally started when the </span><span
+        id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">Tika proposal </span><span
+        id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">was </span><span id="f27"
+                                                                                                     style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">accepted </span><span
+        id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">by the </span><span id="f27"
+                                                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache</span>
+</div>
+<div class="txt" style="position:absolute; left:92px; top:355px;"><span id="f27"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Incubator PMC.</span>
+</div>
+<div class="txt" style="position:absolute; left:18px; top:792px;"><span id="f4"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">1 of 1</span>
+</div>
+<div class="txt" style="position:absolute; left:510px; top:792px;"><span id="f4"
+                                                                         style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">15.9.2007 11:02</span>
+</div>
 </body>
 </html>
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file16_badTags.html b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file16_badTags.html
index 19ed27c0e..6131a54e4 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file16_badTags.html
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file16_badTags.html
@@ -1,31 +1,81 @@
 <html>
 <head>
-<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
-<style type="text/css">
-.txt { white-space:nowrap; }
-#f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }
+    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
+    <style type="text/css">
+        .txt { white-space:nowrap; }
+        #f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }
 
-</style>
+    </style>
 </head>
 <body>
-<img id="background" style="position:absolute; left:0px; top:0px;" width="595" height="842" src="page1.png">
-<div class="txt" style="position:absolute; left:18px; top:20px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache <i><b>bad tag</i></b>- Apache Tika</span></div>
-<div class="txt" style="position:absolute; left:449px; top:20px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">http://incubator.apache.org/tika/</span></div>
-<div class="txt" style="position:absolute; left:62px; top:77px;"><span id="f25" style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Tika - Content Analysis Toolkit</span></div>
-<div class="txt" style="position:absolute; left:57px; top:118px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is a toolkit for detecting and extracting metadata and structured text content</span></div>
-<div class="txt" style="position:absolute; left:57px; top:131px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">from various documents using existing parser libraries.</span></div>
-<div class="txt" style="position:absolute; left:57px; top:154px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is an effort undergoing </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">incubation </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">at </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">The Apache Software Foundation </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">(ASF),</span></div>
-<div class="txt" style="position:absolute; left:57px; top:167px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">sponsored by the </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">Apache Lucene PMC. </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Incubation is required of all newly accepted projects</span></div>
-<div class="txt" style="position:absolute; left:57px; top:180px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">until a further review indicates that the infrastructure, communications, and decision making</span></div>
-<div class="txt" style="position:absolute; left:57px; top:193px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">process have stabilized in a manner consistent with other successful ASF projects. While</span></div>
-<div class="txt" style="position:absolute; left:57px; top:206px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">incubation status is not necessarily a reflection of the completeness or stability of the code, it</span></div>
-<div class="txt" style="position:absolute; left:57px; top:219px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">does indicate that the project has yet to be fully endorsed by the ASF.</span></div>
-<div class="txt" style="position:absolute; left:57px; top:242px;"><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">See the </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache Tika Incubation Status </span><span id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">page for the current incubation status.</span></div>
-<div class="txt" style="position:absolute; left:62px; top:289px;"><span id="f25" style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Latest News</span></div>
-<div class="txt" style="position:absolute; left:62px; top:333px;"><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(152,0,0,1);">March 22nd, 2007: Apache Tika project started</span></div>
-<div class="txt" style="position:absolute; left:92px; top:344px;"><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">The Apache Tika project was formally started when the </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">Tika proposal </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">was </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">accepted </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">by the </span><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache</span></div>
-<div class="txt" style="position:absolute; left:92px; top:355px;"><span id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Incubator PMC.</span></div>
-<div class="txt" style="position:absolute; left:18px; top:792px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">1 of 1</span></div>
-<div class="txt" style="position:absolute; left:510px; top:792px;"><span id="f4" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">15.9.2007 11:02</span></div>
+<img height="842" id="background" src="page1.png" style="position:absolute; left:0px; top:0px;" width="595">
+<div class="txt" style="position:absolute; left:18px; top:20px;"><span id="f4"
+                                                                       style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache <i><b>bad tag</i></b>
+    - Apache Tika</span></div>
+<div class="txt" style="position:absolute; left:449px; top:20px;"><span id="f4"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">http://incubator.apache.org/tika/</span>
+</div>
+<div class="txt" style="position:absolute; left:62px; top:77px;"><span id="f25"
+                                                                       style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Tika - Content Analysis Toolkit</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:118px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is a toolkit for detecting and extracting metadata and structured text content</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:131px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">from various documents using existing parser libraries.</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:154px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Apache Tika is an effort undergoing </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">incubation </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">at </span><span id="f27"
+                                                                                                     style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">The Apache Software Foundation </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">(ASF),</span></div>
+<div class="txt" style="position:absolute; left:57px; top:167px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">sponsored by the </span><span
+        id="f27"
+        style="font-size:10px;vertical-align:baseline;color:rgba(50,101,169,1);">Apache Lucene PMC. </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">Incubation is required of all newly accepted projects</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:180px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">until a further review indicates that the infrastructure, communications, and decision making</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:193px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">process have stabilized in a manner consistent with other successful ASF projects. While</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:206px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">incubation status is not necessarily a reflection of the completeness or stability of the code, it</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:219px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">does indicate that the project has yet to be fully endorsed by the ASF.</span>
+</div>
+<div class="txt" style="position:absolute; left:57px; top:242px;"><span id="f27"
+                                                                        style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">See the </span><span
+        id="f27"
+        style="font-size:10px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache Tika Incubation Status </span><span
+        id="f27" style="font-size:10px;vertical-align:baseline;color:rgba(0,0,0,1);">page for the current incubation status.</span>
+</div>
+<div class="txt" style="position:absolute; left:62px; top:289px;"><span id="f25"
+                                                                        style="font-size:18px;vertical-align:baseline;color:rgba(152,0,0,1);">Latest News</span>
+</div>
+<div class="txt" style="position:absolute; left:62px; top:333px;"><span id="f27"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(152,0,0,1);">March 22nd, 2007: Apache Tika project started</span>
+</div>
+<div class="txt" style="position:absolute; left:92px; top:344px;"><span id="f27"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">The Apache Tika project was formally started when the </span><span
+        id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">Tika proposal </span><span
+        id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">was </span><span id="f27"
+                                                                                                     style="font-size:9px;vertical-align:baseline;color:rgba(50,101,169,1);">accepted </span><span
+        id="f27" style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">by the </span><span id="f27"
+                                                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Apache</span>
+</div>
+<div class="txt" style="position:absolute; left:92px; top:355px;"><span id="f27"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(60,106,152,1);">Incubator PMC.</span>
+</div>
+<div class="txt" style="position:absolute; left:18px; top:792px;"><span id="f4"
+                                                                        style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">1 of 1</span>
+</div>
+<div class="txt" style="position:absolute; left:510px; top:792px;"><span id="f4"
+                                                                         style="font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);">15.9.2007 11:02</span>
+</div>
 </body>
 </html>
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file2_attachANotB.doc.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file2_attachANotB.doc.json
index 18763d1ea..36a84bed7 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file2_attachANotB.doc.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file2_attachANotB.doc.json
@@ -1,4 +1,6 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox fox fox jumped over the lazy lazy dog"
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox fox fox jumped over the lazy lazy dog"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file3_attachBNotA.doc.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file3_attachBNotA.doc.json
index 9120bb393..67f4f2cea 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file3_attachBNotA.doc.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file3_attachBNotA.doc.json
@@ -1,11 +1,12 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox fox fox jumped over the lazy lazy dog"
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox fox fox jumped over the lazy lazy dog"
   },
   {
-    "Content-Type":"text/plain",
-    "X-TIKA:embedded_resource_path":"inner.txt",
-    "X-TIKA:content":"attachment contents",
+    "Content-Type": "text/plain",
+    "X-TIKA:embedded_resource_path": "inner.txt",
+    "X-TIKA:content": "attachment contents",
     "X-TIKA:embedded_depth": "1"
   }
 ]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file5_emptyA.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file5_emptyA.pdf.json
index 18763d1ea..36a84bed7 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file5_emptyA.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file5_emptyA.pdf.json
@@ -1,4 +1,6 @@
-[{
-  "Content-Type":"text/plain",
-  "X-TIKA:content":"the quick brown fox fox fox jumped over the lazy lazy dog"
-}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "text/plain",
+    "X-TIKA:content": "the quick brown fox fox fox jumped over the lazy lazy dog"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file6_accessEx.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file6_accessEx.pdf.json
index 04b321ead..d9bfad0f3 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file6_accessEx.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file6_accessEx.pdf.json
@@ -1 +1,23 @@
-[{"Content-Type":"application/pdf","X-Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"X-TIKA:EXCEPTION:container_exception":"org.apache.tika.exception.AccessPermissionException: Content extraction is not allowed.\n\tat org.apache.tika.parser.pdf.AccessChecker.check(AccessChecker.java:77)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:147)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:123)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:171)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:104)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:44)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n","access_permission:assemble_document":"false","access_permission:can_modify":"false","access_permission:can_print":"true","access_permission:can_print_degraded":"true","access_permission:extract_content":"false","access_permission:extract_for_accessibility":"true","access_permission:fill_in_form":"false","access_permission:modify_annotations":"false","pdf:encrypted":"true","resourceName":"file3_accessEx","tika:file_ext":"pdf","tika_batch_fs:relative_path":"file3_accessEx","xmpTPg:NPages":"4"}]
\ No newline at end of file
+[
+  {
+    "Content-Type": "application/pdf",
+    "X-Parsed-By": [
+      "org.apache.tika.parser.DefaultParser",
+      "org.apache.tika.parser.pdf.PDFParser"
+    ],
+    "X-TIKA:EXCEPTION:container_exception": "org.apache.tika.exception.AccessPermissionException: Content extraction is not allowed.\n\tat org.apache.tika.parser.pdf.AccessChecker.check(AccessChecker.java:77)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:147)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:270)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:123)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:171)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:104)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:44)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n",
+    "access_permission:assemble_document": "false",
+    "access_permission:can_modify": "false",
+    "access_permission:can_print": "true",
+    "access_permission:can_print_degraded": "true",
+    "access_permission:extract_content": "false",
+    "access_permission:extract_for_accessibility": "true",
+    "access_permission:fill_in_form": "false",
+    "access_permission:modify_annotations": "false",
+    "pdf:encrypted": "true",
+    "resourceName": "file3_accessEx",
+    "tika:file_ext": "pdf",
+    "tika_batch_fs:relative_path": "file3_accessEx",
+    "xmpTPg:NPages": "4"
+  }
+]
\ No newline at end of file
diff --git a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file8_IOEx.pdf.json b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file8_IOEx.pdf.json
index 4ecf0e832..ccd8b88c8 100644
--- a/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file8_IOEx.pdf.json
+++ b/tika-eval/tika-eval-app/src/test/resources/test-dirs/extractsB/file8_IOEx.pdf.json
@@ -1 +1,22 @@
-[{"Content-Length":"479562","Content-Type":"application/pdf","X-Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"X-TIKA:EXCEPTION:runtime":"java.lang.RuntimeException: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:186)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.hasNext(PDFStreamParser.java:193)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:255)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:235)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:215)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:456)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:381)\n\tat org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:340)\n\tat org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:106)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:148)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.FileResourceConsumer.parse(FileResourceConsumer.java:410)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:106)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:182)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:115)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:49)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.cos.COSNumber.get(COSNumber.java:104)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.parseNextToken(PDFStreamParser.java:350)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.access$000(PDFStreamParser.java:46)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:181)\n\t... 24 more\n","access_permission:assemble_document":"true","access_permission:can_modify":"true","access_permission:can_print":"true","access_permission:can_print_degraded":"true","access_permission:extract_content":"true","access_permission:extract_for_accessibility":"true","access_permission:fill_in_form":"true","access_permission:modify_annotations":"true", "resourceName":"file8_IOEx.pdf","tika:file_ext":"pdf","tika_batch_fs:relative_path":"file8_IOEx.pdf"}]
\ No newline at end of file
+[
+  {
+    "Content-Length": "479562",
+    "Content-Type": "application/pdf",
+    "X-Parsed-By": [
+      "org.apache.tika.parser.DefaultParser",
+      "org.apache.tika.parser.pdf.PDFParser"
+    ],
+    "X-TIKA:EXCEPTION:runtime": "java.lang.RuntimeException: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:186)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.hasNext(PDFStreamParser.java:193)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:255)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:235)\n\tat org.apache.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:215)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:456)\n\tat org.apache.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:381)\n\tat org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:340)\n\tat org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:106)\n\tat org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:148)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)\n\tat org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\n\tat org.apache.tika.parser.RecursiveParserWrapper.parse(RecursiveParserWrapper.java:130)\n\tat org.apache.tika.batch.FileResourceConsumer.parse(FileResourceConsumer.java:410)\n\tat org.apache.tika.batch.fs.RecursiveParserWrapperFSConsumer.processFileResource(RecursiveParserWrapperFSConsumer.java:106)\n\tat org.apache.tika.batch.FileResourceConsumer._processFileResource(FileResourceConsumer.java:182)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:115)\n\tat org.apache.tika.batch.FileResourceConsumer.call(FileResourceConsumer.java:49)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Value is not an integer: 8546736428538085463808\n\tat org.apache.pdfbox.cos.COSNumber.get(COSNumber.java:104)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.parseNextToken(PDFStreamParser.java:350)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser.access$000(PDFStreamParser.java:46)\n\tat org.apache.pdfbox.pdfparser.PDFStreamParser$1.tryNext(PDFStreamParser.java:181)\n\t... 24 more\n",
+    "access_permission:assemble_document": "true",
+    "access_permission:can_modify": "true",
+    "access_permission:can_print": "true",
+    "access_permission:can_print_degraded": "true",
+    "access_permission:extract_content": "true",
+    "access_permission:extract_for_accessibility": "true",
+    "access_permission:fill_in_form": "true",
+    "access_permission:modify_annotations": "true",
+    "resourceName": "file8_IOEx.pdf",
+    "tika:file_ext": "pdf",
+    "tika_batch_fs:relative_path": "file8_IOEx.pdf"
+  }
+]
\ No newline at end of file
diff --git a/tika-example/src/main/java/org/apache/tika/example/ContentHandlerExample.java b/tika-example/src/main/java/org/apache/tika/example/ContentHandlerExample.java
index 8cb1f8918..af95e5e95 100644
--- a/tika-example/src/main/java/org/apache/tika/example/ContentHandlerExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/ContentHandlerExample.java
@@ -94,10 +94,8 @@ public class ContentHandlerExample {
     public String parseOnePartToHTML() throws IOException, SAXException, TikaException {
         // Only get things under html -> body -> div (class=header)
         XPathParser xhtmlParser = new XPathParser("xhtml", XHTMLContentHandler.XHTML);
-        Matcher divContentMatcher =
-                xhtmlParser.parse("/xhtml:html/xhtml:body/xhtml:div/descendant::node()");
-        ContentHandler handler =
-                new MatchingContentHandler(new ToXMLContentHandler(), divContentMatcher);
+        Matcher divContentMatcher = xhtmlParser.parse("/xhtml:html/xhtml:body/xhtml:div/descendant::node()");
+        ContentHandler handler = new MatchingContentHandler(new ToXMLContentHandler(), divContentMatcher);
 
         AutoDetectParser parser = new AutoDetectParser();
         Metadata metadata = new Metadata();
diff --git a/tika-example/src/main/java/org/apache/tika/example/CustomMimeInfo.java b/tika-example/src/main/java/org/apache/tika/example/CustomMimeInfo.java
index e83243767..c286353ec 100755
--- a/tika-example/src/main/java/org/apache/tika/example/CustomMimeInfo.java
+++ b/tika-example/src/main/java/org/apache/tika/example/CustomMimeInfo.java
@@ -35,8 +35,7 @@ public class CustomMimeInfo {
     public static String customCompositeDetector() throws Exception {
         String path = "file:///path/to/prescription-type.xml";
         MimeTypes typeDatabase = MimeTypesFactory.create(new URL(path));
-        Tika tika =
-                new Tika(new CompositeDetector(typeDatabase, new EncryptedPrescriptionDetector()));
+        Tika tika = new Tika(new CompositeDetector(typeDatabase, new EncryptedPrescriptionDetector()));
         return tika.detect("/path/to/tmp/prescription.xpd");
     }
 
diff --git a/tika-example/src/main/java/org/apache/tika/example/DirListParser.java b/tika-example/src/main/java/org/apache/tika/example/DirListParser.java
index 897804a13..2d92763ea 100755
--- a/tika-example/src/main/java/org/apache/tika/example/DirListParser.java
+++ b/tika-example/src/main/java/org/apache/tika/example/DirListParser.java
@@ -46,8 +46,7 @@ public class DirListParser implements Parser {
 
     private static final long serialVersionUID = 2717930544410610735L;
 
-    private static Set<MediaType> SUPPORTED_TYPES =
-            new HashSet<>(Collections.singletonList(MediaType.TEXT_PLAIN));
+    private static Set<MediaType> SUPPORTED_TYPES = new HashSet<>(Collections.singletonList(MediaType.TEXT_PLAIN));
 
     public static void main(String[] args) throws IOException, SAXException, TikaException {
         DirListParser parser = new DirListParser();
@@ -74,8 +73,7 @@ public class DirListParser implements Parser {
      * @see org.apache.tika.parser.Parser#parse(java.io.InputStream,
      * org.xml.sax.ContentHandler, org.apache.tika.metadata.Metadata)
      */
-    public void parse(InputStream is, ContentHandler handler, Metadata metadata)
-            throws IOException, SAXException, TikaException {
+    public void parse(InputStream is, ContentHandler handler, Metadata metadata) throws IOException, SAXException, TikaException {
         this.parse(is, handler, metadata, new ParseContext());
     }
 
@@ -86,10 +84,11 @@ public class DirListParser implements Parser {
      * org.xml.sax.ContentHandler, org.apache.tika.metadata.Metadata,
      * org.apache.tika.parser.ParseContext)
      */
-    public void parse(InputStream is, ContentHandler handler, Metadata metadata,
-                      ParseContext context) throws IOException, SAXException, TikaException {
+    public void parse(InputStream is, ContentHandler handler, Metadata metadata, ParseContext context) throws IOException, SAXException, TikaException {
 
-        List<String> lines = FileUtils.readLines(TikaInputStream.get(is).getFile(), UTF_8);
+        List<String> lines = FileUtils.readLines(TikaInputStream
+                .get(is)
+                .getFile(), UTF_8);
         for (String line : lines) {
             String[] fileToks = line.split("\\s+");
             if (fileToks.length < 8) {
@@ -112,14 +111,12 @@ public class DirListParser implements Parser {
                 fileName.append(" ");
             }
             fileName.deleteCharAt(fileName.length() - 1);
-            this.addMetadata(metadata, filePermissions, numHardLinks, fileOwner, fileOwnerGroup,
-                    fileSize, lastModDate.toString(), fileName.toString());
+            this.addMetadata(metadata, filePermissions, numHardLinks, fileOwner, fileOwnerGroup, fileSize, lastModDate.toString(), fileName.toString());
         }
     }
 
-    private void addMetadata(Metadata metadata, String filePerms, String numHardLinks,
-                             String fileOwner, String fileOwnerGroup, String fileSize,
-                             String lastModDate, String fileName) {
+    private void addMetadata(Metadata metadata, String filePerms, String numHardLinks, String fileOwner, String fileOwnerGroup, String fileSize, String lastModDate,
+                             String fileName) {
         metadata.add("FilePermissions", filePerms);
         metadata.add("NumHardLinks", numHardLinks);
         metadata.add("FileOwner", fileOwner);
diff --git a/tika-example/src/main/java/org/apache/tika/example/DumpTikaConfigExample.java b/tika-example/src/main/java/org/apache/tika/example/DumpTikaConfigExample.java
index c16ef6881..f960a0978 100644
--- a/tika-example/src/main/java/org/apache/tika/example/DumpTikaConfigExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/DumpTikaConfigExample.java
@@ -60,8 +60,7 @@ public class DumpTikaConfigExample {
                     mode = TikaConfigSerializer.Mode.STATIC;
                 } else {
                     System.out.println("Use:");
-                    System.out.println(
-                            "  DumpTikaConfig [--dump-minimal] [--dump-current] [--dump-static] [filename] [encoding]");
+                    System.out.println("  DumpTikaConfig [--dump-minimal] [--dump-current] [--dump-static] [filename] [encoding]");
                     System.out.println("");
                     System.out.println("--dump-minimal    Produce the minimal config file");
                     System.out.println("--dump-current    The current (with defaults) config file");
diff --git a/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionDetector.java b/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionDetector.java
index 19f94efb1..ed3cfc508 100755
--- a/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionDetector.java
+++ b/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionDetector.java
@@ -45,8 +45,7 @@ public class EncryptedPrescriptionDetector implements Detector {
             InputStream decrypted = new CipherInputStream(lookahead, cipher);
 
             QName name = new XmlRootExtractor().extractRootElement(decrypted);
-            if (name != null && "http://example.com/xpd".equals(name.getNamespaceURI()) &&
-                    "prescription".equals(name.getLocalPart())) {
+            if (name != null && "http://example.com/xpd".equals(name.getNamespaceURI()) && "prescription".equals(name.getLocalPart())) {
                 type = MediaType.application("x-prescription");
             }
         } catch (GeneralSecurityException e) {
diff --git a/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionParser.java b/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionParser.java
index 0459160e6..28ef460ac 100755
--- a/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionParser.java
+++ b/tika-example/src/main/java/org/apache/tika/example/EncryptedPrescriptionParser.java
@@ -38,8 +38,7 @@ import org.apache.tika.parser.Parser;
 public class EncryptedPrescriptionParser implements Parser {
     private static final long serialVersionUID = -7816987249611278541L;
 
-    public void parse(InputStream stream, ContentHandler handler, Metadata metadata,
-                      ParseContext context) throws IOException, SAXException, TikaException {
+    public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context) throws IOException, SAXException, TikaException {
         try {
             Key key = Pharmacy.getKey();
             Cipher cipher = Cipher.getInstance("RSA");
diff --git a/tika-example/src/main/java/org/apache/tika/example/ExtractEmbeddedFiles.java b/tika-example/src/main/java/org/apache/tika/example/ExtractEmbeddedFiles.java
index 091facc21..9f1425da8 100644
--- a/tika-example/src/main/java/org/apache/tika/example/ExtractEmbeddedFiles.java
+++ b/tika-example/src/main/java/org/apache/tika/example/ExtractEmbeddedFiles.java
@@ -46,8 +46,7 @@ public class ExtractEmbeddedFiles {
     private Detector detector = ((AutoDetectParser) parser).getDetector();
     private TikaConfig config = TikaConfig.getDefaultConfig();
 
-    public void extract(InputStream is, Path outputDir)
-            throws SAXException, TikaException, IOException {
+    public void extract(InputStream is, Path outputDir) throws SAXException, TikaException, IOException {
         Metadata m = new Metadata();
         ParseContext c = new ParseContext();
         ContentHandler h = new BodyContentHandler(-1);
@@ -74,8 +73,7 @@ public class ExtractEmbeddedFiles {
         }
 
         @Override
-        public void parseEmbedded(InputStream stream, ContentHandler handler, Metadata metadata,
-                                  boolean outputHtml) throws SAXException, IOException {
+        public void parseEmbedded(InputStream stream, ContentHandler handler, Metadata metadata, boolean outputHtml) throws SAXException, IOException {
 
             //try to get the name of the embedded file from the metadata
             String name = metadata.get(TikaCoreProperties.RESOURCE_NAME_KEY);
@@ -99,7 +97,9 @@ public class ExtractEmbeddedFiles {
 
             if (name.indexOf('.') == -1 && contentType != null) {
                 try {
-                    name += config.getMimeRepository().forName(contentType.toString())
+                    name += config
+                            .getMimeRepository()
+                            .forName(contentType.toString())
                             .getExtension();
                 } catch (MimeTypeException e) {
                     e.printStackTrace();
@@ -108,7 +108,9 @@ public class ExtractEmbeddedFiles {
 
             Path outputFile = outputDir.resolve(name);
             if (Files.exists(outputFile)) {
-                outputFile = outputDir.resolve(UUID.randomUUID().toString() + "-" + name);
+                outputFile = outputDir.resolve(UUID
+                        .randomUUID()
+                        .toString() + "-" + name);
             }
             Files.createDirectories(outputFile.getParent());
             Files.copy(stream, outputFile);
diff --git a/tika-example/src/main/java/org/apache/tika/example/GrabPhoneNumbersExample.java b/tika-example/src/main/java/org/apache/tika/example/GrabPhoneNumbersExample.java
index 5c3e7c695..b38928298 100644
--- a/tika-example/src/main/java/org/apache/tika/example/GrabPhoneNumbersExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/GrabPhoneNumbersExample.java
@@ -66,8 +66,7 @@ public class GrabPhoneNumbersExample {
         try {
             Files.walkFileTree(folder, new SimpleFileVisitor<Path>() {
                 @Override
-                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs)
-                        throws IOException {
+                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
                     try {
                         process(file);
                         successfulFiles++;
@@ -79,8 +78,7 @@ public class GrabPhoneNumbersExample {
                 }
 
                 @Override
-                public FileVisitResult visitFileFailed(Path file, IOException exc)
-                        throws IOException {
+                public FileVisitResult visitFileFailed(Path file, IOException exc) throws IOException {
                     failedFiles++;
                     return FileVisitResult.CONTINUE;
                 }
@@ -95,8 +93,7 @@ public class GrabPhoneNumbersExample {
         Metadata metadata = new Metadata();
         // The PhoneExtractingContentHandler will examine any characters for phone numbers before passing them
         // to the underlying Handler.
-        PhoneExtractingContentHandler handler =
-                new PhoneExtractingContentHandler(new BodyContentHandler(), metadata);
+        PhoneExtractingContentHandler handler = new PhoneExtractingContentHandler(new BodyContentHandler(), metadata);
         try (InputStream stream = new BufferedInputStream(Files.newInputStream(path))) {
             parser.parse(stream, handler, metadata, new ParseContext());
         }
diff --git a/tika-example/src/main/java/org/apache/tika/example/ImportContextImpl.java b/tika-example/src/main/java/org/apache/tika/example/ImportContextImpl.java
index 3f79ef96c..97daa93a2 100755
--- a/tika-example/src/main/java/org/apache/tika/example/ImportContextImpl.java
+++ b/tika-example/src/main/java/org/apache/tika/example/ImportContextImpl.java
@@ -68,8 +68,7 @@ public class ImportContextImpl implements ImportContext {
      * @throws IOException
      * @see ImportContext#informCompleted(boolean)
      */
-    public ImportContextImpl(Item importRoot, String systemId, InputContext ctx, InputStream stream,
-                             IOListener ioListener, Detector detector) throws IOException {
+    public ImportContextImpl(Item importRoot, String systemId, InputContext ctx, InputStream stream, IOListener ioListener, Detector detector) throws IOException {
         this.importRoot = importRoot;
         this.systemId = systemId;
         this.inputCtx = ctx;
@@ -165,8 +164,7 @@ public class ImportContextImpl implements ImportContext {
             length = inputFile.length();
         }
         if (length < 0) {
-            LOG.debug("Unable to determine content length -> default value = {}",
-                    IOUtil.UNDEFINED_LENGTH);
+            LOG.debug("Unable to determine content length -> default value = {}", IOUtil.UNDEFINED_LENGTH);
         }
         return length;
     }
diff --git a/tika-example/src/main/java/org/apache/tika/example/InterruptableParsingExample.java b/tika-example/src/main/java/org/apache/tika/example/InterruptableParsingExample.java
index 652ddc8b2..f8e4bdbe8 100644
--- a/tika-example/src/main/java/org/apache/tika/example/InterruptableParsingExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/InterruptableParsingExample.java
@@ -53,7 +53,9 @@ public class InterruptableParsingExample {
         context.set(Parser.class, tika.getParser());
 
         try (InputStream is = new BufferedInputStream(Files.newInputStream(path))) {
-            tika.getParser().parse(is, handler, metadata, context);
+            tika
+                    .getParser()
+                    .parse(is, handler, metadata, context);
         } catch (QueryMatchedException e) {
             return true;
         } catch (SAXException | TikaException | IOException e) {
@@ -83,7 +85,9 @@ public class InterruptableParsingExample {
         public void characters(char[] ch, int start, int length) throws SAXException {
             sb.append(new String(ch, start, length).toLowerCase(Locale.getDefault()));
 
-            if (sb.toString().contains(query)) {
+            if (sb
+                    .toString()
+                    .contains(query)) {
                 throw new QueryMatchedException();
             }
 
diff --git a/tika-example/src/main/java/org/apache/tika/example/Language.java b/tika-example/src/main/java/org/apache/tika/example/Language.java
index f8081a8b1..c1ac26bd0 100755
--- a/tika-example/src/main/java/org/apache/tika/example/Language.java
+++ b/tika-example/src/main/java/org/apache/tika/example/Language.java
@@ -31,8 +31,7 @@ import org.apache.tika.parser.ParseContext;
 public class Language {
     public static void languageDetection() throws IOException {
         LanguageDetector detector = new OptimaizeLangDetector().loadModels();
-        LanguageResult result =
-                detector.detect("Alla människor är födda fria och lika i värde och rättigheter.");
+        LanguageResult result = detector.detect("Alla människor är födda fria och lika i värde och rättigheter.");
 
         System.out.println(result.getLanguage());
     }
diff --git a/tika-example/src/main/java/org/apache/tika/example/LanguageDetectingParser.java b/tika-example/src/main/java/org/apache/tika/example/LanguageDetectingParser.java
index 501a31727..7f43f134c 100755
--- a/tika-example/src/main/java/org/apache/tika/example/LanguageDetectingParser.java
+++ b/tika-example/src/main/java/org/apache/tika/example/LanguageDetectingParser.java
@@ -35,8 +35,7 @@ import org.apache.tika.sax.TeeContentHandler;
 public class LanguageDetectingParser extends DelegatingParser {
     private static final long serialVersionUID = 4291320409396502774L;
 
-    public void parse(InputStream stream, ContentHandler handler, final Metadata metadata,
-                      ParseContext context) throws SAXException, IOException, TikaException {
+    public void parse(InputStream stream, ContentHandler handler, final Metadata metadata, ParseContext context) throws SAXException, IOException, TikaException {
         LanguageHandler langHandler = new LanguageHandler();
         ContentHandler tee = new TeeContentHandler(handler, langHandler);
 
diff --git a/tika-example/src/main/java/org/apache/tika/example/LuceneIndexerExtended.java b/tika-example/src/main/java/org/apache/tika/example/LuceneIndexerExtended.java
index a2870dde6..6acdbd3b5 100755
--- a/tika-example/src/main/java/org/apache/tika/example/LuceneIndexerExtended.java
+++ b/tika-example/src/main/java/org/apache/tika/example/LuceneIndexerExtended.java
@@ -44,8 +44,7 @@ public class LuceneIndexerExtended {
 
     public static void main(String[] args) throws Exception {
         IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer());
-        try (IndexWriter writer = new IndexWriter(FSDirectory.open(Paths.get(args[0])),
-                indexWriterConfig)) {
+        try (IndexWriter writer = new IndexWriter(FSDirectory.open(Paths.get(args[0])), indexWriterConfig)) {
             LuceneIndexer indexer = new LuceneIndexer(new Tika(), writer);
             for (int i = 1; i < args.length; i++) {
                 indexer.indexDocument(new File(args[i]));
diff --git a/tika-example/src/main/java/org/apache/tika/example/MetadataAwareLuceneIndexer.java b/tika-example/src/main/java/org/apache/tika/example/MetadataAwareLuceneIndexer.java
index 525327698..c474da542 100755
--- a/tika-example/src/main/java/org/apache/tika/example/MetadataAwareLuceneIndexer.java
+++ b/tika-example/src/main/java/org/apache/tika/example/MetadataAwareLuceneIndexer.java
@@ -68,13 +68,14 @@ public class MetadataAwareLuceneIndexer {
         met.add(TikaCoreProperties.CREATOR, "Tika in Action");
         met.set(TikaCoreProperties.CREATED, new Date());
         met.set(TikaCoreProperties.FORMAT, tika.detect(file));
-        met.set(DublinCore.SOURCE, file.toURI().toURL().toString());
+        met.set(DublinCore.SOURCE, file
+                .toURI()
+                .toURL()
+                .toString());
         met.add(TikaCoreProperties.SUBJECT, "File");
         met.add(TikaCoreProperties.SUBJECT, "Indexing");
         met.add(TikaCoreProperties.SUBJECT, "Metadata");
-        met.set(Property
-                        .externalClosedChoise(TikaCoreProperties.RIGHTS.getName(), "public", "private"),
-                "public");
+        met.set(Property.externalClosedChoise(TikaCoreProperties.RIGHTS.getName(), "public", "private"), "public");
         try (InputStream is = new FileInputStream(file)) {
             tika.parse(is, met);
             Document document = new Document();
diff --git a/tika-example/src/main/java/org/apache/tika/example/MyFirstTika.java b/tika-example/src/main/java/org/apache/tika/example/MyFirstTika.java
index 6a7e94d34..fff96bc19 100755
--- a/tika-example/src/main/java/org/apache/tika/example/MyFirstTika.java
+++ b/tika-example/src/main/java/org/apache/tika/example/MyFirstTika.java
@@ -69,8 +69,7 @@ public class MyFirstTika {
         System.out.println(text);
     }
 
-    public static String parseUsingAutoDetect(String filename, TikaConfig tikaConfig,
-                                              Metadata metadata) throws Exception {
+    public static String parseUsingAutoDetect(String filename, TikaConfig tikaConfig, Metadata metadata) throws Exception {
         System.out.println("Handling using AutoDetectParser: [" + filename + "]");
 
         AutoDetectParser parser = new AutoDetectParser(tikaConfig);
@@ -80,30 +79,23 @@ public class MyFirstTika {
         return handler.toString();
     }
 
-    public static String parseUsingComponents(String filename, TikaConfig tikaConfig,
-                                              Metadata metadata) throws Exception {
+    public static String parseUsingComponents(String filename, TikaConfig tikaConfig, Metadata metadata) throws Exception {
         MimeTypes mimeRegistry = tikaConfig.getMimeRepository();
 
         System.out.println("Examining: [" + filename + "]");
 
         metadata.set(TikaCoreProperties.RESOURCE_NAME_KEY, filename);
-        System.out.println(
-                "The MIME type (based on filename) is: [" + mimeRegistry.detect(null, metadata) +
-                        "]");
+        System.out.println("The MIME type (based on filename) is: [" + mimeRegistry.detect(null, metadata) + "]");
 
         InputStream stream = TikaInputStream.get(new File(filename));
-        System.out.println(
-                "The MIME type (based on MAGIC) is: [" + mimeRegistry.detect(stream, metadata) +
-                        "]");
+        System.out.println("The MIME type (based on MAGIC) is: [" + mimeRegistry.detect(stream, metadata) + "]");
 
         stream = TikaInputStream.get(new File(filename));
         Detector detector = tikaConfig.getDetector();
-        System.out.println("The MIME type (based on the Detector interface) is: [" +
-                detector.detect(stream, metadata) + "]");
+        System.out.println("The MIME type (based on the Detector interface) is: [" + detector.detect(stream, metadata) + "]");
 
         LanguageDetector langDetector = new OptimaizeLangDetector().loadModels();
-        LanguageResult lang =
-                langDetector.detect(FileUtils.readFileToString(new File(filename), UTF_8));
+        LanguageResult lang = langDetector.detect(FileUtils.readFileToString(new File(filename), UTF_8));
 
         System.out.println("The language of this content is: [" + lang.getLanguage() + "]");
 
diff --git a/tika-example/src/main/java/org/apache/tika/example/ParsingExample.java b/tika-example/src/main/java/org/apache/tika/example/ParsingExample.java
index fed993a8c..065f89908 100644
--- a/tika-example/src/main/java/org/apache/tika/example/ParsingExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/ParsingExample.java
@@ -33,7 +33,6 @@ import org.apache.tika.exception.TikaException;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
 import org.apache.tika.parser.AutoDetectParser;
 import org.apache.tika.parser.EmptyParser;
 import org.apache.tika.parser.ParseContext;
@@ -43,6 +42,7 @@ import org.apache.tika.sax.BasicContentHandlerFactory;
 import org.apache.tika.sax.BodyContentHandler;
 import org.apache.tika.sax.ContentHandlerFactory;
 import org.apache.tika.sax.RecursiveParserWrapperHandler;
+import org.apache.tika.serialization.JsonMetadataList;
 
 public class ParsingExample {
 
@@ -109,8 +109,7 @@ public class ParsingExample {
         Metadata metadata = new Metadata();
         ParseContext parseContext = new ParseContext();
         parseContext.set(Parser.class, new EmptyParser());
-        try (InputStream stream = ParsingExample.class
-                .getResourceAsStream("test_recursive_embedded.docx")) {
+        try (InputStream stream = ParsingExample.class.getResourceAsStream("test_recursive_embedded.docx")) {
             parser.parse(stream, handler, metadata, parseContext);
             return handler.toString();
         }
@@ -132,8 +131,7 @@ public class ParsingExample {
         Metadata metadata = new Metadata();
         ParseContext context = new ParseContext();
         context.set(Parser.class, parser);
-        try (InputStream stream = ParsingExample.class
-                .getResourceAsStream("test_recursive_embedded.docx")) {
+        try (InputStream stream = ParsingExample.class.getResourceAsStream("test_recursive_embedded.docx")) {
             parser.parse(stream, handler, metadata, context);
             return handler.toString();
         }
@@ -161,19 +159,16 @@ public class ParsingExample {
      * @throws SAXException
      * @throws TikaException
      */
-    public List<Metadata> recursiveParserWrapperExample()
-            throws IOException, SAXException, TikaException {
+    public List<Metadata> recursiveParserWrapperExample() throws IOException, SAXException, TikaException {
         Parser p = new AutoDetectParser();
-        ContentHandlerFactory factory =
-                new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.HTML, -1);
+        ContentHandlerFactory factory = new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.HTML, -1);
 
         RecursiveParserWrapper wrapper = new RecursiveParserWrapper(p);
         Metadata metadata = new Metadata();
         metadata.set(TikaCoreProperties.RESOURCE_NAME_KEY, "test_recursive_embedded.docx");
         ParseContext context = new ParseContext();
         RecursiveParserWrapperHandler handler = new RecursiveParserWrapperHandler(factory, -1);
-        try (InputStream stream = ParsingExample.class
-                .getResourceAsStream("test_recursive_embedded.docx")) {
+        try (InputStream stream = ParsingExample.class.getResourceAsStream("test_recursive_embedded.docx")) {
             wrapper.parse(stream, handler, metadata, context);
         }
 
@@ -182,7 +177,7 @@ public class ParsingExample {
 
     /**
      * We include a simple JSON serializer for a list of metadata with
-     * {@link org.apache.tika.metadata.serialization.JsonMetadataList}.
+     * {@link JsonMetadataList}.
      * That class also includes a deserializer to convert from JSON
      * back to a List<Metadata>.
      * <p>
@@ -195,8 +190,7 @@ public class ParsingExample {
      * @throws SAXException
      * @throws TikaException
      */
-    public String serializedRecursiveParserWrapperExample()
-            throws IOException, SAXException, TikaException {
+    public String serializedRecursiveParserWrapperExample() throws IOException, SAXException, TikaException {
         List<Metadata> metadataList = recursiveParserWrapperExample();
         StringWriter writer = new StringWriter();
         JsonMetadataList.toJson(metadataList, writer);
@@ -211,12 +205,10 @@ public class ParsingExample {
      * @throws SAXException
      * @throws TikaException
      */
-    public List<Path> extractEmbeddedDocumentsExample(Path outputPath)
-            throws IOException, SAXException, TikaException {
+    public List<Path> extractEmbeddedDocumentsExample(Path outputPath) throws IOException, SAXException, TikaException {
         ExtractEmbeddedFiles ex = new ExtractEmbeddedFiles();
         List<Path> ret = new ArrayList<>();
-        try (TikaInputStream stream = TikaInputStream
-                .get(ParsingExample.class.getResourceAsStream("test_recursive_embedded.docx"))) {
+        try (TikaInputStream stream = TikaInputStream.get(ParsingExample.class.getResourceAsStream("test_recursive_embedded.docx"))) {
             ex.extract(stream, outputPath);
             try (DirectoryStream<Path> dirStream = Files.newDirectoryStream(outputPath)) {
                 for (Path entry : dirStream) {
diff --git a/tika-example/src/main/java/org/apache/tika/example/PickBestTextEncodingParser.java b/tika-example/src/main/java/org/apache/tika/example/PickBestTextEncodingParser.java
index 1888769e8..8f925d3d4 100644
--- a/tika-example/src/main/java/org/apache/tika/example/PickBestTextEncodingParser.java
+++ b/tika-example/src/main/java/org/apache/tika/example/PickBestTextEncodingParser.java
@@ -85,14 +85,15 @@ public class PickBestTextEncodingParser extends AbstractMultipleParser {
         super.parserPrepare(parser, metadata, context);
 
         // Specify which charset to try
-        String charset = context.get(CharsetTester.class).getNextCharset();
+        String charset = context
+                .get(CharsetTester.class)
+                .getNextCharset();
         Charset charsetCS = Charset.forName(charset);
         context.set(EncodingDetector.class, new NonDetectingEncodingDetector(charsetCS));
     }
 
     @Override
-    protected boolean parserCompleted(Parser parser, Metadata metadata, ContentHandler handler,
-                                      ParseContext context, Exception exception) {
+    protected boolean parserCompleted(Parser parser, Metadata metadata, ContentHandler handler, ParseContext context, Exception exception) {
         // Get the current charset
         CharsetTester charsetTester = context.get(CharsetTester.class);
         String charset = charsetTester.getCurrentCharset();
@@ -110,8 +111,7 @@ public class PickBestTextEncodingParser extends AbstractMultipleParser {
                     String text = charsetTester.charsetText.get(pcharset);
                     int cEnglish = 0;
                     for (char c : text.toCharArray()) {
-                        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') ||
-                                (c >= '0' && c <= '9')) {
+                        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || (c >= '0' && c <= '9')) {
                             cEnglish++;
                         }
                     }
@@ -129,8 +129,7 @@ public class PickBestTextEncodingParser extends AbstractMultipleParser {
     }
 
     @Override
-    public void parse(InputStream stream, ContentHandler handler, Metadata originalMetadata,
-                      ParseContext context) throws IOException, SAXException, TikaException {
+    public void parse(InputStream stream, ContentHandler handler, Metadata originalMetadata, ParseContext context) throws IOException, SAXException, TikaException {
         // Use a BodyContentHandler for each of the charset test,
         //  then their real ContentHandler for the last one
         CharsetContentHandlerFactory handlerFactory = new CharsetContentHandlerFactory();
@@ -144,8 +143,7 @@ public class PickBestTextEncodingParser extends AbstractMultipleParser {
     }
 
     @Override
-    public void parse(InputStream stream, ContentHandlerFactory handlers, Metadata metadata,
-                      ParseContext context) throws IOException, SAXException, TikaException {
+    public void parse(InputStream stream, ContentHandlerFactory handlers, Metadata metadata, ParseContext context) throws IOException, SAXException, TikaException {
         // We only work with one ContentHandler as far as the user is
         //  concerned, any others are purely internal!
         parse(stream, handlers.getNewContentHandler(), metadata, context);
@@ -165,6 +163,7 @@ public class PickBestTextEncodingParser extends AbstractMultipleParser {
             }
             return handler;
         }
+
         @Override
         public ContentHandler getNewContentHandler(OutputStream os, Charset charset) {
             return getNewContentHandler();
diff --git a/tika-example/src/main/java/org/apache/tika/example/PrescriptionParser.java b/tika-example/src/main/java/org/apache/tika/example/PrescriptionParser.java
index 33510c4e8..ab492aa96 100755
--- a/tika-example/src/main/java/org/apache/tika/example/PrescriptionParser.java
+++ b/tika-example/src/main/java/org/apache/tika/example/PrescriptionParser.java
@@ -33,16 +33,13 @@ public class PrescriptionParser extends XMLParser {
     private static final long serialVersionUID = 7690682277511967388L;
 
     @Override
-    protected ContentHandler getContentHandler(ContentHandler handler, Metadata metadata,
-                                               ParseContext context) {
+    protected ContentHandler getContentHandler(ContentHandler handler, Metadata metadata, ParseContext context) {
         String xpd = "http://example.com/2011/xpd";
 
         ContentHandler doctor = new ElementMetadataHandler(xpd, "doctor", metadata, "xpd:doctor");
-        ContentHandler patient =
-                new ElementMetadataHandler(xpd, "patient", metadata, "xpd:patient");
+        ContentHandler patient = new ElementMetadataHandler(xpd, "patient", metadata, "xpd:patient");
 
-        return new TeeContentHandler(super.getContentHandler(handler, metadata, context), doctor,
-                patient);
+        return new TeeContentHandler(super.getContentHandler(handler, metadata, context), doctor, patient);
     }
 
     @Override
diff --git a/tika-example/src/main/java/org/apache/tika/example/RecentFiles.java b/tika-example/src/main/java/org/apache/tika/example/RecentFiles.java
index b6c82f1bb..8d8523eb2 100755
--- a/tika-example/src/main/java/org/apache/tika/example/RecentFiles.java
+++ b/tika-example/src/main/java/org/apache/tika/example/RecentFiles.java
@@ -49,8 +49,7 @@ import org.apache.tika.metadata.TikaCoreProperties;
 public class RecentFiles {
     private IndexReader reader;
 
-    private SimpleDateFormat rssDateFormat =
-            new SimpleDateFormat("E, dd MMM yyyy HH:mm:ss z", Locale.getDefault());
+    private SimpleDateFormat rssDateFormat = new SimpleDateFormat("E, dd MMM yyyy HH:mm:ss z", Locale.getDefault());
 
     public String generateRSS(Path indexFile) throws CorruptIndexException, IOException {
         StringBuffer output = new StringBuffer();
@@ -59,14 +58,12 @@ public class RecentFiles {
         try {
             reader = DirectoryReader.open(FSDirectory.open(indexFile));
             searcher = new IndexSearcher(reader);
-            GregorianCalendar gc =
-                    new java.util.GregorianCalendar(TimeZone.getDefault(), Locale.getDefault());
+            GregorianCalendar gc = new java.util.GregorianCalendar(TimeZone.getDefault(), Locale.getDefault());
             gc.setTime(new Date());
             String nowDateTime = ISO8601.format(gc);
             gc.add(java.util.GregorianCalendar.MINUTE, -5);
             String fiveMinsAgo = ISO8601.format(gc);
-            TermRangeQuery query = new TermRangeQuery(TikaCoreProperties.CREATED.getName(),
-                    new BytesRef(fiveMinsAgo), new BytesRef(nowDateTime), true, true);
+            TermRangeQuery query = new TermRangeQuery(TikaCoreProperties.CREATED.getName(), new BytesRef(fiveMinsAgo), new BytesRef(nowDateTime), true, true);
             TopScoreDocCollector collector = TopScoreDocCollector.create(20, 10000);
             searcher.search(query, collector);
             ScoreDoc[] hits = collector.topDocs().scoreDocs;
@@ -95,11 +92,8 @@ public class RecentFiles {
         for (String topic : doc.getValues(TikaCoreProperties.SUBJECT.getName())) {
             output.append(emitTag("category", topic, null, null));
         }
-        output.append(emitTag("pubDate",
-                rssDateFormat.format(ISO8601.parse(doc.get(TikaCoreProperties.CREATED.getName()))),
-                null, null));
-        output.append(
-                emitTag("description", doc.get(TikaCoreProperties.TITLE.getName()), null, null));
+        output.append(emitTag("pubDate", rssDateFormat.format(ISO8601.parse(doc.get(TikaCoreProperties.CREATED.getName()))), null, null));
+        output.append(emitTag("description", doc.get(TikaCoreProperties.TITLE.getName()), null, null));
         output.append("</item>");
         return output.toString();
     }
@@ -110,8 +104,7 @@ public class RecentFiles {
         output.append("<rss version=\"2.0\">");
         output.append("  <channel>");
         output.append("     <title>Tika in Action: Recent Files Feed.</title>");
-        output.append("     <description>Chapter 6 Examples demonstrating " +
-                "use of Tika Metadata for RSS.</description>");
+        output.append("     <description>Chapter 6 Examples demonstrating " + "use of Tika Metadata for RSS.</description>");
         output.append("     <link>tikainaction.rss</link>");
         output.append("     <lastBuildDate>");
         output.append(rssDateFormat.format(new Date()));
@@ -125,8 +118,7 @@ public class RecentFiles {
         return "   </channel>";
     }
 
-    private String emitTag(String tagName, String value, String attributeName,
-                           String attributeValue) {
+    private String emitTag(String tagName, String value, String attributeName, String attributeValue) {
         StringBuilder output = new StringBuilder();
         output.append("<");
         output.append(tagName);
diff --git a/tika-example/src/main/java/org/apache/tika/example/RollbackSoftware.java b/tika-example/src/main/java/org/apache/tika/example/RollbackSoftware.java
index 83377cf89..b25871641 100755
--- a/tika-example/src/main/java/org/apache/tika/example/RollbackSoftware.java
+++ b/tika-example/src/main/java/org/apache/tika/example/RollbackSoftware.java
@@ -61,7 +61,9 @@ public class RollbackSoftware {
         }
         links.sort(Comparator.comparing(Link::getText));
 
-        this.updateVersion(links.get(links.size() - 2).getText());
+        this.updateVersion(links
+                .get(links.size() - 2)
+                .getText());
     }
 
     private void updateVersion(String version) {
@@ -69,7 +71,9 @@ public class RollbackSoftware {
     }
 
     private boolean isSymlink(File f) throws IOException {
-        return !f.getAbsolutePath().equals(f.getCanonicalPath());
+        return !f
+                .getAbsolutePath()
+                .equals(f.getCanonicalPath());
     }
 
     class DeploymentAreaParser implements Parser {
@@ -82,8 +86,7 @@ public class RollbackSoftware {
          * org.apache.tika.parser.ParseContext)
          */
         public Set<MediaType> getSupportedTypes(ParseContext context) {
-            return Collections
-                    .unmodifiableSet(new HashSet<>(Collections.singletonList(MediaType.TEXT_PLAIN)));
+            return Collections.unmodifiableSet(new HashSet<>(Collections.singletonList(MediaType.TEXT_PLAIN)));
         }
 
         /*
@@ -92,8 +95,7 @@ public class RollbackSoftware {
          * @see org.apache.tika.parser.Parser#parse(java.io.InputStream,
          * org.xml.sax.ContentHandler, org.apache.tika.metadata.Metadata)
          */
-        public void parse(InputStream is, ContentHandler handler, Metadata metadata)
-                throws IOException, SAXException, TikaException {
+        public void parse(InputStream is, ContentHandler handler, Metadata metadata) throws IOException, SAXException, TikaException {
             parse(is, handler, metadata, new ParseContext());
         }
 
@@ -104,11 +106,12 @@ public class RollbackSoftware {
          * org.xml.sax.ContentHandler, org.apache.tika.metadata.Metadata,
          * org.apache.tika.parser.ParseContext)
          */
-        public void parse(InputStream is, ContentHandler handler, Metadata metadata,
-                          ParseContext context) throws IOException, SAXException, TikaException {
+        public void parse(InputStream is, ContentHandler handler, Metadata metadata, ParseContext context) throws IOException, SAXException, TikaException {
 
             File deployArea = new File(IOUtils.toString(is, UTF_8));
-            File[] versions = deployArea.listFiles(pathname -> !pathname.getName().startsWith("current"));
+            File[] versions = deployArea.listFiles(pathname -> !pathname
+                    .getName()
+                    .startsWith("current"));
 
             XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
             xhtml.startDocument();
@@ -116,7 +119,10 @@ public class RollbackSoftware {
                 if (isSymlink(v)) {
                     continue;
                 }
-                xhtml.startElement("a", "href", v.toURI().toURL().toExternalForm());
+                xhtml.startElement("a", "href", v
+                        .toURI()
+                        .toURL()
+                        .toExternalForm());
                 xhtml.characters(v.getName());
                 xhtml.endElement("a");
             }
diff --git a/tika-example/src/main/java/org/apache/tika/example/SpringExample.java b/tika-example/src/main/java/org/apache/tika/example/SpringExample.java
index 9cf63e8ad..9d85486f7 100755
--- a/tika-example/src/main/java/org/apache/tika/example/SpringExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/SpringExample.java
@@ -32,12 +32,9 @@ import org.apache.tika.sax.WriteOutContentHandler;
 
 public class SpringExample {
     public static void main(String[] args) throws Exception {
-        ApplicationContext context = new ClassPathXmlApplicationContext(
-                new String[]{"org/apache/tika/example/spring.xml"});
+        ApplicationContext context = new ClassPathXmlApplicationContext(new String[]{"org/apache/tika/example/spring.xml"});
         Parser parser = context.getBean("tika", Parser.class);
-        parser.parse(new ByteArrayInputStream("Hello, World!".getBytes(UTF_8)),
-                new WriteOutContentHandler(new OutputStreamWriter(System.out, UTF_8)),
-                        new Metadata(),
+        parser.parse(new ByteArrayInputStream("Hello, World!".getBytes(UTF_8)), new WriteOutContentHandler(new OutputStreamWriter(System.out, UTF_8)), new Metadata(),
                 new ParseContext());
     }
 }
diff --git a/tika-example/src/main/java/org/apache/tika/example/StandardsExtractionExample.java b/tika-example/src/main/java/org/apache/tika/example/StandardsExtractionExample.java
index e64b1659a..69aea730e 100644
--- a/tika-example/src/main/java/org/apache/tika/example/StandardsExtractionExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/StandardsExtractionExample.java
@@ -55,8 +55,7 @@ public class StandardsExtractionExample {
 
     public static void main(String[] args) {
         if (args.length < 1) {
-            System.err.println(
-                    "Usage: " + StandardsExtractionExample.class.getName() + " /path/to/input");
+            System.err.println("Usage: " + StandardsExtractionExample.class.getName() + " /path/to/input");
             System.exit(1);
         }
         String pathname = args[0];
@@ -72,8 +71,7 @@ public class StandardsExtractionExample {
         try {
             Files.walkFileTree(folder, new SimpleFileVisitor<Path>() {
                 @Override
-                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs)
-                        throws IOException {
+                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
                     try {
                         process(file);
                         successfulFiles++;
@@ -85,8 +83,7 @@ public class StandardsExtractionExample {
                 }
 
                 @Override
-                public FileVisitResult visitFileFailed(Path file, IOException exc)
-                        throws IOException {
+                public FileVisitResult visitFileFailed(Path file, IOException exc) throws IOException {
                     failedFiles++;
                     return FileVisitResult.CONTINUE;
                 }
@@ -102,14 +99,12 @@ public class StandardsExtractionExample {
         // The StandardsExtractingContentHandler will examine any characters for
         // standard references before passing them
         // to the underlying Handler.
-        StandardsExtractingContentHandler handler =
-                new StandardsExtractingContentHandler(new BodyContentHandler(-1), metadata);
+        StandardsExtractingContentHandler handler = new StandardsExtractingContentHandler(new BodyContentHandler(-1), metadata);
         handler.setThreshold(0.75);
         try (InputStream stream = new BufferedInputStream(Files.newInputStream(path))) {
             parser.parse(stream, handler, metadata, new ParseContext());
         }
-        String[] references =
-                metadata.getValues(StandardsExtractingContentHandler.STANDARD_REFERENCES);
+        String[] references = metadata.getValues(StandardsExtractingContentHandler.STANDARD_REFERENCES);
         Collections.addAll(standardReferences, references);
     }
 }
diff --git a/tika-example/src/main/java/org/apache/tika/example/TIAParsingExample.java b/tika-example/src/main/java/org/apache/tika/example/TIAParsingExample.java
index 51007bfdf..d2ba1a8b5 100755
--- a/tika-example/src/main/java/org/apache/tika/example/TIAParsingExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/TIAParsingExample.java
@@ -155,10 +155,8 @@ public class TIAParsingExample {
         ParseContext context = new ParseContext();
         Parser parser = new AutoDetectParser();
         LinkContentHandler linkCollector = new LinkContentHandler();
-        try (Writer writer =
-                     Files.newBufferedWriter(Paths.get(filename), StandardCharsets.UTF_8)) {
-            ContentHandler handler =
-                    new TeeContentHandler(new BodyContentHandler(writer), linkCollector);
+        try (Writer writer = Files.newBufferedWriter(Paths.get(filename), StandardCharsets.UTF_8)) {
+            ContentHandler handler = new TeeContentHandler(new BodyContentHandler(writer), linkCollector);
             parser.parse(stream, handler, metadata, context);
         }
     }
@@ -193,9 +191,7 @@ public class TIAParsingExample {
             private static final long serialVersionUID = 4424210691523343833L;
 
             @Override
-            public void parse(InputStream stream, ContentHandler handler, Metadata metadata,
-                              ParseContext context)
-                    throws IOException, SAXException, TikaException {
+            public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context) throws IOException, SAXException, TikaException {
                 // custom processing of the component document
             }
         });
diff --git a/tika-example/src/main/java/org/apache/tika/example/TranscribeTranslateExample.java b/tika-example/src/main/java/org/apache/tika/example/TranscribeTranslateExample.java
index 2debdf84b..a21e5bd39 100644
--- a/tika-example/src/main/java/org/apache/tika/example/TranscribeTranslateExample.java
+++ b/tika-example/src/main/java/org/apache/tika/example/TranscribeTranslateExample.java
@@ -35,8 +35,8 @@ import org.apache.tika.language.translate.impl.GoogleTranslator;
  * into {@link Translator#translate(String, String)}.
  * The {@link GoogleTranslator} is configured with a target
  * language of "en-US".
- * @author lewismc
  *
+ * @author lewismc
  */
 public class TranscribeTranslateExample {
 
@@ -45,6 +45,7 @@ public class TranscribeTranslateExample {
      * input data. This implementation needs configured as explained in the Javadoc.
      * In this implementation, Google will try to guess the input language. The target
      * language is "en-US".
+     *
      * @param text input text to translate.
      * @return translated text String.
      */
@@ -65,6 +66,7 @@ public class TranscribeTranslateExample {
      * Use {@link org.apache.tika.parser.transcribe.aws.AmazonTranscribe} to execute transcription
      * on input data.
      * This implementation needs to be configured as explained in the Javadoc.
+     *
      * @param file the name of the file (which needs to be on the Java Classpath) to transcribe.
      * @return transcribed text.
      */
@@ -78,35 +80,35 @@ public class TranscribeTranslateExample {
      * <li><code>transcribe-translate ${tika-config.xml} ${file}</code>; which executes both
      * transcription then translation on the given resource, or
      * <li><code>transcribe ${tika-config.xml} ${file}</code>; which executes only translation</li>
-     * @param args either of the commands described above and the input file
-     * (which needs to be on the Java Classpath).
-     *
      *
-     *
-     * ${tika-config.xml} must include credentials for aws and a temporary storage bucket:
-     * <pre>
-     * {@code
-     *  <properties>
-     *   <parsers>
-     *     <parser class="org.apache.tika.parser.DefaultParser"/>
-     *     <parser class="org.apache.tika.parser.transcribe.aws.AmazonTranscribe">
-     *       <params>
-     *         <param name="bucket" type="string">bucket</param>
-     *         <param name="clientId" type="string">clientId</param>
-     *         <param name="clientSecret" type="string">clientSecret</param>
-     *       </params>
-     *     </parser>
-     *   </parsers>
-     * </properties>
-     * }
-     * </pre>
+     * @param args either of the commands described above and the input file
+     *             (which needs to be on the Java Classpath).
+     *             <p>
+     *             <p>
+     *             <p>
+     *             ${tika-config.xml} must include credentials for aws and a temporary storage bucket:
+     *             <pre>
+     *             {@code
+     *              <properties>
+     *               <parsers>
+     *                 <parser class="org.apache.tika.parser.DefaultParser"/>
+     *                 <parser class="org.apache.tika.parser.transcribe.aws.AmazonTranscribe">
+     *                   <params>
+     *                     <param name="bucket" type="string">bucket</param>
+     *                     <param name="clientId" type="string">clientId</param>
+     *                     <param name="clientSecret" type="string">clientSecret</param>
+     *                   </params>
+     *                 </parser>
+     *               </parsers>
+     *             </properties>
+     *             }
+     *             </pre>
      */
-    public static void main (String[] args) throws Exception {
+    public static void main(String[] args) throws Exception {
         String text = null;
         if (args.length > 1) {
             if ("transcribe-translate".equals(args[1])) {
-                text = googleTranslateToEnglish(amazonTranscribe(Paths.get(args[0]),
-                        Paths.get(args[1])));
+                text = googleTranslateToEnglish(amazonTranscribe(Paths.get(args[0]), Paths.get(args[1])));
                 System.out.print("Transcription and translation successful!\nEXTRACTED TEXT: " + text);
             } else if ("transcribe".equals(args[1])) {
                 text = amazonTranscribe(Paths.get(args[0]), Paths.get(args[1]));
diff --git a/tika-example/src/main/java/org/apache/tika/example/TrecDocumentGenerator.java b/tika-example/src/main/java/org/apache/tika/example/TrecDocumentGenerator.java
index 4c89b77d6..ef2a4c508 100755
--- a/tika-example/src/main/java/org/apache/tika/example/TrecDocumentGenerator.java
+++ b/tika-example/src/main/java/org/apache/tika/example/TrecDocumentGenerator.java
@@ -34,14 +34,12 @@ import org.apache.tika.metadata.TikaCoreProperties;
  */
 @SuppressWarnings("deprecation")
 public class TrecDocumentGenerator {
-    public TrecDocument summarize(File file)
-            throws FileNotFoundException, IOException, TikaException {
+    public TrecDocument summarize(File file) throws FileNotFoundException, IOException, TikaException {
         Tika tika = new Tika();
         Metadata met = new Metadata();
 
         String contents = tika.parseToString(new FileInputStream(file), met);
-        return new TrecDocument(met.get(TikaCoreProperties.RESOURCE_NAME_KEY), contents,
-                met.getDate(TikaCoreProperties.CREATED));
+        return new TrecDocument(met.get(TikaCoreProperties.RESOURCE_NAME_KEY), contents, met.getDate(TikaCoreProperties.CREATED));
 
     }
 
diff --git a/tika-example/src/main/resources/org/apache/tika/example/spring.xml b/tika-example/src/main/resources/org/apache/tika/example/spring.xml
index 0f85b9b91..e7882d314 100755
--- a/tika-example/src/main/resources/org/apache/tika/example/spring.xml
+++ b/tika-example/src/main/resources/org/apache/tika/example/spring.xml
@@ -14,23 +14,23 @@
  limitations under the License.
 
  -->
-<beans xmlns="http://www.springframework.org/schema/beans"
-       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+<beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+       xmlns="http://www.springframework.org/schema/beans"
        xsi:schemaLocation="http://www.springframework.org/schema/beans
                            http://www.springframework.org/schema/beans/spring-beans-3.0.xsd">
 
-<!--<start id="spring"/>-->
+  <!--<start id="spring"/>-->
   <bean id="tika" class="org.apache.tika.parser.AutoDetectParser">
     <constructor-arg>
-        <list>
-           <ref bean="txt"/>
-           <ref bean="pdf"/>
-        </list>
+      <list>
+        <ref bean="txt"/>
+        <ref bean="pdf"/>
+      </list>
     </constructor-arg>
   </bean>
 
   <bean id="txt" class="TXTParser"/>
   <bean id="pdf" class="org.apache.tika.parser.pdf.PDFParser"/>
-<!--<end id="spring"/>-->
+  <!--<end id="spring"/>-->
 
 </beans>
\ No newline at end of file
diff --git a/tika-example/src/test/java/org/apache/tika/example/ContentHandlerExampleTest.java b/tika-example/src/test/java/org/apache/tika/example/ContentHandlerExampleTest.java
index d36e55a12..2bf625c9f 100644
--- a/tika-example/src/test/java/org/apache/tika/example/ContentHandlerExampleTest.java
+++ b/tika-example/src/test/java/org/apache/tika/example/ContentHandlerExampleTest.java
@@ -41,13 +41,17 @@ public class ContentHandlerExampleTest {
 
     @Test
     public void testParseToPlainText() throws IOException, SAXException, TikaException {
-        String result = example.parseToPlainText().trim();
+        String result = example
+                .parseToPlainText()
+                .trim();
         assertEquals("test", result, "Expected 'test', but got '" + result + "'");
     }
 
     @Test
     public void testParseToHTML() throws IOException, SAXException, TikaException {
-        String result = example.parseToHTML().trim();
+        String result = example
+                .parseToHTML()
+                .trim();
 
         assertContains("<html", result);
         assertContains("<head>", result);
@@ -59,7 +63,9 @@ public class ContentHandlerExampleTest {
 
     @Test
     public void testParseBodyToHTML() throws IOException, SAXException, TikaException {
-        String result = example.parseBodyToHTML().trim();
+        String result = example
+                .parseBodyToHTML()
+                .trim();
 
         assertNotContained("<html", result);
         assertNotContained("<head>", result);
@@ -71,7 +77,9 @@ public class ContentHandlerExampleTest {
 
     @Test
     public void testParseOnePartToHTML() throws IOException, SAXException, TikaException {
-        String result = example.parseOnePartToHTML().trim();
+        String result = example
+                .parseOnePartToHTML()
+                .trim();
 
         assertNotContained("<html", result);
         assertNotContained("<head>", result);
@@ -91,8 +99,7 @@ public class ContentHandlerExampleTest {
 
         assertEquals(3, result.size());
         for (String chunk : result) {
-            assertTrue(chunk.length() <= example.MAXIMUM_TEXT_CHUNK_SIZE,
-                    "Chunk under max size");
+            assertTrue(chunk.length() <= example.MAXIMUM_TEXT_CHUNK_SIZE, "Chunk under max size");
         }
 
         assertContains("This is in the header", result.get(0));
diff --git a/tika-example/src/test/java/org/apache/tika/example/DumpTikaConfigExampleTest.java b/tika-example/src/test/java/org/apache/tika/example/DumpTikaConfigExampleTest.java
index 85696ab68..8e41ae49f 100644
--- a/tika-example/src/test/java/org/apache/tika/example/DumpTikaConfigExampleTest.java
+++ b/tika-example/src/test/java/org/apache/tika/example/DumpTikaConfigExampleTest.java
@@ -46,7 +46,9 @@ public class DumpTikaConfigExampleTest {
     @BeforeEach
     public void setUp() {
         try {
-            configFile = Files.createTempFile("tmp", ".xml").toFile();
+            configFile = Files
+                    .createTempFile("tmp", ".xml")
+                    .toFile();
         } catch (IOException e) {
             throw new RuntimeException("Failed to create tmp file");
         }
@@ -68,21 +70,27 @@ public class DumpTikaConfigExampleTest {
         for (Charset charset : new Charset[]{UTF_8, UTF_16LE}) {
             for (TikaConfigSerializer.Mode mode : TikaConfigSerializer.Mode.values()) {
                 Writer writer = new OutputStreamWriter(new FileOutputStream(configFile), charset);
-                TikaConfigSerializer
-                        .serialize(TikaConfig.getDefaultConfig(), mode, writer, charset);
+                TikaConfigSerializer.serialize(TikaConfig.getDefaultConfig(), mode, writer, charset);
                 writer.flush();
                 writer.close();
 
                 TikaConfig c = new TikaConfig(configFile);
-                assertTrue(c.getParser() instanceof CompositeParser, c.getParser().toString());
-                assertTrue(c.getDetector() instanceof CompositeDetector,
-                        c.getDetector().toString());
+                assertTrue(c.getParser() instanceof CompositeParser, c
+                        .getParser()
+                        .toString());
+                assertTrue(c.getDetector() instanceof CompositeDetector, c
+                        .getDetector()
+                        .toString());
 
                 CompositeParser p = (CompositeParser) c.getParser();
-                assertTrue(p.getParsers().size() > 130, "enough parsers?");
+                assertTrue(p
+                        .getParsers()
+                        .size() > 130, "enough parsers?");
 
                 CompositeDetector d = (CompositeDetector) c.getDetector();
-                assertTrue(d.getDetectors().size() > 3, "enough detectors?");
+                assertTrue(d
+                        .getDetectors()
+                        .size() > 3, "enough detectors?");
 
                 //just try to load it into autodetect to make sure no errors are thrown
                 Parser auto = new AutoDetectParser(c);
diff --git a/tika-example/src/test/java/org/apache/tika/example/SimpleTextExtractorTest.java b/tika-example/src/test/java/org/apache/tika/example/SimpleTextExtractorTest.java
index 6b2727223..84be548d4 100755
--- a/tika-example/src/test/java/org/apache/tika/example/SimpleTextExtractorTest.java
+++ b/tika-example/src/test/java/org/apache/tika/example/SimpleTextExtractorTest.java
@@ -31,13 +31,12 @@ import org.apache.tika.TikaTest;
 
 // because System is caught
 // https://junit.org/junit5/docs/snapshot/user-guide/#writing-tests-parallel-execution-synchronization
-@Isolated 
+@Isolated
 public class SimpleTextExtractorTest extends TikaTest {
     @Test
     public void testSimpleTextExtractor() throws Exception {
-        String message = "This is Tika - Hello, World! This is simple UTF-8 text" +
-                " content written in English to test autodetection of" +
-                " the character encoding of the input stream.";
+        String message =
+                "This is Tika - Hello, World! This is simple UTF-8 text" + " content written in English to test autodetection of" + " the character encoding of the input stream.";
         ByteArrayOutputStream buffer = new ByteArrayOutputStream();
 
         PrintStream out = System.out;
@@ -50,6 +49,8 @@ public class SimpleTextExtractorTest extends TikaTest {
 
         System.setOut(out);
 
-        assertContains(message, buffer.toString(UTF_8.name()).trim());
+        assertContains(message, buffer
+                .toString(UTF_8.name())
+                .trim());
     }
 }
diff --git a/tika-example/src/test/java/org/apache/tika/example/SimpleTypeDetectorTest.java b/tika-example/src/test/java/org/apache/tika/example/SimpleTypeDetectorTest.java
index b92ab60e8..b7d7c9066 100755
--- a/tika-example/src/test/java/org/apache/tika/example/SimpleTypeDetectorTest.java
+++ b/tika-example/src/test/java/org/apache/tika/example/SimpleTypeDetectorTest.java
@@ -40,7 +40,9 @@ public class SimpleTypeDetectorTest extends TikaTest {
 
         System.setOut(out);
 
-        assertContains("pom.xml: application/xml", buffer.toString(UTF_8.name()).trim());
+        assertContains("pom.xml: application/xml", buffer
+                .toString(UTF_8.name())
+                .trim());
     }
 
 }
diff --git a/tika-example/src/test/java/org/apache/tika/example/TestParsingExample.java b/tika-example/src/test/java/org/apache/tika/example/TestParsingExample.java
index bf9a4b774..20feb8b37 100644
--- a/tika-example/src/test/java/org/apache/tika/example/TestParsingExample.java
+++ b/tika-example/src/test/java/org/apache/tika/example/TestParsingExample.java
@@ -31,7 +31,7 @@ import org.xml.sax.SAXException;
 import org.apache.tika.TikaTest;
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadataList;
 
 public class TestParsingExample extends TikaTest {
     ParsingExample parsingExample;
@@ -43,13 +43,17 @@ public class TestParsingExample extends TikaTest {
 
     @Test
     public void testParseToStringExample() throws IOException, SAXException, TikaException {
-        String result = parsingExample.parseToStringExample().trim();
+        String result = parsingExample
+                .parseToStringExample()
+                .trim();
         assertEquals("test", result, "enough detectors?");
     }
 
     @Test
     public void testParseExample() throws IOException, SAXException, TikaException {
-        String result = parsingExample.parseExample().trim();
+        String result = parsingExample
+                .parseExample()
+                .trim();
         assertEquals("test", result, "Expected 'test', but got '" + result + "'");
     }
 
@@ -72,22 +76,18 @@ public class TestParsingExample extends TikaTest {
     }
 
     @Test
-    public void testRecursiveParserWrapperExample()
-            throws IOException, SAXException, TikaException {
+    public void testRecursiveParserWrapperExample() throws IOException, SAXException, TikaException {
         List<Metadata> metadataList = parsingExample.recursiveParserWrapperExample();
-        assertEquals(12, metadataList.size(),
-                "Number of embedded documents + 1 for the container document");
+        assertEquals(12, metadataList.size(), "Number of embedded documents + 1 for the container document");
         Metadata m = metadataList.get(6);
         //this is the location the embed3.txt text file within the outer .docx
-        assertEquals("/embed1.zip/embed2.zip/embed3.zip/embed3.txt",
-                m.get("X-TIKA:embedded_resource_path"));
+        assertEquals("/embed1.zip/embed2.zip/embed3.zip/embed3.txt", m.get("X-TIKA:embedded_resource_path"));
         //it contains some html encoded content
         assertContains("When in the Course", m.get("X-TIKA:content"));
     }
 
     @Test
-    public void testSerializedRecursiveParserWrapperExample()
-            throws IOException, SAXException, TikaException {
+    public void testSerializedRecursiveParserWrapperExample() throws IOException, SAXException, TikaException {
         String json = parsingExample.serializedRecursiveParserWrapperExample();
         assertTrue(json.contains("When in the Course"));
         //now try deserializing the JSON
diff --git a/tika-example/src/test/resources/junit-platform.properties b/tika-example/src/test/resources/junit-platform.properties
index a89c684fe..00cdd87d5 100644
--- a/tika-example/src/test/resources/junit-platform.properties
+++ b/tika-example/src/test/resources/junit-platform.properties
@@ -14,6 +14,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-junit.jupiter.execution.parallel.enabled = true
-junit.jupiter.execution.parallel.mode.default = same_thread
-junit.jupiter.execution.parallel.mode.classes.default = concurrent
+junit.jupiter.execution.parallel.enabled=true
+junit.jupiter.execution.parallel.mode.default=same_thread
+junit.jupiter.execution.parallel.mode.classes.default=concurrent
diff --git a/tika-fuzzing/src/main/java/org/apache/tika/fuzzing/cli/FuzzingCLI.java b/tika-fuzzing/src/main/java/org/apache/tika/fuzzing/cli/FuzzingCLI.java
index 53cb22b40..c6c71c294 100644
--- a/tika-fuzzing/src/main/java/org/apache/tika/fuzzing/cli/FuzzingCLI.java
+++ b/tika-fuzzing/src/main/java/org/apache/tika/fuzzing/cli/FuzzingCLI.java
@@ -46,7 +46,6 @@ import org.apache.tika.fuzzing.general.ByteInjector;
 import org.apache.tika.fuzzing.general.GeneralTransformer;
 import org.apache.tika.fuzzing.general.SpanSwapper;
 import org.apache.tika.fuzzing.general.Truncator;
-import org.apache.tika.metadata.Metadata;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.PipesConfig;
 import org.apache.tika.pipes.PipesParser;
@@ -213,7 +212,8 @@ public class FuzzingCLI {
                     "." + FilenameUtils.getExtension(fetchEmitTuple.getFetchKey().getFetchKey()));
             try (InputStream is = fetcherManager.getFetcher(
                             fetchEmitTuple.getFetchKey().getFetcherName())
-                    .fetch(fetchEmitTuple.getFetchKey().getFetchKey(), new Metadata())) {
+                    .fetch(fetchEmitTuple.getFetchKey().getFetchKey(), fetchEmitTuple.getMetadata(),
+                            fetchEmitTuple.getParseContext())) {
                 try (OutputStream os = Files.newOutputStream(target)) {
                     transformer.transform(is, os);
                 }
diff --git a/tika-integration-tests/tika-pipes-opensearch-integration-tests/src/test/java/org/apache/tika/pipes/xsearch/tests/TikaPipesXSearchBase.java b/tika-integration-tests/tika-pipes-opensearch-integration-tests/src/test/java/org/apache/tika/pipes/xsearch/tests/TikaPipesXSearchBase.java
index 7416083ab..79ed6ce43 100644
--- a/tika-integration-tests/tika-pipes-opensearch-integration-tests/src/test/java/org/apache/tika/pipes/xsearch/tests/TikaPipesXSearchBase.java
+++ b/tika-integration-tests/tika-pipes-opensearch-integration-tests/src/test/java/org/apache/tika/pipes/xsearch/tests/TikaPipesXSearchBase.java
@@ -45,6 +45,7 @@ import org.testcontainers.shaded.org.apache.commons.io.FileUtils;
 import org.apache.tika.cli.TikaCLI;
 import org.apache.tika.client.HttpClientFactory;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.Emitter;
 import org.apache.tika.pipes.emitter.EmitterManager;
@@ -125,6 +126,7 @@ public abstract class TikaPipesXSearchBase {
             }
             statusCounts.put(status, cnt);
         }
+        System.out.println(statusCounts);
         assertEquals(numHtmlDocs, (int) statusCounts.get("PARSE_SUCCESS"));
         //the npe is caught and counted as a "parse success with exception"
         assertEquals(1, (int) statusCounts.get("PARSE_SUCCESS_WITH_EXCEPTION"));
@@ -334,15 +336,15 @@ public abstract class TikaPipesXSearchBase {
         Metadata metadata = new Metadata();
         metadata.set("mime", "mimeA");
         metadata.set("title", "titleA");
-        emitter.emit("1", Collections.singletonList(metadata));
+        emitter.emit("1", Collections.singletonList(metadata), new ParseContext());
         JsonResponse refresh = CLIENT.getJson(endpoint + "/_refresh");
         metadata.set("title", "titleB");
-        emitter.emit("1", Collections.singletonList(metadata));
+        emitter.emit("1", Collections.singletonList(metadata), new ParseContext());
         refresh = CLIENT.getJson(endpoint + "/_refresh");
 
         Metadata metadata2 = new Metadata();
         metadata2.set("content", "the quick brown fox");
-        emitter.emit("1", Collections.singletonList(metadata2));
+        emitter.emit("1", Collections.singletonList(metadata2), new ParseContext());
         refresh = CLIENT.getJson(endpoint + "/_refresh");
 
         String query = "{ " +
diff --git a/tika-integration-tests/tika-pipes-s3-integration-tests/src/test/java/org/apache/tika/pipes/s3/tests/PipeIntegrationTests.java b/tika-integration-tests/tika-pipes-s3-integration-tests/src/test/java/org/apache/tika/pipes/s3/tests/PipeIntegrationTests.java
index 495bcec17..b32304d69 100644
--- a/tika-integration-tests/tika-pipes-s3-integration-tests/src/test/java/org/apache/tika/pipes/s3/tests/PipeIntegrationTests.java
+++ b/tika-integration-tests/tika-pipes-s3-integration-tests/src/test/java/org/apache/tika/pipes/s3/tests/PipeIntegrationTests.java
@@ -198,7 +198,7 @@ public class PipeIntegrationTests {
             if (Files.isRegularFile(targ)) {
                 return;
             }
-            try (InputStream is = fetcher.fetch(t.getFetchKey().getFetchKey(), t.getMetadata())) {
+            try (InputStream is = fetcher.fetch(t.getFetchKey().getFetchKey(), t.getMetadata(), t.getParseContext())) {
                 System.out.println(counter.getAndIncrement() + " : " + t);
                 Files.createDirectories(targ.getParent());
                 Files.copy(is, targ);
@@ -236,11 +236,11 @@ public class PipeIntegrationTests {
         }
 
         private void process(FetchEmitTuple t) throws IOException, TikaException {
-            Metadata userMetadata = new Metadata();
+            Metadata userMetadata = t.getMetadata();
             userMetadata.set("project", "my-project");
 
-            try (InputStream is = fetcher.fetch(t.getFetchKey().getFetchKey(), t.getMetadata())) {
-                emitter.emit(t.getEmitKey().getEmitKey(), is, userMetadata);
+            try (InputStream is = fetcher.fetch(t.getFetchKey().getFetchKey(), t.getMetadata(), t.getParseContext())) {
+                emitter.emit(t.getEmitKey().getEmitKey(), is, userMetadata, t.getParseContext());
             }
         }
     }
diff --git a/tika-pipes/tika-async-cli/src/main/java/org/apache/tika/async/cli/TikaAsyncCLI.java b/tika-pipes/tika-async-cli/src/main/java/org/apache/tika/async/cli/TikaAsyncCLI.java
index 4490e33fd..37b4d4262 100644
--- a/tika-pipes/tika-async-cli/src/main/java/org/apache/tika/async/cli/TikaAsyncCLI.java
+++ b/tika-pipes/tika-async-cli/src/main/java/org/apache/tika/async/cli/TikaAsyncCLI.java
@@ -40,7 +40,7 @@ public class TikaAsyncCLI {
 
             for (FetchEmitTuple t : pipesIterator) {
                 boolean offered = processor.offer(t, TIMEOUT_MS);
-                if (! offered) {
+                if (!offered) {
                     throw new TimeoutException("timed out waiting to add a fetch emit tuple");
                 }
             }
@@ -53,8 +53,7 @@ public class TikaAsyncCLI {
                 }
             }
             long elapsed = System.currentTimeMillis() - start;
-            LOG.info("Successfully finished processing {} files in {} ms",
-                    processor.getTotalProcessed(), elapsed);
+            LOG.info("Successfully finished processing {} files in {} ms", processor.getTotalProcessed(), elapsed);
         }
     }
 }
diff --git a/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/AsyncProcessorTest.java b/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/AsyncProcessorTest.java
index 4bcdacb9e..acadeeb7a 100644
--- a/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/AsyncProcessorTest.java
+++ b/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/AsyncProcessorTest.java
@@ -35,7 +35,7 @@ import org.junit.jupiter.api.io.TempDir;
 import org.apache.tika.TikaTest;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.async.AsyncProcessor;
@@ -43,6 +43,7 @@ import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
 import org.apache.tika.pipes.fetcher.FetchKey;
 import org.apache.tika.pipes.pipesiterator.PipesIterator;
+import org.apache.tika.serialization.JsonMetadataList;
 
 /**
  * This should be in tika-core, but we want to avoid a dependency mess with tika-serialization
@@ -74,41 +75,42 @@ public class AsyncProcessorTest extends TikaTest {
         Files.createDirectories(configDir);
         Files.createDirectories(inputDir);
 
-        String xml = IOUtils.toString(
-                AsyncProcessorTest.class.getResourceAsStream("/configs/TIKA-4207-emitter.xml"),
-                    StandardCharsets.UTF_8);
+        String xml = IOUtils.toString(AsyncProcessorTest.class.getResourceAsStream("/configs/TIKA-4207-emitter.xml"), StandardCharsets.UTF_8);
         //do stuff to xml
-        xml = xml.replace("BASE_PATH", inputDir.toAbsolutePath().toString());
-        xml = xml.replace("JSON_PATH", jsonDir.toAbsolutePath().toString());
-        xml = xml.replace("BYTES_PATH", bytesDir.toAbsolutePath().toString());
+        xml = xml.replace("BASE_PATH", inputDir
+                .toAbsolutePath()
+                .toString());
+        xml = xml.replace("JSON_PATH", jsonDir
+                .toAbsolutePath()
+                .toString());
+        xml = xml.replace("BYTES_PATH", bytesDir
+                .toAbsolutePath()
+                .toString());
 
         Files.writeString(tikaConfig, xml, StandardCharsets.UTF_8);
 
         Path mock = inputDir.resolve("mock.xml");
         try (OutputStream os = Files.newOutputStream(mock)) {
-            IOUtils.copy(getClass().getResourceAsStream("/test-documents/basic_embedded.xml"),
-                    os);
+            IOUtils.copy(getClass().getResourceAsStream("/test-documents/basic_embedded.xml"), os);
         }
     }
 
     @Test
     public void testBasic() throws Exception {
 //        TikaAsyncCLI cli = new TikaAsyncCLI();
-  //      cli.main(new String[]{ configDir.resolve("tika-config.xml").toAbsolutePath().toString()});
+        //      cli.main(new String[]{ configDir.resolve("tika-config.xml").toAbsolutePath().toString()});
         AsyncProcessor processor = new AsyncProcessor(configDir.resolve("tika-config.xml"));
 
-        EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig =
-                new EmbeddedDocumentBytesConfig(true);
+        EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig = new EmbeddedDocumentBytesConfig(true);
         embeddedDocumentBytesConfig.setIncludeOriginal(true);
         embeddedDocumentBytesConfig.setEmitter("bytes");
         embeddedDocumentBytesConfig.setSuffixStrategy(EmbeddedDocumentBytesConfig.SUFFIX_STRATEGY.NONE);
         embeddedDocumentBytesConfig.setEmbeddedIdPrefix("-");
-
-        FetchEmitTuple t = new FetchEmitTuple("myId-1",
-                new FetchKey("fs",  "mock.xml"),
-                new EmitKey("json", "emit-1"),
-                new Metadata(), HandlerConfig.DEFAULT_HANDLER_CONFIG,
-                FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT, embeddedDocumentBytesConfig);
+        ParseContext parseContext = new ParseContext();
+        parseContext.set(HandlerConfig.class, HandlerConfig.DEFAULT_HANDLER_CONFIG);
+        parseContext.set(EmbeddedDocumentBytesConfig.class, embeddedDocumentBytesConfig);
+        FetchEmitTuple t =
+                new FetchEmitTuple("myId-1", new FetchKey("fs", "mock.xml"), new EmitKey("json", "emit-1"), new Metadata(), parseContext, FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT);
 
         processor.offer(t, 1000);
 
@@ -133,8 +135,11 @@ public class AsyncProcessorTest extends TikaTest {
             metadataList = JsonMetadataList.fromJson(reader);
         }
         assertEquals(2, metadataList.size());
-        assertContains("main_content", metadataList.get(0).get(TikaCoreProperties.TIKA_CONTENT));
-        assertContains("some_embedded_content",
-                metadataList.get(1).get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("main_content", metadataList
+                .get(0)
+                .get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("some_embedded_content", metadataList
+                .get(1)
+                .get(TikaCoreProperties.TIKA_CONTENT));
     }
 }
diff --git a/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/TikaAsyncCLITest.java b/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/TikaAsyncCLITest.java
index 08c962f10..2fd9818f2 100644
--- a/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/TikaAsyncCLITest.java
+++ b/tika-pipes/tika-async-cli/src/test/java/org/apache/tika/async/cli/TikaAsyncCLITest.java
@@ -29,15 +29,13 @@ public class TikaAsyncCLITest {
     @Test
     public void testCrash() throws Exception {
         Path config = getPath("/configs/tika-config-broken.xml");
-        assertThrows(TikaConfigException.class,
-                () -> TikaAsyncCLI.main(
-                        new String[] {
-                            config.toAbsolutePath().toString()
-                        })
-        );
+        assertThrows(TikaConfigException.class, () -> TikaAsyncCLI.main(new String[]{config.toAbsolutePath().toString()}));
     }
 
     private Path getPath(String file) throws Exception {
-        return Paths.get(this.getClass().getResource(file).toURI());
+        return Paths.get(this
+                .getClass()
+                .getResource(file)
+                .toURI());
     }
 }
diff --git a/tika-pipes/tika-emitters/tika-emitter-az-blob/src/main/java/org/apache/tika/pipes/emitter/azblob/AZBlobEmitter.java b/tika-pipes/tika-emitters/tika-emitter-az-blob/src/main/java/org/apache/tika/pipes/emitter/azblob/AZBlobEmitter.java
index 180bc204a..0deb0d42c 100644
--- a/tika-pipes/tika-emitters/tika-emitter-az-blob/src/main/java/org/apache/tika/pipes/emitter/azblob/AZBlobEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-az-blob/src/main/java/org/apache/tika/pipes/emitter/azblob/AZBlobEmitter.java
@@ -47,16 +47,16 @@ import org.apache.tika.exception.TikaException;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.StreamEmitter;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.utils.StringUtils;
 
 
 /**
  * Emit files to Azure blob storage. Must set endpoint, sasToken and container via config.
- *
  */
 
 public class AZBlobEmitter extends AbstractEmitter implements Initializable, StreamEmitter {
@@ -72,6 +72,7 @@ public class AZBlobEmitter extends AbstractEmitter implements Initializable, Str
     private BlobServiceClient blobServiceClient;
     private BlobContainerClient blobContainerClient;
     private boolean overwriteExisting = false;
+
     /**
      * Requires the src-bucket/path/to/my/file.txt in the {@link TikaCoreProperties#SOURCE_PATH}.
      *
@@ -80,21 +81,22 @@ public class AZBlobEmitter extends AbstractEmitter implements Initializable, Str
      * @throws TikaException
      */
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
-            throws IOException, TikaEmitterException {
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext) throws IOException, TikaEmitterException {
         if (metadataList == null || metadataList.size() == 0) {
             throw new TikaEmitterException("metadata list must not be null or of size 0");
         }
         //TODO: estimate size of metadata list.  Above a certain size,
         //create a temp file?
-        UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream.builder().get();
+        UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream
+                .builder()
+                .get();
         try (Writer writer = new OutputStreamWriter(bos, StandardCharsets.UTF_8)) {
             JsonMetadataList.toJson(metadataList, writer);
         } catch (IOException e) {
             throw new TikaEmitterException("can't jsonify", e);
         }
         Metadata metadata = new Metadata();
-        emit(emitKey, TikaInputStream.get(bos.toByteArray(), metadata), metadata);
+        emit(emitKey, TikaInputStream.get(bos.toByteArray(), metadata), metadata, parseContext);
 
     }
 
@@ -109,8 +111,7 @@ public class AZBlobEmitter extends AbstractEmitter implements Initializable, Str
      * @throws TikaEmitterException or IOexception if there is a Runtime client exception
      */
     @Override
-    public void emit(String path, InputStream is, Metadata userMetadata)
-            throws IOException, TikaEmitterException {
+    public void emit(String path, InputStream is, Metadata userMetadata, ParseContext parseContext) throws IOException, TikaEmitterException {
         String lengthString = userMetadata.get(Metadata.CONTENT_LENGTH);
         long length = -1;
         if (lengthString != null) {
@@ -126,7 +127,9 @@ public class AZBlobEmitter extends AbstractEmitter implements Initializable, Str
             LOGGER.debug("relying on the content-length set in the metadata object: {}", length);
             write(path, userMetadata, is, length);
         } else {
-            try (UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream.builder().get()) {
+            try (UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream
+                    .builder()
+                    .get()) {
                 IOUtils.copy(is, bos);
                 write(path, userMetadata, bos.toByteArray());
             }
@@ -147,7 +150,9 @@ public class AZBlobEmitter extends AbstractEmitter implements Initializable, Str
         BlobClient blobClient = blobContainerClient.getBlobClient(actualPath);
         updateMetadata(blobClient, userMetadata);
 
-        blobClient.uploadFromFile(file.toAbsolutePath().toString(), overwriteExisting);
+        blobClient.uploadFromFile(file
+                .toAbsolutePath()
+                .toString(), overwriteExisting);
     }
 
     private void write(String path, Metadata userMetadata, byte[] bytes) {
@@ -165,10 +170,12 @@ public class AZBlobEmitter extends AbstractEmitter implements Initializable, Str
             }
             String[] vals = userMetadata.getValues(n);
             if (vals.length > 1) {
-                LOGGER.warn("Can only write the first value for key {}. I see {} values.", n,
-                        vals.length);
+                LOGGER.warn("Can only write the first value for key {}. I see {} values.", n, vals.length);
             }
-            blobClient.getProperties().getMetadata().put(n, vals[0]);
+            blobClient
+                    .getProperties()
+                    .getMetadata()
+                    .put(n, vals[0]);
         }
 
     }
@@ -246,8 +253,7 @@ public class AZBlobEmitter extends AbstractEmitter implements Initializable, Str
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         mustNotBeEmpty("sasToken", this.sasToken);
         mustNotBeEmpty("endpoint", this.endpoint);
         mustNotBeEmpty("container", this.container);
diff --git a/tika-pipes/tika-emitters/tika-emitter-az-blob/src/test/java/org/apache/tika/pipes/emitter/azblob/TestAZBlobEmitter.java b/tika-pipes/tika-emitters/tika-emitter-az-blob/src/test/java/org/apache/tika/pipes/emitter/azblob/TestAZBlobEmitter.java
index 4b939785e..803513e37 100644
--- a/tika-pipes/tika-emitters/tika-emitter-az-blob/src/test/java/org/apache/tika/pipes/emitter/azblob/TestAZBlobEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-az-blob/src/test/java/org/apache/tika/pipes/emitter/azblob/TestAZBlobEmitter.java
@@ -26,6 +26,7 @@ import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.Emitter;
 import org.apache.tika.pipes.emitter.EmitterManager;
 
@@ -43,10 +44,13 @@ public class TestAZBlobEmitter {
         m.set("k2", "v3");
         m.add("k2", "v4");
         metadataList.add(m);
-        emitter.emit("something-or-other/test-out", metadataList);
+        emitter.emit("something-or-other/test-out", metadataList, new ParseContext());
     }
 
     private Path getConfig(String configFile) throws URISyntaxException {
-        return Paths.get(this.getClass().getResource("/config/" + configFile).toURI());
+        return Paths.get(this
+                .getClass()
+                .getResource("/config/" + configFile)
+                .toURI());
     }
 }
diff --git a/tika-pipes/tika-emitters/tika-emitter-fs/src/main/java/org/apache/tika/pipes/emitter/fs/FileSystemEmitter.java b/tika-pipes/tika-emitters/tika-emitter-fs/src/main/java/org/apache/tika/pipes/emitter/fs/FileSystemEmitter.java
index a90c5e509..9142c9b7b 100644
--- a/tika-pipes/tika-emitters/tika-emitter-fs/src/main/java/org/apache/tika/pipes/emitter/fs/FileSystemEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-fs/src/main/java/org/apache/tika/pipes/emitter/fs/FileSystemEmitter.java
@@ -30,10 +30,11 @@ import java.util.List;
 import org.apache.tika.config.Field;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.StreamEmitter;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
+import org.apache.tika.serialization.JsonMetadataList;
 
 /**
  * Emitter to write to a file system.
@@ -73,8 +74,7 @@ public class FileSystemEmitter extends AbstractEmitter implements StreamEmitter
     private boolean prettyPrint = false;
 
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
-            throws IOException, TikaEmitterException {
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext) throws IOException, TikaEmitterException {
         Path output;
         if (metadataList == null || metadataList.size() == 0) {
             throw new TikaEmitterException("metadata list must not be null or of size 0");
@@ -133,8 +133,7 @@ public class FileSystemEmitter extends AbstractEmitter implements StreamEmitter
                 this.onExists = ON_EXISTS.EXCEPTION;
                 break;
             default:
-                throw new IllegalArgumentException("Don't understand '" + onExists +
-                                                   "'; must be one of: 'skip', 'replace', 'exception'");
+                throw new IllegalArgumentException("Don't understand '" + onExists + "'; must be one of: 'skip', 'replace', 'exception'");
         }
     }
 
@@ -144,8 +143,7 @@ public class FileSystemEmitter extends AbstractEmitter implements StreamEmitter
     }
 
     @Override
-    public void emit(String path, InputStream inputStream, Metadata userMetadata)
-            throws IOException, TikaEmitterException {
+    public void emit(String path, InputStream inputStream, Metadata userMetadata, ParseContext parseContext) throws IOException, TikaEmitterException {
         Path target = basePath.resolve(path);
 
         if (!Files.isDirectory(target.getParent())) {
diff --git a/tika-pipes/tika-emitters/tika-emitter-gcs/src/main/java/org/apache/tika/pipes/emitter/gcs/GCSEmitter.java b/tika-pipes/tika-emitters/tika-emitter-gcs/src/main/java/org/apache/tika/pipes/emitter/gcs/GCSEmitter.java
index 9121190d6..02a42b696 100644
--- a/tika-pipes/tika-emitters/tika-emitter-gcs/src/main/java/org/apache/tika/pipes/emitter/gcs/GCSEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-gcs/src/main/java/org/apache/tika/pipes/emitter/gcs/GCSEmitter.java
@@ -45,10 +45,11 @@ import org.apache.tika.exception.TikaException;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.StreamEmitter;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.utils.StringUtils;
 
 
@@ -69,12 +70,13 @@ public class GCSEmitter extends AbstractEmitter implements Initializable, Stream
      * @throws TikaException
      */
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
-            throws IOException, TikaEmitterException {
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext) throws IOException, TikaEmitterException {
         if (metadataList == null || metadataList.size() == 0) {
             throw new TikaEmitterException("metadata list must not be null or of size 0");
         }
-        try (UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream.builder().get()) {
+        try (UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream
+                .builder()
+                .get()) {
             try (Writer writer = new OutputStreamWriter(bos, StandardCharsets.UTF_8)) {
                 JsonMetadataList.toJson(metadataList, writer);
             } catch (IOException e) {
@@ -93,13 +95,14 @@ public class GCSEmitter extends AbstractEmitter implements Initializable, Stream
      * @throws TikaEmitterException or IOexception if there is a Runtime s3 client exception
      */
     @Override
-    public void emit(String path, InputStream is, Metadata userMetadata)
-            throws IOException, TikaEmitterException {
+    public void emit(String path, InputStream is, Metadata userMetadata, ParseContext parseContext) throws IOException, TikaEmitterException {
 
         if (is instanceof TikaInputStream && ((TikaInputStream) is).hasFile()) {
             write(path, userMetadata, Files.readAllBytes(((TikaInputStream) is).getPath()));
         } else {
-            try (UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream.builder().get()) {
+            try (UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream
+                    .builder()
+                    .get()) {
                 IOUtils.copy(is, bos);
                 write(path, userMetadata, bos.toByteArray());
             }
@@ -117,15 +120,18 @@ public class GCSEmitter extends AbstractEmitter implements Initializable, Stream
 
         LOGGER.debug("about to emit to target bucket: ({}) path:({})", bucket, path);
         BlobId blobId = BlobId.of(bucket, path);
-        BlobInfo blobInfo = BlobInfo.newBuilder(blobId).build();
+        BlobInfo blobInfo = BlobInfo
+                .newBuilder(blobId)
+                .build();
 
         for (String n : userMetadata.names()) {
             String[] vals = userMetadata.getValues(n);
             if (vals.length > 1) {
-                LOGGER.warn("Can only write the first value for key {}. I see {} values.", n,
-                        vals.length);
+                LOGGER.warn("Can only write the first value for key {}. I see {} values.", n, vals.length);
             }
-            blobInfo.getMetadata().put(n, vals[0]);
+            blobInfo
+                    .getMetadata()
+                    .put(n, vals[0]);
         }
         storage.create(blobInfo, bytes);
     }
@@ -173,12 +179,15 @@ public class GCSEmitter extends AbstractEmitter implements Initializable, Stream
     public void initialize(Map<String, Param> params) throws TikaConfigException {
         //params have already been set...ignore them
         //TODO -- add other params to the builder as needed
-        storage = StorageOptions.newBuilder().setProjectId(projectId).build().getService();
+        storage = StorageOptions
+                .newBuilder()
+                .setProjectId(projectId)
+                .build()
+                .getService();
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         mustNotBeEmpty("bucket", this.bucket);
         mustNotBeEmpty("projectId", this.projectId);
     }
diff --git a/tika-pipes/tika-emitters/tika-emitter-gcs/src/test/java/org/apache/tika/pipes/emitter/gcs/TestGCSEmitter.java b/tika-pipes/tika-emitters/tika-emitter-gcs/src/test/java/org/apache/tika/pipes/emitter/gcs/TestGCSEmitter.java
index aaee49e92..1f288bf27 100644
--- a/tika-pipes/tika-emitters/tika-emitter-gcs/src/test/java/org/apache/tika/pipes/emitter/gcs/TestGCSEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-gcs/src/test/java/org/apache/tika/pipes/emitter/gcs/TestGCSEmitter.java
@@ -26,6 +26,7 @@ import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.Emitter;
 import org.apache.tika.pipes.emitter.EmitterManager;
 
@@ -42,10 +43,13 @@ public class TestGCSEmitter {
         m.add("k1", "v2");
         m.set("k2", "v3");
         metadataList.add(m);
-        emitter.emit("something-or-other/test-out", metadataList);
+        emitter.emit("something-or-other/test-out", metadataList, new ParseContext());
     }
 
     private Path getConfig(String configFile) throws URISyntaxException {
-        return Paths.get(this.getClass().getResource("/config/" + configFile).toURI());
+        return Paths.get(this
+                .getClass()
+                .getResource("/config/" + configFile)
+                .toURI());
     }
 }
diff --git a/tika-pipes/tika-emitters/tika-emitter-jdbc/src/main/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitter.java b/tika-pipes/tika-emitters/tika-emitter-jdbc/src/main/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitter.java
index 8e5965b54..016850660 100644
--- a/tika-pipes/tika-emitters/tika-emitter-jdbc/src/main/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-jdbc/src/main/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitter.java
@@ -50,6 +50,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.EmitData;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -263,7 +264,7 @@ public class JDBCEmitter extends AbstractEmitter implements Initializable, Close
      * @throws TikaEmitterException
      */
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext)
             throws IOException, TikaEmitterException {
         if (metadataList == null || metadataList.size() < 1) {
             return;
diff --git a/tika-pipes/tika-emitters/tika-emitter-jdbc/src/test/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitterTest.java b/tika-pipes/tika-emitters/tika-emitter-jdbc/src/test/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitterTest.java
index 97b8d2e0f..03a8472d5 100644
--- a/tika-pipes/tika-emitters/tika-emitter-jdbc/src/test/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitterTest.java
+++ b/tika-pipes/tika-emitters/tika-emitter-jdbc/src/test/java/org/apache/tika/pipes/emitter/jdbc/JDBCEmitterTest.java
@@ -40,6 +40,7 @@ import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.io.TempDir;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.Emitter;
 import org.apache.tika.pipes.emitter.EmitterManager;
 
@@ -69,7 +70,7 @@ public class JDBCEmitterTest {
                 "100002", "k6", "2022-11-04T17:10:15"});
         int id = 0;
         for (String[] d : data) {
-            emitter.emit("id" + id++, Collections.singletonList(m(d)));
+            emitter.emit("id" + id++, Collections.singletonList(m(d)), new ParseContext());
         }
 
         try (Connection connection = DriverManager.getConnection(connectionString)) {
@@ -119,7 +120,7 @@ public class JDBCEmitterTest {
         data.add(new String[]{"k1", "true", "k2", "some string3", "k3", "6", "k4", "102"});
         int id = 0;
         for (String[] d : data) {
-            emitter.emit("id" + id++, Collections.singletonList(m(d)));
+            emitter.emit("id" + id++, Collections.singletonList(m(d)), new ParseContext());
         }
 
         try (Connection connection = DriverManager.getConnection(connectionString)) {
@@ -155,7 +156,7 @@ public class JDBCEmitterTest {
         data.add(m("k1", "true", "k2", "some string1", "k3", "4", "k4", "100"));
         data.add(m("k1", "false", "k2", "some string2", "k3", "5", "k4", "101"));
         data.add(m("k1", "true", "k2", "some string3", "k3", "6", "k4", "102"));
-        emitter.emit("id0", data);
+        emitter.emit("id0", data, new ParseContext());
 
 
         try (Connection connection = DriverManager.getConnection(connectionString)) {
@@ -198,7 +199,7 @@ public class JDBCEmitterTest {
         m.add("k1", "third");
         m.add("k1", "fourth");
         data.add(m);
-        emitter.emit("id0", data);
+        emitter.emit("id0", data, new ParseContext());
 
         String expected = "first, second, third, fourth";
         int rows = 0;
@@ -235,7 +236,7 @@ public class JDBCEmitterTest {
         data.add(new String[]{"k1", "abcdefghijk"});
         int id = 0;
         for (String[] d : data) {
-            emitter.emit("id" + id++, Collections.singletonList(m(d)));
+            emitter.emit("id" + id++, Collections.singletonList(m(d)), new ParseContext());
         }
 
         int rows = 0;
diff --git a/tika-pipes/tika-emitters/tika-emitter-kafka/src/main/java/org/apache/tika/pipes/emitter/kafka/KafkaEmitter.java b/tika-pipes/tika-emitters/tika-emitter-kafka/src/main/java/org/apache/tika/pipes/emitter/kafka/KafkaEmitter.java
index 4076e7a40..7dd8f90ad 100644
--- a/tika-pipes/tika-emitters/tika-emitter-kafka/src/main/java/org/apache/tika/pipes/emitter/kafka/KafkaEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-kafka/src/main/java/org/apache/tika/pipes/emitter/kafka/KafkaEmitter.java
@@ -39,6 +39,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
 
@@ -193,7 +194,7 @@ public class KafkaEmitter extends AbstractEmitter implements Initializable {
     }
 
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext)
             throws IOException, TikaEmitterException {
         if (metadataList == null || metadataList.isEmpty()) {
             throw new TikaEmitterException("metadata list must not be null or of size 0");
diff --git a/tika-pipes/tika-emitters/tika-emitter-opensearch/src/main/java/org/apache/tika/pipes/emitter/opensearch/OpenSearchEmitter.java b/tika-pipes/tika-emitters/tika-emitter-opensearch/src/main/java/org/apache/tika/pipes/emitter/opensearch/OpenSearchEmitter.java
index bc010cf46..397dbda8e 100644
--- a/tika-pipes/tika-emitters/tika-emitter-opensearch/src/main/java/org/apache/tika/pipes/emitter/opensearch/OpenSearchEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-opensearch/src/main/java/org/apache/tika/pipes/emitter/opensearch/OpenSearchEmitter.java
@@ -34,6 +34,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.EmitData;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
@@ -86,7 +87,7 @@ public class OpenSearchEmitter extends AbstractEmitter implements Initializable
     }
 
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext)
             throws IOException, TikaEmitterException {
         if (metadataList == null || metadataList.size() == 0) {
             LOG.debug("metadataList is null or empty");
diff --git a/tika-pipes/tika-emitters/tika-emitter-s3/src/main/java/org/apache/tika/pipes/emitter/s3/S3Emitter.java b/tika-pipes/tika-emitters/tika-emitter-s3/src/main/java/org/apache/tika/pipes/emitter/s3/S3Emitter.java
index fe2f27aed..7f5537356 100644
--- a/tika-pipes/tika-emitters/tika-emitter-s3/src/main/java/org/apache/tika/pipes/emitter/s3/S3Emitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-s3/src/main/java/org/apache/tika/pipes/emitter/s3/S3Emitter.java
@@ -56,10 +56,11 @@ import org.apache.tika.io.TemporaryResources;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.StreamEmitter;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.utils.StringUtils;
 
 /**
@@ -120,35 +121,34 @@ public class S3Emitter extends AbstractEmitter implements Initializable, StreamE
      * @throws TikaException
      */
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
-            throws IOException, TikaEmitterException {
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext) throws IOException, TikaEmitterException {
         if (metadataList == null || metadataList.size() == 0) {
             throw new TikaEmitterException("metadata list must not be null or of size 0");
         }
 
         if (!spoolToTemp) {
-            UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream.builder().get();
-            try (Writer writer = new BufferedWriter(
-                    new OutputStreamWriter(bos, StandardCharsets.UTF_8))) {
+            UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream
+                    .builder()
+                    .get();
+            try (Writer writer = new BufferedWriter(new OutputStreamWriter(bos, StandardCharsets.UTF_8))) {
                 JsonMetadataList.toJson(metadataList, writer);
             } catch (IOException e) {
                 throw new TikaEmitterException("can't jsonify", e);
             }
             byte[] bytes = bos.toByteArray();
             try (InputStream is = TikaInputStream.get(bytes)) {
-                emit(emitKey, is, new Metadata());
+                emit(emitKey, is, new Metadata(), parseContext);
             }
         } else {
             try (TemporaryResources tmp = new TemporaryResources()) {
                 Path tmpPath = tmp.createTempFile();
-                try (Writer writer = Files.newBufferedWriter(tmpPath, StandardCharsets.UTF_8,
-                        StandardOpenOption.CREATE)) {
+                try (Writer writer = Files.newBufferedWriter(tmpPath, StandardCharsets.UTF_8, StandardOpenOption.CREATE)) {
                     JsonMetadataList.toJson(metadataList, writer);
                 } catch (IOException e) {
                     throw new TikaEmitterException("can't jsonify", e);
                 }
                 try (InputStream is = TikaInputStream.get(tmpPath)) {
-                    emit(emitKey, is, new Metadata());
+                    emit(emitKey, is, new Metadata(), parseContext);
                 }
             }
         }
@@ -161,8 +161,7 @@ public class S3Emitter extends AbstractEmitter implements Initializable, StreamE
      * @throws TikaEmitterException or IOexception if there is a Runtime s3 client exception
      */
     @Override
-    public void emit(String path, InputStream is, Metadata userMetadata)
-            throws IOException, TikaEmitterException {
+    public void emit(String path, InputStream is, Metadata userMetadata, ParseContext parseContext) throws IOException, TikaEmitterException {
 
         if (!StringUtils.isBlank(prefix)) {
             path = prefix + "/" + path;
@@ -178,9 +177,7 @@ public class S3Emitter extends AbstractEmitter implements Initializable, StreamE
         for (String n : userMetadata.names()) {
             String[] vals = userMetadata.getValues(n);
             if (vals.length > 1) {
-                LOGGER.warn("Can only write the first value for key {}. I see {} values.",
-                        n,
-                        vals.length);
+                LOGGER.warn("Can only write the first value for key {}. I see {} values.", n, vals.length);
             }
             objectMetadata.addUserMetadata(n, vals[0]);
         }
@@ -190,8 +187,7 @@ public class S3Emitter extends AbstractEmitter implements Initializable, StreamE
         if (is instanceof TikaInputStream) {
             if (((TikaInputStream) is).hasFile()) {
                 try {
-                    PutObjectRequest putObjectRequest = new PutObjectRequest(bucket, path,
-                            ((TikaInputStream) is).getFile()).withMetadata(objectMetadata);
+                    PutObjectRequest putObjectRequest = new PutObjectRequest(bucket, path, ((TikaInputStream) is).getFile()).withMetadata(objectMetadata);
                     s3Client.putObject(putObjectRequest);
                 } catch (IOException e) {
                     throw new TikaEmitterException("exception sending underlying file", e);
@@ -245,10 +241,8 @@ public class S3Emitter extends AbstractEmitter implements Initializable, StreamE
 
     @Field
     public void setCredentialsProvider(String credentialsProvider) {
-        if (!credentialsProvider.equals("profile") && !credentialsProvider.equals("instance")
-                && !credentialsProvider.equals("key_secret")) {
-            throw new IllegalArgumentException(
-                    "credentialsProvider must be either 'profile', 'instance' or 'key_secret'");
+        if (!credentialsProvider.equals("profile") && !credentialsProvider.equals("instance") && !credentialsProvider.equals("key_secret")) {
+            throw new IllegalArgumentException("credentialsProvider must be either 'profile', 'instance' or 'key_secret'");
         }
         this.credentialsProvider = credentialsProvider;
     }
@@ -308,20 +302,17 @@ public class S3Emitter extends AbstractEmitter implements Initializable, StreamE
         } else if (credentialsProvider.equals("key_secret")) {
             provider = new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey));
         } else {
-            throw new TikaConfigException("credentialsProvider must be set and " +
-                    "must be either 'instance', 'profile' or 'key_secret'");
+            throw new TikaConfigException("credentialsProvider must be set and " + "must be either 'instance', 'profile' or 'key_secret'");
         }
-        ClientConfiguration clientConfig = new ClientConfiguration()
-                .withMaxConnections(maxConnections);
+        ClientConfiguration clientConfig = new ClientConfiguration().withMaxConnections(maxConnections);
         try {
-            AmazonS3ClientBuilder amazonS3ClientBuilder = AmazonS3ClientBuilder.standard()
+            AmazonS3ClientBuilder amazonS3ClientBuilder = AmazonS3ClientBuilder
+                    .standard()
                     .withClientConfiguration(clientConfig)
                     .withCredentials(provider)
                     .withPathStyleAccessEnabled(pathStyleAccessEnabled);
             if (!StringUtils.isBlank(endpointConfigurationService)) {
-                amazonS3ClientBuilder.setEndpointConfiguration(
-                        new AwsClientBuilder
-                                .EndpointConfiguration(endpointConfigurationService, region));
+                amazonS3ClientBuilder.setEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpointConfigurationService, region));
             } else {
                 amazonS3ClientBuilder.withRegion(region);
             }
@@ -332,8 +323,7 @@ public class S3Emitter extends AbstractEmitter implements Initializable, StreamE
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         mustNotBeEmpty("bucket", this.bucket);
         mustNotBeEmpty("region", this.region);
     }
diff --git a/tika-pipes/tika-emitters/tika-emitter-solr/src/main/java/org/apache/tika/pipes/emitter/solr/SolrEmitter.java b/tika-pipes/tika-emitters/tika-emitter-solr/src/main/java/org/apache/tika/pipes/emitter/solr/SolrEmitter.java
index 7d638f78b..a139b9b7b 100644
--- a/tika-pipes/tika-emitters/tika-emitter-solr/src/main/java/org/apache/tika/pipes/emitter/solr/SolrEmitter.java
+++ b/tika-pipes/tika-emitters/tika-emitter-solr/src/main/java/org/apache/tika/pipes/emitter/solr/SolrEmitter.java
@@ -42,6 +42,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.emitter.AbstractEmitter;
 import org.apache.tika.pipes.emitter.EmitData;
 import org.apache.tika.pipes.emitter.TikaEmitterException;
@@ -73,7 +74,7 @@ public class SolrEmitter extends AbstractEmitter implements Initializable {
     }
 
     @Override
-    public void emit(String emitKey, List<Metadata> metadataList)
+    public void emit(String emitKey, List<Metadata> metadataList, ParseContext parseContext)
             throws IOException, TikaEmitterException {
         if (metadataList == null || metadataList.size() == 0) {
             LOG.warn("metadataList is null or empty");
diff --git a/tika-pipes/tika-emitters/tika-emitter-solr/src/test/java/org/apache/tika/pipes/emitter/solr/SolrEmitterDevTest.java b/tika-pipes/tika-emitters/tika-emitter-solr/src/test/java/org/apache/tika/pipes/emitter/solr/SolrEmitterDevTest.java
index cb7266503..1e68b1d4e 100644
--- a/tika-pipes/tika-emitters/tika-emitter-solr/src/test/java/org/apache/tika/pipes/emitter/solr/SolrEmitterDevTest.java
+++ b/tika-pipes/tika-emitters/tika-emitter-solr/src/test/java/org/apache/tika/pipes/emitter/solr/SolrEmitterDevTest.java
@@ -27,6 +27,7 @@ import org.junit.jupiter.api.Test;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
 import org.apache.tika.metadata.filter.FieldNameMappingFilter;
+import org.apache.tika.parser.ParseContext;
 
 /**
  * This is meant only for one off development tests with a locally
@@ -57,6 +58,6 @@ public class SolrEmitterDevTest {
         filter.setMappings(mappings);
         filter.filter(metadata);
 
-        solrEmitter.emit(emitKey, Collections.singletonList(metadata));
+        solrEmitter.emit(emitKey, Collections.singletonList(metadata), new ParseContext());
     }
 }
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/main/java/org/apache/tika/pipes/fetcher/azblob/AZBlobFetcher.java b/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/main/java/org/apache/tika/pipes/fetcher/azblob/AZBlobFetcher.java
index dee903040..d1f9e80d6 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/main/java/org/apache/tika/pipes/fetcher/azblob/AZBlobFetcher.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/main/java/org/apache/tika/pipes/fetcher/azblob/AZBlobFetcher.java
@@ -43,23 +43,24 @@ import org.apache.tika.exception.TikaException;
 import org.apache.tika.io.TemporaryResources;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.AbstractFetcher;
 import org.apache.tika.utils.StringUtils;
 
 /**
  * Fetches files from Azure blob storage.
- *
+ * <p>
  * There are two modes:
  * 1) If you are only using one endpoint and one sas token and one container,
- *    configure those in the config file.  In this case, your fetchKey will
- *    be the path in the container to the blob.
+ * configure those in the config file.  In this case, your fetchKey will
+ * be the path in the container to the blob.
  * 2) If you have different endpoints or sas tokens or containers across
- *    your requests, your fetchKey will be the complete SAS url pointing to the blob.
+ * your requests, your fetchKey will be the complete SAS url pointing to the blob.
  */
 public class AZBlobFetcher extends AbstractFetcher implements Initializable {
 
-    private static String PREFIX = "az-blob";
     private static final Logger LOGGER = LoggerFactory.getLogger(AZBlobFetcher.class);
+    private static String PREFIX = "az-blob";
     private String sasToken;
     private String container;
     private String endpoint;
@@ -70,7 +71,7 @@ public class AZBlobFetcher extends AbstractFetcher implements Initializable {
     private boolean spoolToTemp = true;
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws TikaException, IOException {
 
         LOGGER.debug("about to fetch fetchkey={} from endpoint ({})", fetchKey, endpoint);
 
@@ -80,7 +81,9 @@ public class AZBlobFetcher extends AbstractFetcher implements Initializable {
             if (extractUserMetadata) {
                 BlobProperties properties = blobClient.getProperties();
                 if (properties.getMetadata() != null) {
-                    for (Map.Entry<String, String> e : properties.getMetadata().entrySet()) {
+                    for (Map.Entry<String, String> e : properties
+                            .getMetadata()
+                            .entrySet()) {
                         metadata.add(PREFIX + ":" + e.getKey(), e.getValue());
                     }
                 }
@@ -123,6 +126,7 @@ public class AZBlobFetcher extends AbstractFetcher implements Initializable {
     public void setContainer(String container) {
         this.container = container;
     }
+
     /**
      * Whether or not to extract user metadata from the blob object
      *
@@ -152,11 +156,9 @@ public class AZBlobFetcher extends AbstractFetcher implements Initializable {
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         //if the user has set one of these, they need to have set all of them
-        if (!StringUtils.isBlank(this.sasToken) ||
-                !StringUtils.isBlank(this.endpoint) || !StringUtils.isBlank(this.container)) {
+        if (!StringUtils.isBlank(this.sasToken) || !StringUtils.isBlank(this.endpoint) || !StringUtils.isBlank(this.container)) {
             mustNotBeEmpty("sasToken", this.sasToken);
             mustNotBeEmpty("endpoint", this.endpoint);
             mustNotBeEmpty("container", this.container);
@@ -189,7 +191,9 @@ public class AZBlobFetcher extends AbstractFetcher implements Initializable {
 
         @Override
         public BlobClient getClient(String fetchKey) {
-            return new BlobClientBuilder().connectionString(fetchKey).buildClient();
+            return new BlobClientBuilder()
+                    .connectionString(fetchKey)
+                    .buildClient();
         }
     }
 }
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/test/java/org/apache/tika/pipes/fetcher/azblob/TestAZBlobFetcher.java b/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/test/java/org/apache/tika/pipes/fetcher/azblob/TestAZBlobFetcher.java
index b499ac9b1..1ba2cfdcd 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/test/java/org/apache/tika/pipes/fetcher/azblob/TestAZBlobFetcher.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-az-blob/src/test/java/org/apache/tika/pipes/fetcher/azblob/TestAZBlobFetcher.java
@@ -29,9 +29,10 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.TikaTest;
 import org.apache.tika.metadata.Metadata;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.Fetcher;
 import org.apache.tika.pipes.fetcher.FetcherManager;
+import org.apache.tika.serialization.JsonMetadataList;
 
 @Disabled("write actual unit tests")
 public class TestAZBlobFetcher extends TikaTest {
@@ -40,12 +41,13 @@ public class TestAZBlobFetcher extends TikaTest {
 
     @Test
     public void testConfig() throws Exception {
-        FetcherManager fetcherManager = FetcherManager.load(
-                Paths.get(this.getClass().getResource("/tika-config-az-blob.xml").toURI()));
+        FetcherManager fetcherManager = FetcherManager.load(Paths.get(this
+                .getClass()
+                .getResource("/tika-config-az-blob.xml")
+                .toURI()));
         Fetcher fetcher = fetcherManager.getFetcher("az-blob");
         List<Metadata> metadataList = null;
-        try (Reader reader = new BufferedReader(new InputStreamReader(
-                fetcher.fetch(FETCH_STRING, new Metadata()), StandardCharsets.UTF_8))) {
+        try (Reader reader = new BufferedReader(new InputStreamReader(fetcher.fetch(FETCH_STRING, new Metadata(), new ParseContext()), StandardCharsets.UTF_8))) {
             metadataList = JsonMetadataList.fromJson(reader);
         }
         debug(metadataList);
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/main/java/org/apache/tika/pipes/fetcher/gcs/GCSFetcher.java b/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/main/java/org/apache/tika/pipes/fetcher/gcs/GCSFetcher.java
index 6881c5a66..661d5f30d 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/main/java/org/apache/tika/pipes/fetcher/gcs/GCSFetcher.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/main/java/org/apache/tika/pipes/fetcher/gcs/GCSFetcher.java
@@ -39,6 +39,7 @@ import org.apache.tika.exception.TikaException;
 import org.apache.tika.io.TemporaryResources;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.AbstractFetcher;
 
 /**
@@ -55,7 +56,7 @@ public class GCSFetcher extends AbstractFetcher implements Initializable {
     private boolean spoolToTemp = true;
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws TikaException, IOException {
 
         LOGGER.debug("about to fetch fetchkey={} from bucket ({})", fetchKey, bucket);
 
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/test/java/org/apache/tika/pipes/fetcher/s3/TestGCSFetcher.java b/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/test/java/org/apache/tika/pipes/fetcher/s3/TestGCSFetcher.java
index 0d25cd1e5..e68552050 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/test/java/org/apache/tika/pipes/fetcher/s3/TestGCSFetcher.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-gcs/src/test/java/org/apache/tika/pipes/fetcher/s3/TestGCSFetcher.java
@@ -30,6 +30,7 @@ import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.io.TempDir;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.Fetcher;
 import org.apache.tika.pipes.fetcher.FetcherManager;
 
@@ -54,7 +55,7 @@ public class TestGCSFetcher {
                 Paths.get(this.getClass().getResource("/tika-config-gcs.xml").toURI()));
         Fetcher fetcher = fetcherManager.getFetcher("gcs");
         Metadata metadata = new Metadata();
-        try (InputStream is = fetcher.fetch(FETCH_STRING, metadata)) {
+        try (InputStream is = fetcher.fetch(FETCH_STRING, metadata, new ParseContext())) {
             Files.copy(is, outputFile, StandardCopyOption.REPLACE_EXISTING);
         }
         assertEquals(20743, Files.size(outputFile));
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-http/pom.xml b/tika-pipes/tika-fetchers/tika-fetcher-http/pom.xml
index e759879c1..c839ca6af 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-http/pom.xml
+++ b/tika-pipes/tika-fetchers/tika-fetcher-http/pom.xml
@@ -45,6 +45,19 @@
       <artifactId>tika-httpclient-commons</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <dependency>
+      <groupId>com.google.guava</groupId>
+      <artifactId>guava</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-databind</artifactId>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-annotations</artifactId>
+    </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
       <artifactId>tika-core</artifactId>
@@ -127,4 +140,4 @@
   <scm>
     <tag>3.0.0-BETA-rc1</tag>
   </scm>
-</project>
\ No newline at end of file
+</project>
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-http/src/main/java/org/apache/tika/pipes/fetcher/http/HttpFetcher.java b/tika-pipes/tika-fetchers/tika-fetcher-http/src/main/java/org/apache/tika/pipes/fetcher/http/HttpFetcher.java
index 26b45f8bf..f9a4ebe0d 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-http/src/main/java/org/apache/tika/pipes/fetcher/http/HttpFetcher.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-http/src/main/java/org/apache/tika/pipes/fetcher/http/HttpFetcher.java
@@ -60,15 +60,16 @@ import org.apache.tika.config.Initializable;
 import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
-import org.apache.tika.exception.TikaException;
 import org.apache.tika.exception.TikaTimeoutException;
 import org.apache.tika.io.TemporaryResources;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.Property;
 import org.apache.tika.metadata.TikaCoreProperties;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.AbstractFetcher;
 import org.apache.tika.pipes.fetcher.RangeFetcher;
+import org.apache.tika.pipes.fetcher.http.config.AdditionalHttpHeaders;
 import org.apache.tika.utils.StringUtils;
 
 /**
@@ -136,30 +137,39 @@ public class HttpFetcher extends AbstractFetcher implements Initializable, Range
 
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws IOException, TikaException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws IOException {
         HttpGet get = new HttpGet(fetchKey);
         RequestConfig requestConfig =
                 RequestConfig.custom()
                         .setMaxRedirects(maxRedirects)
                         .setRedirectsEnabled(true).build();
         get.setConfig(requestConfig);
-        if (! StringUtils.isBlank(userAgent)) {
-            get.setHeader(USER_AGENT, userAgent);
-        }
+        putAdditionalHeadersOnRequest(parseContext, get);
         return execute(get, metadata, httpClient, true);
     }
 
     @Override
-    public InputStream fetch(String fetchKey, long startRange, long endRange, Metadata metadata)
+    public InputStream fetch(String fetchKey, long startRange, long endRange, Metadata metadata, ParseContext parseContext)
             throws IOException {
         HttpGet get = new HttpGet(fetchKey);
-        if (! StringUtils.isBlank(userAgent)) {
-            get.setHeader(USER_AGENT, userAgent);
-        }
+        putAdditionalHeadersOnRequest(parseContext, get);
+
         get.setHeader("Range", "bytes=" + startRange + "-" + endRange);
         return execute(get, metadata, httpClient, true);
     }
 
+    private void putAdditionalHeadersOnRequest(ParseContext parseContext, HttpGet get) {
+        if (!StringUtils.isBlank(userAgent)) {
+            get.setHeader(USER_AGENT, userAgent);
+        }
+        AdditionalHttpHeaders additionalHttpHeaders = parseContext.get(AdditionalHttpHeaders.class);
+        if (additionalHttpHeaders != null) {
+            additionalHttpHeaders
+                    .getHeaders()
+                    .forEach(get::setHeader);
+        }
+    }
+
     private InputStream execute(HttpGet get, Metadata metadata, HttpClient client,
                                 boolean retryOnBadLength) throws IOException {
         HttpClientContext context = HttpClientContext.create();
@@ -455,4 +465,11 @@ public class HttpFetcher extends AbstractFetcher implements Initializable, Range
         this.httpClientFactory = httpClientFactory;
     }
 
+    public void setHttpClient(HttpClient httpClient) {
+        this.httpClient = httpClient;
+    }
+
+    public HttpClient getHttpClient() {
+        return httpClient;
+    }
 }
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-http/src/main/java/org/apache/tika/pipes/fetcher/http/config/AdditionalHttpHeaders.java b/tika-pipes/tika-fetchers/tika-fetcher-http/src/main/java/org/apache/tika/pipes/fetcher/http/config/AdditionalHttpHeaders.java
new file mode 100644
index 000000000..7d9989500
--- /dev/null
+++ b/tika-pipes/tika-fetchers/tika-fetcher-http/src/main/java/org/apache/tika/pipes/fetcher/http/config/AdditionalHttpHeaders.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.pipes.fetcher.http.config;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Objects;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.Multimap;
+
+public class AdditionalHttpHeaders {
+    @JsonIgnore
+    private Multimap<String, String> headers = ArrayListMultimap.create();
+
+    @JsonIgnore
+    public Multimap<String, String> getHeaders() {
+        return headers;
+    }
+
+    public Map<String, Collection<String>> getMap() {
+        return headers.asMap();
+    }
+
+    public void setMap(Map<String, Collection<String>> map) {
+        headers = ArrayListMultimap.create();
+        map.forEach(headers::putAll);
+    }
+
+    public void setHeaders(Multimap<String, String> headers) {
+        this.headers = headers;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) {
+            return true;
+        }
+        if (o == null || getClass() != o.getClass()) {
+            return false;
+        }
+        AdditionalHttpHeaders that = (AdditionalHttpHeaders) o;
+        return Objects.equals(headers, that.headers);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(headers);
+    }
+}
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-http/src/test/java/org/apache/tika/pipes/fetcher/http/HttpFetcherTest.java b/tika-pipes/tika-fetchers/tika-fetcher-http/src/test/java/org/apache/tika/pipes/fetcher/http/HttpFetcherTest.java
index e26e6cfcb..1fca7811e 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-http/src/test/java/org/apache/tika/pipes/fetcher/http/HttpFetcherTest.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-http/src/test/java/org/apache/tika/pipes/fetcher/http/HttpFetcherTest.java
@@ -37,13 +37,20 @@ import org.apache.commons.io.IOUtils;
 import org.apache.http.HttpEntity;
 import org.apache.http.HttpResponse;
 import org.apache.http.HttpStatus;
+import org.apache.http.ProtocolVersion;
 import org.apache.http.StatusLine;
 import org.apache.http.client.HttpClient;
+import org.apache.http.client.methods.CloseableHttpResponse;
+import org.apache.http.client.methods.HttpGet;
 import org.apache.http.client.methods.HttpUriRequest;
+import org.apache.http.entity.StringEntity;
 import org.apache.http.protocol.HttpContext;
+import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
 
 import org.apache.tika.TikaTest;
 import org.apache.tika.client.HttpClientFactory;
@@ -51,7 +58,9 @@ import org.apache.tika.exception.TikaException;
 import org.apache.tika.io.TemporaryResources;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.FetcherManager;
+import org.apache.tika.pipes.fetcher.http.config.AdditionalHttpHeaders;
 
 public class HttpFetcherTest extends TikaTest {
 
@@ -73,7 +82,7 @@ public class HttpFetcherTest extends TikaTest {
         final Metadata meta = new Metadata();
         meta.set(TikaCoreProperties.RESOURCE_NAME_KEY, "fileName");
 
-        try (final InputStream ignored = httpFetcher.fetch(TEST_URL, meta)) {
+        try (final InputStream ignored = httpFetcher.fetch(TEST_URL, meta, new ParseContext())) {
             // HTTP headers added into meta
             assertEquals("200", meta.get("http-header:status-code"));
             assertEquals(TEST_URL, meta.get("http-connection:target-url"));
@@ -91,7 +100,7 @@ public class HttpFetcherTest extends TikaTest {
         mockClientResponse(buildMockResponse(HttpStatus.SC_FORBIDDEN, null));
 
         final Metadata meta = new Metadata();
-        assertThrows(IOException.class, () -> httpFetcher.fetch(TEST_URL, meta));
+        assertThrows(IOException.class, () -> httpFetcher.fetch(TEST_URL, meta, new ParseContext()));
 
         // Meta still populated
         assertEquals("403", meta.get("http-header:status-code"));
@@ -106,7 +115,7 @@ public class HttpFetcherTest extends TikaTest {
         Metadata metadata = new Metadata();
         HttpFetcher httpFetcher =
                 (HttpFetcher) getFetcherManager("tika-config-http.xml").getFetcher("http");
-        try (InputStream is = httpFetcher.fetch(url, metadata)) {
+        try (InputStream is = httpFetcher.fetch(url, metadata, new ParseContext())) {
             IOUtils.copy(is, bos);
         }
         //debug(metadata);
@@ -131,6 +140,45 @@ public class HttpFetcherTest extends TikaTest {
         }
     }
 
+    @Test
+    public void testHttpRequestHeaders() throws Exception {
+        HttpClient httpClient = Mockito.mock(HttpClient.class);
+        httpFetcher.setHttpClient(httpClient);
+        CloseableHttpResponse response = mock(CloseableHttpResponse.class);
+        ArgumentCaptor<HttpGet> httpGetArgumentCaptor = ArgumentCaptor.forClass(HttpGet.class);
+
+        when(httpClient.execute(httpGetArgumentCaptor.capture(), any(HttpContext.class)))
+                .thenReturn(response);
+        when(response.getStatusLine()).thenReturn(new StatusLine() {
+            @Override
+            public ProtocolVersion getProtocolVersion() {
+                return new HttpGet("http://localhost").getProtocolVersion();
+            }
+
+            @Override
+            public int getStatusCode() {
+                return 200;
+            }
+
+            @Override
+            public String getReasonPhrase() {
+                return null;
+            }
+        });
+
+        when(response.getEntity()).thenReturn(new StringEntity("Hi"));
+
+        Metadata metadata = new Metadata();
+        ParseContext parseContext = new ParseContext();
+        AdditionalHttpHeaders additionalHttpHeaders = new AdditionalHttpHeaders();
+        additionalHttpHeaders.getHeaders().put("nick1", "val1");
+        additionalHttpHeaders.getHeaders().put("nick2", "val2");
+        parseContext.set(AdditionalHttpHeaders.class, additionalHttpHeaders);
+        httpFetcher.fetch("http://localhost", metadata, parseContext);
+        HttpGet httpGet = httpGetArgumentCaptor.getValue();
+        Assertions.assertEquals("val1", httpGet.getHeaders("nick1")[0].getValue());
+        Assertions.assertEquals("val2", httpGet.getHeaders("nick2")[0].getValue());
+    }
 
     FetcherManager getFetcherManager(String path) throws Exception {
         return FetcherManager.load(
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-http/src/test/java/org/apache/tika/pipes/fetcher/http/config/AdditionalHttpHeadersTest.java b/tika-pipes/tika-fetchers/tika-fetcher-http/src/test/java/org/apache/tika/pipes/fetcher/http/config/AdditionalHttpHeadersTest.java
new file mode 100644
index 000000000..4c45acc66
--- /dev/null
+++ b/tika-pipes/tika-fetchers/tika-fetcher-http/src/test/java/org/apache/tika/pipes/fetcher/http/config/AdditionalHttpHeadersTest.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.pipes.fetcher.http.config;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.junit.jupiter.api.Test;
+
+class AdditionalHttpHeadersTest {
+
+    ObjectMapper om = new ObjectMapper();
+
+    @Test
+    void testToAndFromJson() throws JsonProcessingException {
+        AdditionalHttpHeaders additionalHttpHeaders = new AdditionalHttpHeaders();
+        additionalHttpHeaders.getHeaders().put("nick1", "val1");
+        additionalHttpHeaders.getHeaders().put("nick2", "val2");
+
+        String json = om.writeValueAsString(additionalHttpHeaders);
+
+        AdditionalHttpHeaders additionalHttpHeaders2 = om.readValue(json, AdditionalHttpHeaders.class);
+        assertEquals(additionalHttpHeaders, additionalHttpHeaders2);
+    }
+}
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-s3/src/main/java/org/apache/tika/pipes/fetcher/s3/S3Fetcher.java b/tika-pipes/tika-fetchers/tika-fetcher-s3/src/main/java/org/apache/tika/pipes/fetcher/s3/S3Fetcher.java
index b57c361b9..7283c9727 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-s3/src/main/java/org/apache/tika/pipes/fetcher/s3/S3Fetcher.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-s3/src/main/java/org/apache/tika/pipes/fetcher/s3/S3Fetcher.java
@@ -55,6 +55,7 @@ import org.apache.tika.io.FilenameUtils;
 import org.apache.tika.io.TemporaryResources;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.AbstractFetcher;
 import org.apache.tika.pipes.fetcher.RangeFetcher;
 import org.apache.tika.utils.StringUtils;
@@ -106,12 +107,12 @@ public class S3Fetcher extends AbstractFetcher implements Initializable, RangeFe
     private boolean pathStyleAccessEnabled = false;
 
     @Override
-    public InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException {
+    public InputStream fetch(String fetchKey, Metadata metadata, ParseContext parseContext) throws TikaException, IOException {
         return fetch(fetchKey, -1, -1, metadata);
     }
 
     @Override
-    public InputStream fetch(String fetchKey, long startRange, long endRange, Metadata metadata)
+    public InputStream fetch(String fetchKey, long startRange, long endRange, Metadata metadata, ParseContext parseContext)
             throws TikaException, IOException {
         String theFetchKey = StringUtils.isBlank(prefix) ? fetchKey : prefix + fetchKey;
 
diff --git a/tika-pipes/tika-fetchers/tika-fetcher-s3/src/test/java/org/apache/tika/pipes/fetcher/s3/TestS3Fetcher.java b/tika-pipes/tika-fetchers/tika-fetcher-s3/src/test/java/org/apache/tika/pipes/fetcher/s3/TestS3Fetcher.java
index 0a05ac247..0055bf68a 100644
--- a/tika-pipes/tika-fetchers/tika-fetcher-s3/src/test/java/org/apache/tika/pipes/fetcher/s3/TestS3Fetcher.java
+++ b/tika-pipes/tika-fetchers/tika-fetcher-s3/src/test/java/org/apache/tika/pipes/fetcher/s3/TestS3Fetcher.java
@@ -27,6 +27,7 @@ import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.Fetcher;
 import org.apache.tika.pipes.fetcher.FetcherManager;
 
@@ -45,7 +46,7 @@ public class TestS3Fetcher {
         fetcher.initialize(Collections.EMPTY_MAP);
 
         Metadata metadata = new Metadata();
-        try (InputStream is = fetcher.fetch(FETCH_STRING, metadata)) {
+        try (InputStream is = fetcher.fetch(FETCH_STRING, metadata, new ParseContext())) {
             Files.copy(is, outputFile, StandardCopyOption.REPLACE_EXISTING);
         }
     }
@@ -56,7 +57,7 @@ public class TestS3Fetcher {
                 Paths.get(this.getClass().getResource("/tika-config-s3.xml").toURI()));
         Fetcher fetcher = fetcherManager.getFetcher("s3");
         Metadata metadata = new Metadata();
-        try (InputStream is = fetcher.fetch(FETCH_STRING, metadata)) {
+        try (InputStream is = fetcher.fetch(FETCH_STRING, metadata, new ParseContext())) {
             Files.copy(is, outputFile, StandardCopyOption.REPLACE_EXISTING);
         }
     }
diff --git a/tika-pipes/tika-pipes-iterators/pom.xml b/tika-pipes/tika-pipes-iterators/pom.xml
index 5cb99fbd1..1a04b710d 100644
--- a/tika-pipes/tika-pipes-iterators/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/pom.xml
index 907f3311c..5024db2f0 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/src/main/java/org/apache/tika/pipes/pipesiterator/azblob/AZBlobPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/src/main/java/org/apache/tika/pipes/pipesiterator/azblob/AZBlobPipesIterator.java
index b207800cc..0c5d6840d 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/src/main/java/org/apache/tika/pipes/pipesiterator/azblob/AZBlobPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-az-blob/src/main/java/org/apache/tika/pipes/pipesiterator/azblob/AZBlobPipesIterator.java
@@ -40,6 +40,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -94,30 +95,26 @@ public class AZBlobPipesIterator extends PipesIterator implements Initializable
 
         PagedIterable<BlobItem> blobs = null;
         if (StringUtils.isBlank(prefix)) {
-            ListBlobsOptions options = new ListBlobsOptions()
-                    .setDetails(
-                            new BlobListDetails()
-                                    .setRetrieveDeletedBlobs(false)
-                                    .setRetrieveMetadata(false)
-                                    .setRetrieveSnapshots(false));
-            blobs = blobContainerClient.listBlobs(options,
-                    Duration.of(timeoutMillis, ChronoUnit.MILLIS));
+            ListBlobsOptions options = new ListBlobsOptions().setDetails(new BlobListDetails()
+                    .setRetrieveDeletedBlobs(false)
+                    .setRetrieveMetadata(false)
+                    .setRetrieveSnapshots(false));
+            blobs = blobContainerClient.listBlobs(options, Duration.of(timeoutMillis, ChronoUnit.MILLIS));
         } else {
             ListBlobsOptions options = new ListBlobsOptions()
                     .setPrefix(prefix)
-                    .setDetails(
-                            new BlobListDetails()
-                                    .setRetrieveDeletedBlobs(false)
-                                    .setRetrieveMetadata(false)
-                                    .setRetrieveSnapshots(false));
-            blobs = blobContainerClient.listBlobs(options,
-                    Duration.of(timeoutMillis, ChronoUnit.MILLIS));
+                    .setDetails(new BlobListDetails()
+                            .setRetrieveDeletedBlobs(false)
+                            .setRetrieveMetadata(false)
+                            .setRetrieveSnapshots(false));
+            blobs = blobContainerClient.listBlobs(options, Duration.of(timeoutMillis, ChronoUnit.MILLIS));
         }
 
         for (BlobItem blob : blobs) {
             //tried blob.isPrefix() and got NPE ... user error?
-            if (blob == null || blob.getProperties() == null ||
-                    blob.getProperties().getContentLength() == 0) {
+            if (blob == null || blob.getProperties() == null || blob
+                    .getProperties()
+                    .getContentLength() == 0) {
                 continue;
             }
             long elapsed = System.currentTimeMillis() - start;
@@ -125,9 +122,9 @@ public class AZBlobPipesIterator extends PipesIterator implements Initializable
                 LOGGER.debug("adding ({}) {} in {} ms", count, blob.getName(), elapsed);
             }
             //TODO -- extract metadata about content length etc from properties
-            tryToAdd(new FetchEmitTuple(blob.getName(), new FetchKey(fetcherName,
-                    blob.getName()),
-                    new EmitKey(emitterName, blob.getName()), new Metadata(), handlerConfig,
+            ParseContext parseContext = new ParseContext();
+            parseContext.set(HandlerConfig.class, handlerConfig);
+            tryToAdd(new FetchEmitTuple(blob.getName(), new FetchKey(fetcherName, blob.getName()), new EmitKey(emitterName, blob.getName()), new Metadata(), parseContext,
                     getOnParseException()));
             count++;
         }
@@ -146,8 +143,7 @@ public class AZBlobPipesIterator extends PipesIterator implements Initializable
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         mustNotBeEmpty("sasToken", this.sasToken);
         mustNotBeEmpty("endpoint", this.endpoint);
         mustNotBeEmpty("container", this.container);
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/pom.xml
index 39387044a..ecd05045c 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/main/java/org/apache/tika/pipes/pipesiterator/csv/CSVPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/main/java/org/apache/tika/pipes/pipesiterator/csv/CSVPipesIterator.java
index 8fb441d8b..1f53a22d3 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/main/java/org/apache/tika/pipes/pipesiterator/csv/CSVPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/main/java/org/apache/tika/pipes/pipesiterator/csv/CSVPipesIterator.java
@@ -40,6 +40,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -134,32 +135,28 @@ public class CSVPipesIterator extends PipesIterator implements Initializable {
                 String fetchKey = record.get(fetchEmitKeyIndices.fetchKeyIndex);
                 String emitKey = record.get(fetchEmitKeyIndices.emitKeyIndex);
                 if (StringUtils.isBlank(fetchKey) && !StringUtils.isBlank(fetcherName)) {
-                    LOGGER.debug("Fetcher specified ({}), but no fetchkey was found in ({})",
-                            fetcherName, record);
+                    LOGGER.debug("Fetcher specified ({}), but no fetchkey was found in ({})", fetcherName, record);
                 }
                 if (StringUtils.isBlank(emitKey)) {
                     throw new IOException("emitKey must not be blank in :" + record);
                 }
 
                 Metadata metadata = loadMetadata(fetchEmitKeyIndices, headers, record);
-                tryToAdd(new FetchEmitTuple(id, new FetchKey(fetcherName, fetchKey),
-                        new EmitKey(emitterName, emitKey), metadata, handlerConfig,
-                        getOnParseException()));
+                ParseContext parseContext = new ParseContext();
+                parseContext.set(HandlerConfig.class, handlerConfig);
+                tryToAdd(new FetchEmitTuple(id, new FetchKey(fetcherName, fetchKey), new EmitKey(emitterName, emitKey), metadata, parseContext, getOnParseException()));
             }
         }
     }
 
-    private void checkFetchEmitValidity(String fetcherName, String emitterName,
-                                        FetchEmitKeyIndices fetchEmitKeyIndices,
-                                        List<String> headers) throws TikaConfigException {
+    private void checkFetchEmitValidity(String fetcherName, String emitterName, FetchEmitKeyIndices fetchEmitKeyIndices, List<String> headers) throws TikaConfigException {
 
         if (StringUtils.isBlank(emitterName)) {
             throw new TikaConfigException("must specify at least an emitterName");
         }
 
         if (StringUtils.isBlank(fetcherName) && !StringUtils.isBlank(fetchKeyColumn)) {
-            new TikaConfigException("If specifying a 'fetchKeyColumn', " +
-                    "you must also specify a 'fetcherName'");
+            new TikaConfigException("If specifying a 'fetchKeyColumn', " + "you must also specify a 'fetcherName'");
         }
 
         if (StringUtils.isBlank(fetcherName)) {
@@ -171,35 +168,26 @@ public class CSVPipesIterator extends PipesIterator implements Initializable {
         }
         //if a fetchkeycolumn is specified, make sure that it was found
         if (!StringUtils.isBlank(fetchKeyColumn) && fetchEmitKeyIndices.fetchKeyIndex < 0) {
-            throw new TikaConfigException(
-                    "Couldn't find fetchKeyColumn (" + fetchKeyColumn + " in header.\n" +
-                            "These are the headers I see: " + headers);
+            throw new TikaConfigException("Couldn't find fetchKeyColumn (" + fetchKeyColumn + " in header.\n" + "These are the headers I see: " + headers);
         }
 
         //if an emitkeycolumn is specified, make sure that it was found
         if (!StringUtils.isBlank(emitKeyColumn) && fetchEmitKeyIndices.emitKeyIndex < 0) {
-            throw new TikaConfigException(
-                    "Couldn't find emitKeyColumn (" + emitKeyColumn + " in header.\n" +
-                            "These are the headers I see: " + headers);
+            throw new TikaConfigException("Couldn't find emitKeyColumn (" + emitKeyColumn + " in header.\n" + "These are the headers I see: " + headers);
         }
 
         //if an idcolumn is specified, make sure that it was found
         if (!StringUtils.isBlank(idColumn) && fetchEmitKeyIndices.idIndex < 0) {
-            throw new TikaConfigException(
-                    "Couldn't find idColumn (" + idColumn + " in header.\n" +
-                            "These are the headers I see: " + headers);
+            throw new TikaConfigException("Couldn't find idColumn (" + idColumn + " in header.\n" + "These are the headers I see: " + headers);
         }
 
         if (StringUtils.isBlank(emitKeyColumn)) {
-            LOGGER.warn("No emitKeyColumn specified. " +
-                            "Will use fetchKeyColumn ({}) for both the fetch key and emit key",
-                    fetchKeyColumn);
+            LOGGER.warn("No emitKeyColumn specified. " + "Will use fetchKeyColumn ({}) for both the fetch key and emit key", fetchKeyColumn);
         }
 
     }
 
-    private Metadata loadMetadata(FetchEmitKeyIndices fetchEmitKeyIndices, List<String> headers,
-                                  CSVRecord record) {
+    private Metadata loadMetadata(FetchEmitKeyIndices fetchEmitKeyIndices, List<String> headers, CSVRecord record) {
         Metadata metadata = new Metadata();
         for (int i = 0; i < record.size(); i++) {
             if (fetchEmitKeyIndices.shouldSkip(i)) {
@@ -211,16 +199,14 @@ public class CSVPipesIterator extends PipesIterator implements Initializable {
     }
 
 
-    private FetchEmitKeyIndices loadHeaders(CSVRecord record, List<String> headers)
-            throws IOException {
+    private FetchEmitKeyIndices loadHeaders(CSVRecord record, List<String> headers) throws IOException {
         int fetchKeyColumnIndex = -1;
         int emitKeyColumnIndex = -1;
         int idIndex = -1;
         for (int col = 0; col < record.size(); col++) {
             String header = record.get(col);
             if (StringUtils.isBlank(header)) {
-                throw new IOException(
-                        new TikaException("Header in column (" + col + ") must not be empty"));
+                throw new IOException(new TikaException("Header in column (" + col + ") must not be empty"));
             }
             headers.add(header);
             if (header.equals(fetchKeyColumn)) {
@@ -245,15 +231,14 @@ public class CSVPipesIterator extends PipesIterator implements Initializable {
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         super.checkInitialization(problemHandler);
         mustNotBeEmpty("csvPath", this.csvPath);
     }
 
     private static class FetchEmitKeyIndices {
-        private int idIndex;
         private final int fetchKeyIndex;
+        private int idIndex;
         private int emitKeyIndex;
 
         public FetchEmitKeyIndices(int idIndex, int fetchKeyIndex, int emitKeyIndex) {
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/test/java/TestCSVPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/test/java/TestCSVPipesIterator.java
index 173524fda..772c399cc 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/test/java/TestCSVPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-csv/src/test/java/TestCSVPipesIterator.java
@@ -76,10 +76,15 @@ public class TestCSVPipesIterator {
         assertEquals(5, completed);
         for (MockFetcher f : fetchers) {
             for (FetchEmitTuple t : f.pairs) {
-                String id = t.getMetadata().get("id");
-                assertEquals("path/to/my/file" + id, t.getFetchKey().getFetchKey());
-                assertEquals("project" + (Integer.parseInt(id) % 2 == 1 ? "a" : "b"),
-                        t.getMetadata().get("project"));
+                String id = t
+                        .getMetadata()
+                        .get("id");
+                assertEquals("path/to/my/file" + id, t
+                        .getFetchKey()
+                        .getFetchKey());
+                assertEquals("project" + (Integer.parseInt(id) % 2 == 1 ? "a" : "b"), t
+                        .getMetadata()
+                        .get("project"));
             }
         }
     }
@@ -99,7 +104,9 @@ public class TestCSVPipesIterator {
     }
 
     private Path get(String testFileName) throws Exception {
-        return Paths.get(TestCSVPipesIterator.class.getResource("/" + testFileName).toURI());
+        return Paths.get(TestCSVPipesIterator.class
+                .getResource("/" + testFileName)
+                .toURI());
     }
 
     private static class MockFetcher implements Callable<Integer> {
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/pom.xml
index ae74ab206..bb2f2500f 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/main/java/org/apache/tika/pipes/pipesiterator/gcs/GCSPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/main/java/org/apache/tika/pipes/pipesiterator/gcs/GCSPipesIterator.java
index a9d052bf5..248d2461e 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/main/java/org/apache/tika/pipes/pipesiterator/gcs/GCSPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/main/java/org/apache/tika/pipes/pipesiterator/gcs/GCSPipesIterator.java
@@ -35,6 +35,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -75,12 +76,15 @@ public class GCSPipesIterator extends PipesIterator implements Initializable {
     @Override
     public void initialize(Map<String, Param> params) throws TikaConfigException {
         //TODO -- add other params to the builder as needed
-        storage = StorageOptions.newBuilder().setProjectId(projectId).build().getService();
+        storage = StorageOptions
+                .newBuilder()
+                .setProjectId(projectId)
+                .build()
+                .getService();
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         super.checkInitialization(problemHandler);
         mustNotBeEmpty("bucket", this.bucket);
         mustNotBeEmpty("projectId", this.projectId);
@@ -98,8 +102,7 @@ public class GCSPipesIterator extends PipesIterator implements Initializable {
         if (StringUtils.isBlank(prefix)) {
             blobs = storage.list(bucket);
         } else {
-            blobs = storage.list(bucket,
-                    Storage.BlobListOption.prefix(prefix));
+            blobs = storage.list(bucket, Storage.BlobListOption.prefix(prefix));
         }
 
         for (Blob blob : blobs.iterateAll()) {
@@ -111,9 +114,9 @@ public class GCSPipesIterator extends PipesIterator implements Initializable {
             long elapsed = System.currentTimeMillis() - start;
             LOGGER.debug("adding ({}) {} in {} ms", count, blob.getName(), elapsed);
             //TODO -- allow user specified metadata as the "id"?
-            tryToAdd(new FetchEmitTuple(blob.getName(), new FetchKey(fetcherName,
-                    blob.getName()),
-                    new EmitKey(emitterName, blob.getName()), new Metadata(), handlerConfig,
+            ParseContext parseContext = new ParseContext();
+            parseContext.set(HandlerConfig.class, handlerConfig);
+            tryToAdd(new FetchEmitTuple(blob.getName(), new FetchKey(fetcherName, blob.getName()), new EmitKey(emitterName, blob.getName()), new Metadata(), parseContext,
                     getOnParseException()));
             count++;
         }
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/test/java/org/apache/tika/pipes/pipesiterator/gcs/TestGCSPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/test/java/org/apache/tika/pipes/pipesiterator/gcs/TestGCSPipesIterator.java
index 5fa51ab65..c6f4b2773 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/test/java/org/apache/tika/pipes/pipesiterator/gcs/TestGCSPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-gcs/src/test/java/org/apache/tika/pipes/pipesiterator/gcs/TestGCSPipesIterator.java
@@ -34,7 +34,6 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.pipesiterator.PipesIterator;
-import org.apache.tika.pipes.pipesiterator.gcs.GCSPipesIterator;
 
 @Disabled("turn into an actual unit test")
 public class TestGCSPipesIterator {
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/pom.xml
index 885691622..6b3bd926b 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/main/java/org/apache/tika/pipes/pipesiterator/jdbc/JDBCPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/main/java/org/apache/tika/pipes/pipesiterator/jdbc/JDBCPipesIterator.java
index 5dafee713..2c178e147 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/main/java/org/apache/tika/pipes/pipesiterator/jdbc/JDBCPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/main/java/org/apache/tika/pipes/pipesiterator/jdbc/JDBCPipesIterator.java
@@ -39,6 +39,7 @@ import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -157,12 +158,10 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
                 while (rs.next()) {
                     if (headers.size() == 0) {
                         fetchEmitKeyIndices = loadHeaders(rs.getMetaData(), headers);
-                        checkFetchEmitValidity(fetcherName, emitterName, fetchEmitKeyIndices,
-                                headers);
+                        checkFetchEmitValidity(fetcherName, emitterName, fetchEmitKeyIndices, headers);
                     }
                     try {
-                        processRow(fetcherName, emitterName, headers, fetchEmitKeyIndices, rs,
-                                handlerConfig);
+                        processRow(fetcherName, emitterName, headers, fetchEmitKeyIndices, rs, handlerConfig);
                     } catch (SQLException e) {
                         LOGGER.warn("Failed to insert: " + rs, e);
                     }
@@ -184,21 +183,16 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
         }
     }
 
-    private void checkFetchEmitValidity(String fetcherName, String emitterName,
-                                        FetchEmitKeyIndices fetchEmitKeyIndices,
-                                        List<String> headers) throws IOException {
+    private void checkFetchEmitValidity(String fetcherName, String emitterName, FetchEmitKeyIndices fetchEmitKeyIndices, List<String> headers) throws IOException {
 
         if (!StringUtils.isBlank(fetchKeyColumn) && fetchEmitKeyIndices.fetchKeyIndex < 0) {
-            throw new IOException(
-                    new TikaConfigException("Couldn't find fetchkey column: " + fetchKeyColumn));
+            throw new IOException(new TikaConfigException("Couldn't find fetchkey column: " + fetchKeyColumn));
         }
         if (!StringUtils.isBlank(emitKeyColumn) && fetchEmitKeyIndices.emitKeyIndex < 0) {
-            throw new IOException(
-                    new TikaConfigException("Couldn't find emitKey column: " + emitKeyColumn));
+            throw new IOException(new TikaConfigException("Couldn't find emitKey column: " + emitKeyColumn));
         }
         if (!StringUtils.isBlank(idColumn) && fetchEmitKeyIndices.idIndex < 0) {
-            throw new IOException(
-                    new TikaConfigException("Couldn't find id column: " + idColumn));
+            throw new IOException(new TikaConfigException("Couldn't find id column: " + idColumn));
         }
         if (StringUtils.isBlank(idColumn)) {
             LOGGER.warn("id column is blank, using fetchkey column as the id column");
@@ -206,9 +200,7 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
         }
     }
 
-    private void processRow(String fetcherName, String emitterName, List<String> headers,
-                            FetchEmitKeyIndices fetchEmitKeyIndices, ResultSet rs,
-                            HandlerConfig handlerConfig)
+    private void processRow(String fetcherName, String emitterName, List<String> headers, FetchEmitKeyIndices fetchEmitKeyIndices, ResultSet rs, HandlerConfig handlerConfig)
             throws SQLException, TimeoutException, InterruptedException {
         Metadata metadata = new Metadata();
         String fetchKey = "";
@@ -216,7 +208,9 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
         long fetchEndRange = -1l;
         String emitKey = "";
         String id = "";
-        for (int i = 1; i <= rs.getMetaData().getColumnCount(); i++) {
+        for (int i = 1; i <= rs
+                .getMetaData()
+                .getColumnCount(); i++) {
             //a single column can be the fetch key and the emit key, etc.
             boolean isUsed = false;
             if (i == fetchEmitKeyIndices.fetchKeyIndex) {
@@ -252,25 +246,34 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
                 isUsed = true;
 
             }
-            if (! isUsed) {
+            if (!isUsed) {
                 String val = getString(i, rs);
-                if (! StringUtils.isBlank(val)) {
+                if (!StringUtils.isBlank(val)) {
                     metadata.set(headers.get(i - 1), val);
                 }
             }
         }
-
-        tryToAdd(new FetchEmitTuple(id, new FetchKey(fetcherName, fetchKey, fetchStartRange, fetchEndRange),
-                new EmitKey(emitterName, emitKey), metadata, handlerConfig, getOnParseException()));
+        ParseContext parseContext = new ParseContext();
+        parseContext.set(HandlerConfig.class, handlerConfig);
+        tryToAdd(new FetchEmitTuple(id, new FetchKey(fetcherName, fetchKey, fetchStartRange, fetchEndRange), new EmitKey(emitterName, emitKey), metadata, parseContext,
+                getOnParseException()));
     }
 
     private String toString(ResultSet rs) throws SQLException {
         StringBuilder sb = new StringBuilder();
-        for (int i = 1; i <= rs.getMetaData().getColumnCount(); i++) {
+        for (int i = 1; i <= rs
+                .getMetaData()
+                .getColumnCount(); i++) {
             String val = rs.getString(i);
             val = (val == null) ? "" : val;
             val = (val.length() > 100) ? val.substring(0, 100) : val;
-            sb.append(rs.getMetaData().getColumnLabel(i)).append(":").append(val).append("\n");
+            sb
+                    .append(rs
+                            .getMetaData()
+                            .getColumnLabel(i))
+                    .append(":")
+                    .append(val)
+                    .append("\n");
         }
         return sb.toString();
     }
@@ -293,8 +296,7 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
     }
 
 
-    private FetchEmitKeyIndices loadHeaders(ResultSetMetaData metaData, List<String> headers)
-            throws SQLException {
+    private FetchEmitKeyIndices loadHeaders(ResultSetMetaData metaData, List<String> headers) throws SQLException {
         int idIndex = -1;
         int fetchKeyIndex = -1;
         int fetchKeyStartRangeIndex = -1;
@@ -319,8 +321,7 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
             }
             headers.add(metaData.getColumnLabel(i));
         }
-        return new FetchEmitKeyIndices(idIndex, fetchKeyIndex,
-                fetchKeyStartRangeIndex, fetchKeyEndRangeIndex, emitKeyIndex);
+        return new FetchEmitKeyIndices(idIndex, fetchKeyIndex, fetchKeyStartRangeIndex, fetchKeyEndRangeIndex, emitKeyIndex);
     }
 
     @Override
@@ -333,20 +334,17 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         super.checkInitialization(problemHandler);
         mustNotBeEmpty("connection", this.connection);
         mustNotBeEmpty("select", this.select);
 
         if (StringUtils.isBlank(getFetcherName()) && !StringUtils.isBlank(fetchKeyColumn)) {
-            throw new TikaConfigException(
-                    "If you specify a 'fetchKeyColumn', you must specify a 'fetcherName'");
+            throw new TikaConfigException("If you specify a 'fetchKeyColumn', you must specify a 'fetcherName'");
         }
 
         if (StringUtils.isBlank(getEmitterName()) && !StringUtils.isBlank(emitKeyColumn)) {
-            throw new TikaConfigException(
-                    "If you specify an 'emitKeyColumn', you must specify an 'emitterName'");
+            throw new TikaConfigException("If you specify an 'emitKeyColumn', you must specify an 'emitterName'");
         }
 
         if (StringUtils.isBlank(getEmitterName()) && StringUtils.isBlank(getFetcherName())) {
@@ -360,15 +358,13 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
     }
 
     private static class FetchEmitKeyIndices {
-        private int idIndex;
         private final int fetchKeyIndex;
         private final int fetchStartRangeIndex;
         private final int fetchEndRangeIndex;
         private final int emitKeyIndex;
+        private int idIndex;
 
-        public FetchEmitKeyIndices(int idIndex, int fetchKeyIndex,
-                                   int fetchStartRangeIndex, int fetchEndRangeIndex,
-                                   int emitKeyIndex) {
+        public FetchEmitKeyIndices(int idIndex, int fetchKeyIndex, int fetchStartRangeIndex, int fetchEndRangeIndex, int emitKeyIndex) {
             this.idIndex = idIndex;
             this.fetchKeyIndex = fetchKeyIndex;
             this.fetchStartRangeIndex = fetchStartRangeIndex;
@@ -377,9 +373,7 @@ public class JDBCPipesIterator extends PipesIterator implements Initializable {
         }
 
         public boolean shouldSkip(int index) {
-            return idIndex == index || fetchKeyIndex == index ||
-                    fetchStartRangeIndex == index || fetchEndRangeIndex == index ||
-                    emitKeyIndex == index;
+            return idIndex == index || fetchKeyIndex == index || fetchStartRangeIndex == index || fetchEndRangeIndex == index || emitKeyIndex == index;
         }
     }
 }
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/java/org/apache/tika/pipes/pipesiterator/jdbc/TestJDBCPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/java/org/apache/tika/pipes/pipesiterator/jdbc/TestJDBCPipesIterator.java
index a993559e9..cad3574ac 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/java/org/apache/tika/pipes/pipesiterator/jdbc/TestJDBCPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/java/org/apache/tika/pipes/pipesiterator/jdbc/TestJDBCPipesIterator.java
@@ -59,21 +59,22 @@ public class TestJDBCPipesIterator {
     @BeforeAll
     public static void setUp() throws Exception {
 
-        CONNECTION =
-                DriverManager.getConnection("jdbc:h2:file:" +
-                        DB_DIR.toAbsolutePath() + "/" + db);
-        String sql = "create table " + TABLE + " (id varchar(128), " +
-                "project varchar(128), " +
-                "fetchKey varchar(128))";
-        CONNECTION.createStatement().execute(sql);
+        CONNECTION = DriverManager.getConnection("jdbc:h2:file:" + DB_DIR.toAbsolutePath() + "/" + db);
+        String sql = "create table " + TABLE + " (id varchar(128), " + "project varchar(128), " + "fetchKey varchar(128))";
+        CONNECTION
+                .createStatement()
+                .execute(sql);
 
         for (int i = 0; i < NUM_ROWS; i++) {
-            sql = "insert into " + TABLE + " (id, project, fetchKey) values ('id" + i +
-                    "','project" + (i % 2 == 0 ? "a" : "b") + "','fk" + i + "')";
-            CONNECTION.createStatement().execute(sql);
+            sql = "insert into " + TABLE + " (id, project, fetchKey) values ('id" + i + "','project" + (i % 2 == 0 ? "a" : "b") + "','fk" + i + "')";
+            CONNECTION
+                    .createStatement()
+                    .execute(sql);
         }
         sql = "select count(1) from " + TABLE;
-        ResultSet rs = CONNECTION.createStatement().executeQuery(sql);
+        ResultSet rs = CONNECTION
+                .createStatement()
+                .executeQuery(sql);
         while (rs.next()) {
             int cnt = rs.getInt(1);
             assertEquals(NUM_ROWS, cnt);
@@ -91,8 +92,7 @@ public class TestJDBCPipesIterator {
 
         PipesIterator pipesIterator = getConfig();
         ExecutorService es = Executors.newFixedThreadPool(numConsumers);
-        ExecutorCompletionService<Integer> completionService =
-                new ExecutorCompletionService<>(es);
+        ExecutorCompletionService<Integer> completionService = new ExecutorCompletionService<>(es);
         ArrayBlockingQueue<FetchEmitTuple> queue = new ArrayBlockingQueue<>(100);
         List<MockFetcher> fetchers = new ArrayList<>();
         for (int i = 0; i < numConsumers; i++) {
@@ -119,21 +119,33 @@ public class TestJDBCPipesIterator {
         }
         assertEquals(NUM_ROWS, processed);
         int cnt = 0;
-        Matcher m = Pattern.compile("fk(\\d+)").matcher("");
+        Matcher m = Pattern
+                .compile("fk(\\d+)")
+                .matcher("");
         for (MockFetcher f : fetchers) {
             for (FetchEmitTuple p : f.pairs) {
-                String k = p.getFetchKey().getFetchKey();
+                String k = p
+                        .getFetchKey()
+                        .getFetchKey();
                 String num = "";
-                if (m.reset(k).find()) {
+                if (m
+                        .reset(k)
+                        .find()) {
                     num = m.group(1);
                 } else {
                     fail("failed to find key pattern: " + k);
                 }
                 String aOrB = Integer.parseInt(num) % 2 == 0 ? "a" : "b";
                 assertEquals("id" + num, p.getId());
-                assertEquals("project" + aOrB, p.getMetadata().get("MY_PROJECT"));
-                assertNull(p.getMetadata().get("fetchKey"));
-                assertNull(p.getMetadata().get("MY_FETCHKEY"));
+                assertEquals("project" + aOrB, p
+                        .getMetadata()
+                        .get("MY_PROJECT"));
+                assertNull(p
+                        .getMetadata()
+                        .get("fetchKey"));
+                assertNull(p
+                        .getMetadata()
+                        .get("MY_FETCHKEY"));
                 cnt++;
             }
         }
@@ -141,26 +153,15 @@ public class TestJDBCPipesIterator {
     }
 
     private PipesIterator getConfig() throws Exception {
-        String config = "<?xml version=\"1.0\" encoding=\"UTF-8\" ?><properties>\n" +
-                "        <pipesIterator " +
-                "       class=\"org.apache.tika.pipes.pipesiterator.jdbc.JDBCPipesIterator\">\n" +
-                "                <fetcherName>s3f</fetcherName>\n" +
-                "                <emitterName>s3e</emitterName>\n" +
-                "                <queueSize>57</queueSize>\n" +
-                "                <idColumn>my_id</idColumn>\n" +
-                "                <fetchKeyColumn>my_fetchkey</fetchKeyColumn>\n" +
-                "                <emitKeyColumn>my_fetchkey</emitKeyColumn>\n" +
-                "                <select>" +
-                "select id as my_id, project as my_project, fetchKey as my_fetchKey " +
-                "from fetchkeys</select>\n" +
-                "                <connection>jdbc:h2:file:" + DB_DIR.toAbsolutePath() + "/" +
-                    db + "</connection>\n" +
-                "        </pipesIterator>\n" +
-                "</properties>";
+        String config = "<?xml version=\"1.0\" encoding=\"UTF-8\" ?><properties>\n" + "        <pipesIterator " +
+                "       class=\"org.apache.tika.pipes.pipesiterator.jdbc.JDBCPipesIterator\">\n" + "                <fetcherName>s3f</fetcherName>\n" +
+                "                <emitterName>s3e</emitterName>\n" + "                <queueSize>57</queueSize>\n" + "                <idColumn>my_id</idColumn>\n" +
+                "                <fetchKeyColumn>my_fetchkey</fetchKeyColumn>\n" + "                <emitKeyColumn>my_fetchkey</emitKeyColumn>\n" + "                <select>" +
+                "select id as my_id, project as my_project, fetchKey as my_fetchKey " + "from fetchkeys</select>\n" + "                <connection>jdbc:h2:file:" +
+                DB_DIR.toAbsolutePath() + "/" + db + "</connection>\n" + "        </pipesIterator>\n" + "</properties>";
         Path tmp = Files.createTempFile("tika-jdbc-", ".xml");
         Files.write(tmp, config.getBytes(StandardCharsets.UTF_8));
-        PipesIterator manager =
-                PipesIterator.build(tmp);
+        PipesIterator manager = PipesIterator.build(tmp);
         Files.delete(tmp);
         return manager;
     }
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/resources/log4j2.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/resources/log4j2.xml
index 5f946e6e5..f5e70eedb 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/resources/log4j2.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-jdbc/src/test/resources/log4j2.xml
@@ -21,12 +21,12 @@
   <Appenders>
     <Console name="console" target="SYSTEM_ERR">
       <PatternLayout
-          pattern="%-5p [%t] %d{HH:mm:ss,SSS} %c %m%n" />
+          pattern="%-5p [%t] %d{HH:mm:ss,SSS} %c %m%n"/>
     </Console>
   </Appenders>
   <Loggers>
     <Root level="info" additivity="false">
-      <AppenderRef ref="console" />
+      <AppenderRef ref="console"/>
     </Root>
   </Loggers>
 </Configuration>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/pom.xml
index 7b3307f5e..c0acce169 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/main/java/org/apache/tika/pipes/pipesiterator/json/JsonPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/main/java/org/apache/tika/pipes/pipesiterator/json/JsonPipesIterator.java
index 4ff338736..6d3ceb6c2 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/main/java/org/apache/tika/pipes/pipesiterator/json/JsonPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/main/java/org/apache/tika/pipes/pipesiterator/json/JsonPipesIterator.java
@@ -30,9 +30,9 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import org.apache.tika.config.Initializable;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTuple;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.pipesiterator.PipesIterator;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTuple;
 
 /**
  * Iterates through a UTF-8 text file with one FetchEmitTuple
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/java/org/apache/tika/pipes/pipesiterator/json/TestJsonPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/java/org/apache/tika/pipes/pipesiterator/json/TestJsonPipesIterator.java
index 671fecc5f..70a34eff2 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/java/org/apache/tika/pipes/pipesiterator/json/TestJsonPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/java/org/apache/tika/pipes/pipesiterator/json/TestJsonPipesIterator.java
@@ -31,9 +31,13 @@ public class TestJsonPipesIterator {
     @Test
     public void testBasic() throws Exception {
         JsonPipesIterator pipesIterator = new JsonPipesIterator();
-        pipesIterator.setJsonPath(
-                Paths.get(this.getClass().getResource("/test-documents/test.json").toURI())
-                        .toAbsolutePath().toString());
+        pipesIterator.setJsonPath(Paths
+                .get(this
+                        .getClass()
+                        .getResource("/test-documents/test.json")
+                        .toURI())
+                .toAbsolutePath()
+                .toString());
         Iterator<FetchEmitTuple> it = pipesIterator.iterator();
         while (it.hasNext()) {
             //System.out.println(it.next());
@@ -43,10 +47,13 @@ public class TestJsonPipesIterator {
     @Test
     public void testWithEmbDocBytes() throws Exception {
         JsonPipesIterator pipesIterator = new JsonPipesIterator();
-        pipesIterator.setJsonPath(
-                Paths.get(
-                        this.getClass().getResource("/test-documents/test-with-embedded-bytes.json").toURI())
-                        .toAbsolutePath().toString());
+        pipesIterator.setJsonPath(Paths
+                .get(this
+                        .getClass()
+                        .getResource("/test-documents/test-with-embedded-bytes.json")
+                        .toURI())
+                .toAbsolutePath()
+                .toString());
         Iterator<FetchEmitTuple> it = pipesIterator.iterator();
         while (it.hasNext()) {
             //System.out.println(it.next());
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test-with-embedded-bytes.json b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test-with-embedded-bytes.json
index 5e064d2d7..748830690 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test-with-embedded-bytes.json
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test-with-embedded-bytes.json
@@ -1,100 +1,2100 @@
-{"id":"myid-0","fetcher":"fs","fetchKey":"0.xml","emitter":"fs","emitKey":"0.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-1","fetcher":"fs","fetchKey":"1.xml","emitter":"fs","emitKey":"1.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-2","fetcher":"fs","fetchKey":"2.xml","emitter":"fs","emitKey":"2.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-3","fetcher":"fs","fetchKey":"3.xml","emitter":"fs","emitKey":"3.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-4","fetcher":"fs","fetchKey":"4.xml","emitter":"fs","emitKey":"4.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-5","fetcher":"fs","fetchKey":"5.xml","emitter":"fs","emitKey":"5.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-6","fetcher":"fs","fetchKey":"6.xml","emitter":"fs","emitKey":"6.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-7","fetcher":"fs","fetchKey":"7.xml","emitter":"fs","emitKey":"7.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-8","fetcher":"fs","fetchKey":"8.xml","emitter":"fs","emitKey":"8.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-9","fetcher":"fs","fetchKey":"9.xml","emitter":"fs","emitKey":"9.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-10","fetcher":"fs","fetchKey":"10.xml","emitter":"fs","emitKey":"10.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-11","fetcher":"fs","fetchKey":"11.xml","emitter":"fs","emitKey":"11.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-12","fetcher":"fs","fetchKey":"12.xml","emitter":"fs","emitKey":"12.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-13","fetcher":"fs","fetchKey":"13.xml","emitter":"fs","emitKey":"13.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-14","fetcher":"fs","fetchKey":"14.xml","emitter":"fs","emitKey":"14.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-15","fetcher":"fs","fetchKey":"15.xml","emitter":"fs","emitKey":"15.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-16","fetcher":"fs","fetchKey":"16.xml","emitter":"fs","emitKey":"16.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-17","fetcher":"fs","fetchKey":"17.xml","emitter":"fs","emitKey":"17.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-18","fetcher":"fs","fetchKey":"18.xml","emitter":"fs","emitKey":"18.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-19","fetcher":"fs","fetchKey":"19.xml","emitter":"fs","emitKey":"19.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-20","fetcher":"fs","fetchKey":"20.xml","emitter":"fs","emitKey":"20.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-21","fetcher":"fs","fetchKey":"21.xml","emitter":"fs","emitKey":"21.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-22","fetcher":"fs","fetchKey":"22.xml","emitter":"fs","emitKey":"22.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-23","fetcher":"fs","fetchKey":"23.xml","emitter":"fs","emitKey":"23.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-24","fetcher":"fs","fetchKey":"24.xml","emitter":"fs","emitKey":"24.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-25","fetcher":"fs","fetchKey":"25.xml","emitter":"fs","emitKey":"25.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-26","fetcher":"fs","fetchKey":"26.xml","emitter":"fs","emitKey":"26.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-27","fetcher":"fs","fetchKey":"27.xml","emitter":"fs","emitKey":"27.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-28","fetcher":"fs","fetchKey":"28.xml","emitter":"fs","emitKey":"28.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-29","fetcher":"fs","fetchKey":"29.xml","emitter":"fs","emitKey":"29.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-30","fetcher":"fs","fetchKey":"30.xml","emitter":"fs","emitKey":"30.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-31","fetcher":"fs","fetchKey":"31.xml","emitter":"fs","emitKey":"31.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-32","fetcher":"fs","fetchKey":"32.xml","emitter":"fs","emitKey":"32.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-33","fetcher":"fs","fetchKey":"33.xml","emitter":"fs","emitKey":"33.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-34","fetcher":"fs","fetchKey":"34.xml","emitter":"fs","emitKey":"34.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-35","fetcher":"fs","fetchKey":"35.xml","emitter":"fs","emitKey":"35.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-36","fetcher":"fs","fetchKey":"36.xml","emitter":"fs","emitKey":"36.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-37","fetcher":"fs","fetchKey":"37.xml","emitter":"fs","emitKey":"37.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-38","fetcher":"fs","fetchKey":"38.xml","emitter":"fs","emitKey":"38.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-39","fetcher":"fs","fetchKey":"39.xml","emitter":"fs","emitKey":"39.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-40","fetcher":"fs","fetchKey":"40.xml","emitter":"fs","emitKey":"40.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-41","fetcher":"fs","fetchKey":"41.xml","emitter":"fs","emitKey":"41.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-42","fetcher":"fs","fetchKey":"42.xml","emitter":"fs","emitKey":"42.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-43","fetcher":"fs","fetchKey":"43.xml","emitter":"fs","emitKey":"43.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-44","fetcher":"fs","fetchKey":"44.xml","emitter":"fs","emitKey":"44.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-45","fetcher":"fs","fetchKey":"45.xml","emitter":"fs","emitKey":"45.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-46","fetcher":"fs","fetchKey":"46.xml","emitter":"fs","emitKey":"46.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-47","fetcher":"fs","fetchKey":"47.xml","emitter":"fs","emitKey":"47.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-48","fetcher":"fs","fetchKey":"48.xml","emitter":"fs","emitKey":"48.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-49","fetcher":"fs","fetchKey":"49.xml","emitter":"fs","emitKey":"49.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-50","fetcher":"fs","fetchKey":"50.xml","emitter":"fs","emitKey":"50.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-51","fetcher":"fs","fetchKey":"51.xml","emitter":"fs","emitKey":"51.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-52","fetcher":"fs","fetchKey":"52.xml","emitter":"fs","emitKey":"52.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-53","fetcher":"fs","fetchKey":"53.xml","emitter":"fs","emitKey":"53.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-54","fetcher":"fs","fetchKey":"54.xml","emitter":"fs","emitKey":"54.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-55","fetcher":"fs","fetchKey":"55.xml","emitter":"fs","emitKey":"55.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-56","fetcher":"fs","fetchKey":"56.xml","emitter":"fs","emitKey":"56.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-57","fetcher":"fs","fetchKey":"57.xml","emitter":"fs","emitKey":"57.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-58","fetcher":"fs","fetchKey":"58.xml","emitter":"fs","emitKey":"58.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-59","fetcher":"fs","fetchKey":"59.xml","emitter":"fs","emitKey":"59.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-60","fetcher":"fs","fetchKey":"60.xml","emitter":"fs","emitKey":"60.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-61","fetcher":"fs","fetchKey":"61.xml","emitter":"fs","emitKey":"61.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-62","fetcher":"fs","fetchKey":"62.xml","emitter":"fs","emitKey":"62.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-63","fetcher":"fs","fetchKey":"63.xml","emitter":"fs","emitKey":"63.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-64","fetcher":"fs","fetchKey":"64.xml","emitter":"fs","emitKey":"64.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-65","fetcher":"fs","fetchKey":"65.xml","emitter":"fs","emitKey":"65.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-66","fetcher":"fs","fetchKey":"66.xml","emitter":"fs","emitKey":"66.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-67","fetcher":"fs","fetchKey":"67.xml","emitter":"fs","emitKey":"67.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-68","fetcher":"fs","fetchKey":"68.xml","emitter":"fs","emitKey":"68.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-69","fetcher":"fs","fetchKey":"69.xml","emitter":"fs","emitKey":"69.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-70","fetcher":"fs","fetchKey":"70.xml","emitter":"fs","emitKey":"70.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-71","fetcher":"fs","fetchKey":"71.xml","emitter":"fs","emitKey":"71.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-72","fetcher":"fs","fetchKey":"72.xml","emitter":"fs","emitKey":"72.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-73","fetcher":"fs","fetchKey":"73.xml","emitter":"fs","emitKey":"73.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-74","fetcher":"fs","fetchKey":"74.xml","emitter":"fs","emitKey":"74.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-75","fetcher":"fs","fetchKey":"75.xml","emitter":"fs","emitKey":"75.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-76","fetcher":"fs","fetchKey":"76.xml","emitter":"fs","emitKey":"76.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-77","fetcher":"fs","fetchKey":"77.xml","emitter":"fs","emitKey":"77.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-78","fetcher":"fs","fetchKey":"78.xml","emitter":"fs","emitKey":"78.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-79","fetcher":"fs","fetchKey":"79.xml","emitter":"fs","emitKey":"79.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-80","fetcher":"fs","fetchKey":"80.xml","emitter":"fs","emitKey":"80.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-81","fetcher":"fs","fetchKey":"81.xml","emitter":"fs","emitKey":"81.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-82","fetcher":"fs","fetchKey":"82.xml","emitter":"fs","emitKey":"82.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-83","fetcher":"fs","fetchKey":"83.xml","emitter":"fs","emitKey":"83.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-84","fetcher":"fs","fetchKey":"84.xml","emitter":"fs","emitKey":"84.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-85","fetcher":"fs","fetchKey":"85.xml","emitter":"fs","emitKey":"85.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-86","fetcher":"fs","fetchKey":"86.xml","emitter":"fs","emitKey":"86.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-87","fetcher":"fs","fetchKey":"87.xml","emitter":"fs","emitKey":"87.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-88","fetcher":"fs","fetchKey":"88.xml","emitter":"fs","emitKey":"88.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-89","fetcher":"fs","fetchKey":"89.xml","emitter":"fs","emitKey":"89.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-90","fetcher":"fs","fetchKey":"90.xml","emitter":"fs","emitKey":"90.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-91","fetcher":"fs","fetchKey":"91.xml","emitter":"fs","emitKey":"91.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-92","fetcher":"fs","fetchKey":"92.xml","emitter":"fs","emitKey":"92.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-93","fetcher":"fs","fetchKey":"93.xml","emitter":"fs","emitKey":"93.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-94","fetcher":"fs","fetchKey":"94.xml","emitter":"fs","emitKey":"94.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-95","fetcher":"fs","fetchKey":"95.xml","emitter":"fs","emitKey":"95.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-96","fetcher":"fs","fetchKey":"96.xml","emitter":"fs","emitKey":"96.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-97","fetcher":"fs","fetchKey":"97.xml","emitter":"fs","emitKey":"97.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-98","fetcher":"fs","fetchKey":"98.xml","emitter":"fs","emitKey":"98.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
-{"id":"myid-99","fetcher":"fs","fetchKey":"99.xml","emitter":"fs","emitKey":"99.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit","embeddedDocumentBytesConfig":{"extractEmbeddedDocumentBytes":true,"zeroPadName":0,"suffixStrategy":"NONE","embeddedIdPrefix":"-","includeOriginal":false}}
+{
+  "id": "myid-0",
+  "fetcher": "fs",
+  "fetchKey": "0.xml",
+  "emitter": "fs",
+  "emitKey": "0.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-1",
+  "fetcher": "fs",
+  "fetchKey": "1.xml",
+  "emitter": "fs",
+  "emitKey": "1.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-2",
+  "fetcher": "fs",
+  "fetchKey": "2.xml",
+  "emitter": "fs",
+  "emitKey": "2.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-3",
+  "fetcher": "fs",
+  "fetchKey": "3.xml",
+  "emitter": "fs",
+  "emitKey": "3.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-4",
+  "fetcher": "fs",
+  "fetchKey": "4.xml",
+  "emitter": "fs",
+  "emitKey": "4.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-5",
+  "fetcher": "fs",
+  "fetchKey": "5.xml",
+  "emitter": "fs",
+  "emitKey": "5.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-6",
+  "fetcher": "fs",
+  "fetchKey": "6.xml",
+  "emitter": "fs",
+  "emitKey": "6.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-7",
+  "fetcher": "fs",
+  "fetchKey": "7.xml",
+  "emitter": "fs",
+  "emitKey": "7.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-8",
+  "fetcher": "fs",
+  "fetchKey": "8.xml",
+  "emitter": "fs",
+  "emitKey": "8.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-9",
+  "fetcher": "fs",
+  "fetchKey": "9.xml",
+  "emitter": "fs",
+  "emitKey": "9.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-10",
+  "fetcher": "fs",
+  "fetchKey": "10.xml",
+  "emitter": "fs",
+  "emitKey": "10.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-11",
+  "fetcher": "fs",
+  "fetchKey": "11.xml",
+  "emitter": "fs",
+  "emitKey": "11.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-12",
+  "fetcher": "fs",
+  "fetchKey": "12.xml",
+  "emitter": "fs",
+  "emitKey": "12.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-13",
+  "fetcher": "fs",
+  "fetchKey": "13.xml",
+  "emitter": "fs",
+  "emitKey": "13.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-14",
+  "fetcher": "fs",
+  "fetchKey": "14.xml",
+  "emitter": "fs",
+  "emitKey": "14.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-15",
+  "fetcher": "fs",
+  "fetchKey": "15.xml",
+  "emitter": "fs",
+  "emitKey": "15.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-16",
+  "fetcher": "fs",
+  "fetchKey": "16.xml",
+  "emitter": "fs",
+  "emitKey": "16.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-17",
+  "fetcher": "fs",
+  "fetchKey": "17.xml",
+  "emitter": "fs",
+  "emitKey": "17.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-18",
+  "fetcher": "fs",
+  "fetchKey": "18.xml",
+  "emitter": "fs",
+  "emitKey": "18.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-19",
+  "fetcher": "fs",
+  "fetchKey": "19.xml",
+  "emitter": "fs",
+  "emitKey": "19.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-20",
+  "fetcher": "fs",
+  "fetchKey": "20.xml",
+  "emitter": "fs",
+  "emitKey": "20.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-21",
+  "fetcher": "fs",
+  "fetchKey": "21.xml",
+  "emitter": "fs",
+  "emitKey": "21.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-22",
+  "fetcher": "fs",
+  "fetchKey": "22.xml",
+  "emitter": "fs",
+  "emitKey": "22.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-23",
+  "fetcher": "fs",
+  "fetchKey": "23.xml",
+  "emitter": "fs",
+  "emitKey": "23.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-24",
+  "fetcher": "fs",
+  "fetchKey": "24.xml",
+  "emitter": "fs",
+  "emitKey": "24.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-25",
+  "fetcher": "fs",
+  "fetchKey": "25.xml",
+  "emitter": "fs",
+  "emitKey": "25.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-26",
+  "fetcher": "fs",
+  "fetchKey": "26.xml",
+  "emitter": "fs",
+  "emitKey": "26.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-27",
+  "fetcher": "fs",
+  "fetchKey": "27.xml",
+  "emitter": "fs",
+  "emitKey": "27.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-28",
+  "fetcher": "fs",
+  "fetchKey": "28.xml",
+  "emitter": "fs",
+  "emitKey": "28.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-29",
+  "fetcher": "fs",
+  "fetchKey": "29.xml",
+  "emitter": "fs",
+  "emitKey": "29.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-30",
+  "fetcher": "fs",
+  "fetchKey": "30.xml",
+  "emitter": "fs",
+  "emitKey": "30.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-31",
+  "fetcher": "fs",
+  "fetchKey": "31.xml",
+  "emitter": "fs",
+  "emitKey": "31.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-32",
+  "fetcher": "fs",
+  "fetchKey": "32.xml",
+  "emitter": "fs",
+  "emitKey": "32.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-33",
+  "fetcher": "fs",
+  "fetchKey": "33.xml",
+  "emitter": "fs",
+  "emitKey": "33.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-34",
+  "fetcher": "fs",
+  "fetchKey": "34.xml",
+  "emitter": "fs",
+  "emitKey": "34.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-35",
+  "fetcher": "fs",
+  "fetchKey": "35.xml",
+  "emitter": "fs",
+  "emitKey": "35.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-36",
+  "fetcher": "fs",
+  "fetchKey": "36.xml",
+  "emitter": "fs",
+  "emitKey": "36.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-37",
+  "fetcher": "fs",
+  "fetchKey": "37.xml",
+  "emitter": "fs",
+  "emitKey": "37.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-38",
+  "fetcher": "fs",
+  "fetchKey": "38.xml",
+  "emitter": "fs",
+  "emitKey": "38.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-39",
+  "fetcher": "fs",
+  "fetchKey": "39.xml",
+  "emitter": "fs",
+  "emitKey": "39.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-40",
+  "fetcher": "fs",
+  "fetchKey": "40.xml",
+  "emitter": "fs",
+  "emitKey": "40.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-41",
+  "fetcher": "fs",
+  "fetchKey": "41.xml",
+  "emitter": "fs",
+  "emitKey": "41.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-42",
+  "fetcher": "fs",
+  "fetchKey": "42.xml",
+  "emitter": "fs",
+  "emitKey": "42.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-43",
+  "fetcher": "fs",
+  "fetchKey": "43.xml",
+  "emitter": "fs",
+  "emitKey": "43.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-44",
+  "fetcher": "fs",
+  "fetchKey": "44.xml",
+  "emitter": "fs",
+  "emitKey": "44.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-45",
+  "fetcher": "fs",
+  "fetchKey": "45.xml",
+  "emitter": "fs",
+  "emitKey": "45.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-46",
+  "fetcher": "fs",
+  "fetchKey": "46.xml",
+  "emitter": "fs",
+  "emitKey": "46.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-47",
+  "fetcher": "fs",
+  "fetchKey": "47.xml",
+  "emitter": "fs",
+  "emitKey": "47.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-48",
+  "fetcher": "fs",
+  "fetchKey": "48.xml",
+  "emitter": "fs",
+  "emitKey": "48.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-49",
+  "fetcher": "fs",
+  "fetchKey": "49.xml",
+  "emitter": "fs",
+  "emitKey": "49.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-50",
+  "fetcher": "fs",
+  "fetchKey": "50.xml",
+  "emitter": "fs",
+  "emitKey": "50.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-51",
+  "fetcher": "fs",
+  "fetchKey": "51.xml",
+  "emitter": "fs",
+  "emitKey": "51.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-52",
+  "fetcher": "fs",
+  "fetchKey": "52.xml",
+  "emitter": "fs",
+  "emitKey": "52.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-53",
+  "fetcher": "fs",
+  "fetchKey": "53.xml",
+  "emitter": "fs",
+  "emitKey": "53.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-54",
+  "fetcher": "fs",
+  "fetchKey": "54.xml",
+  "emitter": "fs",
+  "emitKey": "54.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-55",
+  "fetcher": "fs",
+  "fetchKey": "55.xml",
+  "emitter": "fs",
+  "emitKey": "55.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-56",
+  "fetcher": "fs",
+  "fetchKey": "56.xml",
+  "emitter": "fs",
+  "emitKey": "56.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-57",
+  "fetcher": "fs",
+  "fetchKey": "57.xml",
+  "emitter": "fs",
+  "emitKey": "57.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-58",
+  "fetcher": "fs",
+  "fetchKey": "58.xml",
+  "emitter": "fs",
+  "emitKey": "58.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-59",
+  "fetcher": "fs",
+  "fetchKey": "59.xml",
+  "emitter": "fs",
+  "emitKey": "59.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-60",
+  "fetcher": "fs",
+  "fetchKey": "60.xml",
+  "emitter": "fs",
+  "emitKey": "60.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-61",
+  "fetcher": "fs",
+  "fetchKey": "61.xml",
+  "emitter": "fs",
+  "emitKey": "61.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-62",
+  "fetcher": "fs",
+  "fetchKey": "62.xml",
+  "emitter": "fs",
+  "emitKey": "62.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-63",
+  "fetcher": "fs",
+  "fetchKey": "63.xml",
+  "emitter": "fs",
+  "emitKey": "63.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-64",
+  "fetcher": "fs",
+  "fetchKey": "64.xml",
+  "emitter": "fs",
+  "emitKey": "64.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-65",
+  "fetcher": "fs",
+  "fetchKey": "65.xml",
+  "emitter": "fs",
+  "emitKey": "65.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-66",
+  "fetcher": "fs",
+  "fetchKey": "66.xml",
+  "emitter": "fs",
+  "emitKey": "66.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-67",
+  "fetcher": "fs",
+  "fetchKey": "67.xml",
+  "emitter": "fs",
+  "emitKey": "67.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-68",
+  "fetcher": "fs",
+  "fetchKey": "68.xml",
+  "emitter": "fs",
+  "emitKey": "68.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-69",
+  "fetcher": "fs",
+  "fetchKey": "69.xml",
+  "emitter": "fs",
+  "emitKey": "69.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-70",
+  "fetcher": "fs",
+  "fetchKey": "70.xml",
+  "emitter": "fs",
+  "emitKey": "70.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-71",
+  "fetcher": "fs",
+  "fetchKey": "71.xml",
+  "emitter": "fs",
+  "emitKey": "71.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-72",
+  "fetcher": "fs",
+  "fetchKey": "72.xml",
+  "emitter": "fs",
+  "emitKey": "72.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-73",
+  "fetcher": "fs",
+  "fetchKey": "73.xml",
+  "emitter": "fs",
+  "emitKey": "73.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-74",
+  "fetcher": "fs",
+  "fetchKey": "74.xml",
+  "emitter": "fs",
+  "emitKey": "74.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-75",
+  "fetcher": "fs",
+  "fetchKey": "75.xml",
+  "emitter": "fs",
+  "emitKey": "75.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-76",
+  "fetcher": "fs",
+  "fetchKey": "76.xml",
+  "emitter": "fs",
+  "emitKey": "76.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-77",
+  "fetcher": "fs",
+  "fetchKey": "77.xml",
+  "emitter": "fs",
+  "emitKey": "77.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-78",
+  "fetcher": "fs",
+  "fetchKey": "78.xml",
+  "emitter": "fs",
+  "emitKey": "78.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-79",
+  "fetcher": "fs",
+  "fetchKey": "79.xml",
+  "emitter": "fs",
+  "emitKey": "79.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-80",
+  "fetcher": "fs",
+  "fetchKey": "80.xml",
+  "emitter": "fs",
+  "emitKey": "80.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-81",
+  "fetcher": "fs",
+  "fetchKey": "81.xml",
+  "emitter": "fs",
+  "emitKey": "81.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-82",
+  "fetcher": "fs",
+  "fetchKey": "82.xml",
+  "emitter": "fs",
+  "emitKey": "82.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-83",
+  "fetcher": "fs",
+  "fetchKey": "83.xml",
+  "emitter": "fs",
+  "emitKey": "83.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-84",
+  "fetcher": "fs",
+  "fetchKey": "84.xml",
+  "emitter": "fs",
+  "emitKey": "84.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-85",
+  "fetcher": "fs",
+  "fetchKey": "85.xml",
+  "emitter": "fs",
+  "emitKey": "85.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-86",
+  "fetcher": "fs",
+  "fetchKey": "86.xml",
+  "emitter": "fs",
+  "emitKey": "86.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-87",
+  "fetcher": "fs",
+  "fetchKey": "87.xml",
+  "emitter": "fs",
+  "emitKey": "87.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-88",
+  "fetcher": "fs",
+  "fetchKey": "88.xml",
+  "emitter": "fs",
+  "emitKey": "88.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-89",
+  "fetcher": "fs",
+  "fetchKey": "89.xml",
+  "emitter": "fs",
+  "emitKey": "89.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-90",
+  "fetcher": "fs",
+  "fetchKey": "90.xml",
+  "emitter": "fs",
+  "emitKey": "90.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-91",
+  "fetcher": "fs",
+  "fetchKey": "91.xml",
+  "emitter": "fs",
+  "emitKey": "91.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-92",
+  "fetcher": "fs",
+  "fetchKey": "92.xml",
+  "emitter": "fs",
+  "emitKey": "92.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-93",
+  "fetcher": "fs",
+  "fetchKey": "93.xml",
+  "emitter": "fs",
+  "emitKey": "93.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-94",
+  "fetcher": "fs",
+  "fetchKey": "94.xml",
+  "emitter": "fs",
+  "emitKey": "94.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-95",
+  "fetcher": "fs",
+  "fetchKey": "95.xml",
+  "emitter": "fs",
+  "emitKey": "95.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-96",
+  "fetcher": "fs",
+  "fetchKey": "96.xml",
+  "emitter": "fs",
+  "emitKey": "96.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-97",
+  "fetcher": "fs",
+  "fetchKey": "97.xml",
+  "emitter": "fs",
+  "emitKey": "97.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-98",
+  "fetcher": "fs",
+  "fetchKey": "98.xml",
+  "emitter": "fs",
+  "emitKey": "98.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
+{
+  "id": "myid-99",
+  "fetcher": "fs",
+  "fetchKey": "99.xml",
+  "emitter": "fs",
+  "emitKey": "99.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit",
+  "embeddedDocumentBytesConfig": {
+    "extractEmbeddedDocumentBytes": true,
+    "zeroPadName": 0,
+    "suffixStrategy": "NONE",
+    "embeddedIdPrefix": "-",
+    "includeOriginal": false
+  }
+}
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test.json b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test.json
index 199772ecb..721410fd3 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test.json
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-json/src/test/resources/test-documents/test.json
@@ -1,100 +1,1400 @@
-{"id":"myid-0","fetcher":"fs","fetchKey":"0.xml","emitter":"fs","emitKey":"0.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-1","fetcher":"fs","fetchKey":"1.xml","emitter":"fs","emitKey":"1.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-2","fetcher":"fs","fetchKey":"2.xml","emitter":"fs","emitKey":"2.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-3","fetcher":"fs","fetchKey":"3.xml","emitter":"fs","emitKey":"3.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-4","fetcher":"fs","fetchKey":"4.xml","emitter":"fs","emitKey":"4.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-5","fetcher":"fs","fetchKey":"5.xml","emitter":"fs","emitKey":"5.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-6","fetcher":"fs","fetchKey":"6.xml","emitter":"fs","emitKey":"6.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-7","fetcher":"fs","fetchKey":"7.xml","emitter":"fs","emitKey":"7.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-8","fetcher":"fs","fetchKey":"8.xml","emitter":"fs","emitKey":"8.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-9","fetcher":"fs","fetchKey":"9.xml","emitter":"fs","emitKey":"9.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-10","fetcher":"fs","fetchKey":"10.xml","emitter":"fs","emitKey":"10.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-11","fetcher":"fs","fetchKey":"11.xml","emitter":"fs","emitKey":"11.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-12","fetcher":"fs","fetchKey":"12.xml","emitter":"fs","emitKey":"12.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-13","fetcher":"fs","fetchKey":"13.xml","emitter":"fs","emitKey":"13.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-14","fetcher":"fs","fetchKey":"14.xml","emitter":"fs","emitKey":"14.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-15","fetcher":"fs","fetchKey":"15.xml","emitter":"fs","emitKey":"15.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-16","fetcher":"fs","fetchKey":"16.xml","emitter":"fs","emitKey":"16.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-17","fetcher":"fs","fetchKey":"17.xml","emitter":"fs","emitKey":"17.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-18","fetcher":"fs","fetchKey":"18.xml","emitter":"fs","emitKey":"18.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-19","fetcher":"fs","fetchKey":"19.xml","emitter":"fs","emitKey":"19.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-20","fetcher":"fs","fetchKey":"20.xml","emitter":"fs","emitKey":"20.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-21","fetcher":"fs","fetchKey":"21.xml","emitter":"fs","emitKey":"21.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-22","fetcher":"fs","fetchKey":"22.xml","emitter":"fs","emitKey":"22.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-23","fetcher":"fs","fetchKey":"23.xml","emitter":"fs","emitKey":"23.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-24","fetcher":"fs","fetchKey":"24.xml","emitter":"fs","emitKey":"24.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-25","fetcher":"fs","fetchKey":"25.xml","emitter":"fs","emitKey":"25.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-26","fetcher":"fs","fetchKey":"26.xml","emitter":"fs","emitKey":"26.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-27","fetcher":"fs","fetchKey":"27.xml","emitter":"fs","emitKey":"27.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-28","fetcher":"fs","fetchKey":"28.xml","emitter":"fs","emitKey":"28.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-29","fetcher":"fs","fetchKey":"29.xml","emitter":"fs","emitKey":"29.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-30","fetcher":"fs","fetchKey":"30.xml","emitter":"fs","emitKey":"30.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-31","fetcher":"fs","fetchKey":"31.xml","emitter":"fs","emitKey":"31.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-32","fetcher":"fs","fetchKey":"32.xml","emitter":"fs","emitKey":"32.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-33","fetcher":"fs","fetchKey":"33.xml","emitter":"fs","emitKey":"33.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-34","fetcher":"fs","fetchKey":"34.xml","emitter":"fs","emitKey":"34.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-35","fetcher":"fs","fetchKey":"35.xml","emitter":"fs","emitKey":"35.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-36","fetcher":"fs","fetchKey":"36.xml","emitter":"fs","emitKey":"36.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-37","fetcher":"fs","fetchKey":"37.xml","emitter":"fs","emitKey":"37.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-38","fetcher":"fs","fetchKey":"38.xml","emitter":"fs","emitKey":"38.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-39","fetcher":"fs","fetchKey":"39.xml","emitter":"fs","emitKey":"39.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-40","fetcher":"fs","fetchKey":"40.xml","emitter":"fs","emitKey":"40.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-41","fetcher":"fs","fetchKey":"41.xml","emitter":"fs","emitKey":"41.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-42","fetcher":"fs","fetchKey":"42.xml","emitter":"fs","emitKey":"42.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-43","fetcher":"fs","fetchKey":"43.xml","emitter":"fs","emitKey":"43.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-44","fetcher":"fs","fetchKey":"44.xml","emitter":"fs","emitKey":"44.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-45","fetcher":"fs","fetchKey":"45.xml","emitter":"fs","emitKey":"45.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-46","fetcher":"fs","fetchKey":"46.xml","emitter":"fs","emitKey":"46.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-47","fetcher":"fs","fetchKey":"47.xml","emitter":"fs","emitKey":"47.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-48","fetcher":"fs","fetchKey":"48.xml","emitter":"fs","emitKey":"48.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-49","fetcher":"fs","fetchKey":"49.xml","emitter":"fs","emitKey":"49.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-50","fetcher":"fs","fetchKey":"50.xml","emitter":"fs","emitKey":"50.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-51","fetcher":"fs","fetchKey":"51.xml","emitter":"fs","emitKey":"51.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-52","fetcher":"fs","fetchKey":"52.xml","emitter":"fs","emitKey":"52.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-53","fetcher":"fs","fetchKey":"53.xml","emitter":"fs","emitKey":"53.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-54","fetcher":"fs","fetchKey":"54.xml","emitter":"fs","emitKey":"54.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-55","fetcher":"fs","fetchKey":"55.xml","emitter":"fs","emitKey":"55.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-56","fetcher":"fs","fetchKey":"56.xml","emitter":"fs","emitKey":"56.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-57","fetcher":"fs","fetchKey":"57.xml","emitter":"fs","emitKey":"57.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-58","fetcher":"fs","fetchKey":"58.xml","emitter":"fs","emitKey":"58.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-59","fetcher":"fs","fetchKey":"59.xml","emitter":"fs","emitKey":"59.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-60","fetcher":"fs","fetchKey":"60.xml","emitter":"fs","emitKey":"60.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-61","fetcher":"fs","fetchKey":"61.xml","emitter":"fs","emitKey":"61.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-62","fetcher":"fs","fetchKey":"62.xml","emitter":"fs","emitKey":"62.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-63","fetcher":"fs","fetchKey":"63.xml","emitter":"fs","emitKey":"63.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-64","fetcher":"fs","fetchKey":"64.xml","emitter":"fs","emitKey":"64.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-65","fetcher":"fs","fetchKey":"65.xml","emitter":"fs","emitKey":"65.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-66","fetcher":"fs","fetchKey":"66.xml","emitter":"fs","emitKey":"66.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-67","fetcher":"fs","fetchKey":"67.xml","emitter":"fs","emitKey":"67.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-68","fetcher":"fs","fetchKey":"68.xml","emitter":"fs","emitKey":"68.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-69","fetcher":"fs","fetchKey":"69.xml","emitter":"fs","emitKey":"69.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-70","fetcher":"fs","fetchKey":"70.xml","emitter":"fs","emitKey":"70.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-71","fetcher":"fs","fetchKey":"71.xml","emitter":"fs","emitKey":"71.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-72","fetcher":"fs","fetchKey":"72.xml","emitter":"fs","emitKey":"72.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-73","fetcher":"fs","fetchKey":"73.xml","emitter":"fs","emitKey":"73.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-74","fetcher":"fs","fetchKey":"74.xml","emitter":"fs","emitKey":"74.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-75","fetcher":"fs","fetchKey":"75.xml","emitter":"fs","emitKey":"75.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-76","fetcher":"fs","fetchKey":"76.xml","emitter":"fs","emitKey":"76.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-77","fetcher":"fs","fetchKey":"77.xml","emitter":"fs","emitKey":"77.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-78","fetcher":"fs","fetchKey":"78.xml","emitter":"fs","emitKey":"78.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-79","fetcher":"fs","fetchKey":"79.xml","emitter":"fs","emitKey":"79.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-80","fetcher":"fs","fetchKey":"80.xml","emitter":"fs","emitKey":"80.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-81","fetcher":"fs","fetchKey":"81.xml","emitter":"fs","emitKey":"81.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-82","fetcher":"fs","fetchKey":"82.xml","emitter":"fs","emitKey":"82.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-83","fetcher":"fs","fetchKey":"83.xml","emitter":"fs","emitKey":"83.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-84","fetcher":"fs","fetchKey":"84.xml","emitter":"fs","emitKey":"84.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-85","fetcher":"fs","fetchKey":"85.xml","emitter":"fs","emitKey":"85.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-86","fetcher":"fs","fetchKey":"86.xml","emitter":"fs","emitKey":"86.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-87","fetcher":"fs","fetchKey":"87.xml","emitter":"fs","emitKey":"87.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-88","fetcher":"fs","fetchKey":"88.xml","emitter":"fs","emitKey":"88.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-89","fetcher":"fs","fetchKey":"89.xml","emitter":"fs","emitKey":"89.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-90","fetcher":"fs","fetchKey":"90.xml","emitter":"fs","emitKey":"90.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-91","fetcher":"fs","fetchKey":"91.xml","emitter":"fs","emitKey":"91.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-92","fetcher":"fs","fetchKey":"92.xml","emitter":"fs","emitKey":"92.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-93","fetcher":"fs","fetchKey":"93.xml","emitter":"fs","emitKey":"93.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-94","fetcher":"fs","fetchKey":"94.xml","emitter":"fs","emitKey":"94.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-95","fetcher":"fs","fetchKey":"95.xml","emitter":"fs","emitKey":"95.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-96","fetcher":"fs","fetchKey":"96.xml","emitter":"fs","emitKey":"96.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-97","fetcher":"fs","fetchKey":"97.xml","emitter":"fs","emitKey":"97.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-98","fetcher":"fs","fetchKey":"98.xml","emitter":"fs","emitKey":"98.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
-{"id":"myid-99","fetcher":"fs","fetchKey":"99.xml","emitter":"fs","emitKey":"99.xml.json","handlerConfig":{"type":"text","parseMode":"rmeta","writeLimit":-1,"maxEmbeddedResources":-1},"onParseException":"emit"}
+{
+  "id": "myid-0",
+  "fetcher": "fs",
+  "fetchKey": "0.xml",
+  "emitter": "fs",
+  "emitKey": "0.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-1",
+  "fetcher": "fs",
+  "fetchKey": "1.xml",
+  "emitter": "fs",
+  "emitKey": "1.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-2",
+  "fetcher": "fs",
+  "fetchKey": "2.xml",
+  "emitter": "fs",
+  "emitKey": "2.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-3",
+  "fetcher": "fs",
+  "fetchKey": "3.xml",
+  "emitter": "fs",
+  "emitKey": "3.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-4",
+  "fetcher": "fs",
+  "fetchKey": "4.xml",
+  "emitter": "fs",
+  "emitKey": "4.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-5",
+  "fetcher": "fs",
+  "fetchKey": "5.xml",
+  "emitter": "fs",
+  "emitKey": "5.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-6",
+  "fetcher": "fs",
+  "fetchKey": "6.xml",
+  "emitter": "fs",
+  "emitKey": "6.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-7",
+  "fetcher": "fs",
+  "fetchKey": "7.xml",
+  "emitter": "fs",
+  "emitKey": "7.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-8",
+  "fetcher": "fs",
+  "fetchKey": "8.xml",
+  "emitter": "fs",
+  "emitKey": "8.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-9",
+  "fetcher": "fs",
+  "fetchKey": "9.xml",
+  "emitter": "fs",
+  "emitKey": "9.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-10",
+  "fetcher": "fs",
+  "fetchKey": "10.xml",
+  "emitter": "fs",
+  "emitKey": "10.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-11",
+  "fetcher": "fs",
+  "fetchKey": "11.xml",
+  "emitter": "fs",
+  "emitKey": "11.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-12",
+  "fetcher": "fs",
+  "fetchKey": "12.xml",
+  "emitter": "fs",
+  "emitKey": "12.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-13",
+  "fetcher": "fs",
+  "fetchKey": "13.xml",
+  "emitter": "fs",
+  "emitKey": "13.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-14",
+  "fetcher": "fs",
+  "fetchKey": "14.xml",
+  "emitter": "fs",
+  "emitKey": "14.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-15",
+  "fetcher": "fs",
+  "fetchKey": "15.xml",
+  "emitter": "fs",
+  "emitKey": "15.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-16",
+  "fetcher": "fs",
+  "fetchKey": "16.xml",
+  "emitter": "fs",
+  "emitKey": "16.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-17",
+  "fetcher": "fs",
+  "fetchKey": "17.xml",
+  "emitter": "fs",
+  "emitKey": "17.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-18",
+  "fetcher": "fs",
+  "fetchKey": "18.xml",
+  "emitter": "fs",
+  "emitKey": "18.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-19",
+  "fetcher": "fs",
+  "fetchKey": "19.xml",
+  "emitter": "fs",
+  "emitKey": "19.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-20",
+  "fetcher": "fs",
+  "fetchKey": "20.xml",
+  "emitter": "fs",
+  "emitKey": "20.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-21",
+  "fetcher": "fs",
+  "fetchKey": "21.xml",
+  "emitter": "fs",
+  "emitKey": "21.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-22",
+  "fetcher": "fs",
+  "fetchKey": "22.xml",
+  "emitter": "fs",
+  "emitKey": "22.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-23",
+  "fetcher": "fs",
+  "fetchKey": "23.xml",
+  "emitter": "fs",
+  "emitKey": "23.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-24",
+  "fetcher": "fs",
+  "fetchKey": "24.xml",
+  "emitter": "fs",
+  "emitKey": "24.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-25",
+  "fetcher": "fs",
+  "fetchKey": "25.xml",
+  "emitter": "fs",
+  "emitKey": "25.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-26",
+  "fetcher": "fs",
+  "fetchKey": "26.xml",
+  "emitter": "fs",
+  "emitKey": "26.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-27",
+  "fetcher": "fs",
+  "fetchKey": "27.xml",
+  "emitter": "fs",
+  "emitKey": "27.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-28",
+  "fetcher": "fs",
+  "fetchKey": "28.xml",
+  "emitter": "fs",
+  "emitKey": "28.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-29",
+  "fetcher": "fs",
+  "fetchKey": "29.xml",
+  "emitter": "fs",
+  "emitKey": "29.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-30",
+  "fetcher": "fs",
+  "fetchKey": "30.xml",
+  "emitter": "fs",
+  "emitKey": "30.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-31",
+  "fetcher": "fs",
+  "fetchKey": "31.xml",
+  "emitter": "fs",
+  "emitKey": "31.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-32",
+  "fetcher": "fs",
+  "fetchKey": "32.xml",
+  "emitter": "fs",
+  "emitKey": "32.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-33",
+  "fetcher": "fs",
+  "fetchKey": "33.xml",
+  "emitter": "fs",
+  "emitKey": "33.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-34",
+  "fetcher": "fs",
+  "fetchKey": "34.xml",
+  "emitter": "fs",
+  "emitKey": "34.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-35",
+  "fetcher": "fs",
+  "fetchKey": "35.xml",
+  "emitter": "fs",
+  "emitKey": "35.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-36",
+  "fetcher": "fs",
+  "fetchKey": "36.xml",
+  "emitter": "fs",
+  "emitKey": "36.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-37",
+  "fetcher": "fs",
+  "fetchKey": "37.xml",
+  "emitter": "fs",
+  "emitKey": "37.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-38",
+  "fetcher": "fs",
+  "fetchKey": "38.xml",
+  "emitter": "fs",
+  "emitKey": "38.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-39",
+  "fetcher": "fs",
+  "fetchKey": "39.xml",
+  "emitter": "fs",
+  "emitKey": "39.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-40",
+  "fetcher": "fs",
+  "fetchKey": "40.xml",
+  "emitter": "fs",
+  "emitKey": "40.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-41",
+  "fetcher": "fs",
+  "fetchKey": "41.xml",
+  "emitter": "fs",
+  "emitKey": "41.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-42",
+  "fetcher": "fs",
+  "fetchKey": "42.xml",
+  "emitter": "fs",
+  "emitKey": "42.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-43",
+  "fetcher": "fs",
+  "fetchKey": "43.xml",
+  "emitter": "fs",
+  "emitKey": "43.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-44",
+  "fetcher": "fs",
+  "fetchKey": "44.xml",
+  "emitter": "fs",
+  "emitKey": "44.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-45",
+  "fetcher": "fs",
+  "fetchKey": "45.xml",
+  "emitter": "fs",
+  "emitKey": "45.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-46",
+  "fetcher": "fs",
+  "fetchKey": "46.xml",
+  "emitter": "fs",
+  "emitKey": "46.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-47",
+  "fetcher": "fs",
+  "fetchKey": "47.xml",
+  "emitter": "fs",
+  "emitKey": "47.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-48",
+  "fetcher": "fs",
+  "fetchKey": "48.xml",
+  "emitter": "fs",
+  "emitKey": "48.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-49",
+  "fetcher": "fs",
+  "fetchKey": "49.xml",
+  "emitter": "fs",
+  "emitKey": "49.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-50",
+  "fetcher": "fs",
+  "fetchKey": "50.xml",
+  "emitter": "fs",
+  "emitKey": "50.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-51",
+  "fetcher": "fs",
+  "fetchKey": "51.xml",
+  "emitter": "fs",
+  "emitKey": "51.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-52",
+  "fetcher": "fs",
+  "fetchKey": "52.xml",
+  "emitter": "fs",
+  "emitKey": "52.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-53",
+  "fetcher": "fs",
+  "fetchKey": "53.xml",
+  "emitter": "fs",
+  "emitKey": "53.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-54",
+  "fetcher": "fs",
+  "fetchKey": "54.xml",
+  "emitter": "fs",
+  "emitKey": "54.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-55",
+  "fetcher": "fs",
+  "fetchKey": "55.xml",
+  "emitter": "fs",
+  "emitKey": "55.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-56",
+  "fetcher": "fs",
+  "fetchKey": "56.xml",
+  "emitter": "fs",
+  "emitKey": "56.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-57",
+  "fetcher": "fs",
+  "fetchKey": "57.xml",
+  "emitter": "fs",
+  "emitKey": "57.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-58",
+  "fetcher": "fs",
+  "fetchKey": "58.xml",
+  "emitter": "fs",
+  "emitKey": "58.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-59",
+  "fetcher": "fs",
+  "fetchKey": "59.xml",
+  "emitter": "fs",
+  "emitKey": "59.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-60",
+  "fetcher": "fs",
+  "fetchKey": "60.xml",
+  "emitter": "fs",
+  "emitKey": "60.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-61",
+  "fetcher": "fs",
+  "fetchKey": "61.xml",
+  "emitter": "fs",
+  "emitKey": "61.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-62",
+  "fetcher": "fs",
+  "fetchKey": "62.xml",
+  "emitter": "fs",
+  "emitKey": "62.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-63",
+  "fetcher": "fs",
+  "fetchKey": "63.xml",
+  "emitter": "fs",
+  "emitKey": "63.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-64",
+  "fetcher": "fs",
+  "fetchKey": "64.xml",
+  "emitter": "fs",
+  "emitKey": "64.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-65",
+  "fetcher": "fs",
+  "fetchKey": "65.xml",
+  "emitter": "fs",
+  "emitKey": "65.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-66",
+  "fetcher": "fs",
+  "fetchKey": "66.xml",
+  "emitter": "fs",
+  "emitKey": "66.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-67",
+  "fetcher": "fs",
+  "fetchKey": "67.xml",
+  "emitter": "fs",
+  "emitKey": "67.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-68",
+  "fetcher": "fs",
+  "fetchKey": "68.xml",
+  "emitter": "fs",
+  "emitKey": "68.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-69",
+  "fetcher": "fs",
+  "fetchKey": "69.xml",
+  "emitter": "fs",
+  "emitKey": "69.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-70",
+  "fetcher": "fs",
+  "fetchKey": "70.xml",
+  "emitter": "fs",
+  "emitKey": "70.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-71",
+  "fetcher": "fs",
+  "fetchKey": "71.xml",
+  "emitter": "fs",
+  "emitKey": "71.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-72",
+  "fetcher": "fs",
+  "fetchKey": "72.xml",
+  "emitter": "fs",
+  "emitKey": "72.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-73",
+  "fetcher": "fs",
+  "fetchKey": "73.xml",
+  "emitter": "fs",
+  "emitKey": "73.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-74",
+  "fetcher": "fs",
+  "fetchKey": "74.xml",
+  "emitter": "fs",
+  "emitKey": "74.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-75",
+  "fetcher": "fs",
+  "fetchKey": "75.xml",
+  "emitter": "fs",
+  "emitKey": "75.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-76",
+  "fetcher": "fs",
+  "fetchKey": "76.xml",
+  "emitter": "fs",
+  "emitKey": "76.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-77",
+  "fetcher": "fs",
+  "fetchKey": "77.xml",
+  "emitter": "fs",
+  "emitKey": "77.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-78",
+  "fetcher": "fs",
+  "fetchKey": "78.xml",
+  "emitter": "fs",
+  "emitKey": "78.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-79",
+  "fetcher": "fs",
+  "fetchKey": "79.xml",
+  "emitter": "fs",
+  "emitKey": "79.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-80",
+  "fetcher": "fs",
+  "fetchKey": "80.xml",
+  "emitter": "fs",
+  "emitKey": "80.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-81",
+  "fetcher": "fs",
+  "fetchKey": "81.xml",
+  "emitter": "fs",
+  "emitKey": "81.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-82",
+  "fetcher": "fs",
+  "fetchKey": "82.xml",
+  "emitter": "fs",
+  "emitKey": "82.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-83",
+  "fetcher": "fs",
+  "fetchKey": "83.xml",
+  "emitter": "fs",
+  "emitKey": "83.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-84",
+  "fetcher": "fs",
+  "fetchKey": "84.xml",
+  "emitter": "fs",
+  "emitKey": "84.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-85",
+  "fetcher": "fs",
+  "fetchKey": "85.xml",
+  "emitter": "fs",
+  "emitKey": "85.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-86",
+  "fetcher": "fs",
+  "fetchKey": "86.xml",
+  "emitter": "fs",
+  "emitKey": "86.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-87",
+  "fetcher": "fs",
+  "fetchKey": "87.xml",
+  "emitter": "fs",
+  "emitKey": "87.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-88",
+  "fetcher": "fs",
+  "fetchKey": "88.xml",
+  "emitter": "fs",
+  "emitKey": "88.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-89",
+  "fetcher": "fs",
+  "fetchKey": "89.xml",
+  "emitter": "fs",
+  "emitKey": "89.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-90",
+  "fetcher": "fs",
+  "fetchKey": "90.xml",
+  "emitter": "fs",
+  "emitKey": "90.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-91",
+  "fetcher": "fs",
+  "fetchKey": "91.xml",
+  "emitter": "fs",
+  "emitKey": "91.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-92",
+  "fetcher": "fs",
+  "fetchKey": "92.xml",
+  "emitter": "fs",
+  "emitKey": "92.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-93",
+  "fetcher": "fs",
+  "fetchKey": "93.xml",
+  "emitter": "fs",
+  "emitKey": "93.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-94",
+  "fetcher": "fs",
+  "fetchKey": "94.xml",
+  "emitter": "fs",
+  "emitKey": "94.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-95",
+  "fetcher": "fs",
+  "fetchKey": "95.xml",
+  "emitter": "fs",
+  "emitKey": "95.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-96",
+  "fetcher": "fs",
+  "fetchKey": "96.xml",
+  "emitter": "fs",
+  "emitKey": "96.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-97",
+  "fetcher": "fs",
+  "fetchKey": "97.xml",
+  "emitter": "fs",
+  "emitKey": "97.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-98",
+  "fetcher": "fs",
+  "fetchKey": "98.xml",
+  "emitter": "fs",
+  "emitKey": "98.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
+{
+  "id": "myid-99",
+  "fetcher": "fs",
+  "fetchKey": "99.xml",
+  "emitter": "fs",
+  "emitKey": "99.xml.json",
+  "handlerConfig": {
+    "type": "text",
+    "parseMode": "rmeta",
+    "writeLimit": -1,
+    "maxEmbeddedResources": -1
+  },
+  "onParseException": "emit"
+}
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/pom.xml
index 5f69ec939..29eb7b8db 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/src/main/java/org/apache/tika/pipes/pipesiterator/kafka/KafkaPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/src/main/java/org/apache/tika/pipes/pipesiterator/kafka/KafkaPipesIterator.java
index 19df4408f..9fbebcfda 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/src/main/java/org/apache/tika/pipes/pipesiterator/kafka/KafkaPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-kafka/src/main/java/org/apache/tika/pipes/pipesiterator/kafka/KafkaPipesIterator.java
@@ -37,6 +37,7 @@ import org.apache.tika.config.Param;
 import org.apache.tika.config.TikaConfig;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -120,10 +121,8 @@ public class KafkaPipesIterator extends PipesIterator implements Initializable {
     public void initialize(Map<String, Param> params) {
         props = new Properties();
         safePut(props, ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
-        safePut(props, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
-                serializerClass(keySerializer, StringDeserializer.class));
-        safePut(props, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
-                serializerClass(valueSerializer, StringDeserializer.class));
+        safePut(props, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, serializerClass(keySerializer, StringDeserializer.class));
+        safePut(props, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, serializerClass(valueSerializer, StringDeserializer.class));
         safePut(props, ConsumerConfig.GROUP_ID_CONFIG, groupId);
         safePut(props, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);
         safePut(props, "group.inital.rebalance.delay.ms", groupInitialRebalanceDelayMs);
@@ -141,8 +140,7 @@ public class KafkaPipesIterator extends PipesIterator implements Initializable {
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         super.checkInitialization(problemHandler);
         TikaConfig.mustNotBeEmpty("bootstrapServers", this.bootstrapServers);
         TikaConfig.mustNotBeEmpty("topic", this.topic);
@@ -164,10 +162,9 @@ public class KafkaPipesIterator extends PipesIterator implements Initializable {
                 if (LOGGER.isDebugEnabled()) {
                     LOGGER.debug("adding ({}) {} in {} ms", count, r.key(), elapsed);
                 }
-                tryToAdd(new FetchEmitTuple(r.key(), new FetchKey(fetcherName,
-                        r.key()),
-                        new EmitKey(emitterName, r.key()), new Metadata(), handlerConfig,
-                        getOnParseException()));
+                ParseContext parseContext = new ParseContext();
+                parseContext.set(HandlerConfig.class, handlerConfig);
+                tryToAdd(new FetchEmitTuple(r.key(), new FetchKey(fetcherName, r.key()), new EmitKey(emitterName, r.key()), new Metadata(), parseContext, getOnParseException()));
                 ++count;
             }
         } while ((emitMax > 0 || count < emitMax) && !records.isEmpty());
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/pom.xml
index 53a8e8c15..542337495 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/src/main/java/org/apache/tika/pipes/pipesiterator/s3/S3PipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/src/main/java/org/apache/tika/pipes/pipesiterator/s3/S3PipesIterator.java
index eee1b8173..38fc1889c 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/src/main/java/org/apache/tika/pipes/pipesiterator/s3/S3PipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-s3/src/main/java/org/apache/tika/pipes/pipesiterator/s3/S3PipesIterator.java
@@ -46,6 +46,7 @@ import org.apache.tika.config.Param;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.io.FilenameUtils;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -112,10 +113,8 @@ public class S3PipesIterator extends PipesIterator implements Initializable {
 
     @Field
     public void setCredentialsProvider(String credentialsProvider) {
-        if (!credentialsProvider.equals("profile") && !credentialsProvider.equals("instance")
-                && !credentialsProvider.equals("key_secret")) {
-            throw new IllegalArgumentException(
-                    "credentialsProvider must be either 'profile', 'instance' or 'key_secret'");
+        if (!credentialsProvider.equals("profile") && !credentialsProvider.equals("instance") && !credentialsProvider.equals("key_secret")) {
+            throw new IllegalArgumentException("credentialsProvider must be either 'profile', 'instance' or 'key_secret'");
         }
         this.credentialsProvider = credentialsProvider;
     }
@@ -153,21 +152,18 @@ public class S3PipesIterator extends PipesIterator implements Initializable {
         } else if (credentialsProvider.equals("key_secret")) {
             provider = new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey));
         } else {
-            throw new TikaConfigException("credentialsProvider must be set and " +
-                    "must be either 'instance', 'profile' or 'key_secret'");
+            throw new TikaConfigException("credentialsProvider must be set and " + "must be either 'instance', 'profile' or 'key_secret'");
         }
 
-        ClientConfiguration clientConfig = new ClientConfiguration()
-                .withMaxConnections(maxConnections);
+        ClientConfiguration clientConfig = new ClientConfiguration().withMaxConnections(maxConnections);
         try {
-            AmazonS3ClientBuilder amazonS3ClientBuilder = AmazonS3ClientBuilder.standard()
+            AmazonS3ClientBuilder amazonS3ClientBuilder = AmazonS3ClientBuilder
+                    .standard()
                     .withClientConfiguration(clientConfig)
                     .withCredentials(provider)
                     .withPathStyleAccessEnabled(pathStyleAccessEnabled);
             if (!StringUtils.isBlank(endpointConfigurationService)) {
-                amazonS3ClientBuilder.setEndpointConfiguration(
-                        new AwsClientBuilder
-                                .EndpointConfiguration(endpointConfigurationService, region));
+                amazonS3ClientBuilder.setEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpointConfigurationService, region));
             } else {
                 amazonS3ClientBuilder.withRegion(region);
             }
@@ -178,8 +174,7 @@ public class S3PipesIterator extends PipesIterator implements Initializable {
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         super.checkInitialization(problemHandler);
         mustNotBeEmpty("bucket", this.bucket);
         mustNotBeEmpty("region", this.region);
@@ -203,9 +198,9 @@ public class S3PipesIterator extends PipesIterator implements Initializable {
             long elapsed = System.currentTimeMillis() - start;
             LOGGER.debug("adding ({}) {} in {} ms", count, summary.getKey(), elapsed);
             //TODO -- allow user specified metadata as the "id"?
-            tryToAdd(new FetchEmitTuple(summary.getKey(), new FetchKey(fetcherName,
-                    summary.getKey()),
-                    new EmitKey(emitterName, summary.getKey()), new Metadata(), handlerConfig,
+            ParseContext parseContext = new ParseContext();
+            parseContext.set(HandlerConfig.class, handlerConfig);
+            tryToAdd(new FetchEmitTuple(summary.getKey(), new FetchKey(fetcherName, summary.getKey()), new EmitKey(emitterName, summary.getKey()), new Metadata(), parseContext,
                     getOnParseException()));
             count++;
         }
@@ -215,6 +210,8 @@ public class S3PipesIterator extends PipesIterator implements Initializable {
 
     private boolean accept(Matcher fileNameMatcher, String key) {
         String fName = FilenameUtils.getName(key);
-        return fileNameMatcher.reset(fName).find();
+        return fileNameMatcher
+                .reset(fName)
+                .find();
     }
 }
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/pom.xml b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/pom.xml
index 731d3c140..43429fafe 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/pom.xml
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/pom.xml
@@ -17,7 +17,8 @@
   specific language governing permissions and limitations
   under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.tika</groupId>
     <artifactId>tika-pipes-iterators</artifactId>
diff --git a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/src/main/java/org/apache/tika/pipes/pipesiterator/solr/SolrPipesIterator.java b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/src/main/java/org/apache/tika/pipes/pipesiterator/solr/SolrPipesIterator.java
index 4cc50d082..9ecead289 100644
--- a/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/src/main/java/org/apache/tika/pipes/pipesiterator/solr/SolrPipesIterator.java
+++ b/tika-pipes/tika-pipes-iterators/tika-pipes-iterator-solr/src/main/java/org/apache/tika/pipes/pipesiterator/solr/SolrPipesIterator.java
@@ -43,6 +43,7 @@ import org.apache.tika.config.Initializable;
 import org.apache.tika.config.InitializableProblemHandler;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -200,11 +201,11 @@ public class SolrPipesIterator extends PipesIterator implements Initializable {
             while (!done) {
                 query.set(CursorMarkParams.CURSOR_MARK_PARAM, cursorMark);
                 QueryResponse qr = solrClient.query(solrCollection, query);
-                long totalToFetch = qr.getResults().getNumFound();
+                long totalToFetch = qr
+                        .getResults()
+                        .getNumFound();
                 String nextCursorMark = qr.getNextCursorMark();
-                LOGGER.info(
-                        "Query to fetch files to parse collection={}, q={}, onCount={}, totalCount={}",
-                        solrCollection, query, fileCount, totalToFetch);
+                LOGGER.info("Query to fetch files to parse collection={}, q={}, onCount={}, totalCount={}", solrCollection, query, fileCount, totalToFetch);
                 for (SolrDocument sd : qr.getResults()) {
                     ++fileCount;
                     String fetchKey = (String) sd.getFieldValue(idField);
@@ -214,8 +215,9 @@ public class SolrPipesIterator extends PipesIterator implements Initializable {
                         metadata.add(nextField, (String) sd.getFieldValue(nextField));
                     }
                     LOGGER.info("iterator doc: {}, idField={}, fetchKey={}", sd, idField, fetchKey);
-                    tryToAdd(new FetchEmitTuple(fetchKey, new FetchKey(fetcherName, fetchKey),
-                            new EmitKey(emitterName, emitKey), new Metadata(), handlerConfig,
+                    ParseContext parseContext = new ParseContext();
+                    parseContext.set(HandlerConfig.class, handlerConfig);
+                    tryToAdd(new FetchEmitTuple(fetchKey, new FetchKey(fetcherName, fetchKey), new EmitKey(emitterName, emitKey), new Metadata(), parseContext,
                             getOnParseException()));
                 }
                 if (cursorMark.equals(nextCursorMark)) {
@@ -232,32 +234,31 @@ public class SolrPipesIterator extends PipesIterator implements Initializable {
         if (solrUrls == null || solrUrls.isEmpty()) {
             return new CloudSolrClient.Builder(solrZkHosts, Optional.ofNullable(solrZkChroot))
                     .withHttpClient(httpClientFactory.build())
-                    .withConnectionTimeout(connectionTimeout).withSocketTimeout(socketTimeout)
+                    .withConnectionTimeout(connectionTimeout)
+                    .withSocketTimeout(socketTimeout)
                     .build();
         }
-        return new LBHttpSolrClient.Builder().withConnectionTimeout(connectionTimeout)
-                .withSocketTimeout(socketTimeout).withHttpClient(httpClientFactory.build())
-                .withBaseSolrUrls(solrUrls.toArray(new String[]{})).build();
+        return new LBHttpSolrClient.Builder()
+                .withConnectionTimeout(connectionTimeout)
+                .withSocketTimeout(socketTimeout)
+                .withHttpClient(httpClientFactory.build())
+                .withBaseSolrUrls(solrUrls.toArray(new String[]{}))
+                .build();
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         super.checkInitialization(problemHandler);
         mustNotBeEmpty("solrCollection", this.solrCollection);
         mustNotBeEmpty("urlFieldName", this.idField);
         mustNotBeEmpty("parsingIdField", this.parsingIdField);
         mustNotBeEmpty("failCountField", this.failCountField);
         mustNotBeEmpty("sizeFieldName", this.sizeFieldName);
-        if ((this.solrUrls == null || this.solrUrls.isEmpty()) &&
-                (this.solrZkHosts == null || this.solrZkHosts.isEmpty())) {
-            throw new IllegalArgumentException(
-                    "expected either param solrUrls or param solrZkHosts, but neither was specified");
+        if ((this.solrUrls == null || this.solrUrls.isEmpty()) && (this.solrZkHosts == null || this.solrZkHosts.isEmpty())) {
+            throw new IllegalArgumentException("expected either param solrUrls or param solrZkHosts, but neither was specified");
         }
-        if (this.solrUrls != null && !this.solrUrls.isEmpty() && this.solrZkHosts != null &&
-                !this.solrZkHosts.isEmpty()) {
-            throw new IllegalArgumentException(
-                    "expected either param solrUrls or param solrZkHosts, but both were specified");
+        if (this.solrUrls != null && !this.solrUrls.isEmpty() && this.solrZkHosts != null && !this.solrZkHosts.isEmpty()) {
+            throw new IllegalArgumentException("expected either param solrUrls or param solrZkHosts, but both were specified");
         }
     }
 }
diff --git a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonFetchEmitTuple.java b/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonFetchEmitTuple.java
deleted file mode 100644
index ed5931932..000000000
--- a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonFetchEmitTuple.java
+++ /dev/null
@@ -1,306 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.tika.metadata.serialization;
-
-import java.io.IOException;
-import java.io.Reader;
-import java.io.StringWriter;
-import java.io.Writer;
-import java.util.Locale;
-
-import com.fasterxml.jackson.core.JsonFactory;
-import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.core.JsonParser;
-import com.fasterxml.jackson.core.JsonToken;
-import com.fasterxml.jackson.core.StreamReadConstraints;
-
-import org.apache.tika.config.TikaConfig;
-import org.apache.tika.metadata.Metadata;
-import org.apache.tika.pipes.FetchEmitTuple;
-import org.apache.tika.pipes.HandlerConfig;
-import org.apache.tika.pipes.emitter.EmitKey;
-import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
-import org.apache.tika.pipes.fetcher.FetchKey;
-import org.apache.tika.sax.BasicContentHandlerFactory;
-import org.apache.tika.utils.StringUtils;
-
-public class JsonFetchEmitTuple {
-
-    public static final String ID = "id";
-    public static final String FETCHER = "fetcher";
-    public static final String FETCHKEY = "fetchKey";
-    public static final String FETCH_RANGE_START = "fetchRangeStart";
-    public static final String FETCH_RANGE_END = "fetchRangeEnd";
-    public static final String EMITTER = "emitter";
-    public static final String EMITKEY = "emitKey";
-    public static final String METADATAKEY = "metadata";
-    public static final String HANDLER_CONFIG = "handlerConfig";
-    public static final String ON_PARSE_EXCEPTION = "onParseException";
-    private static final String HANDLER_CONFIG_TYPE = "type";
-    private static final String HANDLER_CONFIG_WRITE_LIMIT = "writeLimit";
-    private static final String HANDLER_CONFIG_MAX_EMBEDDED_RESOURCES = "maxEmbeddedResources";
-    private static final String HANDLER_CONFIG_PARSE_MODE = "parseMode";
-
-    private static final String EMBEDDED_DOCUMENT_BYTES_CONFIG = "embeddedDocumentBytesConfig";
-    private static final String ZERO_PAD_NAME = "zeroPadName";
-    private static final String EXTRACT_EMBEDDED_DOCUMENT_BYTES = "extractEmbeddedDocumentBytes";
-    private static final String SUFFIX_STRATEGY = "suffixStrategy";
-    private static final String EMBEDDED_ID_PREFIX = "embeddedIdPrefix";
-    private static final String INCLUDE_ORIGINAL = "includeOriginal";
-
-
-    public static FetchEmitTuple fromJson(Reader reader) throws IOException {
-        try (JsonParser jParser = new JsonFactory().setStreamReadConstraints(StreamReadConstraints.builder()
-                .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build()).createParser(reader)) {
-            JsonToken token = jParser.nextToken();
-            if (token != JsonToken.START_OBJECT) {
-                throw new IOException("require start object, but see: " + token.name());
-            }
-            return parseFetchEmitTuple(jParser);
-        }
-    }
-
-
-    static FetchEmitTuple parseFetchEmitTuple(JsonParser jParser) throws IOException {
-        JsonToken token = jParser.nextToken();
-        if (token == JsonToken.START_OBJECT) {
-            token = jParser.nextToken();
-        }
-        String id = null;
-        String fetcherName = null;
-        String fetchKey = null;
-        String emitterName = null;
-        String emitKey = null;
-        long fetchRangeStart = -1l;
-        long fetchRangeEnd = -1l;
-
-        FetchEmitTuple.ON_PARSE_EXCEPTION onParseException =
-                FetchEmitTuple.DEFAULT_ON_PARSE_EXCEPTION;
-        HandlerConfig handlerConfig = HandlerConfig.DEFAULT_HANDLER_CONFIG;
-        Metadata metadata = new Metadata();
-        EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig = EmbeddedDocumentBytesConfig.SKIP;
-
-        while (token != JsonToken.END_OBJECT) {
-            if (token != JsonToken.FIELD_NAME) {
-                throw new IOException("required field name, but see: " + token.name());
-            }
-            String name = jParser.getCurrentName();
-            if (ID.equals(name)) {
-                id = getValue(jParser);
-            } else if (FETCHER.equals(name)) {
-                fetcherName = getValue(jParser);
-            } else if (FETCHKEY.equals(name)) {
-                fetchKey = getValue(jParser);
-            } else if (EMITTER.equals(name)) {
-                emitterName = getValue(jParser);
-            } else if (EMITKEY.equals(name)) {
-                emitKey = getValue(jParser);
-            } else if (METADATAKEY.equals(name)) {
-                token = jParser.nextToken();
-                if (token != JsonToken.START_OBJECT) {
-                    throw new IOException("required start object, but see: " + token.name());
-                }
-                metadata = JsonMetadata.readMetadataObject(jParser);
-            } else if (ON_PARSE_EXCEPTION.equals(name)) {
-                String value = getValue(jParser);
-                if ("skip".equalsIgnoreCase(value)) {
-                    onParseException = FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP;
-                } else if ("emit".equalsIgnoreCase(value)) {
-                    onParseException = FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT;
-                } else {
-                    throw new IOException(ON_PARSE_EXCEPTION + " must be either 'skip' or 'emit'");
-                }
-            } else if (HANDLER_CONFIG.equals(name)) {
-                handlerConfig = getHandlerConfig(jParser);
-            } else if (FETCH_RANGE_START.equals(name)) {
-                fetchRangeStart = getLong(jParser);
-            } else if (FETCH_RANGE_END.equals(name)) {
-                fetchRangeEnd = getLong(jParser);
-            } else if (EMBEDDED_DOCUMENT_BYTES_CONFIG.equals(name)) {
-                embeddedDocumentBytesConfig = getEmbeddedDocumentBytesConfig(jParser);
-            }
-            token = jParser.nextToken();
-        }
-        if (id == null) {
-            id = fetchKey;
-        }
-        return new FetchEmitTuple(id, new FetchKey(fetcherName, fetchKey, fetchRangeStart, fetchRangeEnd),
-                new EmitKey(emitterName, emitKey), metadata, handlerConfig, onParseException,
-                embeddedDocumentBytesConfig);
-    }
-
-    private static EmbeddedDocumentBytesConfig getEmbeddedDocumentBytesConfig(JsonParser jParser) throws IOException {
-        JsonToken token = jParser.nextToken();
-        if (token != JsonToken.START_OBJECT) {
-            throw new IOException("required start object, but see: " + token.name());
-        }
-        String fieldName = jParser.nextFieldName();
-        EmbeddedDocumentBytesConfig config = new EmbeddedDocumentBytesConfig(true);
-        while (fieldName != null) {
-            switch (fieldName) {
-                case EXTRACT_EMBEDDED_DOCUMENT_BYTES:
-                    boolean extract = jParser.nextBooleanValue();
-                    if (! extract) {
-                        return new EmbeddedDocumentBytesConfig(false);
-                    }
-                    break;
-                case INCLUDE_ORIGINAL:
-                    config.setIncludeOriginal(jParser.nextBooleanValue());
-                    break;
-                case EMITTER:
-                    config.setEmitter(jParser.nextTextValue());
-                    break;
-                case ZERO_PAD_NAME:
-                    config.setZeroPadNameLength(jParser.nextIntValue(0));
-                    break;
-                case SUFFIX_STRATEGY:
-                    config.setSuffixStrategy(EmbeddedDocumentBytesConfig.SUFFIX_STRATEGY.parse(
-                            jParser.nextTextValue()));
-                    break;
-                case EMBEDDED_ID_PREFIX:
-                    config.setEmbeddedIdPrefix(jParser.nextTextValue());
-                    break;
-                default:
-                    throw new IllegalArgumentException("I regret I don't understand '" + fieldName +
-                            "' in the context of an embeddedDocumentBytesConfig");
-            }
-            fieldName = jParser.nextFieldName();
-        }
-        return config;
-    }
-
-    private static HandlerConfig getHandlerConfig(JsonParser jParser) throws IOException {
-
-        JsonToken token = jParser.nextToken();
-        if (token != JsonToken.START_OBJECT) {
-            throw new IOException("required start object, but see: " + token.name());
-        }
-        BasicContentHandlerFactory.HANDLER_TYPE handlerType =
-                BasicContentHandlerFactory.HANDLER_TYPE.TEXT;
-        int writeLimit = -1;
-        int maxEmbeddedResources = -1;
-        HandlerConfig.PARSE_MODE parseMode = HandlerConfig.PARSE_MODE.RMETA;
-        String fieldName = jParser.nextFieldName();
-        while (fieldName != null) {
-            switch (fieldName) {
-                case HANDLER_CONFIG_TYPE:
-                    String value = jParser.nextTextValue();
-                    handlerType = BasicContentHandlerFactory
-                            .parseHandlerType(value, HandlerConfig.DEFAULT_HANDLER_CONFIG.getType());
-                    break;
-                case HANDLER_CONFIG_WRITE_LIMIT:
-                    writeLimit = jParser.nextIntValue(-1);
-                    break;
-                case HANDLER_CONFIG_MAX_EMBEDDED_RESOURCES:
-                    maxEmbeddedResources = jParser.nextIntValue(-1);
-                    break;
-                case HANDLER_CONFIG_PARSE_MODE:
-                    String modeString = jParser.nextTextValue();
-                    parseMode = HandlerConfig.PARSE_MODE.parseMode(modeString);
-                    break;
-                default:
-                    throw new IllegalArgumentException("I regret I don't understand '" + fieldName +
-                                                       "' in the context of a handler config");
-            }
-            fieldName = jParser.nextFieldName();
-        }
-        //TODO: implement configuration of throwOnWriteLimitReached
-        return new HandlerConfig(handlerType, parseMode, writeLimit, maxEmbeddedResources,
-                true);
-    }
-
-    private static String getValue(JsonParser jParser) throws IOException {
-        JsonToken token = jParser.nextToken();
-        if (token != JsonToken.VALUE_STRING) {
-            throw new IOException("required value string, but see: " + token.name());
-        }
-        return jParser.getValueAsString();
-    }
-
-    private static long getLong(JsonParser jParser) throws IOException {
-        JsonToken token = jParser.nextToken();
-        if (token != JsonToken.VALUE_NUMBER_INT) {
-            throw new IOException("required value long, but see: " + token.name());
-        }
-        return jParser.getValueAsLong();
-    }
-
-    public static String toJson(FetchEmitTuple t) throws IOException {
-        StringWriter writer = new StringWriter();
-        toJson(t, writer);
-        return writer.toString();
-    }
-
-    public static void toJson(FetchEmitTuple t, Writer writer) throws IOException {
-
-        try (JsonGenerator jsonGenerator = new JsonFactory().createGenerator(writer)) {
-            writeTuple(t, jsonGenerator);
-        }
-    }
-
-    static void writeTuple(FetchEmitTuple t, JsonGenerator jsonGenerator) throws IOException {
-        jsonGenerator.writeStartObject();
-        jsonGenerator.writeStringField(ID, t.getId());
-        jsonGenerator.writeStringField(FETCHER, t.getFetchKey().getFetcherName());
-        jsonGenerator.writeStringField(FETCHKEY, t.getFetchKey().getFetchKey());
-        if (t.getFetchKey().hasRange()) {
-            jsonGenerator.writeNumberField(FETCH_RANGE_START, t.getFetchKey().getRangeStart());
-            jsonGenerator.writeNumberField(FETCH_RANGE_END, t.getFetchKey().getRangeEnd());
-        }
-        jsonGenerator.writeStringField(EMITTER, t.getEmitKey().getEmitterName());
-        if (!StringUtils.isBlank(t.getEmitKey().getEmitKey())) {
-            jsonGenerator.writeStringField(EMITKEY, t.getEmitKey().getEmitKey());
-        }
-        if (t.getMetadata().size() > 0) {
-            jsonGenerator.writeFieldName(METADATAKEY);
-            JsonMetadata.writeMetadataObject(t.getMetadata(), jsonGenerator, false);
-        }
-        if (t.getHandlerConfig() != HandlerConfig.DEFAULT_HANDLER_CONFIG) {
-            jsonGenerator.writeFieldName(HANDLER_CONFIG);
-            jsonGenerator.writeStartObject();
-            jsonGenerator.writeStringField(HANDLER_CONFIG_TYPE,
-                    t.getHandlerConfig().getType().name().toLowerCase(Locale.ROOT));
-            jsonGenerator.writeStringField(HANDLER_CONFIG_PARSE_MODE,
-                    t.getHandlerConfig().getParseMode().name().toLowerCase(Locale.ROOT));
-            jsonGenerator.writeNumberField(HANDLER_CONFIG_WRITE_LIMIT,
-                    t.getHandlerConfig().getWriteLimit());
-            jsonGenerator.writeNumberField(HANDLER_CONFIG_MAX_EMBEDDED_RESOURCES,
-                    t.getHandlerConfig().getMaxEmbeddedResources());
-            jsonGenerator.writeEndObject();
-        }
-        jsonGenerator.writeStringField(ON_PARSE_EXCEPTION,
-                t.getOnParseException().name().toLowerCase(Locale.US));
-        if (t.getEmbeddedDocumentBytesConfig().isExtractEmbeddedDocumentBytes()) {
-            EmbeddedDocumentBytesConfig edbc = t.getEmbeddedDocumentBytesConfig();
-            jsonGenerator.writeFieldName(EMBEDDED_DOCUMENT_BYTES_CONFIG);
-            jsonGenerator.writeStartObject();
-            jsonGenerator.writeBooleanField(EXTRACT_EMBEDDED_DOCUMENT_BYTES,
-                    edbc.isExtractEmbeddedDocumentBytes());
-            jsonGenerator.writeNumberField(ZERO_PAD_NAME, edbc.getZeroPadName());
-            jsonGenerator.writeStringField(SUFFIX_STRATEGY,
-                    edbc.getSuffixStrategy().toString());
-            jsonGenerator.writeStringField(EMBEDDED_ID_PREFIX, edbc.getEmbeddedIdPrefix());
-            if (! StringUtils.isBlank(edbc.getEmitter())) {
-                jsonGenerator.writeStringField(EMITTER, edbc.getEmitter());
-            }
-            jsonGenerator.writeBooleanField(INCLUDE_ORIGINAL, edbc.isIncludeOriginal());
-            jsonGenerator.writeEndObject();
-        }
-        jsonGenerator.writeEndObject();
-
-    }
-}
diff --git a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadata.java b/tika-serialization/src/main/java/org/apache/tika/serialization/JsonMetadata.java
similarity index 88%
rename from tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadata.java
rename to tika-serialization/src/main/java/org/apache/tika/serialization/JsonMetadata.java
index 2e7a931b2..7281a3fa4 100644
--- a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadata.java
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/JsonMetadata.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization;
 
 
 import java.io.IOException;
@@ -51,9 +51,11 @@ public class JsonMetadata {
             return;
         }
         long max = TikaConfig.getMaxJsonStringFieldLength();
-        try (JsonGenerator jsonGenerator = new JsonFactory().setStreamReadConstraints(
-                        StreamReadConstraints.builder()
-                                .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build())
+        try (JsonGenerator jsonGenerator = new JsonFactory()
+                .setStreamReadConstraints(StreamReadConstraints
+                        .builder()
+                        .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
+                        .build())
                 .createGenerator(new CloseShieldWriter(writer))) {
             if (PRETTY_PRINT) {
                 jsonGenerator.useDefaultPrettyPrinter();
@@ -62,8 +64,7 @@ public class JsonMetadata {
         }
     }
 
-    static void writeMetadataObject(Metadata metadata, JsonGenerator jsonGenerator,
-                                    boolean prettyPrint) throws IOException {
+    public static void writeMetadataObject(Metadata metadata, JsonGenerator jsonGenerator, boolean prettyPrint) throws IOException {
         jsonGenerator.writeStartObject();
         String[] names = metadata.names();
         if (prettyPrint) {
@@ -97,8 +98,11 @@ public class JsonMetadata {
      */
     public static Metadata fromJson(Reader reader) throws IOException {
         Metadata m = null;
-        try (JsonParser jParser = new JsonFactory().setStreamReadConstraints(StreamReadConstraints.builder()
-                .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build())
+        try (JsonParser jParser = new JsonFactory()
+                .setStreamReadConstraints(StreamReadConstraints
+                        .builder()
+                        .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
+                        .build())
                 .createParser(new CloseShieldReader(reader))) {
             m = readMetadataObject(jParser);
         }
diff --git a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadataList.java b/tika-serialization/src/main/java/org/apache/tika/serialization/JsonMetadataList.java
similarity index 83%
rename from tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadataList.java
rename to tika-serialization/src/main/java/org/apache/tika/serialization/JsonMetadataList.java
index 88c21588d..e9082c721 100644
--- a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadataList.java
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/JsonMetadataList.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization;
 
 import java.io.IOException;
 import java.io.Reader;
@@ -45,15 +45,16 @@ public class JsonMetadataList {
      * @param prettyPrint  whether or not to pretty print the output
      * @throws org.apache.tika.exception.TikaException if there is an IOException during writing
      */
-    public static void toJson(List<Metadata> metadataList, Writer writer, boolean prettyPrint)
-            throws IOException {
+    public static void toJson(List<Metadata> metadataList, Writer writer, boolean prettyPrint) throws IOException {
         if (metadataList == null) {
             writer.write("null");
             return;
         }
-        try (JsonGenerator jsonGenerator = new JsonFactory().setStreamReadConstraints(
-                        StreamReadConstraints.builder().maxStringLength(
-                                TikaConfig.getMaxJsonStringFieldLength()).build())
+        try (JsonGenerator jsonGenerator = new JsonFactory()
+                .setStreamReadConstraints(StreamReadConstraints
+                        .builder()
+                        .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
+                        .build())
                 .createGenerator(new CloseShieldWriter(writer))) {
             if (prettyPrint) {
                 jsonGenerator.useDefaultPrettyPrinter();
@@ -90,14 +91,16 @@ public class JsonMetadataList {
             return ms;
         }
         ms = new ArrayList<>();
-        try (JsonParser jParser = new JsonFactory().setStreamReadConstraints(StreamReadConstraints.builder()
-                .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build())
+        try (JsonParser jParser = new JsonFactory()
+                .setStreamReadConstraints(StreamReadConstraints
+                        .builder()
+                        .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
+                        .build())
                 .createParser(new CloseShieldReader(reader))) {
 
             JsonToken token = jParser.nextToken();
             if (token != JsonToken.START_ARRAY) {
-                throw new IOException(
-                        "metadata list must start with an array, but I see: " + token.name());
+                throw new IOException("metadata list must start with an array, but I see: " + token.name());
             }
             token = jParser.nextToken();
             while (token != JsonToken.END_ARRAY) {
@@ -116,8 +119,9 @@ public class JsonMetadataList {
         if (ms.size() > 1) {
             Metadata last = ms.get(ms.size() - 1);
             String embResourcePath = last.get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH);
-            if (embResourcePath == null &&
-                    ms.get(0).get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH) != null) {
+            if (embResourcePath == null && ms
+                    .get(0)
+                    .get(TikaCoreProperties.EMBEDDED_RESOURCE_PATH) != null) {
                 ms.add(0, ms.remove(ms.size() - 1));
             }
         }
diff --git a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonStreamingSerializer.java b/tika-serialization/src/main/java/org/apache/tika/serialization/JsonStreamingSerializer.java
similarity index 86%
rename from tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonStreamingSerializer.java
rename to tika-serialization/src/main/java/org/apache/tika/serialization/JsonStreamingSerializer.java
index a0e6c4739..259695ada 100644
--- a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonStreamingSerializer.java
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/JsonStreamingSerializer.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization;
 
 
 import java.io.IOException;
@@ -41,10 +41,12 @@ public class JsonStreamingSerializer implements AutoCloseable {
 
     public void add(Metadata metadata) throws IOException {
         if (!hasStartedArray) {
-            jsonGenerator =
-                    new JsonFactory().setStreamReadConstraints(StreamReadConstraints.builder()
-                            .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build())
-                            .createGenerator(writer);
+            jsonGenerator = new JsonFactory()
+                    .setStreamReadConstraints(StreamReadConstraints
+                            .builder()
+                            .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
+                            .build())
+                    .createGenerator(writer);
             jsonGenerator.writeStartArray();
             hasStartedArray = true;
         }
diff --git a/tika-serialization/src/main/java/org/apache/tika/serialization/ParseContextSerializer.java b/tika-serialization/src/main/java/org/apache/tika/serialization/ParseContextSerializer.java
new file mode 100644
index 000000000..ece9eb27b
--- /dev/null
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/ParseContextSerializer.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization;
+
+import java.io.IOException;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+
+import org.apache.tika.parser.ParseContext;
+
+public class ParseContextSerializer extends JsonSerializer<ParseContext> {
+
+
+    @Override
+    public void serialize(ParseContext parseContext, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException {
+        jsonGenerator.writeFieldName("parseContext");
+        jsonGenerator.writeStartObject();
+        for (String className : parseContext.keySet()) {
+            try {
+                Class clazz = Class.forName(className);
+                TikaJsonSerializer.serialize(className, parseContext.get(clazz), clazz, jsonGenerator);
+            } catch (TikaSerializationException e) {
+                throw new IOException(e);
+            } catch (ClassNotFoundException e) {
+                throw new IllegalArgumentException(e);
+            }
+        }
+        jsonGenerator.writeEndObject();
+    }
+}
diff --git a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/PrettyMetadataKeyComparator.java b/tika-serialization/src/main/java/org/apache/tika/serialization/PrettyMetadataKeyComparator.java
similarity index 96%
rename from tika-serialization/src/main/java/org/apache/tika/metadata/serialization/PrettyMetadataKeyComparator.java
rename to tika-serialization/src/main/java/org/apache/tika/serialization/PrettyMetadataKeyComparator.java
index 31258ac4a..5585f415c 100644
--- a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/PrettyMetadataKeyComparator.java
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/PrettyMetadataKeyComparator.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization;
 
 import org.apache.tika.metadata.TikaCoreProperties;
 
diff --git a/tika-serialization/src/main/java/org/apache/tika/serialization/TikaJsonDeserializer.java b/tika-serialization/src/main/java/org/apache/tika/serialization/TikaJsonDeserializer.java
new file mode 100644
index 000000000..7d9cb4fc7
--- /dev/null
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/TikaJsonDeserializer.java
@@ -0,0 +1,401 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization;
+
+import java.lang.reflect.Array;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+
+import com.fasterxml.jackson.databind.JsonNode;
+
+/**
+ * See the notes @link{TikaJsonSerializer}.
+ * <p>
+ * This currently requires a setString() option on objects that have enum parameters.
+ */
+public class TikaJsonDeserializer {
+
+    public static Optional deserializeObject(JsonNode root) {
+        if (!root.isObject()) {
+            throw new IllegalArgumentException("root needs to be an object");
+        }
+        if (!root.has(TikaJsonSerializer.INSTANTIATED_CLASS_KEY)) {
+            throw new IllegalArgumentException("need to specify: " + TikaJsonSerializer.INSTANTIATED_CLASS_KEY);
+        }
+        String className = root
+                .get(TikaJsonSerializer.INSTANTIATED_CLASS_KEY)
+                .asText();
+        String superClass = root.has(TikaJsonSerializer.SUPER_CLASS_KEY) ? root
+                .get(TikaJsonSerializer.SUPER_CLASS_KEY)
+                .asText() : className;
+
+        try {
+            return Optional.of(deserialize(Class.forName(className), Class.forName(superClass), root));
+        } catch (Exception e) {
+            throw new IllegalArgumentException(e);
+        }
+    }
+
+    public static <T> T deserialize(Class<? extends T> clazz, Class<? extends T> superClazz, JsonNode root) throws ReflectiveOperationException {
+        T obj = clazz
+                .getDeclaredConstructor()
+                .newInstance();
+        Map<String, List<Method>> setters = getSetters(obj);
+        if (!root.isObject()) {
+            throw new IllegalArgumentException("must be object");
+        }
+        Iterator<Map.Entry<String, JsonNode>> fields = root.fields();
+        while (fields.hasNext()) {
+            Map.Entry<String, JsonNode> e = fields.next();
+            String name = e.getKey();
+            JsonNode child = e.getValue();
+            if (TikaJsonSerializer.INSTANTIATED_CLASS_KEY.equals(name) || TikaJsonSerializer.SUPER_CLASS_KEY.equals(name)) {
+                continue;
+            }
+            setValue(name, child, obj, setters);
+        }
+        return obj;
+    }
+
+    private static Map<String, List<Method>> getSetters(Object obj) {
+        Map<String, List<Method>> setters = new HashMap<>();
+        for (Method m : obj
+                .getClass()
+                .getMethods()) {
+            String n = m.getName();
+            if (n.startsWith(TikaJsonSerializer.SET) && n.length() > 3 && Character.isUpperCase(n.charAt(3))) {
+                if (m.getParameters().length == 1) {
+                    String paramName = TikaJsonSerializer.getParam(TikaJsonSerializer.SET, n);
+                    List<Method> methods = setters.get(paramName);
+                    if (methods == null) {
+                        methods = new ArrayList<>();
+                        setters.put(paramName, methods);
+                    }
+                    methods.add(m);
+                }
+            }
+        }
+        return setters;
+    }
+
+    private static void setValue(String name, JsonNode node, Object obj, Map<String, List<Method>> setters) throws ReflectiveOperationException {
+        List<Method> mySetters = setters.get(name);
+        if (mySetters == null || mySetters.size() == 0) {
+            throw new IllegalArgumentException("can't find any setter for " + name);
+        }
+        if (node.isNull()) {
+            setNull(name, node, obj, mySetters);
+        } else if (node.isNumber()) {
+            setNumericValue(name, node, obj, mySetters);
+        } else if (node.isTextual()) {
+            setStringValue(name, node.asText(), obj, mySetters);
+        } else if (node.isArray()) {
+            setArray(name, node, obj, mySetters);
+        } else if (node.isObject()) {
+            setObject(name, node, obj, mySetters);
+        } else if (node.isBoolean()) {
+            setBoolean(name, node, obj, mySetters);
+        }
+    }
+
+    private static void setArray(String name, JsonNode node, Object obj, List<Method> mySetters) {
+        //there's much more to be done here. :(
+        for (Method setter : mySetters) {
+            try {
+                tryArray(name, node, obj, setter);
+            } catch (InvocationTargetException | IllegalAccessException e) {
+                throw new IllegalArgumentException("couldn't create array for " + name);
+            }
+        }
+    }
+
+    private static void tryArray(String name, JsonNode node, Object obj, Method setter) throws InvocationTargetException, IllegalAccessException {
+        Class argClass = setter.getReturnType();
+        Class componentType = argClass.getComponentType();
+        if (argClass.isArray()) {
+            int len = node.size();
+            Object arrayObject = Array.newInstance(componentType, len);
+            for (int i = 0; i < len; i++) {
+                Array.set(arrayObject, i, getVal(componentType, node.get(i)));
+            }
+            setter.invoke(obj, arrayObject);
+
+        } else if (List.class.isAssignableFrom(argClass)) {
+            int len = node.size();
+            List<Object> list = new ArrayList<>();
+            for (int i = 0; i < len; i++) {
+                list.add(getVal(componentType, node.get(i)));
+            }
+            setter.invoke(obj, list);
+        }
+    }
+
+    private static <T> T getVal(T clazz, JsonNode node) {
+        if (clazz.equals(String.class)) {
+            return (T) node.asText();
+        } else if (clazz.equals(Integer.class) || clazz.equals(int.class)) {
+            return (T) Integer.valueOf(node.intValue());
+        } else if (clazz.equals(Long.class) || clazz.equals(long.class)) {
+            return (T) Long.valueOf(node.longValue());
+        } else if (clazz.equals(Float.class) || clazz.equals(float.class)) {
+            return (T) Float.valueOf(node.floatValue());
+        } else if (clazz.equals(Double.class) || clazz.equals(double.class)) {
+            return (T) Double.valueOf(node.doubleValue());
+        }
+        //add short, boolean and full class objects?
+        throw new IllegalArgumentException("I regret I don't yet support: " + clazz);
+    }
+
+    private static void setObject(String name, JsonNode node, Object obj, List<Method> mySetters) {
+        if (!node.has(TikaJsonSerializer.INSTANTIATED_CLASS_KEY)) {
+            setMap(name, node, obj, mySetters);
+            return;
+        }
+
+        Optional object = deserializeObject(node);
+        if (object.isEmpty()) {
+            //log, throw exception?!
+            return;
+        }
+        for (Method m : mySetters) {
+            Class argClass = m.getParameters()[0].getType();
+            if (argClass.isAssignableFrom(object
+                    .get()
+                    .getClass())) {
+                try {
+                    m.invoke(obj, object.get());
+                    return;
+                } catch (IllegalAccessException | InvocationTargetException e) {
+                    //swallow
+                }
+            }
+        }
+        throw new IllegalArgumentException("can't set object on " + name);
+    }
+
+    private static void setMap(String name, JsonNode node, Object obj, List<Method> setters) {
+        //TODO this should try to match the map setters with the data types
+        //for now, we're just doing <String,String>
+        Map<String, String> val = new HashMap<>();
+        Iterator<Map.Entry<String, JsonNode>> it = node.fields();
+        while (it.hasNext()) {
+            Map.Entry<String, JsonNode> e = it.next();
+            val.put(e.getKey(), e
+                    .getValue()
+                    .textValue());
+        }
+        for (Method m : setters) {
+            try {
+                m.invoke(obj, val);
+                return;
+            } catch (ReflectiveOperationException e) {
+                //swallow
+            }
+        }
+        throw new IllegalArgumentException("can't find map setter for: " + name);
+    }
+
+    private static void setBoolean(String name, JsonNode node, Object obj, List<Method> setters) throws ReflectiveOperationException {
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if (argClass.equals(Boolean.class) || argClass.equals(boolean.class)) {
+                m.invoke(obj, node.booleanValue());
+                return;
+            }
+        }
+        //TODO -- maybe check for string?
+        throw new IllegalArgumentException("can't set boolean on " + name);
+    }
+
+    private static void setNull(String name, JsonNode node, Object obj, List<Method> setters) {
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if (!TikaJsonSerializer.PRIMITIVES.contains(argClass)) {
+                try {
+
+                    m.invoke(obj, argClass.cast(null));
+                    return;
+                } catch (Exception e) {
+                    e.printStackTrace();
+                    //swallow
+                }
+            }
+        }
+        throw new IllegalArgumentException("can't set null on " + name);
+    }
+
+    private static void setStringValue(String name, String txt, Object obj, List<Method> setters) throws ReflectiveOperationException {
+
+        //try for exact match first
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if (argClass.equals(String.class)) {
+                m.invoke(obj, txt);
+                return;
+            }
+        }
+        Method intMethod = null;
+        Method longMethod = null;
+        Method doubleMethod = null;
+        Method floatMethod = null;
+        Method shortMethod = null;
+        Method boolMethod = null;
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if (argClass.equals(Integer.class) || argClass.equals(int.class)) {
+                intMethod = m;
+            } else if (argClass.equals(Long.class) || argClass.equals(long.class)) {
+                longMethod = m;
+            } else if (argClass.equals(Float.class) || argClass.equals(float.class)) {
+                floatMethod = m;
+            } else if (argClass.equals(Double.class) || argClass.equals(double.class)) {
+                doubleMethod = m;
+            } else if (argClass.equals(Short.class) || argClass.equals(short.class)) {
+                shortMethod = m;
+            } else if (argClass.equals(Boolean.class) || argClass.equals(boolean.class)) {
+                boolMethod = m;
+            }
+        }
+
+        if (shortMethod != null) {
+            try {
+                short val = Short.parseShort(txt);
+                shortMethod.invoke(obj, val);
+                return;
+            } catch (NumberFormatException e) {
+                //swallow
+            }
+        } else if (intMethod != null) {
+            try {
+                int val = Integer.parseInt(txt);
+                intMethod.invoke(obj, val);
+                return;
+            } catch (NumberFormatException e) {
+                //swallow
+            }
+        } else if (floatMethod != null) {
+            try {
+                float val = Float.parseFloat(txt);
+                floatMethod.invoke(obj, val);
+                return;
+            } catch (NumberFormatException e) {
+                //swallow
+            }
+        } else if (longMethod != null) {
+            try {
+                long val = Long.parseLong(txt);
+                longMethod.invoke(obj, val);
+                return;
+            } catch (NumberFormatException e) {
+                //swallow
+            }
+        } else if (doubleMethod != null) {
+            try {
+                double val = Double.parseDouble(txt);
+                doubleMethod.invoke(obj, val);
+                return;
+            } catch (NumberFormatException e) {
+                //swallow
+            }
+        } else if (boolMethod != null) {
+            if (txt.equalsIgnoreCase("true")) {
+                boolMethod.invoke(obj, true);
+            } else if (txt.equalsIgnoreCase("false")) {
+                boolMethod.invoke(obj, false);
+            }
+        }
+        throw new IllegalArgumentException("I regret I couldn't find a setter for: " + name);
+
+    }
+
+    private static void setNumericValue(String name, JsonNode node, Object obj, List<Method> setters) throws ReflectiveOperationException {
+
+        //try numeric and equals first
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if ((argClass.equals(int.class) || argClass.equals(Integer.class)) && node.isInt()) {
+                m.invoke(obj, node.intValue());
+                return;
+            } else if ((argClass.equals(long.class) || argClass.equals(Long.class)) && node.isLong()) {
+                m.invoke(obj, node.asLong());
+                return;
+            } else if ((argClass.equals(float.class) || argClass.equals(Float.class)) && node.isFloat()) {
+                m.invoke(obj, node.floatValue());
+                return;
+            } else if ((argClass.equals(double.class) || argClass.equals(Double.class)) && node.isDouble()) {
+                m.invoke(obj, node.doubleValue());
+                return;
+            } else if ((argClass.equals(short.class) || argClass.equals(Short.class)) && node.isShort()) {
+                m.invoke(obj, node.shortValue());
+                return;
+            }
+        }
+        //try for higher precision setters
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if ((argClass.equals(long.class) || argClass.equals(Long.class)) && node.isInt()) {
+                m.invoke(obj, node.asLong());
+                return;
+            } else if ((argClass.equals(double.class) || argClass.equals(Double.class)) && node.isFloat()) {
+                m.invoke(obj, node.floatValue());
+                return;
+            }
+        }
+        //try for lower precision setters
+        //we have to do this for node=double, type=float; should we do this for long->integer?!
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if ((argClass.equals(int.class) || argClass.equals(Integer.class)) && node.isLong()) {
+                long val = node.longValue();
+                if (val >= Integer.MAX_VALUE || val <= Integer.MIN_VALUE) {
+                    //don't do this
+                } else {
+                    m.invoke(obj, node.intValue());
+                }
+                return;
+            } else if ((argClass.equals(float.class) || argClass.equals(Float.class)) && node.isDouble()) {
+                //TODO -- check for over/underflow
+                m.invoke(obj, node.floatValue());
+                return;
+            } else if ((argClass.equals(short.class) || argClass.equals(Short.class)) && node.isInt()) {
+                int val = node.intValue();
+                if (val > Short.MAX_VALUE || val < Short.MIN_VALUE) {
+                    //don't do this
+                } else {
+                    m.invoke(obj, node.shortValue());
+                    return;
+                }
+            }
+        }
+        //finally try for String
+        for (Method m : setters) {
+            Class argClass = m.getParameters()[0].getType();
+            if (argClass.equals(String.class)) {
+                m.invoke(obj, node.asText());
+                return;
+            }
+        }
+        throw new IllegalArgumentException("Couldn't find numeric setter for: " + name);
+
+    }
+}
diff --git a/tika-serialization/src/main/java/org/apache/tika/serialization/TikaJsonSerializer.java b/tika-serialization/src/main/java/org/apache/tika/serialization/TikaJsonSerializer.java
new file mode 100644
index 000000000..531406847
--- /dev/null
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/TikaJsonSerializer.java
@@ -0,0 +1,282 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization;
+
+import java.io.IOException;
+import java.lang.reflect.Constructor;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ * This is a basic serializer that requires that an object:
+ * a) have a no-arg constructor
+ * b) have both setters and getters for the same parameters with the same names, e.g. setXYZ and getXYZ
+ * c) setters and getters have to follow the pattern setX where x is a capital letter
+ * d) have maps as parameters where the keys are strings (and the values are strings for now)
+ * e) at deserialization time, objects that have setters for enums also have to have a setter for a string value of that enum
+ */
+public class TikaJsonSerializer {
+
+    public static String INSTANTIATED_CLASS_KEY = "_class";
+    public static String SUPER_CLASS_KEY = "_super_class";
+    static Set<Class> PRIMITIVES = Set.of(int.class, double.class, float.class, long.class, short.class, boolean.class, String.class, byte.class, char.class);
+    static Set<Class> BOXED = Set.of(Integer.class, Double.class, Float.class, Long.class, Short.class, Boolean.class, Byte.class, Character.class);
+    static String SET = "set";
+    private static Logger LOG = LoggerFactory.getLogger(TikaJsonSerializer.class);
+    private static String GET = "get";
+    private static String IS = "is";
+
+    public static void serialize(Object obj, JsonGenerator jsonGenerator) throws TikaSerializationException, IOException {
+        serialize(null, obj, obj.getClass(), jsonGenerator);
+    }
+
+    public static void serialize(String fieldName, Object obj, Class superClass, JsonGenerator jsonGenerator) throws TikaSerializationException, IOException {
+        if (obj == null) {
+            if (fieldName == null) {
+                jsonGenerator.writeNull();
+            } else {
+                jsonGenerator.writeNullField(fieldName);
+            }
+        } else if (PRIMITIVES.contains(obj.getClass()) || BOXED.contains(obj.getClass())) {
+            try {
+                serializePrimitiveAndBoxed(fieldName, obj, jsonGenerator);
+            } catch (IOException e) {
+                throw new TikaSerializationException("problem serializing", e);
+            }
+        } else if (isCollection(obj)) {
+            serializeCollection(fieldName, obj, jsonGenerator);
+        } else if (obj
+                .getClass()
+                .isEnum()) {
+            jsonGenerator.writeStringField(fieldName, ((Enum) obj).name());
+        } else {
+            serializeObject(fieldName, obj, superClass, jsonGenerator);
+        }
+    }
+
+    /**
+     * limited to array, list and map
+     *
+     * @param obj
+     * @return
+     */
+    private static boolean isCollection(Object obj) {
+        Class clazz = obj.getClass();
+        return clazz.isArray() || List.class.isAssignableFrom(clazz) || Map.class.isAssignableFrom(clazz);
+    }
+
+
+    /**
+     * @param fieldName     can be null -- used only for logging and debugging
+     * @param obj
+     * @param superClass
+     * @param jsonGenerator
+     * @throws TikaSerializationException
+     */
+    public static void serializeObject(String fieldName, Object obj, Class superClass, JsonGenerator jsonGenerator) throws TikaSerializationException {
+
+
+        try {
+            Constructor constructor = obj
+                    .getClass()
+                    .getConstructor();
+        } catch (NoSuchMethodException e) {
+            throw new IllegalArgumentException("class (" + obj.getClass() + ") doesn't have a no-arg constructor. Respectfully not seralizing.");
+        }
+        try {
+            if (fieldName != null) {
+                jsonGenerator.writeFieldName(fieldName);
+            }
+            jsonGenerator.writeStartObject();
+            jsonGenerator.writeStringField(INSTANTIATED_CLASS_KEY, obj
+                    .getClass()
+                    .getName());
+            if (!obj
+                    .getClass()
+                    .equals(superClass)) {
+                jsonGenerator.writeStringField(SUPER_CLASS_KEY, superClass.getName());
+            }
+            Map<String, Method> matches = getGetters(obj
+                    .getClass()
+                    .getMethods());
+            //iterate through the getters
+            for (Map.Entry<String, Method> e : matches.entrySet()) {
+                try {
+                    Object methodVal = e
+                            .getValue()
+                            .invoke(obj);
+                    Class valSuperClass = e
+                            .getValue()
+                            .getReturnType();
+                    serialize(e.getKey(), methodVal, valSuperClass, jsonGenerator);
+                } catch (IllegalAccessException | InvocationTargetException ex) {
+                    throw new TikaSerializationException("couldn't write paramName=" + e.getKey(), ex);
+                }
+            }
+
+            jsonGenerator.writeEndObject();
+        } catch (IOException e) {
+            throw new TikaSerializationException("couldn't serialize", e);
+        }
+    }
+
+    private static Map<String, Method> getGetters(Method[] methods) {
+        Map<String, List<Method>> getters = new HashMap<>();
+        Map<String, List<Method>> setters = new HashMap<>();
+
+        for (Method m : methods) {
+            String name = m.getName();
+            if (name.startsWith("get") && name.length() > 3 && Character.isUpperCase(name.charAt(3))) {
+                String param = getParam(GET, name);
+                add(param, m, getters);
+            } else if (name.startsWith("is") && name.length() > 2 && Character.isUpperCase(name.charAt(2))) {
+                String param = getParam(IS, name);
+                add(param, m, getters);
+            } else if (name.startsWith("set") && name.length() > 3 && Character.isUpperCase(name.charAt(3))) {
+                //take only single param setters
+                if (m.getParameters().length == 1) {
+                    String param = getParam(SET, name);
+                    add(param, m, setters);
+                }
+            }
+        }
+        //this strictly looks for classA.equals(classB)
+        //this does not look for instance of, nor does it look for boxed vs. primitives
+        //Also, TODO -- this should favor getters and setters with Strings over those
+        //with complex types
+        Map<String, Method> ret = new HashMap<>();
+        for (Map.Entry<String, List<Method>> e : getters.entrySet()) {
+            String paramName = e.getKey();
+            List<Method> setterList = setters.get(paramName);
+            if (setterList == null || setterList.size() == 0) {
+                LOG.debug("Couldn't find setter for getter: " + paramName);
+                continue;
+            }
+            for (Method getter : e.getValue()) {
+                for (Method setter : setterList) {
+                    Class setClass = setter.getParameters()[0].getType();
+                    if (getter
+                            .getReturnType()
+                            .equals(setClass)) {
+                        ret.put(paramName, getter);
+                    }
+                }
+            }
+        }
+        return ret;
+    }
+
+    private static void serializeCollection(String fieldName, Object obj, JsonGenerator jsonGenerator) throws IOException, TikaSerializationException {
+        if (fieldName != null) {
+            jsonGenerator.writeFieldName(fieldName);
+        }
+        Class clazz = obj.getClass();
+        if (clazz.isArray()) {
+            jsonGenerator.writeStartArray();
+            for (Object item : (Object[]) obj) {
+                serialize(item, jsonGenerator);
+            }
+            jsonGenerator.writeEndArray();
+        } else if (List.class.isAssignableFrom(clazz)) {
+            jsonGenerator.writeStartArray();
+            for (Object item : (List) obj) {
+                serialize(item, jsonGenerator);
+            }
+            jsonGenerator.writeEndArray();
+        } else if (Map.class.isAssignableFrom(clazz)) {
+            jsonGenerator.writeStartObject();
+            for (Map.Entry<String, Object> e : ((Map<String, Object>) obj).entrySet()) {
+                serialize(e.getKey(), e.getValue(), e
+                        .getValue()
+                        .getClass(), jsonGenerator);
+            }
+            jsonGenerator.writeEndObject();
+        } else {
+            throw new UnsupportedOperationException("Should have been a collection?! " + clazz);
+        }
+    }
+
+    private static void serializeItem(String field, Object obj, Class<?> generalType, JsonGenerator jsonGenerator) {
+
+    }
+
+    private static void serializePrimitiveAndBoxed(String paramName, Object obj, JsonGenerator jsonGenerator) throws IOException {
+        Class clazz = obj.getClass();
+        if (paramName != null) {
+            jsonGenerator.writeFieldName(paramName);
+        }
+        if (clazz.equals(String.class)) {
+            jsonGenerator.writeString((String) obj);
+        } else if (clazz.equals(Integer.class)) {
+            jsonGenerator.writeNumber((Integer) obj);
+        } else if (clazz.equals(Short.class)) {
+            jsonGenerator.writeNumber((Short) obj);
+        } else if (clazz.equals(Long.class)) {
+            jsonGenerator.writeNumber((Long) obj);
+        } else if (clazz.equals(Float.class)) {
+            jsonGenerator.writeNumber((Float) obj);
+        } else if (clazz.equals(Double.class)) {
+            jsonGenerator.writeNumber((Double) obj);
+        } else if (clazz.equals(Boolean.class)) {
+            jsonGenerator.writeBoolean((Boolean) obj);
+        } else if (clazz.equals(short.class)) {
+            jsonGenerator.writeNumber((short) obj);
+        } else if (clazz.equals(int.class)) {
+            jsonGenerator.writeNumber((int) obj);
+        } else if (clazz.equals(long.class)) {
+            jsonGenerator.writeNumber((long) obj);
+        } else if (clazz.equals(float.class)) {
+            jsonGenerator.writeNumber((float) obj);
+        } else if (clazz.equals(double.class)) {
+            jsonGenerator.writeNumber((double) obj);
+        } else if (clazz.equals(boolean.class)) {
+            jsonGenerator.writeBoolean((boolean) obj);
+        } else {
+            throw new UnsupportedOperationException("I regret that I don't yet support " + clazz);
+        }
+
+    }
+
+    private static void add(String param, Method method, Map<String, List<Method>> map) {
+        List<Method> methods = map.get(param);
+        if (methods == null) {
+            methods = new ArrayList<>();
+            map.put(param, methods);
+        }
+        methods.add(method);
+    }
+
+    static String getParam(String prefix, String name) {
+        String ret = name.substring(prefix.length());
+        ret = ret
+                .substring(0, 1)
+                .toLowerCase(Locale.ROOT) + ret.substring(1);
+        return ret;
+    }
+
+}
diff --git a/tika-serialization/src/main/java/org/apache/tika/serialization/TikaSerializationException.java b/tika-serialization/src/main/java/org/apache/tika/serialization/TikaSerializationException.java
new file mode 100644
index 000000000..ddc670015
--- /dev/null
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/TikaSerializationException.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization;
+
+import org.apache.tika.exception.TikaException;
+
+public class TikaSerializationException extends TikaException {
+
+    public TikaSerializationException(String msg) {
+        super(msg);
+    }
+
+    public TikaSerializationException(String msg, Throwable cause) {
+        super(msg, cause);
+    }
+}
diff --git a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonEmitData.java b/tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonEmitData.java
similarity index 78%
rename from tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonEmitData.java
rename to tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonEmitData.java
index cc9cd9320..a8e9788b7 100644
--- a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonEmitData.java
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonEmitData.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization.pipes;
 
 import java.io.IOException;
 import java.io.Writer;
@@ -27,18 +27,26 @@ import org.apache.tika.config.TikaConfig;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.pipes.emitter.EmitData;
 import org.apache.tika.pipes.emitter.EmitKey;
+import org.apache.tika.serialization.JsonMetadata;
 
 public class JsonEmitData {
 
     public static void toJson(EmitData emitData, Writer writer) throws IOException {
         try (JsonGenerator jsonGenerator = new JsonFactory()
-                .setStreamReadConstraints(StreamReadConstraints.builder()
-                            .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
-                        .build()).createGenerator(writer)) {
+                .setStreamReadConstraints(StreamReadConstraints
+                        .builder()
+                        .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
+                        .build())
+                .createGenerator(writer)) {
             jsonGenerator.writeStartObject();
             EmitKey key = emitData.getEmitKey();
             jsonGenerator.writeStringField(JsonFetchEmitTuple.EMITTER, key.getEmitterName());
             jsonGenerator.writeStringField(JsonFetchEmitTuple.EMITKEY, key.getEmitKey());
+            if (!emitData
+                    .getParseContext()
+                    .isEmpty()) {
+                jsonGenerator.writeObject(emitData.getParseContext());
+            }
             jsonGenerator.writeFieldName("data");
             jsonGenerator.writeStartArray();
             for (Metadata m : emitData.getMetadataList()) {
diff --git a/tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonFetchEmitTuple.java b/tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonFetchEmitTuple.java
new file mode 100644
index 000000000..72082996b
--- /dev/null
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonFetchEmitTuple.java
@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization.pipes;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringWriter;
+import java.io.Writer;
+import java.util.Iterator;
+import java.util.Locale;
+import java.util.Map;
+
+import com.fasterxml.jackson.core.JsonFactory;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.commons.io.IOExceptionWithCause;
+
+import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
+import org.apache.tika.pipes.FetchEmitTuple;
+import org.apache.tika.pipes.emitter.EmitKey;
+import org.apache.tika.pipes.fetcher.FetchKey;
+import org.apache.tika.serialization.JsonMetadata;
+import org.apache.tika.serialization.ParseContextSerializer;
+import org.apache.tika.serialization.TikaJsonDeserializer;
+import org.apache.tika.serialization.TikaJsonSerializer;
+import org.apache.tika.utils.StringUtils;
+
+public class JsonFetchEmitTuple {
+
+    public static final String ID = "id";
+    public static final String FETCHER = "fetcher";
+    public static final String FETCHKEY = "fetchKey";
+    public static final String FETCH_RANGE_START = "fetchRangeStart";
+    public static final String FETCH_RANGE_END = "fetchRangeEnd";
+    public static final String EMITTER = "emitter";
+    public static final String EMITKEY = "emitKey";
+    public static final String METADATAKEY = "metadata";
+    public static final String ON_PARSE_EXCEPTION = "onParseException";
+    public static final String PARSE_CONTEXT = "parseContext";
+
+    public static FetchEmitTuple fromJson(Reader reader) throws IOException {
+        //try (JsonParser jParser = new JsonFactory().setStreamReadConstraints(StreamReadConstraints.builder()
+        //      .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build()).createParser(reader)) {
+
+        JsonNode root = new ObjectMapper().readTree(reader);
+        return parseFetchEmitTuple(root);
+    }
+
+
+    static FetchEmitTuple parseFetchEmitTuple(JsonNode root) throws IOException {
+        String id = readVal(ID, root, null, true);
+        String fetcherName = readVal(FETCHER, root, null, true);
+        String fetchKey = readVal(FETCHKEY, root, null, true);
+        String emitterName = readVal(EMITTER, root, "", false);
+        String emitKey = readVal(EMITKEY, root, "", false);
+        long fetchRangeStart = readLong(FETCH_RANGE_START, root, -1l, false);
+        long fetchRangeEnd = readLong(FETCH_RANGE_END, root, -1l, false);
+        Metadata metadata = readMetadata(root);
+        ParseContext parseContext = readParseContext(root);
+        FetchEmitTuple.ON_PARSE_EXCEPTION onParseException = readOnParseException(root);
+
+        return new FetchEmitTuple(id, new FetchKey(fetcherName, fetchKey, fetchRangeStart, fetchRangeEnd), new EmitKey(emitterName, emitKey), metadata, parseContext,
+                onParseException);
+    }
+
+    private static FetchEmitTuple.ON_PARSE_EXCEPTION readOnParseException(JsonNode root) throws IOException {
+        JsonNode onParseExNode = root.get(ON_PARSE_EXCEPTION);
+        if (onParseExNode == null) {
+            return FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT;
+        }
+        String txt = onParseExNode.asText();
+        if ("skip".equalsIgnoreCase(txt)) {
+            return FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP;
+        } else if ("emit".equalsIgnoreCase(txt)) {
+            return FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT;
+        } else {
+            throw new IOException(ON_PARSE_EXCEPTION + " must be either 'skip' or 'emit'");
+        }
+    }
+
+    private static ParseContext readParseContext(JsonNode root) throws IOException {
+        JsonNode contextNode = root.get(PARSE_CONTEXT);
+        if (contextNode == null) {
+            return new ParseContext();
+        }
+        ParseContext parseContext = new ParseContext();
+        Iterator<Map.Entry<String, JsonNode>> it = contextNode.fields();
+        while (it.hasNext()) {
+            Map.Entry<String, JsonNode> e = it.next();
+            String clazzName = e.getKey();
+            JsonNode obj = e.getValue();
+            String className = readVal(TikaJsonSerializer.INSTANTIATED_CLASS_KEY, obj, null, true);
+            String superClassName = readVal(TikaJsonSerializer.SUPER_CLASS_KEY, obj, className, false);
+            try {
+                Class clazz = Class.forName(className);
+                Class superClazz = clazz.equals(superClassName) ? clazz : Class.forName(superClassName);
+                parseContext.set(clazz, TikaJsonDeserializer.deserialize(clazz, superClazz, obj));
+            } catch (ReflectiveOperationException ex) {
+                throw new IOExceptionWithCause(ex);
+            }
+        }
+        return parseContext;
+    }
+
+    private static Metadata readMetadata(JsonNode root) {
+        JsonNode metadataNode = root.get(METADATAKEY);
+        if (metadataNode == null) {
+            return new Metadata();
+        }
+        Metadata metadata = new Metadata();
+        Iterator<Map.Entry<String, JsonNode>> it = metadataNode.fields();
+        while (it.hasNext()) {
+            Map.Entry<String, JsonNode> e = it.next();
+            JsonNode vals = e.getValue();
+            String k = e.getKey();
+            if (vals.isArray()) {
+                Iterator<JsonNode> arrIt = vals.iterator();
+                while (arrIt.hasNext()) {
+                    JsonNode arrVal = arrIt.next();
+                    metadata.add(k, arrVal.textValue());
+                }
+            } else {
+                metadata.set(k, vals.asText());
+            }
+        }
+        return metadata;
+    }
+
+    private static String readVal(String key, JsonNode jsonObj, String defaultRet, boolean isRequired) throws IOException {
+        JsonNode valNode = jsonObj.get(key);
+        if (valNode == null) {
+            if (isRequired) {
+                throw new IOException("required value string, but see: " + key);
+            }
+            return defaultRet;
+        }
+        return valNode.asText();
+    }
+
+    private static long readLong(String key, JsonNode jsonObj, long defaultVal, boolean isRequired) throws IOException {
+        JsonNode val = jsonObj.get(key);
+        if (val == null) {
+            if (isRequired) {
+                throw new IOException("required value long, but see: " + key);
+            }
+            return defaultVal;
+        }
+        return val.longValue();
+    }
+
+    public static String toJson(FetchEmitTuple t) throws IOException {
+        StringWriter writer = new StringWriter();
+        toJson(t, writer);
+        return writer.toString();
+    }
+
+    public static void toJson(FetchEmitTuple t, Writer writer) throws IOException {
+
+        try (JsonGenerator jsonGenerator = new JsonFactory().createGenerator(writer)) {
+            writeTuple(t, jsonGenerator);
+        }
+    }
+
+    static void writeTuple(FetchEmitTuple t, JsonGenerator jsonGenerator) throws IOException {
+        jsonGenerator.writeStartObject();
+        jsonGenerator.writeStringField(ID, t.getId());
+        jsonGenerator.writeStringField(FETCHER, t
+                .getFetchKey()
+                .getFetcherName());
+        jsonGenerator.writeStringField(FETCHKEY, t
+                .getFetchKey()
+                .getFetchKey());
+        if (t
+                .getFetchKey()
+                .hasRange()) {
+            jsonGenerator.writeNumberField(FETCH_RANGE_START, t
+                    .getFetchKey()
+                    .getRangeStart());
+            jsonGenerator.writeNumberField(FETCH_RANGE_END, t
+                    .getFetchKey()
+                    .getRangeEnd());
+        }
+        jsonGenerator.writeStringField(EMITTER, t
+                .getEmitKey()
+                .getEmitterName());
+        if (!StringUtils.isBlank(t
+                .getEmitKey()
+                .getEmitKey())) {
+            jsonGenerator.writeStringField(EMITKEY, t
+                    .getEmitKey()
+                    .getEmitKey());
+        }
+        if (t
+                .getMetadata()
+                .size() > 0) {
+            jsonGenerator.writeFieldName(METADATAKEY);
+            JsonMetadata.writeMetadataObject(t.getMetadata(), jsonGenerator, false);
+        }
+
+        jsonGenerator.writeStringField(ON_PARSE_EXCEPTION, t
+                .getOnParseException()
+                .name()
+                .toLowerCase(Locale.US));
+        if (!t
+                .getParseContext()
+                .isEmpty()) {
+            ParseContextSerializer s = new ParseContextSerializer();
+            s.serialize(t.getParseContext(), jsonGenerator, null);
+        }
+        jsonGenerator.writeEndObject();
+
+    }
+}
diff --git a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleList.java b/tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleList.java
similarity index 64%
rename from tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleList.java
rename to tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleList.java
index e7181aeb3..652c724f1 100644
--- a/tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleList.java
+++ b/tika-serialization/src/main/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleList.java
@@ -14,20 +14,21 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization.pipes;
 
 import java.io.IOException;
 import java.io.Reader;
 import java.io.StringWriter;
 import java.io.Writer;
 import java.util.ArrayList;
+import java.util.Iterator;
 import java.util.List;
 
 import com.fasterxml.jackson.core.JsonFactory;
 import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.core.JsonParser;
-import com.fasterxml.jackson.core.JsonToken;
 import com.fasterxml.jackson.core.StreamReadConstraints;
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.ObjectMapper;
 
 import org.apache.tika.config.TikaConfig;
 import org.apache.tika.pipes.FetchEmitTuple;
@@ -35,18 +36,17 @@ import org.apache.tika.pipes.FetchEmitTuple;
 public class JsonFetchEmitTupleList {
 
     public static List<FetchEmitTuple> fromJson(Reader reader) throws IOException {
-        List<FetchEmitTuple> list;
-        try (JsonParser jParser = new JsonFactory().setStreamReadConstraints(StreamReadConstraints.builder()
-                .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build()).createParser(reader)) {
-            JsonToken token = jParser.nextToken();
-            if (token != JsonToken.START_ARRAY) {
-                throw new IOException("require start array, but see: " + token.name());
-            }
-            list = new ArrayList<>();
-            while (token != JsonToken.END_ARRAY) {
-                list.add(JsonFetchEmitTuple.parseFetchEmitTuple(jParser));
-                token = jParser.nextToken();
-            }
+        JsonNode root = new ObjectMapper().readTree(reader);
+
+        if (!root.isArray()) {
+            throw new IOException("FetchEmitTupleList must be an array");
+        }
+        List<FetchEmitTuple> list = new ArrayList<>();
+        Iterator<JsonNode> it = root.iterator();
+        while (it.hasNext()) {
+            JsonNode n = it.next();
+            FetchEmitTuple t = JsonFetchEmitTuple.parseFetchEmitTuple(n);
+            list.add(t);
         }
         return list;
     }
@@ -59,8 +59,12 @@ public class JsonFetchEmitTupleList {
 
     public static void toJson(List<FetchEmitTuple> list, Writer writer) throws IOException {
 
-        try (JsonGenerator jsonGenerator = new JsonFactory().setStreamReadConstraints(StreamReadConstraints.builder()
-                .maxStringLength(TikaConfig.getMaxJsonStringFieldLength()).build()).createGenerator(writer)) {
+        try (JsonGenerator jsonGenerator = new JsonFactory()
+                .setStreamReadConstraints(StreamReadConstraints
+                        .builder()
+                        .maxStringLength(TikaConfig.getMaxJsonStringFieldLength())
+                        .build())
+                .createGenerator(writer)) {
             jsonGenerator.writeStartArray();
             for (FetchEmitTuple t : list) {
                 JsonFetchEmitTuple.writeTuple(t, jsonGenerator);
diff --git a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonMetadataListTest.java b/tika-serialization/src/test/java/org/apache/tika/serialization/JsonMetadataListTest.java
similarity index 86%
rename from tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonMetadataListTest.java
rename to tika-serialization/src/test/java/org/apache/tika/serialization/JsonMetadataListTest.java
index 930a594ef..79ecf8df4 100644
--- a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonMetadataListTest.java
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/JsonMetadataListTest.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertNull;
@@ -58,8 +58,7 @@ public class JsonMetadataListTest {
         metadataList.add(m2);
         StringWriter writer = new StringWriter();
         JsonMetadataList.toJson(metadataList, writer);
-        List<Metadata> deserialized =
-                JsonMetadataList.fromJson(new StringReader(writer.toString()));
+        List<Metadata> deserialized = JsonMetadataList.fromJson(new StringReader(writer.toString()));
         assertEquals(metadataList, deserialized);
 
         //now test streaming serializer
@@ -77,7 +76,9 @@ public class JsonMetadataListTest {
     public void testListNull() throws Exception {
         StringWriter writer = new StringWriter();
         JsonMetadataList.toJson(null, writer);
-        assertEquals("null", writer.toString().trim());
+        assertEquals("null", writer
+                .toString()
+                .trim());
 
         List<Metadata> m = JsonMetadataList.fromJson(null);
         assertNull(m);
@@ -85,8 +86,7 @@ public class JsonMetadataListTest {
 
     @Test
     public void testListCorrupted() throws Exception {
-        String json = "[{\"k1\":[\"v1\",\"v2\",\"v3\",\"v4\",\"v4\"],\"k2\":\"v1\"}," +
-                "\"k3\":[\"v1\",\"v2\",\"v3\",\"v4\",\"v4\"],\"k4\":\"v1\"}]";
+        String json = "[{\"k1\":[\"v1\",\"v2\",\"v3\",\"v4\",\"v4\"],\"k2\":\"v1\"}," + "\"k3\":[\"v1\",\"v2\",\"v3\",\"v4\",\"v4\"],\"k4\":\"v1\"}]";
         List<Metadata> m = JsonMetadataList.fromJson(null);
         assertNull(m);
     }
@@ -115,21 +115,26 @@ public class JsonMetadataListTest {
         metadataList.add(m2);
         StringWriter writer = new StringWriter();
         JsonMetadataList.toJson(metadataList, writer);
-        assertTrue(writer.toString().startsWith("["));
+        assertTrue(writer
+                .toString()
+                .startsWith("["));
         writer = new StringWriter();
         JsonMetadataList.setPrettyPrinting(true);
         JsonMetadataList.toJson(metadataList, writer);
-        assertTrue(writer.toString().replaceAll("\r\n", "\n").startsWith(
-                "[ {\n" + "  \"zk1\" : [ \"v1\", \"v2\", \"v3\", \"v4\", \"v4\" ],\n" +
-                        "  \"zk2\" : \"v1\",\n" +
-                        "  \"X-TIKA:content\" : \"this is the content\"\n" + "},"));
+        assertTrue(writer
+                .toString()
+                .replaceAll("\r\n", "\n")
+                .startsWith("[ {\n" + "  \"zk1\" : [ \"v1\", \"v2\", \"v3\", \"v4\", \"v4\" ],\n" + "  \"zk2\" : \"v1\",\n" + "  \"X-TIKA:content\" : \"this is the content\"\n" +
+                        "},"));
 
 
         //now set it back to false
         JsonMetadataList.setPrettyPrinting(false);
         writer = new StringWriter();
         JsonMetadataList.toJson(metadataList, writer);
-        assertTrue(writer.toString().startsWith("["));
+        assertTrue(writer
+                .toString()
+                .startsWith("["));
     }
 
     @Test
diff --git a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonMetadataTest.java b/tika-serialization/src/test/java/org/apache/tika/serialization/JsonMetadataTest.java
similarity index 87%
rename from tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonMetadataTest.java
rename to tika-serialization/src/test/java/org/apache/tika/serialization/JsonMetadataTest.java
index a7b16e9ef..aa9b8ccad 100644
--- a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonMetadataTest.java
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/JsonMetadataTest.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
@@ -58,17 +58,19 @@ public class JsonMetadataTest {
         assertEquals(metadata, deserialized);
 
         //test that this really is 6 Chinese characters
-        assertEquals(6, deserialized.get("alma_mater").length());
+        assertEquals(6, deserialized
+                .get("alma_mater")
+                .length());
 
         //now test pretty print;
         writer = new StringWriter();
         JsonMetadata.setPrettyPrinting(true);
         JsonMetadata.toJson(metadata, writer);
-        assertTrue(writer.toString().replaceAll("\r\n", "\n").contains(
-                "\"json_escapes\" : \"the: \\\"quick\\\" brown, fox\",\n" +
-                        "  \"k1\" : [ \"v1\", \"v2\" ],\n" + "  \"k3\" : [ \"v3\", \"v3\" ],\n" +
-                        "  \"k4\" : \"500,000\",\n" +
-                        "  \"url\" : \"/myApp/myAction.html?method=router&cmd=1\"\n" + "}"));
+        assertTrue(writer
+                .toString()
+                .replaceAll("\r\n", "\n")
+                .contains("\"json_escapes\" : \"the: \\\"quick\\\" brown, fox\",\n" + "  \"k1\" : [ \"v1\", \"v2\" ],\n" + "  \"k3\" : [ \"v3\", \"v3\" ],\n" +
+                        "  \"k4\" : \"500,000\",\n" + "  \"url\" : \"/myApp/myAction.html?method=router&cmd=1\"\n" + "}"));
     }
 
     @Test
@@ -113,8 +115,7 @@ public class JsonMetadataTest {
     public void testLargeValues() throws Exception {
         //TIKA-4154
         TikaConfig tikaConfig = null;
-        try (InputStream is =
-                     JsonMetadata.class.getResourceAsStream("/config/tika-config-json.xml")) {
+        try (InputStream is = JsonMetadata.class.getResourceAsStream("/config/tika-config-json.xml")) {
             tikaConfig = new TikaConfig(is);
         }
         StringBuilder sb = new StringBuilder();
diff --git a/tika-serialization/src/test/java/org/apache/tika/serialization/TikaJsonSerializationTest.java b/tika-serialization/src/test/java/org/apache/tika/serialization/TikaJsonSerializationTest.java
new file mode 100644
index 000000000..8aefbc428
--- /dev/null
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/TikaJsonSerializationTest.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.StringReader;
+import java.io.StringWriter;
+import java.util.Optional;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.junit.jupiter.api.Test;
+
+import org.apache.tika.serialization.mocks.ClassC;
+
+public class TikaJsonSerializationTest {
+
+    @Test
+    public void testBasic() throws Exception {
+        StringWriter sw = new StringWriter();
+        ClassC classA = new ClassC();
+        try (JsonGenerator jsonGenerator = new ObjectMapper().createGenerator(sw)) {
+            TikaJsonSerializer.serialize(classA, jsonGenerator);
+        }
+        JsonNode root = new ObjectMapper().readTree(new StringReader(sw.toString()));
+        Optional opt = TikaJsonDeserializer.deserializeObject(root);
+        assertTrue(opt.isPresent());
+        assertEquals(classA, opt.get());
+
+    }
+
+}
diff --git a/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassA.java b/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassA.java
new file mode 100644
index 000000000..5b17d7342
--- /dev/null
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassA.java
@@ -0,0 +1,156 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization.mocks;
+
+import java.util.Objects;
+
+public class ClassA {
+    private int a = 10;
+    private float b = 11.1f;
+    private short c = 2;
+    private long d = 13l;
+    private boolean e = false;
+    private Integer f = 14;
+    private Integer g = null;
+    private Long h = 15l;
+    private Long i = null;
+    private Boolean j = Boolean.TRUE;
+    private Boolean k = null;
+
+    public int getA() {
+        return a;
+    }
+
+    public void setA(int a) {
+        this.a = a;
+    }
+
+    public float getB() {
+        return b;
+    }
+
+    public void setB(float b) {
+        this.b = b;
+    }
+
+    public short getC() {
+        return c;
+    }
+
+    public void setC(short c) {
+        this.c = c;
+    }
+
+    public long getD() {
+        return d;
+    }
+
+    public void setD(long d) {
+        this.d = d;
+    }
+
+    public boolean isE() {
+        return e;
+    }
+
+    public void setE(boolean e) {
+        this.e = e;
+    }
+
+    public Integer getF() {
+        return f;
+    }
+
+    public void setF(Integer f) {
+        this.f = f;
+    }
+
+    public Integer getG() {
+        return g;
+    }
+
+    public void setG(Integer g) {
+        this.g = g;
+    }
+
+    public Long getH() {
+        return h;
+    }
+
+    public void setH(Long h) {
+        this.h = h;
+    }
+
+    public Long getI() {
+        return i;
+    }
+
+    public void setI(Long i) {
+        this.i = i;
+    }
+
+    public Boolean getJ() {
+        return j;
+    }
+
+    public void setJ(Boolean j) {
+        this.j = j;
+    }
+
+    public Boolean getK() {
+        return k;
+    }
+
+    public void setK(Boolean k) {
+        this.k = k;
+    }
+
+    @Override
+    public String toString() {
+        return "ClassA{" + "a=" + a + ", b=" + b + ", c=" + c + ", d=" + d + ", e=" + e + ", f=" + f + ", g=" + g + ", h=" + h + ", i=" + i + ", j=" + j + ", k=" + k + '}';
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) {
+            return true;
+        }
+        if (o == null || getClass() != o.getClass()) {
+            return false;
+        }
+
+        ClassA classA = (ClassA) o;
+        return a == classA.a && Float.compare(b, classA.b) == 0 && c == classA.c && d == classA.d && e == classA.e && Objects.equals(f, classA.f) && Objects.equals(g, classA.g) &&
+                Objects.equals(h, classA.h) && Objects.equals(i, classA.i) && Objects.equals(j, classA.j) && Objects.equals(k, classA.k);
+    }
+
+    @Override
+    public int hashCode() {
+        int result = a;
+        result = 31 * result + Float.hashCode(b);
+        result = 31 * result + c;
+        result = 31 * result + Long.hashCode(d);
+        result = 31 * result + Boolean.hashCode(e);
+        result = 31 * result + Objects.hashCode(f);
+        result = 31 * result + Objects.hashCode(g);
+        result = 31 * result + Objects.hashCode(h);
+        result = 31 * result + Objects.hashCode(i);
+        result = 31 * result + Objects.hashCode(j);
+        result = 31 * result + Objects.hashCode(k);
+        return result;
+    }
+}
diff --git a/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassB.java b/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassB.java
new file mode 100644
index 000000000..e5b8d1d17
--- /dev/null
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassB.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization.mocks;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
+public class ClassB extends ClassA {
+    private String s = "hello world";
+    private Map<String, String> counts = new HashMap<>();
+    private Integer[] ints = new Integer[]{1, 2, 3, 4};
+    private List<Float> floats = new ArrayList<>();
+
+    public ClassB() {
+        floats.add(2.3f);
+        floats.add(3.4f);
+        counts.put("k1", "v1");
+        counts.put("k2", "v2");
+    }
+
+    public String getS() {
+        return s;
+    }
+
+    public void setS(String s) {
+        this.s = s;
+    }
+
+    public Map<String, String> getCounts() {
+        return counts;
+    }
+
+    public void setCounts(Map<String, String> counts) {
+        this.counts = counts;
+    }
+
+    public Integer[] getInts() {
+        return ints;
+    }
+
+    public void setInts(Integer[] ints) {
+        this.ints = ints;
+    }
+
+    public List<Float> getFloats() {
+        return floats;
+    }
+
+    public void setFloats(List<Float> floats) {
+        this.floats = floats;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) {
+            return true;
+        }
+        if (o == null || getClass() != o.getClass()) {
+            return false;
+        }
+        if (!super.equals(o)) {
+            return false;
+        }
+
+        ClassB classB = (ClassB) o;
+        return Objects.equals(s, classB.s) && Objects.equals(counts, classB.counts) && Arrays.equals(ints, classB.ints) && Objects.equals(floats, classB.floats);
+    }
+
+    @Override
+    public int hashCode() {
+        int result = super.hashCode();
+        result = 31 * result + Objects.hashCode(s);
+        result = 31 * result + Objects.hashCode(counts);
+        result = 31 * result + Arrays.hashCode(ints);
+        result = 31 * result + Objects.hashCode(floats);
+        return result;
+    }
+}
diff --git a/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassC.java b/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassC.java
new file mode 100644
index 000000000..7da5752c7
--- /dev/null
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/mocks/ClassC.java
@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.tika.serialization.mocks;
+
+import java.util.Objects;
+
+public class ClassC {
+
+    ClassB classB = new ClassB();
+
+    public ClassB getClassB() {
+        return classB;
+    }
+
+    public void setClassB(ClassB classB) {
+        this.classB = classB;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) {
+            return true;
+        }
+        if (o == null || getClass() != o.getClass()) {
+            return false;
+        }
+
+        ClassC classC = (ClassC) o;
+        return Objects.equals(classB, classC.classB);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hashCode(classB);
+    }
+}
diff --git a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleListTest.java b/tika-serialization/src/test/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleListTest.java
similarity index 90%
rename from tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleListTest.java
rename to tika-serialization/src/test/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleListTest.java
index cf57e52bd..1ff068d5d 100644
--- a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleListTest.java
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleListTest.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization.pipes;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 
@@ -55,8 +55,6 @@ public class JsonFetchEmitTupleListTest {
         m.add("m2", "v3-" + i);
         m.add("m3", "v4-" + i);
 
-        return new FetchEmitTuple("id-" + i,
-                new FetchKey("fetcher-" + i, "fetchkey-" + i),
-                new EmitKey("emitter-" + i, "emitKey-" + i), m);
+        return new FetchEmitTuple("id-" + i, new FetchKey("fetcher-" + i, "fetchkey-" + i), new EmitKey("emitter-" + i, "emitKey-" + i), m);
     }
 }
diff --git a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleTest.java b/tika-serialization/src/test/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleTest.java
similarity index 64%
rename from tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleTest.java
rename to tika-serialization/src/test/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleTest.java
index 4484478dc..ec74a0f7e 100644
--- a/tika-serialization/src/test/java/org/apache/tika/metadata/serialization/JsonFetchEmitTupleTest.java
+++ b/tika-serialization/src/test/java/org/apache/tika/serialization/pipes/JsonFetchEmitTupleTest.java
@@ -14,7 +14,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.tika.metadata.serialization;
+package org.apache.tika.serialization.pipes;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 
@@ -25,10 +25,10 @@ import java.io.StringWriter;
 import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
-import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
 import org.apache.tika.pipes.fetcher.FetchKey;
 import org.apache.tika.sax.BasicContentHandlerFactory;
 
@@ -43,14 +43,16 @@ public class JsonFetchEmitTupleTest {
         m.add("m2", "v3");
         m.add("m3", "v4");
 
-        FetchEmitTuple t = new FetchEmitTuple("my_id", new FetchKey("my_fetcher", "fetchKey1"),
-                new EmitKey("my_emitter", "emitKey1"), m,
-                new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML,
-                        HandlerConfig.PARSE_MODE.CONCATENATE,
-                        10000,10, true),
+        ParseContext parseContext = new ParseContext();
+
+        HandlerConfig h = new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML, HandlerConfig.PARSE_MODE.CONCATENATE, 10000, 10, true);
+        parseContext.set(HandlerConfig.class, h);
+
+        FetchEmitTuple t = new FetchEmitTuple("my_id", new FetchKey("my_fetcher", "fetchKey1"), new EmitKey("my_emitter", "emitKey1"), m, parseContext,
                 FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
+        System.out.println(writer);
         Reader reader = new StringReader(writer.toString());
         FetchEmitTuple deserialized = JsonFetchEmitTuple.fromJson(reader);
         assertEquals(t, deserialized);
@@ -65,12 +67,13 @@ public class JsonFetchEmitTupleTest {
         m.add("m2", "v3");
         m.add("m3", "v4");
 
-        FetchEmitTuple t = new FetchEmitTuple("my_id",
-                new FetchKey("my_fetcher", "fetchKey1", 10, 1000),
-                new EmitKey("my_emitter", "emitKey1"), m,
-                new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML,
-                        HandlerConfig.PARSE_MODE.CONCATENATE,
-                        10000,10, true),
+        /**
+         *                TODO -- add this to the ParseContext
+         *                new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML,
+         *                         HandlerConfig.PARSE_MODE.CONCATENATE,
+         *                         10000,10, true),
+         */
+        FetchEmitTuple t = new FetchEmitTuple("my_id", new FetchKey("my_fetcher", "fetchKey1", 10, 1000), new EmitKey("my_emitter", "emitKey1"), m, new ParseContext(),
                 FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
@@ -81,15 +84,16 @@ public class JsonFetchEmitTupleTest {
 
     @Test
     public void testBytes() throws Exception {
-        EmbeddedDocumentBytesConfig bytesConfig = new EmbeddedDocumentBytesConfig(true);
-        bytesConfig.setEmitter("emitter");
-        FetchEmitTuple t = new FetchEmitTuple("my_id",
-                new FetchKey("my_fetcher", "fetchKey1", 10, 1000),
-                new EmitKey("my_emitter", "emitKey1"), new Metadata(),
-                new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML,
-                        HandlerConfig.PARSE_MODE.CONCATENATE,
-                        10000,10, true),
-                FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP, bytesConfig);
+        /**
+         * TODO -- add these to the ParseContext
+         EmbeddedDocumentBytesConfig bytesConfig = new EmbeddedDocumentBytesConfig(true);
+         bytesConfig.setEmitter("emitter");
+         * new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML,
+         *                         HandlerConfig.PARSE_MODE.CONCATENATE,
+         *                         10000,10, true)
+         */
+        FetchEmitTuple t = new FetchEmitTuple("my_id", new FetchKey("my_fetcher", "fetchKey1", 10, 1000), new EmitKey("my_emitter", "emitKey1"), new Metadata(), new ParseContext(),
+                FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
         Reader reader = new StringReader(writer.toString());
diff --git a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaAsyncHttpClient.java b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaAsyncHttpClient.java
index 8da79c736..af9d370c2 100644
--- a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaAsyncHttpClient.java
+++ b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaAsyncHttpClient.java
@@ -24,13 +24,12 @@ import org.apache.tika.exception.TikaConfigException;
  */
 class TikaAsyncHttpClient extends TikaPipesHttpClient {
 
-    private TikaAsyncHttpClient(String baseUrl, HttpClientFactory httpClientFactory)
-            throws TikaConfigException {
+    private final String endPoint = "async";
+
+    private TikaAsyncHttpClient(String baseUrl, HttpClientFactory httpClientFactory) throws TikaConfigException {
         super(baseUrl, httpClientFactory);
     }
 
-    private final String endPoint = "async";
-
     String getEndpoint() {
         return endPoint;
     }
diff --git a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClient.java b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClient.java
index 99e4f18e5..bb4cb365b 100644
--- a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClient.java
+++ b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClient.java
@@ -25,8 +25,8 @@ import java.util.Random;
 import org.apache.tika.client.HttpClientFactory;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.exception.TikaException;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTuple;
 import org.apache.tika.pipes.FetchEmitTuple;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTuple;
 
 public class TikaClient {
 
@@ -38,8 +38,7 @@ public class TikaClient {
         this.clients = clients;
     }
 
-    public static TikaClient get(HttpClientFactory httpClientFactory, List<String> tikaServers)
-            throws TikaConfigException {
+    public static TikaClient get(HttpClientFactory httpClientFactory, List<String> tikaServers) throws TikaConfigException {
         List clients = new ArrayList<>();
         for (String url : tikaServers) {
             //client factory is not thread safe, create a copy per client
diff --git a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClientCLI.java b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClientCLI.java
index a02915c9b..ed962290f 100644
--- a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClientCLI.java
+++ b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaClientCLI.java
@@ -50,15 +50,12 @@ public class TikaClientCLI {
         cli.execute(tikaConfigPath);
     }
 
-    private void execute(Path tikaConfigPath)
-            throws TikaException, IOException, SAXException {
+    private void execute(Path tikaConfigPath) throws TikaException, IOException, SAXException {
         TikaServerClientConfig clientConfig = TikaServerClientConfig.build(tikaConfigPath);
 
-        ExecutorService executorService =
-                Executors.newFixedThreadPool(clientConfig.getNumThreads() + 1);
+        ExecutorService executorService = Executors.newFixedThreadPool(clientConfig.getNumThreads() + 1);
 
-        ExecutorCompletionService<Long> completionService =
-                new ExecutorCompletionService<>(executorService);
+        ExecutorCompletionService<Long> completionService = new ExecutorCompletionService<>(executorService);
 
         final PipesIterator pipesIterator = PipesIterator.build(tikaConfigPath);
 
@@ -66,21 +63,22 @@ public class TikaClientCLI {
 
         completionService.submit(new CallablePipesIterator(pipesIterator, queue));
 
-        if (clientConfig.getTikaEndpoints().size() == clientConfig.getNumThreads()) {
-            logDiffSizes(clientConfig.getTikaEndpoints().size(), clientConfig.getNumThreads());
+        if (clientConfig
+                .getTikaEndpoints()
+                .size() == clientConfig.getNumThreads()) {
+            logDiffSizes(clientConfig
+                    .getTikaEndpoints()
+                    .size(), clientConfig.getNumThreads());
             for (int i = 0; i < clientConfig.getNumThreads(); i++) {
-                TikaClient client =
-                        TikaClient.get(clientConfig.getHttpClientFactory(),
-                                Collections.singletonList(clientConfig.getTikaEndpoints().get(i)));
-                completionService.submit(new FetchWorker(queue, client,
-                        clientConfig.getMaxWaitMillis()));
+                TikaClient client = TikaClient.get(clientConfig.getHttpClientFactory(), Collections.singletonList(clientConfig
+                        .getTikaEndpoints()
+                        .get(i)));
+                completionService.submit(new FetchWorker(queue, client, clientConfig.getMaxWaitMillis()));
             }
         } else {
             for (int i = 0; i < clientConfig.getNumThreads(); i++) {
-                TikaClient client = TikaClient.get(clientConfig.getHttpClientFactory(),
-                        clientConfig.getTikaEndpoints());
-                completionService.submit(new FetchWorker(queue, client,
-                        clientConfig.getMaxWaitMillis()));
+                TikaClient client = TikaClient.get(clientConfig.getHttpClientFactory(), clientConfig.getTikaEndpoints());
+                completionService.submit(new FetchWorker(queue, client, clientConfig.getMaxWaitMillis()));
             }
         }
 
@@ -108,8 +106,7 @@ public class TikaClientCLI {
     }
 
     private void logDiffSizes(int servers, int numThreads) {
-        LOGGER.info("tika server count ({}) != numThreads ({}). " +
-                "Each client will randomly select a server from this list", servers, numThreads);
+        LOGGER.info("tika server count ({}) != numThreads ({}). " + "Each client will randomly select a server from this list", servers, numThreads);
     }
 
     private class FetchWorker implements Callable<Long> {
@@ -118,8 +115,7 @@ public class TikaClientCLI {
 
         private final long maxWaitMs;
 
-        public FetchWorker(ArrayBlockingQueue<FetchEmitTuple> queue,
-                           TikaClient client, long maxWaitMs) {
+        public FetchWorker(ArrayBlockingQueue<FetchEmitTuple> queue, TikaClient client, long maxWaitMs) {
             this.queue = queue;
             this.client = client;
             this.maxWaitMs = maxWaitMs;
@@ -142,7 +138,9 @@ public class TikaClientCLI {
                     LOGGER.debug("about to parse: {}", t.getFetchKey());
                     client.parse(t);
                 } catch (IOException | TikaException e) {
-                    LOGGER.warn(t.getFetchKey().toString(), e);
+                    LOGGER.warn(t
+                            .getFetchKey()
+                            .toString(), e);
                 }
             }
         }
diff --git a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaEmitterResult.java b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaEmitterResult.java
index 4cb134c72..63b15fd1b 100644
--- a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaEmitterResult.java
+++ b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaEmitterResult.java
@@ -31,8 +31,7 @@ public class TikaEmitterResult {
 
     @Override
     public String toString() {
-        return "TikaEmitterResult{" + "status=" + status + ", msg='" + msg + '\'' +
-                ", timeElapsed=" + timeElapsed + '}';
+        return "TikaEmitterResult{" + "status=" + status + ", msg='" + msg + '\'' + ", timeElapsed=" + timeElapsed + '}';
     }
 
     public STATUS getStatus() {
diff --git a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaPipesHttpClient.java b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaPipesHttpClient.java
index 24a4ae4be..83edf2d8c 100644
--- a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaPipesHttpClient.java
+++ b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaPipesHttpClient.java
@@ -43,7 +43,6 @@ class TikaPipesHttpClient {
     private final String endPoint = "pipes";
 
     private final HttpClientFactory httpClientFactory;
-    private HttpClient httpClient;
     private final String endPointUrl;
     private final String tikaUrl;
     private final int maxRetries = 3;
@@ -51,12 +50,12 @@ class TikaPipesHttpClient {
     private final long maxWaitForTikaMs = 120000;
     //how often to ping /tika (in ms) to see if the server is up and running
     private final long pulseWaitForTikaMs = 1000;
+    private HttpClient httpClient;
 
     /**
-     * @param baseUrl    url to base endpoint
+     * @param baseUrl url to base endpoint
      */
-    TikaPipesHttpClient(String baseUrl, HttpClientFactory httpClientFactory)
-            throws TikaConfigException {
+    TikaPipesHttpClient(String baseUrl, HttpClientFactory httpClientFactory) throws TikaConfigException {
         if (!baseUrl.endsWith("/")) {
             baseUrl += "/";
         }
@@ -70,8 +69,7 @@ class TikaPipesHttpClient {
         return endPoint;
     }
 
-    private HttpClient getNewClient(String baseUrl)
-            throws TikaConfigException {
+    private HttpClient getNewClient(String baseUrl) throws TikaConfigException {
         if (httpClient instanceof CloseableHttpClient) {
             try {
                 ((CloseableHttpClient) httpClient).close();
@@ -112,7 +110,9 @@ class TikaPipesHttpClient {
                 }
                 long elapsed = System.currentTimeMillis() - start;
                 TikaEmitterResult.STATUS status = TikaEmitterResult.STATUS.OK;
-                if (response.getStatusLine().getStatusCode() != 200) {
+                if (response
+                        .getStatusLine()
+                        .getStatusCode() != 200) {
                     status = TikaEmitterResult.STATUS.NOT_OK;
                 } else {
                     //pull out stacktrace from parse exception?
@@ -121,8 +121,7 @@ class TikaPipesHttpClient {
             }
         } catch (TimeoutWaitingForTikaException e) {
             long elapsed = System.currentTimeMillis() - start;
-            return new TikaEmitterResult(TikaEmitterResult.STATUS.TIMED_OUT_WAITING_FOR_TIKA,
-                    elapsed, "");
+            return new TikaEmitterResult(TikaEmitterResult.STATUS.TIMED_OUT_WAITING_FOR_TIKA, elapsed, "");
         }
         long elapsed = System.currentTimeMillis() - start;
         return new TikaEmitterResult(TikaEmitterResult.STATUS.EXCEEDED_MAX_RETRIES, elapsed, "");
@@ -143,7 +142,9 @@ class TikaPipesHttpClient {
             HttpGet get = new HttpGet(tikaUrl);
             try {
                 HttpResponse response = httpClient.execute(get);
-                if (response.getStatusLine().getStatusCode() == 200) {
+                if (response
+                        .getStatusLine()
+                        .getStatusCode() == 200) {
                     LOGGER.debug("server back up");
                     return;
                 }
diff --git a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaServerClientConfig.java b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaServerClientConfig.java
index 6fe64e973..01a6d16e1 100644
--- a/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaServerClientConfig.java
+++ b/tika-server/tika-server-client/src/main/java/org/apache/tika/server/client/TikaServerClientConfig.java
@@ -34,24 +34,18 @@ import org.apache.tika.exception.TikaConfigException;
 public class TikaServerClientConfig extends ConfigBase implements Initializable {
 
 
-    public static TikaServerClientConfig build(Path configFile)
-            throws IOException, TikaConfigException {
-        try (InputStream is = Files.newInputStream(configFile)) {
-            return buildSingle("serverClientConfig", TikaServerClientConfig.class, is);
-        }
-    }
-
-    enum MODE {
-        PIPES, ASYNC
-    }
-
     private HttpClientFactory httpClientFactory;
     private int numThreads = 1;
     private MODE mode = MODE.PIPES;
     private List<String> tikaEndpoints = new ArrayList<>();
-
     private long maxWaitMillis = 60000;
 
+    public static TikaServerClientConfig build(Path configFile) throws IOException, TikaConfigException {
+        try (InputStream is = Files.newInputStream(configFile)) {
+            return buildSingle("serverClientConfig", TikaServerClientConfig.class, is);
+        }
+    }
+
     public long getMaxWaitMillis() {
         return maxWaitMillis;
     }
@@ -67,14 +61,6 @@ public class TikaServerClientConfig extends ConfigBase implements Initializable
         this.maxWaitMillis = maxWaitMs;
     }
 
-    public void setMode(String mode) {
-        if ("pipes".equals(mode)) {
-            this.mode = MODE.PIPES;
-            return;
-        }
-        throw new IllegalArgumentException("I regret that we have not yet implemented: '" + mode + "'");
-    }
-
     public HttpClientFactory getHttpClientFactory() {
         return httpClientFactory;
     }
@@ -95,6 +81,14 @@ public class TikaServerClientConfig extends ConfigBase implements Initializable
         return mode;
     }
 
+    public void setMode(String mode) {
+        if ("pipes".equals(mode)) {
+            this.mode = MODE.PIPES;
+            return;
+        }
+        throw new IllegalArgumentException("I regret that we have not yet implemented: '" + mode + "'");
+    }
+
     public List<String> getTikaEndpoints() {
         return tikaEndpoints;
     }
@@ -109,10 +103,13 @@ public class TikaServerClientConfig extends ConfigBase implements Initializable
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         if (tikaEndpoints.size() == 0) {
             throw new TikaConfigException("tikaEndpoints must not be empty");
         }
     }
+
+    enum MODE {
+        PIPES, ASYNC
+    }
 }
diff --git a/tika-server/tika-server-client/src/test/java/org/apache/tika/server/client/TestBasic.java b/tika-server/tika-server-client/src/test/java/org/apache/tika/server/client/TestBasic.java
index 7e58e8127..78d76c7cc 100644
--- a/tika-server/tika-server-client/src/test/java/org/apache/tika/server/client/TestBasic.java
+++ b/tika-server/tika-server-client/src/test/java/org/apache/tika/server/client/TestBasic.java
@@ -30,20 +30,24 @@ public class TestBasic {
 
     @Test
     public void testConfig() throws Exception {
-        Path p = Paths.get(
-                TestBasic.class.getResource("/tika-config-simple-fs-emitter.xml").toURI());
+        Path p = Paths.get(TestBasic.class
+                .getResource("/tika-config-simple-fs-emitter.xml")
+                .toURI());
         assertTrue(Files.isRegularFile(p));
 
         TikaServerClientConfig clientConfig = TikaServerClientConfig.build(p);
         assertEquals(6, clientConfig.getNumThreads());
-        assertEquals(5, clientConfig.getHttpClientFactory().getMaxConnections());
+        assertEquals(5, clientConfig
+                .getHttpClientFactory()
+                .getMaxConnections());
     }
 
     @Test
     @Disabled("turn this into an actual test in tika-integration-tests?")
     public void testBasic() throws Exception {
-        Path p = Paths.get(
-                TestBasic.class.getResource("/tika-config-simple-fs-emitter.xml").toURI());
+        Path p = Paths.get(TestBasic.class
+                .getResource("/tika-config-simple-fs-emitter.xml")
+                .toURI());
         assertTrue(Files.isRegularFile(p));
         String[] args = new String[]{p.toAbsolutePath().toString()};
         long start = System.currentTimeMillis();
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/CompositeParseContextConfig.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/CompositeParseContextConfig.java
index 3158cf03f..29ab876b1 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/CompositeParseContextConfig.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/CompositeParseContextConfig.java
@@ -30,14 +30,11 @@ public class CompositeParseContextConfig implements ParseContextConfig {
     final List<ParseContextConfig> configs;
 
     public CompositeParseContextConfig() {
-        configs = new ServiceLoader(
-                CompositeParseContextConfig.class.getClassLoader()).loadServiceProviders(
-                ParseContextConfig.class);
+        configs = new ServiceLoader(CompositeParseContextConfig.class.getClassLoader()).loadServiceProviders(ParseContextConfig.class);
     }
 
     @Override
-    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata,
-                          ParseContext context) {
+    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata, ParseContext context) {
         for (ParseContextConfig config : configs) {
             config.configure(httpHeaders, metadata, context);
         }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/DefaultInputStreamFactory.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/DefaultInputStreamFactory.java
index 4b12ee97b..e33eb607e 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/DefaultInputStreamFactory.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/DefaultInputStreamFactory.java
@@ -31,8 +31,7 @@ import org.apache.tika.metadata.Metadata;
 public class DefaultInputStreamFactory implements InputStreamFactory {
 
     @Override
-    public InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders httpHeaders,
-                                      UriInfo uriInfo) throws IOException {
+    public InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders httpHeaders, UriInfo uriInfo) throws IOException {
         return is;
     }
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/FetcherStreamFactory.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/FetcherStreamFactory.java
index f173808d6..6331767a5 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/FetcherStreamFactory.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/FetcherStreamFactory.java
@@ -31,9 +31,11 @@ import org.slf4j.LoggerFactory;
 
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.fetcher.Fetcher;
 import org.apache.tika.pipes.fetcher.FetcherManager;
 import org.apache.tika.pipes.fetcher.RangeFetcher;
+import org.apache.tika.server.core.resource.TikaResource;
 
 /**
  * This class looks for &quot;fetcherName&quot; in the http header.  If it is not null
@@ -62,9 +64,22 @@ public class FetcherStreamFactory implements InputStreamFactory {
         this.fetcherManager = fetcherManager;
     }
 
+    /**
+     * Tries to parse a long out of the value.  If the val is blank, it returns -1.
+     * Throws {@link NumberFormatException}
+     *
+     * @param val
+     * @return
+     */
+    private static long getLong(String val) {
+        if (StringUtils.isBlank(val)) {
+            return -1;
+        }
+        return Long.parseLong(val);
+    }
+
     @Override
-    public InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders httpHeaders,
-                                      UriInfo uriInfo) throws IOException {
+    public InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders httpHeaders, UriInfo uriInfo) throws IOException {
         MultivaluedMap params = (uriInfo == null) ? null : uriInfo.getQueryParameters();
         String fetcherName = getParam("fetcherName", httpHeaders, params);
         String fetchKey = getParam("fetchKey", httpHeaders, params);
@@ -72,38 +87,30 @@ public class FetcherStreamFactory implements InputStreamFactory {
         if (StringUtils.isBlank(fetchKey)) {
             fetchKey = getParam("fetchKeyLiteral", httpHeaders, params);
         }
-
+        ParseContext parseContext = new ParseContext();
+        TikaResource.fillParseContext(httpHeaders.getRequestHeaders(), metadata, parseContext);
         long fetchRangeStart = getLong(getParam("fetchRangeStart", httpHeaders, params));
         long fetchRangeEnd = getLong(getParam("fetchRangeEnd", httpHeaders, params));
         if (StringUtils.isBlank(fetcherName) != StringUtils.isBlank(fetchKey)) {
-            throw new IOException("Must specify both a 'fetcherName' and a 'fetchKey'. I see: " +
-                    " fetcherName:" + fetcherName + " and fetchKey:" + fetchKey);
+            throw new IOException("Must specify both a 'fetcherName' and a 'fetchKey'. I see: " + " fetcherName:" + fetcherName + " and fetchKey:" + fetchKey);
         }
         if (fetchRangeStart < 0 && fetchRangeEnd > -1) {
-            throw new IllegalArgumentException("fetchRangeStart must be > -1 if a fetchRangeEnd " +
-                    "is specified");
+            throw new IllegalArgumentException("fetchRangeStart must be > -1 if a fetchRangeEnd " + "is specified");
         }
 
         if (fetchRangeStart > -1 && fetchRangeEnd < 0) {
-            throw new IllegalArgumentException("fetchRangeEnd must be > -1 if a fetchRangeStart " +
-                    "is specified");
+            throw new IllegalArgumentException("fetchRangeEnd must be > -1 if a fetchRangeStart " + "is specified");
         }
 
         if (!StringUtils.isBlank(fetcherName)) {
             try {
                 LOG.debug("going to fetch '{}' from fetcher: {}", fetchKey, fetcherName);
                 Fetcher fetcher = fetcherManager.getFetcher(fetcherName);
-                if (fetchRangeStart > -1 && fetchRangeEnd > -1) {
-                    if (!(fetcher instanceof RangeFetcher)) {
-                        throw new IllegalArgumentException(
-                                "Requesting a range fetch from a fetcher " +
-                                        "that doesn't support range fetching?!");
-                    }
-                    return ((RangeFetcher) fetcher).fetch(fetchKey, fetchRangeStart, fetchRangeEnd,
-                            metadata);
-                } else {
-                    return fetcher.fetch(fetchKey, metadata);
+                if (fetchRangeStart > -1 && fetchRangeEnd > -1 && !(fetcher instanceof RangeFetcher)) {
+                    throw new IllegalArgumentException(
+                            "Can't call a fetch with a range on a fetcher that" + " is not a RangeFetcher: name=" + fetcher.getName() + " class=" + fetcher.getClass());
                 }
+                return fetcher.fetch(fetchKey, metadata, parseContext);
             } catch (TikaException e) {
                 throw new IOException(e);
             }
@@ -124,24 +131,10 @@ public class FetcherStreamFactory implements InputStreamFactory {
     }
 
     private String getParam(String paramName, HttpHeaders httpHeaders, MultivaluedMap uriParams) {
-        if (uriParams == null || ! uriParams.containsKey(paramName)) {
+        if (uriParams == null || !uriParams.containsKey(paramName)) {
             return httpHeaders.getHeaderString(paramName);
         }
 
-        return (String)uriParams.getFirst(paramName);
-    }
-
-    /**
-     * Tries to parse a long out of the value.  If the val is blank, it returns -1.
-     * Throws {@link NumberFormatException}
-     *
-     * @param val
-     * @return
-     */
-    private static long getLong(String val) {
-        if (StringUtils.isBlank(val)) {
-            return -1;
-        }
-        return Long.parseLong(val);
+        return (String) uriParams.getFirst(paramName);
     }
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/InputStreamFactory.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/InputStreamFactory.java
index 5346d5ced..07d8dbaaa 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/InputStreamFactory.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/InputStreamFactory.java
@@ -33,8 +33,6 @@ import org.apache.tika.metadata.Metadata;
  */
 public interface InputStreamFactory {
 
-    InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders httpHeaders,
-                               UriInfo uriInfo)
-            throws IOException;
+    InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders httpHeaders, UriInfo uriInfo) throws IOException;
 
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ProduceTypeResourceComparator.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ProduceTypeResourceComparator.java
index 267d83561..53b120059 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ProduceTypeResourceComparator.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ProduceTypeResourceComparator.java
@@ -41,8 +41,7 @@ public class ProduceTypeResourceComparator implements ResourceComparator {
      * In case of no matching in this list, it will be treated as media type all.
      */
     public static final List<MediaType> PRIORITIZED_MEDIA_LIST =
-            Arrays.asList(MediaType.TEXT_PLAIN_TYPE, MediaType.APPLICATION_JSON_TYPE,
-                    MediaType.TEXT_HTML_TYPE, MediaType.TEXT_XML_TYPE);
+            Arrays.asList(MediaType.TEXT_PLAIN_TYPE, MediaType.APPLICATION_JSON_TYPE, MediaType.TEXT_HTML_TYPE, MediaType.TEXT_XML_TYPE);
     /**
      * The logger
      */
@@ -83,25 +82,29 @@ public class ProduceTypeResourceComparator implements ResourceComparator {
     public int compare(OperationResourceInfo oper1, OperationResourceInfo oper2, Message message) {
         // getting all message context data
         final String httpMethod = (String) message.get(Message.HTTP_REQUEST_METHOD);
-        final MediaType contentType =
-                JAXRSUtils.toMediaType((String) message.get(Message.CONTENT_TYPE));
-        final List<MediaType> acceptTypes =
-                JAXRSUtils.parseMediaTypes((String) message.get(Message.ACCEPT_CONTENT_TYPE));
+        final MediaType contentType = JAXRSUtils.toMediaType((String) message.get(Message.CONTENT_TYPE));
+        final List<MediaType> acceptTypes = JAXRSUtils.parseMediaTypes((String) message.get(Message.ACCEPT_CONTENT_TYPE));
 
-        LOG.debug("Message Method : " + httpMethod + ", ContentType : " + contentType + ", " +
-                "Accept Types : " + acceptTypes);
+        LOG.debug("Message Method : " + httpMethod + ", ContentType : " + contentType + ", " + "Accept Types : " + acceptTypes);
 
         int result = compareProduceTypes(oper1, oper2, acceptTypes);
 
-        String m1Name = oper1.getClassResourceInfo().getServiceClass().getName() + "#" +
-                oper1.getMethodToInvoke().getName();
-        String m2Name = oper2.getClassResourceInfo().getServiceClass().getName() + "#" +
-                oper2.getMethodToInvoke().getName();
+        String m1Name = oper1
+                .getClassResourceInfo()
+                .getServiceClass()
+                .getName() + "#" + oper1
+                .getMethodToInvoke()
+                .getName();
+        String m2Name = oper2
+                .getClassResourceInfo()
+                .getServiceClass()
+                .getName() + "#" + oper2
+                .getMethodToInvoke()
+                .getName();
 
         if (result != 0) {
             String chosen = result == -1 ? m1Name : m2Name;
-            LOG.debug("Between " + m1Name + " and " + m2Name + ", " + chosen +
-                    " is chosen for handling the current request");
+            LOG.debug("Between " + m1Name + " and " + m2Name + ", " + chosen + " is chosen for handling the current request");
         }
 
         return result;
@@ -117,20 +120,23 @@ public class ProduceTypeResourceComparator implements ResourceComparator {
      * @param acceptTypes the list acceptable response mime type, for the message.
      * @return value based on chosen handler. Returns -1 if first, 1 if second and 0 if no decision.
      */
-    private int compareProduceTypes(OperationResourceInfo oper1, OperationResourceInfo oper2,
-                                    final List<MediaType> acceptTypes) {
+    private int compareProduceTypes(OperationResourceInfo oper1, OperationResourceInfo oper2, final List<MediaType> acceptTypes) {
         // getting matched produce type for both handlers.
         // this is required if a method can produce multiple types.
-        List<MediaType> op1Matched =
-                JAXRSUtils.intersectMimeTypes(acceptTypes, oper1.getProduceTypes(), true);
-        List<MediaType> op2Matched =
-                JAXRSUtils.intersectMimeTypes(acceptTypes, oper2.getProduceTypes(), true);
+        List<MediaType> op1Matched = JAXRSUtils.intersectMimeTypes(acceptTypes, oper1.getProduceTypes(), true);
+        List<MediaType> op2Matched = JAXRSUtils.intersectMimeTypes(acceptTypes, oper2.getProduceTypes(), true);
 
         // calculate the max priority for both handlers
-        int oper1Priority =
-                op1Matched.stream().mapToInt(PRIORITIZED_MEDIA_LIST::indexOf).max().getAsInt();
-        int oper2Priority =
-                op2Matched.stream().mapToInt(PRIORITIZED_MEDIA_LIST::indexOf).max().getAsInt();
+        int oper1Priority = op1Matched
+                .stream()
+                .mapToInt(PRIORITIZED_MEDIA_LIST::indexOf)
+                .max()
+                .getAsInt();
+        int oper2Priority = op2Matched
+                .stream()
+                .mapToInt(PRIORITIZED_MEDIA_LIST::indexOf)
+                .max()
+                .getAsInt();
 
         // final calculation
         return oper1Priority == oper2Priority ? 0 : (oper1Priority > oper2Priority ? -1 : 1);
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatus.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatus.java
index fff678a66..b53683de5 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatus.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatus.java
@@ -34,7 +34,9 @@ public class ServerStatus {
     private AtomicLong counter = new AtomicLong(0);
     private Map<Long, TaskStatus> tasks = new HashMap<>();
     private STATUS status = STATUS.OPERATING;
-    private volatile long lastStarted = Instant.now().toEpochMilli();
+    private volatile long lastStarted = Instant
+            .now()
+            .toEpochMilli();
 
     public ServerStatus(String serverId, int numRestarts) {
         this(serverId, numRestarts, false);
@@ -84,7 +86,9 @@ public class ServerStatus {
     }
 
     public long getMillisSinceLastParseStarted() {
-        return Instant.now().toEpochMilli() - lastStarted;
+        return Instant
+                .now()
+                .toEpochMilli() - lastStarted;
     }
 
     /**
@@ -120,8 +124,7 @@ public class ServerStatus {
     }
 
     public enum STATUS {
-        INITIALIZING(0), OPERATING(1), HIT_MAX_FILES(2), TIMEOUT(3), ERROR(4),
-        PARENT_REQUESTED_SHUTDOWN(5), PARENT_EXCEPTION(6), OFF(7);
+        INITIALIZING(0), OPERATING(1), HIT_MAX_FILES(2), TIMEOUT(3), ERROR(4), PARENT_REQUESTED_SHUTDOWN(5), PARENT_EXCEPTION(6), OFF(7);
 
         private final int shutdownCode;
 
@@ -132,8 +135,7 @@ public class ServerStatus {
         static STATUS lookup(int i) {
             STATUS[] values = STATUS.values();
             if (i < 0 || i >= values.length) {
-                throw new ArrayIndexOutOfBoundsException(
-                        i + " is not acceptable for an array of length " + values.length);
+                throw new ArrayIndexOutOfBoundsException(i + " is not acceptable for an array of length " + values.length);
             }
             return STATUS.values()[i];
         }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatusWatcher.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatusWatcher.java
index 77b7b6416..55ce47a9b 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatusWatcher.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/ServerStatusWatcher.java
@@ -45,9 +45,7 @@ public class ServerStatusWatcher implements Runnable {
     private final ByteBuffer statusBuffer = ByteBuffer.allocate(16);
     private volatile boolean shuttingDown = false;
 
-    public ServerStatusWatcher(ServerStatus serverStatus, InputStream inputStream,
-                               Path forkedStatusPath, TikaServerConfig tikaServerConfig)
-            throws InterruptedException {
+    public ServerStatusWatcher(ServerStatus serverStatus, InputStream inputStream, Path forkedStatusPath, TikaServerConfig tikaServerConfig) throws InterruptedException {
         this.serverStatus = serverStatus;
         this.tikaServerConfig = tikaServerConfig;
         this.forkedStatusPath = forkedStatusPath;
@@ -67,8 +65,7 @@ public class ServerStatusWatcher implements Runnable {
             //this should block forever until the parent dies
             int directive = fromParent.read();
             if (directive != -1) {
-                LOG.debug("Read byte ({}) from forking process. Shouldn't have received anything",
-                        directive);
+                LOG.debug("Read byte ({}) from forking process. Shouldn't have received anything", directive);
             }
         } catch (Exception e) {
             LOG.debug("Exception reading from parent", e);
@@ -89,16 +86,23 @@ public class ServerStatusWatcher implements Runnable {
         }
 
         Instant started = Instant.now();
-        long elapsed = Duration.between(started, Instant.now()).toMillis();
-        try (FileChannel channel = FileChannel.open(forkedStatusPath, StandardOpenOption.CREATE,
-                StandardOpenOption.WRITE)) {
+        long elapsed = Duration
+                .between(started, Instant.now())
+                .toMillis();
+        try (FileChannel channel = FileChannel.open(forkedStatusPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE)) {
             while (elapsed < tikaServerConfig.getTaskTimeoutMillis()) {
                 try (FileLock lock = channel.tryLock()) {
                     if (lock != null) {
                         ((Buffer) statusBuffer).position(0);
-                        statusBuffer.putLong(0, Instant.now().toEpochMilli());
-                        statusBuffer.putInt(8, serverStatus.getStatus().getInt());
-                        statusBuffer.putInt(12, serverStatus.getTasks().size());
+                        statusBuffer.putLong(0, Instant
+                                .now()
+                                .toEpochMilli());
+                        statusBuffer.putInt(8, serverStatus
+                                .getStatus()
+                                .getInt());
+                        statusBuffer.putInt(12, serverStatus
+                                .getTasks()
+                                .size());
                         channel.write(statusBuffer);
                         channel.force(true);
                         return;
@@ -107,13 +111,14 @@ public class ServerStatusWatcher implements Runnable {
                     LOG.warn("Problem writing to status file", e);
                 }
                 Thread.sleep(100);
-                elapsed = Duration.between(started, Instant.now()).toMillis();
+                elapsed = Duration
+                        .between(started, Instant.now())
+                        .toMillis();
             }
         } catch (IOException e) {
             LOG.warn("Couldn't open forked status file for writing", e);
         }
-        throw new FatalException(
-                "Couldn't write to status file after trying for " + elapsed + " millis.");
+        throw new FatalException("Couldn't write to status file after trying for " + elapsed + " millis.");
     }
 
     private void checkForHitMaxFiles() {
@@ -128,22 +133,21 @@ public class ServerStatusWatcher implements Runnable {
 
     private void checkForTaskTimeouts() {
         Instant now = Instant.now();
-        for (TaskStatus status : serverStatus.getTasks().values()) {
-            long millisElapsed = Duration.between(status.started, now).toMillis();
+        for (TaskStatus status : serverStatus
+                .getTasks()
+                .values()) {
+            long millisElapsed = Duration
+                    .between(status.started, now)
+                    .toMillis();
             if (millisElapsed > status.timeoutMillis) {
                 serverStatus.setStatus(ServerStatus.STATUS.TIMEOUT);
                 if (status.fileName.isPresent()) {
-                    LOG.error("Timeout task {}, millis elapsed {}, timeoutMillis {}, file id {}" +
-                                    "consider increasing the allowable time with the " +
-                                    "<taskTimeoutMillis/> parameter or the {} header",
-                            status.task.toString(), millisElapsed, status.timeoutMillis,
-                            status.fileName.get(), TimeoutConfig.X_TIKA_TIMEOUT_MILLIS);
-                } else {
-                    LOG.error("Timeout task {}, millis elapsed {}; " +
-                                    "consider increasing the allowable time with the " +
-                                    "<taskTimeoutMillis/> parameter or the {} header",
-                            status.task.toString(), millisElapsed,
+                    LOG.error("Timeout task {}, millis elapsed {}, timeoutMillis {}, file id {}" + "consider increasing the allowable time with the " +
+                                    "<taskTimeoutMillis/> parameter or the {} header", status.task.toString(), millisElapsed, status.timeoutMillis, status.fileName.get(),
                             TimeoutConfig.X_TIKA_TIMEOUT_MILLIS);
+                } else {
+                    LOG.error("Timeout task {}, millis elapsed {}; " + "consider increasing the allowable time with the " + "<taskTimeoutMillis/> parameter or the {} header",
+                            status.task.toString(), millisElapsed, TimeoutConfig.X_TIKA_TIMEOUT_MILLIS);
                 }
             }
         }
@@ -192,11 +196,12 @@ public class ServerStatusWatcher implements Runnable {
                 checkForTaskTimeouts();
                 ServerStatus.STATUS currStatus = serverStatus.getStatus();
                 if (currStatus != ServerStatus.STATUS.OPERATING) {
-                    LOG.warn("forked process observed " + currStatus.name() +
-                            " and is shutting down.");
+                    LOG.warn("forked process observed " + currStatus.name() + " and is shutting down.");
                     shutdown(currStatus);
                 } else {
-                    long elapsed = Duration.between(lastWrite, Instant.now()).toMillis();
+                    long elapsed = Duration
+                            .between(lastWrite, Instant.now())
+                            .toMillis();
                     if (elapsed > tikaServerConfig.getTaskPulseMillis()) {
                         try {
                             writeStatus(false);
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TaskStatus.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TaskStatus.java
index 016c21445..1dad230a7 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TaskStatus.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TaskStatus.java
@@ -35,7 +35,6 @@ public class TaskStatus {
 
     @Override
     public String toString() {
-        return "TaskStatus{" + "task=" + task + ", started=" + started + ", fileName=" + fileName +
-                ", timeoutMillis=" + timeoutMillis + '}';
+        return "TaskStatus{" + "task=" + task + ", started=" + started + ", fileName=" + fileName + ", timeoutMillis=" + timeoutMillis + '}';
     }
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaLoggingFilter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaLoggingFilter.java
index 93d68f1cd..c07e8410b 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaLoggingFilter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaLoggingFilter.java
@@ -39,7 +39,10 @@ public class TikaLoggingFilter implements ContainerRequestFilter {
 
     @Override
     public void filter(ContainerRequestContext requestContext) throws IOException {
-        String requestUri = requestContext.getUriInfo().getRequestUri().toString();
+        String requestUri = requestContext
+                .getUriInfo()
+                .getRequestUri()
+                .toString();
         if (infoLevel) {
             LOG.info("Request URI: {}", requestUri);
         } else {
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerCli.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerCli.java
index 85f5cd7a8..e818d04e1 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerCli.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerCli.java
@@ -48,18 +48,14 @@ public class TikaServerCli {
 
     private static Options getOptions() {
         Options options = new Options();
-        options.addOption("h", "host", true,
-                "host name (default = " + DEFAULT_HOST + ", use * for all)");
-        options.addOption("p", "port", true, "listen port(s) (default = 9998)\n" +
-                "Can specify multiple ports with inclusive ranges (e.g. 9990-9999)\n" +
-                "or with comma delimited list (e.g. 9996,9998,9995)");
+        options.addOption("h", "host", true, "host name (default = " + DEFAULT_HOST + ", use * for all)");
+        options.addOption("p", "port", true,
+                "listen port(s) (default = 9998)\n" + "Can specify multiple ports with inclusive ranges (e.g. 9990-9999)\n" + "or with comma delimited list (e.g. 9996,9998,9995)");
         options.addOption("?", "help", false, "this help message");
         options.addOption("c", "config", true, "tika-config file");
 
-        options.addOption("i", "id", true,
-                "id to use for server in" + " the server status endpoint and logging");
-        options.addOption("noFork", "noFork", false, "runs in legacy 1.x mode -- " +
-                "server runs in process and is not safely isolated in a forked process");
+        options.addOption("i", "id", true, "id to use for server in" + " the server status endpoint and logging");
+        options.addOption("noFork", "noFork", false, "runs in legacy 1.x mode -- " + "server runs in process and is not safely isolated in a forked process");
 
         return options;
     }
@@ -103,8 +99,7 @@ public class TikaServerCli {
         List<PortIdPair> portIdPairs = getPortIdPairs(tikaServerConfig);
 
         ExecutorService executorService = Executors.newFixedThreadPool(portIdPairs.size());
-        ExecutorCompletionService<WatchDogResult> executorCompletionService =
-                new ExecutorCompletionService<>(executorService);
+        ExecutorCompletionService<WatchDogResult> executorCompletionService = new ExecutorCompletionService<>(executorService);
 
         for (PortIdPair p : portIdPairs) {
             TikaServerWatchDog watcher = new TikaServerWatchDog(p.port, p.id, tikaServerConfig);
@@ -165,8 +160,7 @@ public class TikaServerCli {
     private static Options getStopOptions() {
         Options options = new Options();
         options.addOption("preventSystemExit", false,
-                "Prevent the stop method from calling system.exit, " +
-                        "which would terminate the JVM. This is useful for integration tests.");
+                "Prevent the stop method from calling system.exit, " + "which would terminate the JVM. This is useful for integration tests.");
         return options;
     }
 
@@ -181,8 +175,7 @@ public class TikaServerCli {
     }
 
     public static void noFork(TikaServerConfig tikaServerConfig) throws Exception {
-        List<String> args = tikaServerConfig.getForkedProcessArgs(tikaServerConfig.getPort(),
-                tikaServerConfig.getIdBase());
+        List<String> args = tikaServerConfig.getForkedProcessArgs(tikaServerConfig.getPort(), tikaServerConfig.getIdBase());
         args.add("--noFork");
         TikaServerProcess.main(args.toArray(new String[0]));
     }
@@ -198,8 +191,7 @@ public class TikaServerCli {
         int[] ports = tikaServerConfig.getPorts();
         //if there's only one port, use only the idbase, otherwise append -$port
         if (ports.length == 0) {
-            throw new IllegalArgumentException(
-                    "Couldn't find any ports in: " + tikaServerConfig.getPort());
+            throw new IllegalArgumentException("Couldn't find any ports in: " + tikaServerConfig.getPort());
         } else if (ports.length == 1) {
             pairs.add(new PortIdPair(ports[0], tikaServerConfig.getIdBase()));
         } else {
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerConfig.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerConfig.java
index 6fe158830..402a90627 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerConfig.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerConfig.java
@@ -74,17 +74,12 @@ public class TikaServerConfig extends ConfigBase {
     private static final long DEFAULT_MAX_FILES = 100000;
     private static final int DEFAULT_DIGEST_MARK_LIMIT = 20 * 1024 * 1024;
     private static final String UNSECURE_WARNING =
-            "WARNING: You have chosen to run tika-server with unsecure features enabled.\n" +
-                    "Whoever has access to your service now has the same read permissions\n" +
-                    "as you've given your fetchers and the same write permissions " +
-                    "as your emitters.\n" +
-                    "Users could request and receive a sensitive file from your\n" +
-                    "drive or a webpage from your intranet and/or send malicious content to\n" +
-                    " your emitter endpoints.  See CVE-2015-3271.\n" +
+            "WARNING: You have chosen to run tika-server with unsecure features enabled.\n" + "Whoever has access to your service now has the same read permissions\n" +
+                    "as you've given your fetchers and the same write permissions " + "as your emitters.\n" + "Users could request and receive a sensitive file from your\n" +
+                    "drive or a webpage from your intranet and/or send malicious content to\n" + " your emitter endpoints.  See CVE-2015-3271.\n" +
                     "Please make sure you know what you are doing.";
     private static final List<String> ONLY_IN_FORK_MODE = Arrays.asList(
-            new String[]{"taskTimeoutMillis", "taskPulseMillis", "maxFiles", "javaPath",
-                    "maxRestarts", "numRestarts", "forkedStatusFile", "maxForkedStartupMillis",
+            new String[]{"taskTimeoutMillis", "taskPulseMillis", "maxFiles", "javaPath", "maxRestarts", "numRestarts", "forkedStatusFile", "maxForkedStartupMillis",
                     "tmpFilePrefix"});
     private static Pattern SYS_PROPS = Pattern.compile("\\$\\{sys:([-_0-9A-Za-z]+)\\}");
     /*
@@ -116,7 +111,9 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
     private Set<String> supportedFetchers = new HashSet<>();
     private Set<String> supportedEmitters = new HashSet<>();
     private List<String> forkedJvmArgs = new ArrayList<>();
-    private String idBase = UUID.randomUUID().toString();
+    private String idBase = UUID
+            .randomUUID()
+            .toString();
     private String port = Integer.toString(DEFAULT_PORT);
     private String host = DEFAULT_HOST;
     private int digestMarkLimit = DEFAULT_DIGEST_MARK_LIMIT;
@@ -187,20 +184,20 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
         return config;
     }
 
-    static TikaServerConfig load(Path p, CommandLine commandLine, Set<String> settings)
-            throws IOException, TikaException {
+    static TikaServerConfig load(Path p, CommandLine commandLine, Set<String> settings) throws IOException, TikaException {
         try (InputStream is = Files.newInputStream(p)) {
             TikaServerConfig config = TikaServerConfig.load(is, commandLine, settings);
             if (config.getConfigPath() == null) {
-                config.setConfigPath(p.toAbsolutePath().toString());
+                config.setConfigPath(p
+                        .toAbsolutePath()
+                        .toString());
             }
             loadSupportedFetchersEmitters(config);
             return config;
         }
     }
 
-    private static TikaServerConfig load(InputStream is, CommandLine commandLine,
-                                         Set<String> settings) throws IOException, TikaException {
+    private static TikaServerConfig load(InputStream is, CommandLine commandLine, Set<String> settings) throws IOException, TikaException {
         TikaServerConfig tikaServerConfig = new TikaServerConfig();
         Set<String> configSettings = tikaServerConfig.configure("server", is);
         settings.addAll(configSettings);
@@ -211,19 +208,22 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
         return tikaServerConfig;
     }
 
-    private static void loadSupportedFetchersEmitters(TikaServerConfig tikaServerConfig)
-            throws IOException, TikaConfigException {
+    private static void loadSupportedFetchersEmitters(TikaServerConfig tikaServerConfig) throws IOException, TikaConfigException {
         //this is an abomination... clean up this double read
         try (InputStream is = Files.newInputStream(tikaServerConfig.getConfigPath())) {
             Node properties = null;
             try {
-                properties = XMLReaderUtils.buildDOM(is).getDocumentElement();
+                properties = XMLReaderUtils
+                        .buildDOM(is)
+                        .getDocumentElement();
             } catch (SAXException e) {
                 throw new IOException(e);
             } catch (TikaException e) {
                 throw new TikaConfigException("problem loading xml to dom", e);
             }
-            if (!properties.getLocalName().equals("properties")) {
+            if (!properties
+                    .getLocalName()
+                    .equals("properties")) {
                 throw new TikaConfigException("expect properties as root node");
             }
             NodeList children = properties.getChildNodes();
@@ -371,8 +371,7 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
     public List<String> getForkedProcessArgs(String portString, String id) {
         int[] ports = getPorts(portString);
         if (ports.length > 1 || ports.length == 0) {
-            throw new IllegalArgumentException(
-                    "must specify one and only one port here:" + portString);
+            throw new IllegalArgumentException("must specify one and only one port here:" + portString);
         }
         int port = ports[0];
         return getForkedProcessArgs(port, id);
@@ -389,7 +388,9 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
         args.add(id);
         if (hasConfigFile()) {
             args.add("-c");
-            args.add(ProcessUtils.escapeCommandLine(configPath.toAbsolutePath().toString()));
+            args.add(ProcessUtils.escapeCommandLine(configPath
+                    .toAbsolutePath()
+                    .toString()));
         }
         return args;
     }
@@ -452,14 +453,14 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
         if (isNoFork()) {
             for (String onlyFork : ONLY_IN_FORK_MODE) {
                 if (settings.contains(onlyFork)) {
-                    throw new TikaConfigException(
-                            "Can't set param=" + onlyFork + "if you've selected noFork");
+                    throw new TikaConfigException("Can't set param=" + onlyFork + "if you've selected noFork");
                 }
             }
         }
         //add headless if not already configured
-        boolean foundHeadlessOption =
-                forkedJvmArgs.stream().anyMatch(arg -> arg.contains("java.awt.headless"));
+        boolean foundHeadlessOption = forkedJvmArgs
+                .stream()
+                .anyMatch(arg -> arg.contains("java.awt.headless"));
         //if user has already specified headless...don't modify
         if (!foundHeadlessOption) {
             forkedJvmArgs.add("-Djava.awt.headless=true");
@@ -533,8 +534,7 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
     }
 
     public void setDigest(String digest) {
-        LOG.info("As of Tika 2.5.0, you can set the digester via the AutoDetectParserConfig in " +
-                "tika-config.xml. We plan to remove this commandline option in 2.8.0");
+        LOG.info("As of Tika 2.5.0, you can set the digester via the AutoDetectParserConfig in " + "tika-config.xml. We plan to remove this commandline option in 2.8.0");
         this.digest = digest;
     }
 
@@ -632,16 +632,16 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
 
     private int[] getPorts(String portString) {
         //throws NumberFormatException
-        Matcher rangeMatcher = Pattern.compile("^(\\d+)-(\\d+)\\Z").matcher("");
+        Matcher rangeMatcher = Pattern
+                .compile("^(\\d+)-(\\d+)\\Z")
+                .matcher("");
         String[] commaDelimited = portString.split(",");
         List<Integer> indivPorts = new ArrayList<>();
         for (String val : commaDelimited) {
             rangeMatcher.reset(val);
             if (rangeMatcher.find()) {
-                int min = Math.min(Integer.parseInt(rangeMatcher.group(1)),
-                        Integer.parseInt(rangeMatcher.group(2)));
-                int max = Math.max(Integer.parseInt(rangeMatcher.group(1)),
-                        Integer.parseInt(rangeMatcher.group(2)));
+                int min = Math.min(Integer.parseInt(rangeMatcher.group(1)), Integer.parseInt(rangeMatcher.group(2)));
+                int max = Math.max(Integer.parseInt(rangeMatcher.group(1)), Integer.parseInt(rangeMatcher.group(2)));
                 for (int i = min; i <= max; i++) {
                     indivPorts.add(i);
                 }
@@ -649,7 +649,10 @@ private long forkedProcessShutdownMillis = DEFAULT_FORKED_PROCESS_SHUTDOWN_MILLI
                 indivPorts.add(Integer.parseInt(val));
             }
         }
-        return indivPorts.stream().mapToInt(Integer::intValue).toArray();
+        return indivPorts
+                .stream()
+                .mapToInt(Integer::intValue)
+                .toArray();
     }
 
     public Set<String> getSupportedFetchers() {
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerParseExceptionMapper.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerParseExceptionMapper.java
index ea3280ef4..cbf5bfbd1 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerParseExceptionMapper.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerParseExceptionMapper.java
@@ -41,8 +41,9 @@ public class TikaServerParseExceptionMapper implements ExceptionMapper<TikaServe
     }
 
     public Response toResponse(TikaServerParseException e) {
-        if (e.getMessage() != null &&
-                e.getMessage().equals(Response.Status.UNSUPPORTED_MEDIA_TYPE.toString())) {
+        if (e.getMessage() != null && e
+                .getMessage()
+                .equals(Response.Status.UNSUPPORTED_MEDIA_TYPE.toString())) {
             return buildResponse(e, 415);
         }
         Throwable cause = e.getCause();
@@ -80,11 +81,19 @@ public class TikaServerParseExceptionMapper implements ExceptionMapper<TikaServe
                 result.flush();
             } catch (IOException e) {
                 //something went seriously wrong
-                return Response.status(500).build();
+                return Response
+                        .status(500)
+                        .build();
             }
-            return Response.status(i).entity(result.toString()).type("text/plain").build();
+            return Response
+                    .status(i)
+                    .entity(result.toString())
+                    .type("text/plain")
+                    .build();
         } else {
-            return Response.status(i).build();
+            return Response
+                    .status(i)
+                    .build();
         }
     }
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerProcess.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerProcess.java
index 10fb951e0..6c7cc97f7 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerProcess.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerProcess.java
@@ -103,19 +103,14 @@ public class TikaServerProcess {
         Options options = new Options();
         options.addOption("h", "host", true, "host name, use * for all)");
         options.addOption("p", "port", true, "listen port");
-        options.addOption("c", "config", true,
-                "Tika Configuration file to override default config with.");
+        options.addOption("c", "config", true, "Tika Configuration file to override default config with.");
         options.addOption("i", "id", true, "id to use for server in server status endpoint");
         options.addOption("?", "help", false, "this help message");
         options.addOption("noFork", "noFork", false, "if launched in no fork mode");
         options.addOption("forkedStatusFile", true,
-                "Not allowed in -noFork: temporary file used to communicate " +
-                        "with forking process -- do not use this! " +
-                        "Should only be invoked by forking process.");
-        options.addOption("tmpFilePrefix", true,
-                "Not allowed in -noFork: prefix for temp file - for debugging only");
-        options.addOption("numRestarts", true, "Not allowed in -noFork: number of times that " +
-                "the forked server has had to be restarted.");
+                "Not allowed in -noFork: temporary file used to communicate " + "with forking process -- do not use this! " + "Should only be invoked by forking process.");
+        options.addOption("tmpFilePrefix", true, "Not allowed in -noFork: prefix for temp file - for debugging only");
+        options.addOption("numRestarts", true, "Not allowed in -noFork: number of times that " + "the forked server has had to be restarted.");
         return options;
     }
 
@@ -147,8 +142,7 @@ public class TikaServerProcess {
         return isBindException(e.getCause());
     }
 
-    private static void startServer(ServerDetails serverDetails, TikaServerConfig tikaServerConfig)
-            throws Exception {
+    private static void startServer(ServerDetails serverDetails, TikaServerConfig tikaServerConfig) throws Exception {
 
         try {
             //start the server
@@ -167,8 +161,7 @@ public class TikaServerProcess {
             System.setIn(new ByteArrayInputStream(new byte[0]));
 
             String forkedStatusFile = tikaServerConfig.getForkedStatusFile();
-            Thread serverThread = new Thread(new ServerStatusWatcher(serverDetails.serverStatus, in,
-                    Paths.get(forkedStatusFile), tikaServerConfig));
+            Thread serverThread = new Thread(new ServerStatusWatcher(serverDetails.serverStatus, in, Paths.get(forkedStatusFile), tikaServerConfig));
 
             serverThread.start();
         }
@@ -181,8 +174,7 @@ public class TikaServerProcess {
         String host = tikaServerConfig.getHost();
         int[] ports = tikaServerConfig.getPorts();
         if (ports.length > 1) {
-            throw new IllegalArgumentException(
-                    "there must be only one port here! " + "I see: " + tikaServerConfig.getPort());
+            throw new IllegalArgumentException("there must be only one port here! " + "I see: " + tikaServerConfig.getPort());
         }
 
         int port = ports[0];
@@ -199,17 +191,13 @@ public class TikaServerProcess {
         DigestingParser.Digester digester = null;
         if (!StringUtils.isBlank(tikaServerConfig.getDigest())) {
             try {
-                digester = new CommonsDigester(tikaServerConfig.getDigestMarkLimit(),
-                        tikaServerConfig.getDigest());
+                digester = new CommonsDigester(tikaServerConfig.getDigestMarkLimit(), tikaServerConfig.getDigest());
             } catch (IllegalArgumentException commonsException) {
                 try {
-                    digester = new BouncyCastleDigester(tikaServerConfig.getDigestMarkLimit(),
-                            tikaServerConfig.getDigest());
+                    digester = new BouncyCastleDigester(tikaServerConfig.getDigestMarkLimit(), tikaServerConfig.getDigest());
                 } catch (IllegalArgumentException bcException) {
                     throw new IllegalArgumentException(
-                            "Tried both CommonsDigester (" + commonsException.getMessage() +
-                                    ") and BouncyCastleDigester (" + bcException.getMessage() + ")",
-                            bcException);
+                            "Tried both CommonsDigester (" + commonsException.getMessage() + ") and BouncyCastleDigester (" + bcException.getMessage() + ")", bcException);
                 }
             }
         }
@@ -252,14 +240,19 @@ public class TikaServerProcess {
         sf.setOutInterceptors(Collections.singletonList(new GZIPOutInterceptor()));
         sf.setInInterceptors(Collections.singletonList(new GZIPInInterceptor()));
 
-        String protocol = tikaServerConfig.getTlsConfig().isActive() ? "https" : "http";
+        String protocol = tikaServerConfig
+                .getTlsConfig()
+                .isActive() ? "https" : "http";
         String url = protocol + "://" + host + ":" + port + "/";
         sf.setAddress(url);
         sf.setResourceComparator(new ProduceTypeResourceComparator());
-        BindingFactoryManager manager = sf.getBus().getExtension(BindingFactoryManager.class);
-        if (tikaServerConfig.getTlsConfig().isActive()) {
-            LOG.warn("The TLS configuration is in BETA and might change " +
-                    "dramatically in future releases.");
+        BindingFactoryManager manager = sf
+                .getBus()
+                .getExtension(BindingFactoryManager.class);
+        if (tikaServerConfig
+                .getTlsConfig()
+                .isActive()) {
+            LOG.warn("The TLS configuration is in BETA and might change " + "dramatically in future releases.");
             TLSServerParameters tlsParams = getTlsParams(tikaServerConfig.getTlsConfig());
             JettyHTTPServerEngineFactory factory = new JettyHTTPServerEngineFactory();
             factory.setBus(sf.getBus());
@@ -278,8 +271,7 @@ public class TikaServerProcess {
         return details;
     }
 
-    private static TLSServerParameters getTlsParams(TlsConfig tlsConfig)
-            throws GeneralSecurityException, IOException {
+    private static TLSServerParameters getTlsParams(TlsConfig tlsConfig) throws GeneralSecurityException, IOException {
         KeyStoreType keyStore = new KeyStoreType();
         keyStore.setType(tlsConfig.getKeyStoreType());
         keyStore.setPassword(tlsConfig.getKeyStorePassword());
@@ -306,10 +298,7 @@ public class TikaServerProcess {
         return parameters;
     }
 
-    private static void loadAllProviders(TikaServerConfig tikaServerConfig,
-                                         ServerStatus serverStatus,
-                                         List<ResourceProvider> resourceProviders,
-                                         List<Object> writers)
+    private static void loadAllProviders(TikaServerConfig tikaServerConfig, ServerStatus serverStatus, List<ResourceProvider> resourceProviders, List<Object> writers)
             throws TikaException, SAXException, IOException {
         List<ResourceProvider> tmpCoreProviders = loadCoreProviders(tikaServerConfig, serverStatus);
 
@@ -354,20 +343,18 @@ public class TikaServerProcess {
 
     }
 
-    private static List<ResourceProvider> loadCoreProviders(TikaServerConfig tikaServerConfig,
-                                                            ServerStatus serverStatus)
-            throws TikaException, IOException, SAXException {
+    private static List<ResourceProvider> loadCoreProviders(TikaServerConfig tikaServerConfig, ServerStatus serverStatus) throws TikaException, IOException, SAXException {
         List<ResourceProvider> resourceProviders = new ArrayList<>();
         boolean addAsyncResource = false;
         boolean addPipesResource = false;
-        if (tikaServerConfig.getEndpoints().size() == 0) {
+        if (tikaServerConfig
+                .getEndpoints()
+                .size() == 0) {
             resourceProviders.add(new SingletonResourceProvider(new MetadataResource()));
             resourceProviders.add(new SingletonResourceProvider(new RecursiveMetadataResource()));
-            resourceProviders.add(
-                    new SingletonResourceProvider(new DetectorResource(serverStatus)));
+            resourceProviders.add(new SingletonResourceProvider(new DetectorResource(serverStatus)));
             resourceProviders.add(new SingletonResourceProvider(new LanguageResource()));
-            resourceProviders.add(new SingletonResourceProvider(
-                    new TranslateResource(serverStatus, tikaServerConfig.getTaskTimeoutMillis())));
+            resourceProviders.add(new SingletonResourceProvider(new TranslateResource(serverStatus, tikaServerConfig.getTaskTimeoutMillis())));
             resourceProviders.add(new SingletonResourceProvider(new TikaResource()));
             resourceProviders.add(new SingletonResourceProvider(new UnpackerResource()));
             resourceProviders.add(new SingletonResourceProvider(new TikaMimeTypes()));
@@ -378,30 +365,28 @@ public class TikaServerProcess {
                 //check to make sure there are both fetchers and emitters
                 //specified.  It is possible that users may only specify fetchers
                 //for legacy endpoints.
-                if (tikaServerConfig.getSupportedFetchers().size() > 0 &&
-                        tikaServerConfig.getSupportedEmitters().size() > 0) {
+                if (tikaServerConfig
+                        .getSupportedFetchers()
+                        .size() > 0 && tikaServerConfig
+                        .getSupportedEmitters()
+                        .size() > 0) {
                     addAsyncResource = true;
                     addPipesResource = true;
                 }
-                resourceProviders.add(
-                        new SingletonResourceProvider(new TikaServerStatus(serverStatus)));
+                resourceProviders.add(new SingletonResourceProvider(new TikaServerStatus(serverStatus)));
             }
         } else {
             for (String endPoint : tikaServerConfig.getEndpoints()) {
                 if ("meta".equals(endPoint)) {
                     resourceProviders.add(new SingletonResourceProvider(new MetadataResource()));
                 } else if ("rmeta".equals(endPoint)) {
-                    resourceProviders.add(
-                            new SingletonResourceProvider(new RecursiveMetadataResource()));
+                    resourceProviders.add(new SingletonResourceProvider(new RecursiveMetadataResource()));
                 } else if ("detect".equals(endPoint)) {
-                    resourceProviders.add(
-                            new SingletonResourceProvider(new DetectorResource(serverStatus)));
+                    resourceProviders.add(new SingletonResourceProvider(new DetectorResource(serverStatus)));
                 } else if ("language".equals(endPoint)) {
                     resourceProviders.add(new SingletonResourceProvider(new LanguageResource()));
                 } else if ("translate".equals(endPoint)) {
-                    resourceProviders.add(new SingletonResourceProvider(
-                            new TranslateResource(serverStatus,
-                                    tikaServerConfig.getTaskTimeoutMillis())));
+                    resourceProviders.add(new SingletonResourceProvider(new TranslateResource(serverStatus, tikaServerConfig.getTaskTimeoutMillis())));
                 } else if ("tika".equals(endPoint)) {
                     resourceProviders.add(new SingletonResourceProvider(new TikaResource()));
                 } else if ("unpack".equals(endPoint)) {
@@ -419,44 +404,42 @@ public class TikaServerProcess {
                 } else if ("async".equals(endPoint)) {
                     addAsyncResource = true;
                 } else if ("status".equals(endPoint)) {
-                    resourceProviders.add(
-                            new SingletonResourceProvider(new TikaServerStatus(serverStatus)));
+                    resourceProviders.add(new SingletonResourceProvider(new TikaServerStatus(serverStatus)));
                 }
             }
         }
 
         if (addAsyncResource) {
-            final AsyncResource localAsyncResource =
-                    new AsyncResource(tikaServerConfig.getConfigPath(),
-                            tikaServerConfig.getSupportedFetchers());
-            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
-                try {
-                    localAsyncResource.shutdownNow();
-                } catch (Exception e) {
-                    LOG.warn("problem shutting down local async resource", e);
-                }
-            }));
+            final AsyncResource localAsyncResource = new AsyncResource(tikaServerConfig.getConfigPath(), tikaServerConfig.getSupportedFetchers());
+            Runtime
+                    .getRuntime()
+                    .addShutdownHook(new Thread(() -> {
+                        try {
+                            localAsyncResource.shutdownNow();
+                        } catch (Exception e) {
+                            LOG.warn("problem shutting down local async resource", e);
+                        }
+                    }));
             resourceProviders.add(new SingletonResourceProvider(localAsyncResource));
         }
         if (addPipesResource) {
-            final PipesResource localPipesResource =
-                    new PipesResource(tikaServerConfig.getConfigPath());
-            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
-                try {
-                    localPipesResource.close();
-                } catch (Exception e) {
-                    LOG.warn("exception closing local pipes resource", e);
-                }
-            }));
+            final PipesResource localPipesResource = new PipesResource(tikaServerConfig.getConfigPath());
+            Runtime
+                    .getRuntime()
+                    .addShutdownHook(new Thread(() -> {
+                        try {
+                            localPipesResource.close();
+                        } catch (Exception e) {
+                            LOG.warn("exception closing local pipes resource", e);
+                        }
+                    }));
             resourceProviders.add(new SingletonResourceProvider(localPipesResource));
         }
         resourceProviders.addAll(loadResourceServices(serverStatus));
         return resourceProviders;
     }
 
-    private static void logFetchersAndEmitters(boolean enableUnsecureFeatures,
-                                               FetcherManager fetcherManager,
-                                               EmitterManager emitterManager) {
+    private static void logFetchersAndEmitters(boolean enableUnsecureFeatures, FetcherManager fetcherManager, EmitterManager emitterManager) {
         if (enableUnsecureFeatures) {
             StringBuilder sb = new StringBuilder();
             Set<String> supportedFetchers = fetcherManager.getSupported();
@@ -464,54 +447,49 @@ public class TikaServerProcess {
             if (supportedFetchers.size() == 0) {
                 sb.append("There are no fetchers specified in the TikaConfig");
             } else {
-                sb.append("The following fetchers are available to whomever has " +
-                        "access to this server:\n");
+                sb.append("The following fetchers are available to whomever has " + "access to this server:\n");
                 for (String p : supportedFetchers) {
-                    sb.append(p).append("\n");
+                    sb
+                            .append(p)
+                            .append("\n");
                 }
             }
             Set<String> emitters = emitterManager.getSupported();
             if (supportedFetchers.size() == 0) {
                 sb.append("There are no emitters specified in the TikaConfig");
             } else {
-                sb.append("The following emitters are available to whomever has " +
-                        "access to this server:\n");
+                sb.append("The following emitters are available to whomever has " + "access to this server:\n");
                 for (String e : emitters) {
-                    sb.append(e).append("\n");
+                    sb
+                            .append(e)
+                            .append("\n");
                 }
             }
             LOG.info(sb.toString());
         } else {
-            if (emitterManager.getSupported().size() > 0) {
-                String warn = "enableUnsecureFeatures has not been set to 'true' in the server " +
-                        "config file.\n" + "The " + emitterManager.getSupported().size() +
-                        " emitter(s) that you've\n" +
-                        "specified in TikaConfig will not be available on the /emit " +
-                        "or /async endpoints.\n" +
-                        "To enable your emitters, start tika-server with " +
-                        "<enableUnsecureFeatures>true</enableUnsecureFeatures> " + "parameter in " +
-                        "the TikaConfig\n\n";
+            if (emitterManager
+                    .getSupported()
+                    .size() > 0) {
+                String warn = "enableUnsecureFeatures has not been set to 'true' in the server " + "config file.\n" + "The " + emitterManager
+                        .getSupported()
+                        .size() + " emitter(s) that you've\n" + "specified in TikaConfig will not be available on the /emit " + "or /async endpoints.\n" +
+                        "To enable your emitters, start tika-server with " + "<enableUnsecureFeatures>true</enableUnsecureFeatures> " + "parameter in " + "the TikaConfig\n\n";
                 LOG.warn(warn);
             }
-            if (emitterManager.getSupported().size() > 0) {
-                String warn = "enableUnsecureFeatures has not been set to 'true' in the server " +
-                        "config file.\n" + "The " + emitterManager.getSupported().size() +
-                        " fetcher(s) that you've\n" +
-                        "specified in TikaConfig will not be available on the /emit " +
-                        "or /async endpoints.\n" +
-                        "To enable your emitters, start tika-server with " +
-                        "<enableUnsecureFeatures>true</enableUnsecureFeatures> " + "parameter in " +
-                        "the TikaConfig\n\n";
+            if (emitterManager
+                    .getSupported()
+                    .size() > 0) {
+                String warn = "enableUnsecureFeatures has not been set to 'true' in the server " + "config file.\n" + "The " + emitterManager
+                        .getSupported()
+                        .size() + " fetcher(s) that you've\n" + "specified in TikaConfig will not be available on the /emit " + "or /async endpoints.\n" +
+                        "To enable your emitters, start tika-server with " + "<enableUnsecureFeatures>true</enableUnsecureFeatures> " + "parameter in " + "the TikaConfig\n\n";
                 LOG.warn(warn);
             }
         }
     }
 
-    private static Collection<? extends ResourceProvider> loadResourceServices(
-            ServerStatus serverStatus) {
-        List<TikaServerResource> resources =
-                new ServiceLoader(TikaServerProcess.class.getClassLoader()).loadServiceProviders(
-                        TikaServerResource.class);
+    private static Collection<? extends ResourceProvider> loadResourceServices(ServerStatus serverStatus) {
+        List<TikaServerResource> resources = new ServiceLoader(TikaServerProcess.class.getClassLoader()).loadServiceProviders(TikaServerResource.class);
         List<ResourceProvider> providers = new ArrayList<>();
         for (TikaServerResource r : resources) {
             LOG.info("loading resource from SPI: " + r.getClass());
@@ -524,8 +502,7 @@ public class TikaServerProcess {
     }
 
     private static Collection<?> loadWriterServices() {
-        return new ServiceLoader(TikaServerProcess.class.getClassLoader()).loadServiceProviders(
-                org.apache.tika.server.core.writer.TikaServerWriter.class);
+        return new ServiceLoader(TikaServerProcess.class.getClassLoader()).loadServiceProviders(org.apache.tika.server.core.writer.TikaServerWriter.class);
     }
 
     private static class ServerDetails {
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerWatchDog.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerWatchDog.java
index 72d78ad4b..a2c973f66 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerWatchDog.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TikaServerWatchDog.java
@@ -71,7 +71,9 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
                 }
             }
         });
-        Runtime.getRuntime().addShutdownHook(shutdownHook);
+        Runtime
+                .getRuntime()
+                .addShutdownHook(shutdownHook);
     }
 
     private final int port;
@@ -92,8 +94,7 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
 
     private static void redirectIO(final InputStream src, final PrintStream targ) {
         Thread gobbler = new Thread(() -> {
-            BufferedReader reader =
-                    new BufferedReader(new InputStreamReader(src, StandardCharsets.UTF_8));
+            BufferedReader reader = new BufferedReader(new InputStreamReader(src, StandardCharsets.UTF_8));
             String line;
             try {
                 line = reader.readLine();
@@ -109,16 +110,14 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
         gobbler.start();
     }
 
-    private static synchronized void destroyForkedForcibly(Process process)
-            throws InterruptedException {
+    private static synchronized void destroyForkedForcibly(Process process) throws InterruptedException {
 
         process = process.destroyForcibly();
         try {
             boolean destroyed = process.waitFor(60, TimeUnit.SECONDS);
 
             if (!destroyed) {
-                LOG.error("Forked process still alive after 60 seconds. " +
-                        "Shutting down the forking process.");
+                LOG.error("Forked process still alive after 60 seconds. " + "Shutting down the forking process.");
                 System.exit(1);
             }
         } finally {
@@ -126,8 +125,7 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
         }
     }
 
-    private static void closeForkedProcess(ForkedProcess forkedProcess)
-            throws DoNotRestartException, InterruptedException {
+    private static void closeForkedProcess(ForkedProcess forkedProcess) throws DoNotRestartException, InterruptedException {
         try {
             forkedProcess.close();
         } finally {
@@ -140,10 +138,8 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
         boolean mustRestart = true;
         try {
             while (true) {
-                if (tikaServerConfig.getMaxRestarts() > 0 &&
-                        restarts >= tikaServerConfig.getMaxRestarts()) {
-                    LOG.warn("hit max restarts ({}). Ending processing for {} {}", restarts, id,
-                            port);
+                if (tikaServerConfig.getMaxRestarts() > 0 && restarts >= tikaServerConfig.getMaxRestarts()) {
+                    LOG.warn("hit max restarts ({}). Ending processing for {} {}", restarts, id, port);
                     return new WatchDogResult(port, id, restarts);
                 }
 
@@ -152,8 +148,7 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
                         forkedProcess = startForkedProcess(restarts++);
                         if (forkedProcess == null) {
                             if (!shutDown) {
-                                throw new IllegalArgumentException("forked process should not be " +
-                                        "null when not in shutdown mode");
+                                throw new IllegalArgumentException("forked process should not be " + "null when not in shutdown mode");
                             } else {
                                 return new WatchDogResult(port, id, restarts);
                             }
@@ -161,12 +156,9 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
                         setForkedStatus(FORKED_STATUS.RUNNING);
                         mustRestart = false;
                     }
-                    boolean exited =
-                            forkedProcess.process.waitFor(tikaServerConfig.getTaskPulseMillis(),
-                                    TimeUnit.MILLISECONDS);
+                    boolean exited = forkedProcess.process.waitFor(tikaServerConfig.getTaskPulseMillis(), TimeUnit.MILLISECONDS);
                     if (exited) {
-                        LOG.info("forked process exited with exit value {}",
-                                forkedProcess.process.exitValue());
+                        LOG.info("forked process exited with exit value {}", forkedProcess.process.exitValue());
                         closeForkedProcess(forkedProcess);
                         mustRestart = true;
                     } else {
@@ -177,17 +169,15 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
                             mustRestart = true;
                         } else if (status.status == FORKED_STATUS.SHUTTING_DOWN.ordinal()) {
                             LOG.info("Forked process is in shutting down mode.  Will wait a bit");
-                            forkedProcess.process.waitFor(tikaServerConfig.getTaskTimeoutMillis(),
-                                    TimeUnit.MILLISECONDS);
+                            forkedProcess.process.waitFor(tikaServerConfig.getTaskTimeoutMillis(), TimeUnit.MILLISECONDS);
                             closeForkedProcess(forkedProcess);
                             mustRestart = true;
                         } else {
-                            long elapsed = Duration.between(Instant.ofEpochMilli(status.timestamp),
-                                    Instant.now()).toMillis();
+                            long elapsed = Duration
+                                    .between(Instant.ofEpochMilli(status.timestamp), Instant.now())
+                                    .toMillis();
                             if (elapsed > tikaServerConfig.getTaskTimeoutMillis()) {
-                                LOG.info("{} ms have elapsed since forked process " +
-                                                "last updated status. " + "Shutting down and restarting.",
-                                        elapsed);
+                                LOG.info("{} ms have elapsed since forked process " + "last updated status. " + "Shutting down and restarting.", elapsed);
                                 closeForkedProcess(forkedProcess);
                                 mustRestart = true;
                             }
@@ -226,8 +216,7 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
                 FORKED_PROCESSES.add(forkedProcess);
                 return forkedProcess;
             } catch (BindException e) {
-                LOG.warn("WatchDog observes bind exception on retry {}. " + "Will retry {} times.",
-                        consecutiveRestarts, maxBind);
+                LOG.warn("WatchDog observes bind exception on retry {}. " + "Will retry {} times.", consecutiveRestarts, maxBind);
                 consecutiveRestarts++;
                 Thread.sleep(1000);
                 if (consecutiveRestarts >= maxBind) {
@@ -269,8 +258,7 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
 
         @Override
         public String toString() {
-            return "ForkedStatus{" + "timestamp=" + timestamp + ", status=" + status +
-                    ", numTasks=" + numTasks + '}';
+            return "ForkedStatus{" + "timestamp=" + timestamp + ", status=" + status + ", numTasks=" + numTasks + '}';
         }
     }
 
@@ -299,12 +287,15 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
 
             //wait for file to be written/initialized by forked process
             Instant start = Instant.now();
-            long elapsed = Duration.between(start, Instant.now()).toMillis();
+            long elapsed = Duration
+                    .between(start, Instant.now())
+                    .toMillis();
             try {
-                while (process.isAlive() && Files.size(forkedStatusFile) < 12 &&
-                        elapsed < tikaServerConfig.getMaxForkedStartupMillis()) {
+                while (process.isAlive() && Files.size(forkedStatusFile) < 12 && elapsed < tikaServerConfig.getMaxForkedStartupMillis()) {
                     Thread.sleep(50);
-                    elapsed = Duration.between(start, Instant.now()).toMillis();
+                    elapsed = Duration
+                            .between(start, Instant.now())
+                            .toMillis();
                 }
             } catch (IOException e) {
                 //the forkedStatusFile can be deleted by the
@@ -314,8 +305,7 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
 
             if (elapsed > tikaServerConfig.getMaxForkedStartupMillis()) {
                 close();
-                throw new RuntimeException(
-                        "Forked process failed to start after " + elapsed + " (ms)");
+                throw new RuntimeException("Forked process failed to start after " + elapsed + " (ms)");
             }
             if (!process.isAlive()) {
                 close();
@@ -326,8 +316,7 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
             }
             if (!Files.exists(forkedStatusFile)) {
                 close();
-                throw new RuntimeException(
-                        "Failed to start forked process -- forked status file does not exist");
+                throw new RuntimeException("Failed to start forked process -- forked status file does not exist");
             }
 
             lastPing = Instant.now();
@@ -335,7 +324,9 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
 
         private ForkedStatus readStatus() throws Exception {
             Instant started = Instant.now();
-            long elapsed = Duration.between(started, Instant.now()).toMillis();
+            long elapsed = Duration
+                    .between(started, Instant.now())
+                    .toMillis();
             //only reading, but need to include write to allow for locking
             try (FileChannel fc = FileChannel.open(forkedStatusFile, READ, WRITE)) {
 
@@ -353,7 +344,9 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
                         //swallow
                     }
                     Thread.sleep(100);
-                    elapsed = Duration.between(started, Instant.now()).toMillis();
+                    elapsed = Duration
+                            .between(started, Instant.now())
+                            .toMillis();
                 }
             }
             return new ForkedStatus(-1, FORKED_STATUS.FAILED_COMMUNICATION.ordinal(), -1);
@@ -396,8 +389,9 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
             List<String> jvmArgs = tikaServerConfig.getForkedJvmArgs();
             List<String> forkedArgs = tikaServerConfig.getForkedProcessArgs(port, id);
             forkedArgs.add("-forkedStatusFile");
-            forkedArgs.add(
-                    ProcessUtils.escapeCommandLine(forkedStatusFile.toAbsolutePath().toString()));
+            forkedArgs.add(ProcessUtils.escapeCommandLine(forkedStatusFile
+                    .toAbsolutePath()
+                    .toString()));
 
             argList.add(javaPath);
             if (!jvmArgs.contains("-cp") && !jvmArgs.contains("--classpath")) {
@@ -420,7 +414,9 @@ public class TikaServerWatchDog implements Callable<WatchDogResult> {
             //now overwrite with the specific server id
             //this is mostly for log4j 2.x so that different processes
             //can log to different log files via {env:tika.server.id}
-            builder.environment().put(TIKA_SERVER_ID_ENV, id);
+            builder
+                    .environment()
+                    .put(TIKA_SERVER_ID_ENV, id);
             Process process = builder.start();
             PROCESSES.add(process);
             //redirect stdout to parent stderr to avoid error msgs
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TlsConfig.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TlsConfig.java
index c3922a5e4..df0ddfc70 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TlsConfig.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/TlsConfig.java
@@ -106,8 +106,7 @@ public class TlsConfig implements Initializable {
     }
 
     @Override
-    public void checkInitialization(InitializableProblemHandler problemHandler)
-            throws TikaConfigException {
+    public void checkInitialization(InitializableProblemHandler problemHandler) throws TikaConfigException {
         if (active) {
             if (StringUtils.isBlank(keyStoreType)) {
                 throw new TikaConfigException("must initialize keyStoreType");
@@ -118,19 +117,15 @@ public class TlsConfig implements Initializable {
             }
             if (hasTrustStore()) {
                 if (StringUtils.isBlank(trustStoreType)) {
-                    throw new TikaConfigException(
-                            "must initialize trustStoreType " + "if there's any trustStore info");
+                    throw new TikaConfigException("must initialize trustStoreType " + "if there's any trustStore info");
                 } else if (StringUtils.isBlank(trustStoreFile)) {
-                    throw new TikaConfigException(
-                            "must initialize trustStoreFile " + "if there's any trustStore info");
+                    throw new TikaConfigException("must initialize trustStoreFile " + "if there's any trustStore info");
                 } else if (StringUtils.isBlank(trustStorePassword)) {
-                    throw new TikaConfigException("must initialize trustStorePassword " +
-                            "if there's any trustStore info");
+                    throw new TikaConfigException("must initialize trustStorePassword " + "if there's any trustStore info");
                 }
             }
             if (!hasTrustStore() && isClientAuthenticationRequired()) {
-                throw new TikaConfigException("requiring client authentication, but no trust " +
-                        "store has been specified?!");
+                throw new TikaConfigException("requiring client authentication, but no trust " + "store has been specified?!");
             }
         }
     }
@@ -153,18 +148,13 @@ public class TlsConfig implements Initializable {
 
     @Override
     public String toString() {
-        return "TlsConfig{" + "active=" + active + ", passwordsAESEncrypted=" +
-                passwordsAESEncrypted + ", keyStoreType='" + keyStoreType + '\'' +
-                ", keyStorePassword='" + keyStorePassword + '\'' + ", keyStoreFile='" +
-                keyStoreFile + '\'' + ", trustStoreType='" + trustStoreType + '\'' +
-                ", trustStorePassword='" + trustStorePassword + '\'' + ", trustStoreFile='" +
-                trustStoreFile + '\'' + ", clientAuthenticationWanted=" +
-                clientAuthenticationWanted + ", isClientAuthenticationRequired=" +
+        return "TlsConfig{" + "active=" + active + ", passwordsAESEncrypted=" + passwordsAESEncrypted + ", keyStoreType='" + keyStoreType + '\'' + ", keyStorePassword='" +
+                keyStorePassword + '\'' + ", keyStoreFile='" + keyStoreFile + '\'' + ", trustStoreType='" + trustStoreType + '\'' + ", trustStorePassword='" + trustStorePassword +
+                '\'' + ", trustStoreFile='" + trustStoreFile + '\'' + ", clientAuthenticationWanted=" + clientAuthenticationWanted + ", isClientAuthenticationRequired=" +
                 clientAuthenticationRequired + '}';
     }
 
     public boolean hasTrustStore() {
-        return !StringUtils.isBlank(trustStoreType) && !StringUtils.isBlank(trustStorePassword) &&
-                !StringUtils.isBlank(trustStoreFile);
+        return !StringUtils.isBlank(trustStoreType) && !StringUtils.isBlank(trustStorePassword) && !StringUtils.isBlank(trustStoreFile);
     }
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/WatchDogResult.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/WatchDogResult.java
index 1fa2b3def..4471ff605 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/WatchDogResult.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/WatchDogResult.java
@@ -42,7 +42,6 @@ public class WatchDogResult {
 
     @Override
     public String toString() {
-        return "WatchDogResult{" + "port=" + port + ", id='" + id + '\'' + ", numRetries=" +
-                numRetries + '}';
+        return "WatchDogResult{" + "port=" + port + ", id='" + id + '\'' + ", numRetries=" + numRetries + '}';
     }
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/DocumentSelectorConfig.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/DocumentSelectorConfig.java
index 7e35a40eb..304c02760 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/DocumentSelectorConfig.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/DocumentSelectorConfig.java
@@ -29,8 +29,7 @@ public class DocumentSelectorConfig implements ParseContextConfig {
     public static final String X_TIKA_SKIP_EMBEDDED_HEADER = "X-Tika-Skip-Embedded";
 
     @Override
-    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata mtadata,
-                          ParseContext context) {
+    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata mtadata, ParseContext context) {
         DocumentSelector documentSelector = null;
         for (String key : httpHeaders.keySet()) {
             if (StringUtils.endsWithIgnoreCase(key, X_TIKA_SKIP_EMBEDDED_HEADER)) {
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/PasswordProviderConfig.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/PasswordProviderConfig.java
index 0ea95ec39..60438f19e 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/PasswordProviderConfig.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/PasswordProviderConfig.java
@@ -37,8 +37,7 @@ public class PasswordProviderConfig implements ParseContextConfig {
     }
 
     @Override
-    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata,
-                          ParseContext context) {
+    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata, ParseContext context) {
         String tmpPassword = httpHeaders.getFirst(PASSWORD_BASE64_UTF8);
         if (tmpPassword != null) {
             tmpPassword = decodeBase64UTF8(tmpPassword);
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/TimeoutConfig.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/TimeoutConfig.java
index 6882f404a..88a9e102f 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/TimeoutConfig.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/config/TimeoutConfig.java
@@ -28,8 +28,7 @@ public class TimeoutConfig implements ParseContextConfig {
     public static final String X_TIKA_TIMEOUT_MILLIS = "X-Tika-Timeout-Millis";
 
     @Override
-    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata,
-                          ParseContext context) {
+    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata, ParseContext context) {
         if (httpHeaders.containsKey(X_TIKA_TIMEOUT_MILLIS)) {
             long timeout = Long.parseLong(httpHeaders.getFirst(X_TIKA_TIMEOUT_MILLIS));
             context.set(TikaTaskTimeout.class, new TikaTaskTimeout(timeout));
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/AsyncResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/AsyncResource.java
index e5c00eb41..79107476e 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/AsyncResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/AsyncResource.java
@@ -42,13 +42,15 @@ import org.xml.sax.SAXException;
 import org.apache.tika.exception.TikaException;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTupleList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.async.AsyncProcessor;
 import org.apache.tika.pipes.async.OfferLargerThanQueueSize;
 import org.apache.tika.pipes.emitter.EmitData;
 import org.apache.tika.pipes.emitter.EmitterManager;
+import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
 import org.apache.tika.pipes.fetcher.FetchKey;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTupleList;
 
 @Path("/async")
 public class AsyncResource {
@@ -60,8 +62,7 @@ public class AsyncResource {
     long maxQueuePauseMs = 60000;
     private ArrayBlockingQueue<FetchEmitTuple> queue;
 
-    public AsyncResource(java.nio.file.Path tikaConfigPath, Set<String> supportedFetchers)
-            throws TikaException, IOException, SAXException {
+    public AsyncResource(java.nio.file.Path tikaConfigPath, Set<String> supportedFetchers) throws TikaException, IOException, SAXException {
         this.asyncProcessor = new AsyncProcessor(tikaConfigPath);
         this.supportedFetchers = supportedFetchers;
         this.emitterManager = EmitterManager.load(tikaConfigPath);
@@ -94,8 +95,7 @@ public class AsyncResource {
      */
     @POST
     @Produces("application/json")
-    public Map<String, Object> post(InputStream is, @Context HttpHeaders httpHeaders,
-                                    @Context UriInfo info) throws Exception {
+    public Map<String, Object> post(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
 
         AsyncRequest request = deserializeASyncRequest(is);
 
@@ -103,16 +103,28 @@ public class AsyncResource {
         //the requested fetchers and emitters
         //throw early
         for (FetchEmitTuple t : request.getTuples()) {
-            if (!supportedFetchers.contains(t.getFetchKey().getFetcherName())) {
+            if (!supportedFetchers.contains(t
+                    .getFetchKey()
+                    .getFetcherName())) {
                 return badFetcher(t.getFetchKey());
             }
-            if (!emitterManager.getSupported().contains(t.getEmitKey().getEmitterName())) {
-                return badEmitter(t.getEmitKey().getEmitterName());
+            if (!emitterManager
+                    .getSupported()
+                    .contains(t
+                            .getEmitKey()
+                            .getEmitterName())) {
+                return badEmitter(t
+                        .getEmitKey()
+                        .getEmitterName());
             }
-            if (t.getEmbeddedDocumentBytesConfig().isExtractEmbeddedDocumentBytes() &&
-                    !StringUtils.isAllBlank(t.getEmbeddedDocumentBytesConfig().getEmitter())) {
-                String bytesEmitter = t.getEmbeddedDocumentBytesConfig().getEmitter();
-                if (!emitterManager.getSupported().contains(bytesEmitter)) {
+            ParseContext parseContext = t.getParseContext();
+            EmbeddedDocumentBytesConfig embeddedDocumentBytesConfig = parseContext.get(EmbeddedDocumentBytesConfig.class);
+            if (embeddedDocumentBytesConfig != null && embeddedDocumentBytesConfig.isExtractEmbeddedDocumentBytes() &&
+                    !StringUtils.isAllBlank(embeddedDocumentBytesConfig.getEmitter())) {
+                String bytesEmitter = embeddedDocumentBytesConfig.getEmitter();
+                if (!emitterManager
+                        .getSupported()
+                        .contains(bytesEmitter)) {
                     return badEmitter(bytesEmitter);
                 }
             }
@@ -121,15 +133,27 @@ public class AsyncResource {
         try {
             boolean offered = asyncProcessor.offer(request.getTuples(), maxQueuePauseMs);
             if (offered) {
-                LOG.info("accepted {} tuples, capacity={}", request.getTuples().size(), asyncProcessor.getCapacity());
-                return ok(request.getTuples().size());
+                LOG.info("accepted {} tuples, capacity={}", request
+                        .getTuples()
+                        .size(), asyncProcessor.getCapacity());
+                return ok(request
+                        .getTuples()
+                        .size());
             } else {
-                LOG.info("throttling {} tuples, capacity={}", request.getTuples().size(), asyncProcessor.getCapacity());
-                return throttle(request.getTuples().size());
+                LOG.info("throttling {} tuples, capacity={}", request
+                        .getTuples()
+                        .size(), asyncProcessor.getCapacity());
+                return throttle(request
+                        .getTuples()
+                        .size());
             }
         } catch (OfferLargerThanQueueSize e) {
-            LOG.info("throttling {} tuples, capacity={}", request.getTuples().size(), asyncProcessor.getCapacity());
-            return throttle(request.getTuples().size());
+            LOG.info("throttling {} tuples, capacity={}", request
+                    .getTuples()
+                    .size(), asyncProcessor.getCapacity());
+            return throttle(request
+                    .getTuples()
+                    .size());
         }
     }
 
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/DetectorResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/DetectorResource.java
index a8602a18f..51e7ab3d6 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/DetectorResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/DetectorResource.java
@@ -50,8 +50,7 @@ public class DetectorResource {
     @Path("stream")
     @Consumes("*/*")
     @Produces("text/plain")
-    public String detect(final InputStream is, @Context HttpHeaders httpHeaders,
-                         @Context final UriInfo info) {
+    public String detect(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
         Metadata met = new Metadata();
 
         String filename = TikaResource.detectFilename(httpHeaders.getRequestHeaders());
@@ -62,12 +61,14 @@ public class DetectorResource {
         long timeoutMillis = TikaResource.getTaskTimeout(parseContext);
         long taskId = serverStatus.start(ServerStatus.TASK.DETECT, filename, timeoutMillis);
 
-        try (TikaInputStream tis = TikaInputStream.get(
-                TikaResource.getInputStream(is, met, httpHeaders, info))) {
-            return TikaResource.getConfig().getDetector().detect(tis, met).toString();
+        try (TikaInputStream tis = TikaInputStream.get(TikaResource.getInputStream(is, met, httpHeaders, info))) {
+            return TikaResource
+                    .getConfig()
+                    .getDetector()
+                    .detect(tis, met)
+                    .toString();
         } catch (IOException e) {
-            LOG.warn("Unable to detect MIME type for file. Reason: {} ({})", e.getMessage(),
-                    filename, e);
+            LOG.warn("Unable to detect MIME type for file. Reason: {} ({})", e.getMessage(), filename, e);
             return MediaType.OCTET_STREAM.toString();
         } catch (OutOfMemoryError e) {
             LOG.error("OOM while detecting: ({})", filename, e);
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/LanguageResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/LanguageResource.java
index 54fdd339f..8e2a2b1df 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/LanguageResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/LanguageResource.java
@@ -45,7 +45,9 @@ public class LanguageResource {
     @Produces("text/plain")
     public String detect(final InputStream is) throws IOException {
         String fileTxt = IOUtils.toString(is, UTF_8);
-        LanguageResult language = new OptimaizeLangDetector().loadModels().detect(fileTxt);
+        LanguageResult language = new OptimaizeLangDetector()
+                .loadModels()
+                .detect(fileTxt);
         String detectedLang = language.getLanguage();
         LOG.info("Detecting language for incoming resource: [{}]", detectedLang);
         return detectedLang;
@@ -57,7 +59,9 @@ public class LanguageResource {
     @Consumes("*/*")
     @Produces("text/plain")
     public String detect(final String string) throws IOException {
-        LanguageResult language = new OptimaizeLangDetector().loadModels().detect(string);
+        LanguageResult language = new OptimaizeLangDetector()
+                .loadModels()
+                .detect(string);
         String detectedLang = language.getLanguage();
         LOG.info("Detecting language for incoming resource: [{}]", detectedLang);
         return detectedLang;
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/MetadataResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/MetadataResource.java
index 467256946..29e452d94 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/MetadataResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/MetadataResource.java
@@ -53,21 +53,19 @@ public class MetadataResource {
     @Consumes("multipart/form-data")
     @Produces({"text/csv", "application/json"})
     @Path("form")
-    public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info)
-            throws Exception {
-        return Response.ok(
-                parseMetadata(att.getObject(InputStream.class), new Metadata(), att.getHeaders(),
-                        info)).build();
+    public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info) throws Exception {
+        return Response
+                .ok(parseMetadata(att.getObject(InputStream.class), new Metadata(), att.getHeaders(), info))
+                .build();
     }
 
     @PUT
     @Produces({"text/csv", "application/json"})
-    public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders,
-                                @Context UriInfo info) throws Exception {
+    public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
         Metadata metadata = new Metadata();
-        return Response.ok(
-                parseMetadata(TikaResource.getInputStream(is, metadata, httpHeaders, info),
-                        metadata, httpHeaders.getRequestHeaders(), info)).build();
+        return Response
+                .ok(parseMetadata(TikaResource.getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info))
+                .build();
     }
 
     /**
@@ -96,9 +94,7 @@ public class MetadataResource {
     @PUT
     @Path("{field}")
     @Produces({"text/csv", "application/json", "text/plain"})
-    public Response getMetadataField(InputStream is, @Context HttpHeaders httpHeaders,
-                                     @Context UriInfo info, @PathParam("field") String field)
-            throws Exception {
+    public Response getMetadataField(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info, @PathParam("field") String field) throws Exception {
 
         // use BAD request to indicate that we may not have had enough data to
         // process the request
@@ -106,8 +102,7 @@ public class MetadataResource {
         Metadata metadata = new Metadata();
         boolean success = false;
         try {
-            parseMetadata(TikaResource.getInputStream(is, metadata, httpHeaders, info), metadata,
-                    httpHeaders.getRequestHeaders(), info);
+            parseMetadata(TikaResource.getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info);
             // once we've parsed the document successfully, we should use NOT_FOUND
             // if we did not see the field
             defaultErrorResponse = Response.Status.NOT_FOUND;
@@ -117,8 +112,10 @@ public class MetadataResource {
         }
 
         if (success == false || metadata.get(field) == null) {
-            return Response.status(defaultErrorResponse)
-                    .entity("Failed to get metadata field " + field).build();
+            return Response
+                    .status(defaultErrorResponse)
+                    .entity("Failed to get metadata field " + field)
+                    .build();
         }
 
         // remove fields we don't care about for the response
@@ -127,12 +124,12 @@ public class MetadataResource {
                 metadata.remove(name);
             }
         }
-        return Response.ok(metadata).build();
+        return Response
+                .ok(metadata)
+                .build();
     }
 
-    protected Metadata parseMetadata(InputStream is, Metadata metadata,
-                                     MultivaluedMap<String, String> httpHeaders, UriInfo info)
-            throws IOException {
+    protected Metadata parseMetadata(InputStream is, Metadata metadata, MultivaluedMap<String, String> httpHeaders, UriInfo info) throws IOException {
         final ParseContext context = new ParseContext();
         Parser parser = TikaResource.createParser();
         fillMetadata(parser, metadata, httpHeaders);
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/PipesResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/PipesResource.java
index 333ffa6ce..ab4c38add 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/PipesResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/PipesResource.java
@@ -37,12 +37,12 @@ import org.slf4j.LoggerFactory;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTuple;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.PipesConfig;
 import org.apache.tika.pipes.PipesException;
 import org.apache.tika.pipes.PipesParser;
 import org.apache.tika.pipes.PipesResult;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTuple;
 
 @Path("/pipes")
 public class PipesResource {
@@ -84,8 +84,7 @@ public class PipesResource {
      */
     @POST
     @Produces("application/json")
-    public Map<String, String> postRmeta(InputStream is, @Context HttpHeaders httpHeaders,
-                                         @Context UriInfo info) throws Exception {
+    public Map<String, String> postRmeta(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
         FetchEmitTuple t = null;
         try (Reader reader = new InputStreamReader(is, StandardCharsets.UTF_8)) {
             t = JsonFetchEmitTuple.fromJson(reader);
@@ -93,14 +92,12 @@ public class PipesResource {
         return processTuple(t);
     }
 
-    private Map<String, String> processTuple(FetchEmitTuple fetchEmitTuple)
-            throws InterruptedException, PipesException, IOException {
+    private Map<String, String> processTuple(FetchEmitTuple fetchEmitTuple) throws InterruptedException, PipesException, IOException {
 
         PipesResult pipesResult = pipesParser.parse(fetchEmitTuple);
         switch (pipesResult.getStatus()) {
             case CLIENT_UNAVAILABLE_WITHIN_MS:
-                throw new IllegalStateException(
-                        "client not available within " + "allotted amount of time");
+                throw new IllegalStateException("client not available within " + "allotted amount of time");
             case EMIT_EXCEPTION:
                 return returnEmitException(pipesResult.getMessage());
             case PARSE_SUCCESS:
@@ -111,8 +108,7 @@ public class PipesResource {
             case EMIT_SUCCESS_PARSE_EXCEPTION:
                 return parseException(pipesResult.getMessage(), true);
             case PARSE_EXCEPTION_EMIT:
-                throw new IllegalArgumentException(
-                        "Should have tried to emit in forked " + "process?!");
+                throw new IllegalArgumentException("Should have tried to emit in forked " + "process?!");
             case PARSE_EXCEPTION_NO_EMIT:
                 return parseException(pipesResult.getMessage(), false);
             case TIMEOUT:
@@ -122,13 +118,12 @@ public class PipesResource {
             case UNSPECIFIED_CRASH:
                 return returnError("unknown_crash");
             case NO_EMITTER_FOUND: {
-                throw new IllegalArgumentException("Couldn't find emitter that matched: " +
-                        fetchEmitTuple.getEmitKey().getEmitterName());
+                throw new IllegalArgumentException("Couldn't find emitter that matched: " + fetchEmitTuple
+                        .getEmitKey()
+                        .getEmitterName());
             }
             default:
-                throw new IllegalArgumentException(
-                        "I'm sorry, I don't yet handle a status of " + "this type: " +
-                                pipesResult.getStatus());
+                throw new IllegalArgumentException("I'm sorry, I don't yet handle a status of " + "this type: " + pipesResult.getStatus());
         }
     }
 
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/RecursiveMetadataResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/RecursiveMetadataResource.java
index 574c74aa8..a180ddfba 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/RecursiveMetadataResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/RecursiveMetadataResource.java
@@ -53,13 +53,10 @@ import org.apache.tika.server.core.TikaServerParseException;
 public class RecursiveMetadataResource {
 
     protected static final String HANDLER_TYPE_PARAM = "handler";
-    protected static final BasicContentHandlerFactory.HANDLER_TYPE DEFAULT_HANDLER_TYPE =
-            BasicContentHandlerFactory.HANDLER_TYPE.XML;
+    protected static final BasicContentHandlerFactory.HANDLER_TYPE DEFAULT_HANDLER_TYPE = BasicContentHandlerFactory.HANDLER_TYPE.XML;
     private static final Logger LOG = LoggerFactory.getLogger(RecursiveMetadataResource.class);
 
-    public static List<Metadata> parseMetadata(InputStream is, Metadata metadata,
-                                               MultivaluedMap<String, String> httpHeaders,
-                                               UriInfo info, HandlerConfig handlerConfig)
+    public static List<Metadata> parseMetadata(InputStream is, Metadata metadata, MultivaluedMap<String, String> httpHeaders, UriInfo info, HandlerConfig handlerConfig)
             throws Exception {
 
         final ParseContext context = new ParseContext();
@@ -71,11 +68,11 @@ public class RecursiveMetadataResource {
         TikaResource.logRequest(LOG, "/rmeta", metadata);
 
         BasicContentHandlerFactory.HANDLER_TYPE type = handlerConfig.getType();
-        RecursiveParserWrapperHandler handler = new RecursiveParserWrapperHandler(
-                new BasicContentHandlerFactory(type, handlerConfig.getWriteLimit(),
-                        handlerConfig.isThrowOnWriteLimitReached(), context),
-                handlerConfig.getMaxEmbeddedResources(),
-                TikaResource.getConfig().getMetadataFilter());
+        RecursiveParserWrapperHandler handler =
+                new RecursiveParserWrapperHandler(new BasicContentHandlerFactory(type, handlerConfig.getWriteLimit(), handlerConfig.isThrowOnWriteLimitReached(), context),
+                        handlerConfig.getMaxEmbeddedResources(), TikaResource
+                        .getConfig()
+                        .getMetadataFilter());
         try {
             TikaResource.parse(wrapper, LOG, "/rmeta", is, handler, metadata, context);
         } catch (TikaServerParseException e) {
@@ -91,9 +88,7 @@ public class RecursiveMetadataResource {
         return handler.getMetadataList();
     }
 
-    static HandlerConfig buildHandlerConfig(MultivaluedMap<String, String> httpHeaders,
-                                            String handlerTypeName,
-                                            HandlerConfig.PARSE_MODE parseMode) {
+    static HandlerConfig buildHandlerConfig(MultivaluedMap<String, String> httpHeaders, String handlerTypeName, HandlerConfig.PARSE_MODE parseMode) {
         int writeLimit = -1;
         if (httpHeaders.containsKey("writeLimit")) {
             writeLimit = Integer.parseInt(httpHeaders.getFirst("writeLimit"));
@@ -103,9 +98,7 @@ public class RecursiveMetadataResource {
         if (httpHeaders.containsKey("maxEmbeddedResources")) {
             maxEmbeddedResources = Integer.parseInt(httpHeaders.getFirst("maxEmbeddedResources"));
         }
-        return new HandlerConfig(
-                BasicContentHandlerFactory.parseHandlerType(handlerTypeName, DEFAULT_HANDLER_TYPE),
-                parseMode, writeLimit, maxEmbeddedResources,
+        return new HandlerConfig(BasicContentHandlerFactory.parseHandlerType(handlerTypeName, DEFAULT_HANDLER_TYPE), parseMode, writeLimit, maxEmbeddedResources,
                 TikaResource.getThrowOnWriteLimitReached(httpHeaders));
     }
 
@@ -137,14 +130,11 @@ public class RecursiveMetadataResource {
     @Consumes("multipart/form-data")
     @Produces({"application/json"})
     @Path("form{" + HANDLER_TYPE_PARAM + " : (\\w+)?}")
-    public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info,
-                                             @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName)
-            throws Exception {
-        return Response.ok(
-                parseMetadataToMetadataList(att.getObject(InputStream.class), new Metadata(),
-                        att.getHeaders(), info,
-                        buildHandlerConfig(att.getHeaders(), handlerTypeName,
-                                HandlerConfig.PARSE_MODE.RMETA))).build();
+    public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info, @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName) throws Exception {
+        return Response
+                .ok(parseMetadataToMetadataList(att.getObject(InputStream.class), new Metadata(), att.getHeaders(), info,
+                        buildHandlerConfig(att.getHeaders(), handlerTypeName, HandlerConfig.PARSE_MODE.RMETA)))
+                .build();
     }
 
     /**
@@ -174,21 +164,15 @@ public class RecursiveMetadataResource {
     @PUT
     @Produces("application/json")
     @Path("{" + HANDLER_TYPE_PARAM + " : (\\w+)?}")
-    public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders,
-                                @Context UriInfo info,
-                                @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName)
-            throws Exception {
+    public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info, @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName) throws Exception {
         Metadata metadata = new Metadata();
-        return Response.ok(parseMetadataToMetadataList(
-                TikaResource.getInputStream(is, metadata, httpHeaders, info), metadata,
-                httpHeaders.getRequestHeaders(), info,
-                buildHandlerConfig(httpHeaders.getRequestHeaders(), handlerTypeName,
-                        HandlerConfig.PARSE_MODE.RMETA))).build();
+        return Response
+                .ok(parseMetadataToMetadataList(TikaResource.getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info,
+                        buildHandlerConfig(httpHeaders.getRequestHeaders(), handlerTypeName, HandlerConfig.PARSE_MODE.RMETA)))
+                .build();
     }
 
-    private MetadataList parseMetadataToMetadataList(InputStream is, Metadata metadata,
-                                                     MultivaluedMap<String, String> httpHeaders,
-                                                     UriInfo info, HandlerConfig handlerConfig)
+    private MetadataList parseMetadataToMetadataList(InputStream is, Metadata metadata, MultivaluedMap<String, String> httpHeaders, UriInfo info, HandlerConfig handlerConfig)
             throws Exception {
         return new MetadataList(parseMetadata(is, metadata, httpHeaders, info, handlerConfig));
     }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaDetectors.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaDetectors.java
index 1cc0071be..d9166dca3 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaDetectors.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaDetectors.java
@@ -49,7 +49,9 @@ public class TikaDetectors {
     public String getDectorsHTML() {
         StringBuffer h = new StringBuffer();
         html.generateHeader(h, "Detectors available to Apache Tika");
-        detectorAsHTML(TikaResource.getConfig().getDetector(), h, 2);
+        detectorAsHTML(TikaResource
+                .getConfig()
+                .getDetector(), h, 2);
         html.generateFooter(h);
         return h.toString();
     }
@@ -58,7 +60,9 @@ public class TikaDetectors {
         html.append("<h");
         html.append(level);
         html.append(">");
-        String name = d.getClass().getName();
+        String name = d
+                .getClass()
+                .getName();
         html.append(name.substring(name.lastIndexOf('.') + 1));
         html.append("</h");
         html.append(level);
@@ -78,12 +82,16 @@ public class TikaDetectors {
     @Produces(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
     public String getDetectorsJSON() throws IOException {
         Map<String, Object> details = new HashMap<>();
-        detectorAsMap(TikaResource.getConfig().getDetector(), details);
+        detectorAsMap(TikaResource
+                .getConfig()
+                .getDetector(), details);
         return new ObjectMapper().writeValueAsString(details);
     }
 
     private void detectorAsMap(Detector d, Map<String, Object> details) {
-        details.put("name", d.getClass().getName());
+        details.put("name", d
+                .getClass()
+                .getName());
 
         boolean isComposite = (d instanceof CompositeDetector);
         details.put("composite", isComposite);
@@ -102,13 +110,17 @@ public class TikaDetectors {
     @Produces("text/plain")
     public String getDetectorsPlain() {
         StringBuffer text = new StringBuffer();
-        renderDetector(TikaResource.getConfig().getDetector(), text, 0);
+        renderDetector(TikaResource
+                .getConfig()
+                .getDetector(), text, 0);
         return text.toString();
     }
 
     private void renderDetector(Detector d, StringBuffer text, int indent) {
         boolean isComposite = (d instanceof CompositeDetector);
-        String name = d.getClass().getName();
+        String name = d
+                .getClass()
+                .getName();
 
         for (int i = 0; i < indent; i++) {
             text.append("  ");
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaMimeTypes.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaMimeTypes.java
index 198ce7a9e..9d2e728c4 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaMimeTypes.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaMimeTypes.java
@@ -82,40 +82,74 @@ public class TikaMimeTypes {
         }
         h.append("<ul>");
         for (String section : firstType.keySet()) {
-            h.append("<li><a href=\"#").append(firstType.get(section)).append("\">").append(section)
+            h
+                    .append("<li><a href=\"#")
+                    .append(firstType.get(section))
+                    .append("\">")
+                    .append(section)
                     .append("</a></li>\n");
         }
         h.append("</ul>");
 
         // Output all of them
         for (MediaTypeDetails type : types) {
-            h.append("<a name=\"").append(type.type).append("\"></a>\n");
-            h.append("<h2><a href=\"mime-types/").append(type.type).append("\">").append(type.type)
+            h
+                    .append("<a name=\"")
+                    .append(type.type)
+                    .append("\"></a>\n");
+            h
+                    .append("<h2><a href=\"mime-types/")
+                    .append(type.type)
+                    .append("\">")
+                    .append(type.type)
                     .append("</a></h2>\n");
 
             for (MediaType alias : type.aliases) {
-                h.append("<div>Alias: ").append(alias).append("</div>\n");
+                h
+                        .append("<div>Alias: ")
+                        .append(alias)
+                        .append("</div>\n");
             }
             if (type.supertype != null) {
-                h.append("<div>Super Type: <a href=\"#").append(type.supertype).append("\">")
-                        .append(type.supertype).append("</a></div>\n");
+                h
+                        .append("<div>Super Type: <a href=\"#")
+                        .append(type.supertype)
+                        .append("\">")
+                        .append(type.supertype)
+                        .append("</a></div>\n");
             }
             if (type.mime != null) {
-                if (!type.mime.getDescription().isEmpty()) {
-                    h.append("<div>Description: ").append(type.mime.getDescription())
+                if (!type.mime
+                        .getDescription()
+                        .isEmpty()) {
+                    h
+                            .append("<div>Description: ")
+                            .append(type.mime.getDescription())
                             .append("</div>\n");
                 }
-                if (!type.mime.getAcronym().isEmpty()) {
-                    h.append("<div>Acronym: ").append(type.mime.getAcronym()).append("</div>\n");
+                if (!type.mime
+                        .getAcronym()
+                        .isEmpty()) {
+                    h
+                            .append("<div>Acronym: ")
+                            .append(type.mime.getAcronym())
+                            .append("</div>\n");
                 }
-                if (!type.mime.getExtension().isEmpty()) {
-                    h.append("<div>Default Extension: ").append(type.mime.getExtension())
+                if (!type.mime
+                        .getExtension()
+                        .isEmpty()) {
+                    h
+                            .append("<div>Default Extension: ")
+                            .append(type.mime.getExtension())
                             .append("</div>\n");
                 }
             }
 
             if (type.parser != null) {
-                h.append("<div>Parser: ").append(type.parser).append("</div>\n");
+                h
+                        .append("<div>Parser: ")
+                        .append(type.parser)
+                        .append("</div>\n");
             }
         }
 
@@ -126,47 +160,82 @@ public class TikaMimeTypes {
     @GET
     @Path("/{type}/{subtype}")
     @Produces("text/html")
-    public String getMimeTypeDetailsHTML(@PathParam("type") String typePart,
-                                         @PathParam("subtype") String subtype) {
+    public String getMimeTypeDetailsHTML(@PathParam("type") String typePart, @PathParam("subtype") String subtype) {
         MediaTypeDetails type = getMediaType(typePart, subtype);
 
         StringBuffer h = new StringBuffer();
         html.generateHeader(h, "Apache Tika Details on Mime Type " + type.type);
-        h.append("<h2>").append(type.type).append("</h2>\n");
+        h
+                .append("<h2>")
+                .append(type.type)
+                .append("</h2>\n");
 
         for (MediaType alias : type.aliases) {
-            h.append("<div>Alias: ").append(alias).append("</div>\n");
+            h
+                    .append("<div>Alias: ")
+                    .append(alias)
+                    .append("</div>\n");
         }
         if (type.supertype != null) {
-            h.append("<div>Super Type: <a href=\"#").append(type.supertype).append("\">")
-                    .append(type.supertype).append("</a></div>\n");
+            h
+                    .append("<div>Super Type: <a href=\"#")
+                    .append(type.supertype)
+                    .append("\">")
+                    .append(type.supertype)
+                    .append("</a></div>\n");
         }
         if (type.mime != null) {
-            if (!type.mime.getDescription().isEmpty()) {
-                h.append("<div>Description: ").append(type.mime.getDescription())
+            if (!type.mime
+                    .getDescription()
+                    .isEmpty()) {
+                h
+                        .append("<div>Description: ")
+                        .append(type.mime.getDescription())
                         .append("</div>\n");
             }
-            if (!type.mime.getAcronym().isEmpty()) {
-                h.append("<div>Acronym: ").append(type.mime.getAcronym()).append("</div>\n");
+            if (!type.mime
+                    .getAcronym()
+                    .isEmpty()) {
+                h
+                        .append("<div>Acronym: ")
+                        .append(type.mime.getAcronym())
+                        .append("</div>\n");
             }
-            if (!type.mime.getUniformTypeIdentifier().isEmpty()) {
-                h.append("<div>Uniform Type Identifier: ")
-                        .append(type.mime.getUniformTypeIdentifier()).append("</div>\n");
+            if (!type.mime
+                    .getUniformTypeIdentifier()
+                    .isEmpty()) {
+                h
+                        .append("<div>Uniform Type Identifier: ")
+                        .append(type.mime.getUniformTypeIdentifier())
+                        .append("</div>\n");
             }
             for (URI uri : type.mime.getLinks()) {
-                h.append("<div>Link: ").append(uri).append("</div>\n");
+                h
+                        .append("<div>Link: ")
+                        .append(uri)
+                        .append("</div>\n");
             }
-            if (!type.mime.getExtension().isEmpty()) {
-                h.append("<div>Default Extension: ").append(type.mime.getExtension())
+            if (!type.mime
+                    .getExtension()
+                    .isEmpty()) {
+                h
+                        .append("<div>Default Extension: ")
+                        .append(type.mime.getExtension())
                         .append("</div>\n");
             }
             for (String ext : type.mime.getExtensions()) {
-                h.append("<div>Extension: ").append(ext).append("</div>\n");
+                h
+                        .append("<div>Extension: ")
+                        .append(ext)
+                        .append("</div>\n");
             }
         }
 
         if (type.parser != null) {
-            h.append("<div>Parser: ").append(type.parser).append("</div>\n");
+            h
+                    .append("<div>Parser: ")
+                    .append(type.parser)
+                    .append("</div>\n");
         }
 
         html.generateFooter(h);
@@ -192,14 +261,15 @@ public class TikaMimeTypes {
             details.put(type.type.toString(), typeDets);
         }
 
-        return new ObjectMapper().writerWithDefaultPrettyPrinter().writeValueAsString(details);
+        return new ObjectMapper()
+                .writerWithDefaultPrettyPrinter()
+                .writeValueAsString(details);
     }
 
     @GET
     @Path("/{type}/{subtype}")
     @Produces(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
-    public String getMimeTypeDetailsJSON(@PathParam("type") String typePart,
-                                         @PathParam("subtype") String subtype) throws IOException {
+    public String getMimeTypeDetailsJSON(@PathParam("type") String typePart, @PathParam("subtype") String subtype) throws IOException {
         MediaTypeDetails type = getMediaType(typePart, subtype);
         Map<String, Object> details = new HashMap<>();
 
@@ -212,27 +282,41 @@ public class TikaMimeTypes {
             details.put("parser", type.parser);
         }
         if (type.mime != null) {
-            if (!type.mime.getDescription().isEmpty()) {
+            if (!type.mime
+                    .getDescription()
+                    .isEmpty()) {
                 details.put("description", type.mime.getDescription());
             }
-            if (!type.mime.getAcronym().isEmpty()) {
+            if (!type.mime
+                    .getAcronym()
+                    .isEmpty()) {
                 details.put("acronym", type.mime.getAcronym());
             }
-            if (!type.mime.getUniformTypeIdentifier().isEmpty()) {
+            if (!type.mime
+                    .getUniformTypeIdentifier()
+                    .isEmpty()) {
                 details.put("uniformTypeIdentifier", type.mime.getUniformTypeIdentifier());
             }
-            if (!type.mime.getLinks().isEmpty()) {
+            if (!type.mime
+                    .getLinks()
+                    .isEmpty()) {
                 details.put("links", type.mime.getLinks());
             }
-            if (!type.mime.getExtension().isEmpty()) {
+            if (!type.mime
+                    .getExtension()
+                    .isEmpty()) {
                 details.put("defaultExtension", type.mime.getExtension());
             }
-            if (!type.mime.getExtensions().isEmpty()) {
+            if (!type.mime
+                    .getExtensions()
+                    .isEmpty()) {
                 details.put("extensions", type.mime.getExtensions());
             }
         }
 
-        return new ObjectMapper().writerWithDefaultPrettyPrinter().writeValueAsString(details);
+        return new ObjectMapper()
+                .writerWithDefaultPrettyPrinter()
+                .writeValueAsString(details);
     }
 
     @GET
@@ -245,14 +329,23 @@ public class TikaMimeTypes {
             text.append("\n");
 
             for (MediaType alias : type.aliases) {
-                text.append("  alias:     ").append(alias).append("\n");
+                text
+                        .append("  alias:     ")
+                        .append(alias)
+                        .append("\n");
             }
             if (type.supertype != null) {
-                text.append("  supertype: ").append(type.supertype.toString()).append("\n");
+                text
+                        .append("  supertype: ")
+                        .append(type.supertype.toString())
+                        .append("\n");
             }
 
             if (type.parser != null) {
-                text.append("  parser:    ").append(type.parser).append("\n");
+                text
+                        .append("  parser:    ")
+                        .append(type.parser)
+                        .append("\n");
             }
         }
 
@@ -275,12 +368,16 @@ public class TikaMimeTypes {
         MediaTypeRegistry registry = config.getMediaTypeRegistry();
         Map<MediaType, Parser> parsers = ((CompositeParser) config.getParser()).getParsers();
 
-        List<MediaTypeDetails> types = new ArrayList<>(registry.getTypes().size());
+        List<MediaTypeDetails> types = new ArrayList<>(registry
+                .getTypes()
+                .size());
 
         for (MediaType type : registry.getTypes()) {
             MediaTypeDetails details = new MediaTypeDetails();
             details.type = type;
-            details.aliases = registry.getAliases(type).toArray(new MediaType[0]);
+            details.aliases = registry
+                    .getAliases(type)
+                    .toArray(new MediaType[0]);
 
             try {
                 details.mime = mimeTypes.getRegisteredMimeType(type.toString());
@@ -296,9 +393,13 @@ public class TikaMimeTypes {
             Parser p = parsers.get(type);
             if (p != null) {
                 if (p instanceof CompositeParser) {
-                    p = ((CompositeParser) p).getParsers().get(type);
+                    p = ((CompositeParser) p)
+                            .getParsers()
+                            .get(type);
                 }
-                details.parser = p.getClass().getName();
+                details.parser = p
+                        .getClass()
+                        .getName();
             }
 
             types.add(details);
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaParsers.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaParsers.java
index 32da8036b..64e645c59 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaParsers.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaParsers.java
@@ -66,7 +66,9 @@ public class TikaParsers {
     }
 
     protected String getParsersHTML(boolean withMimeTypes) {
-        ParserDetails p = new ParserDetails(TikaResource.getConfig().getParser());
+        ParserDetails p = new ParserDetails(TikaResource
+                .getConfig()
+                .getParser());
 
         StringBuffer h = new StringBuffer();
         html.generateHeader(h, "Parsers available to Apache Tika");
@@ -75,8 +77,7 @@ public class TikaParsers {
         return h.toString();
     }
 
-    private void parserAsHTML(ParserDetails p, boolean withMimeTypes, StringBuffer html,
-                              int level) {
+    private void parserAsHTML(ParserDetails p, boolean withMimeTypes, StringBuffer html, int level) {
         html.append("<h");
         html.append(level);
         html.append(">");
@@ -90,7 +91,9 @@ public class TikaParsers {
         if (p.isDecorated) {
             html.append("<p>Decorated Parser");
             if (p.decoratedBy != null) {
-                html.append(" - ").append(p.decoratedBy);
+                html
+                        .append(" - ")
+                        .append(p.decoratedBy);
             }
             html.append("</p>");
         }
@@ -130,10 +133,13 @@ public class TikaParsers {
 
     protected String getParsersJSON(boolean withMimeTypes) throws IOException {
         Map<String, Object> details = new HashMap<>();
-        parserAsMap(new ParserDetails(TikaResource.getConfig().getParser()), withMimeTypes,
-                details);
+        parserAsMap(new ParserDetails(TikaResource
+                .getConfig()
+                .getParser()), withMimeTypes, details);
         ObjectMapper objectMapper = new ObjectMapper();
-        return objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(details);
+        return objectMapper
+                .writerWithDefaultPrettyPrinter()
+                .writeValueAsString(details);
     }
 
     private void parserAsMap(ParserDetails p, boolean withMimeTypes, Map<String, Object> details) {
@@ -173,13 +179,13 @@ public class TikaParsers {
 
     protected String getParsersPlain(boolean withMimeTypes) {
         StringBuffer text = new StringBuffer();
-        renderParser(new ParserDetails(TikaResource.getConfig().getParser()), withMimeTypes, text,
-                "");
+        renderParser(new ParserDetails(TikaResource
+                .getConfig()
+                .getParser()), withMimeTypes, text, "");
         return text.toString();
     }
 
-    private void renderParser(ParserDetails p, boolean withMimeTypes, StringBuffer text,
-                              String indent) {
+    private void renderParser(ParserDetails p, boolean withMimeTypes, StringBuffer text, String indent) {
         String nextIndent = indent + "  ";
 
         text.append(indent);
@@ -187,7 +193,9 @@ public class TikaParsers {
         if (p.isDecorated) {
             text.append(" (Decorated Parser");
             if (p.decoratedBy != null) {
-                text.append(" ").append(p.decoratedBy);
+                text
+                        .append(" ")
+                        .append(p.decoratedBy);
             }
             text.append(")");
         }
@@ -226,7 +234,9 @@ public class TikaParsers {
                 p = ((ParserDecorator) p).getWrappedParser();
             }
 
-            className = p.getClass().getName();
+            className = p
+                    .getClass()
+                    .getName();
             shortName = className.substring(className.lastIndexOf('.') + 1);
 
             if (p instanceof CompositeParser) {
@@ -234,11 +244,14 @@ public class TikaParsers {
                 supportedTypes = Collections.emptySet();
 
                 // Get the unique set of child parsers
-                Set<Parser> children =
-                        new HashSet<>(((CompositeParser) p).getParsers(EMPTY_PC).values());
+                Set<Parser> children = new HashSet<>(((CompositeParser) p)
+                        .getParsers(EMPTY_PC)
+                        .values());
                 // Sort it by class name
                 childParsers = new ArrayList<>(children);
-                childParsers.sort(Comparator.comparing(parser -> parser.getClass().getName()));
+                childParsers.sort(Comparator.comparing(parser -> parser
+                        .getClass()
+                        .getName()));
             } else {
                 supportedTypes = p.getSupportedTypes(EMPTY_PC);
             }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaResource.java
index 5f0e76ec8..6e76dafdb 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaResource.java
@@ -90,8 +90,7 @@ import org.apache.tika.utils.ExceptionUtils;
 @Path("/tika")
 public class TikaResource {
 
-    public static final String GREETING =
-            "This is Tika Server (" + Tika.getString() + "). Please PUT\n";
+    public static final String GREETING = "This is Tika Server (" + Tika.getString() + "). Please PUT\n";
     private static final String META_PREFIX = "meta_";
     private static final Logger LOG = LoggerFactory.getLogger(TikaResource.class);
     private static Pattern ALLOWABLE_HEADER_CHARS = Pattern.compile("(?i)^[-/_+\\.A-Z0-9 ]+$");
@@ -104,9 +103,8 @@ public class TikaResource {
     private static ParseContextConfig PARSE_CONTEXT_CONFIG = new CompositeParseContextConfig();
 
 
-    public static void init(TikaConfig config, TikaServerConfig tikaServerConfg,
-                            DigestingParser.Digester digester,
-                            InputStreamFactory inputStreamFactory, ServerStatus serverStatus) {
+    public static void init(TikaConfig config, TikaServerConfig tikaServerConfg, DigestingParser.Digester digester, InputStreamFactory inputStreamFactory,
+                            ServerStatus serverStatus) {
         TIKA_CONFIG = config;
         TIKA_SERVER_CONFIG = tikaServerConfg;
         DIGESTER = digester;
@@ -121,9 +119,12 @@ public class TikaResource {
 
         if (DIGESTER != null) {
             boolean skipContainer = false;
-            if (TIKA_CONFIG.getAutoDetectParserConfig().getDigesterFactory() != null &&
-                    TIKA_CONFIG.getAutoDetectParserConfig().getDigesterFactory()
-                            .isSkipContainerDocument()) {
+            if (TIKA_CONFIG
+                    .getAutoDetectParserConfig()
+                    .getDigesterFactory() != null && TIKA_CONFIG
+                    .getAutoDetectParserConfig()
+                    .getDigesterFactory()
+                    .isSkipContainerDocument()) {
                 skipContainer = true;
             }
             return new DigestingParser(parser, DIGESTER, skipContainer);
@@ -154,13 +155,11 @@ public class TikaResource {
         return httpHeaders.getFirst("File-Name");
     }
 
-    public static void fillParseContext(MultivaluedMap<String, String> httpHeaders,
-                                        Metadata metadata, ParseContext parseContext) {
+    public static void fillParseContext(MultivaluedMap<String, String> httpHeaders, Metadata metadata, ParseContext parseContext) {
         PARSE_CONTEXT_CONFIG.configure(httpHeaders, metadata, parseContext);
     }
 
-    public static InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders headers,
-                                             UriInfo uriInfo) {
+    public static InputStream getInputStream(InputStream is, Metadata metadata, HttpHeaders headers, UriInfo uriInfo) {
         try {
             return INPUTSTREAM_FACTORY.getInputStream(is, metadata, headers, uriInfo);
         } catch (IOException e) {
@@ -182,18 +181,26 @@ public class TikaResource {
             String property = StringUtils.removeStartIgnoreCase(key, prefix);
             Field field = null;
             try {
-                field = object.getClass().getDeclaredField(StringUtils.uncapitalize(property));
+                field = object
+                        .getClass()
+                        .getDeclaredField(StringUtils.uncapitalize(property));
             } catch (NoSuchFieldException e) {
                 // try to match field case-insensitive way
-                for (Field aField : object.getClass().getDeclaredFields()) {
-                    if (aField.getName().equalsIgnoreCase(property)) {
+                for (Field aField : object
+                        .getClass()
+                        .getDeclaredFields()) {
+                    if (aField
+                            .getName()
+                            .equalsIgnoreCase(property)) {
                         field = aField;
                         break;
                     }
                 }
             }
             String setter = field != null ? field.getName() : property;
-            setter = "set" + setter.substring(0, 1).toUpperCase(Locale.US) + setter.substring(1);
+            setter = "set" + setter
+                    .substring(0, 1)
+                    .toUpperCase(Locale.US) + setter.substring(1);
             //default assume string class
             //if there's a more specific type, e.g. double, int, boolean
             //try that.
@@ -242,8 +249,7 @@ public class TikaResource {
                 } else if (clazz == long.class || clazz == Long.class) {
                     m.invoke(object, Long.parseLong(val));
                 } else {
-                    throw new IllegalArgumentException(
-                            "setter must be String, int, float, double or boolean...for now");
+                    throw new IllegalArgumentException("setter must be String, int, float, double or boolean...for now");
                 }
             } else {
                 throw new NoSuchMethodException("Couldn't find: " + setter);
@@ -251,8 +257,7 @@ public class TikaResource {
 
         } catch (Throwable ex) {
             // TIKA-3345
-            String error = (!(ex.getCause() instanceof IllegalArgumentException)) ?
-                    String.format(Locale.ROOT, "%s is an invalid %s header", key, prefix) :
+            String error = (!(ex.getCause() instanceof IllegalArgumentException)) ? String.format(Locale.ROOT, "%s is an invalid %s header", key, prefix) :
                     String.format(Locale.ROOT, "%s is an invalid %s header value", val, key);
             throw new WebApplicationException(error, Response.Status.BAD_REQUEST);
         }
@@ -262,15 +267,14 @@ public class TikaResource {
         if (setter == null || val == null) {
             throw new IllegalArgumentException("setter and val must not be null");
         }
-        if (setter.toLowerCase(Locale.US).contains("trusted")) {
-            throw new IllegalArgumentException(
-                    "Can't call a trusted method via tika-server headers");
+        if (setter
+                .toLowerCase(Locale.US)
+                .contains("trusted")) {
+            throw new IllegalArgumentException("Can't call a trusted method via tika-server headers");
         }
         Matcher m = ALLOWABLE_HEADER_CHARS.matcher(val);
         if (!m.find()) {
-            throw new IllegalArgumentException(
-                    "Header val: " + val + " contains illegal characters. " +
-                            "Must contain: TikaResource.ALLOWABLE_HEADER_CHARS");
+            throw new IllegalArgumentException("Header val: " + val + " contains illegal characters. " + "Must contain: TikaResource.ALLOWABLE_HEADER_CHARS");
         }
     }
 
@@ -285,30 +289,28 @@ public class TikaResource {
      */
     private static Method tryToGetMethod(Object object, String method, Class clazz) {
         try {
-            return object.getClass().getMethod(method, clazz);
+            return object
+                    .getClass()
+                    .getMethod(method, clazz);
         } catch (NoSuchMethodException e) {
             return null;
         }
     }
 
     @SuppressWarnings("serial")
-    public static void fillMetadata(Parser parser, Metadata metadata,
-                                    MultivaluedMap<String, String> httpHeaders) {
+    public static void fillMetadata(Parser parser, Metadata metadata, MultivaluedMap<String, String> httpHeaders) {
         String fileName = detectFilename(httpHeaders);
         if (fileName != null) {
             metadata.set(TikaCoreProperties.RESOURCE_NAME_KEY, fileName);
         }
 
         String contentTypeHeader = httpHeaders.getFirst(HttpHeaders.CONTENT_TYPE);
-        jakarta.ws.rs.core.MediaType mediaType =
-                (contentTypeHeader == null || "*/*".equals(contentTypeHeader)) ? null :
-                        jakarta.ws.rs.core.MediaType.valueOf(contentTypeHeader);
+        jakarta.ws.rs.core.MediaType mediaType = (contentTypeHeader == null || "*/*".equals(contentTypeHeader)) ? null : jakarta.ws.rs.core.MediaType.valueOf(contentTypeHeader);
         if (mediaType != null && "xml".equals(mediaType.getSubtype())) {
             mediaType = null;
         }
 
-        if (mediaType != null &&
-                mediaType.equals(jakarta.ws.rs.core.MediaType.APPLICATION_OCTET_STREAM_TYPE)) {
+        if (mediaType != null && mediaType.equals(jakarta.ws.rs.core.MediaType.APPLICATION_OCTET_STREAM_TYPE)) {
             mediaType = null;
         }
 
@@ -322,8 +324,12 @@ public class TikaResource {
         }
 
         for (Map.Entry<String, List<String>> e : httpHeaders.entrySet()) {
-            if (e.getKey().startsWith(META_PREFIX)) {
-                String tikaKey = e.getKey().substring(META_PREFIX.length());
+            if (e
+                    .getKey()
+                    .startsWith(META_PREFIX)) {
+                String tikaKey = e
+                        .getKey()
+                        .substring(META_PREFIX.length());
                 for (String value : e.getValue()) {
                     metadata.add(tikaKey, value);
                 }
@@ -345,8 +351,7 @@ public class TikaResource {
      * @param parseContext parse context
      * @throws IOException wrapper for all exceptions
      */
-    public static void parse(Parser parser, Logger logger, String path, InputStream inputStream,
-                             ContentHandler handler, Metadata metadata, ParseContext parseContext)
+    public static void parse(Parser parser, Logger logger, String path, InputStream inputStream, ContentHandler handler, Metadata metadata, ParseContext parseContext)
             throws IOException {
 
         checkIsOperating();
@@ -384,16 +389,13 @@ public class TikaResource {
         if (tikaTaskTimeout != null) {
             if (tikaTaskTimeout.getTimeoutMillis() > TIKA_SERVER_CONFIG.getTaskTimeoutMillis()) {
                 throw new IllegalArgumentException(
-                        "Can't request a timeout ( " + tikaTaskTimeout.getTimeoutMillis() +
-                                "ms) greater than the taskTimeoutMillis set in the server config (" +
+                        "Can't request a timeout ( " + tikaTaskTimeout.getTimeoutMillis() + "ms) greater than the taskTimeoutMillis set in the server config (" +
                                 TIKA_SERVER_CONFIG.getTaskTimeoutMillis() + "ms)");
             }
             timeoutMillis = tikaTaskTimeout.getTimeoutMillis();
             if (timeoutMillis < TIKA_SERVER_CONFIG.getMinimumTimeoutMillis()) {
                 throw new WebApplicationException(new IllegalArgumentException(
-                        "taskTimeoutMillis must be > " +
-                                "minimumTimeoutMillis, currently set to (" +
-                                TIKA_SERVER_CONFIG.getMinimumTimeoutMillis() + "ms)"),
+                        "taskTimeoutMillis must be > " + "minimumTimeoutMillis, currently set to (" + TIKA_SERVER_CONFIG.getMinimumTimeoutMillis() + "ms)"),
                         Response.Status.BAD_REQUEST);
             }
         }
@@ -412,8 +414,7 @@ public class TikaResource {
         if (metadata.get(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE) == null) {
             logger.info("{} (autodetecting type)", endpoint);
         } else {
-            logger.info("{} ({})", endpoint,
-                    metadata.get(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE));
+            logger.info("{} ({})", endpoint, metadata.get(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE));
         }
     }
 
@@ -425,8 +426,7 @@ public class TikaResource {
             } else if ("false".equalsIgnoreCase(val)) {
                 return false;
             } else {
-                throw new IllegalArgumentException(
-                        "'throwOnWriteLimitReached' must be either 'true' or 'false'");
+                throw new IllegalArgumentException("'throwOnWriteLimitReached' must be either 'true' or 'false'");
             }
         }
         return HandlerConfig.DEFAULT_HANDLER_CONFIG.isThrowOnWriteLimitReached();
@@ -443,10 +443,8 @@ public class TikaResource {
     @Consumes("multipart/form-data")
     @Produces("text/plain")
     @Path("form")
-    public StreamingOutput getTextFromMultipart(Attachment att, @Context HttpHeaders httpHeaders,
-                                                @Context final UriInfo info) {
-        return produceText(att.getObject(InputStream.class), new Metadata(),
-                preparePostHeaderMap(att, httpHeaders), info);
+    public StreamingOutput getTextFromMultipart(Attachment att, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
+        return produceText(att.getObject(InputStream.class), new Metadata(), preparePostHeaderMap(att, httpHeaders), info);
     }
 
     //this is equivalent to text-main in tika-app
@@ -454,8 +452,7 @@ public class TikaResource {
     @Consumes("*/*")
     @Produces("text/plain")
     @Path("main")
-    public StreamingOutput getTextMain(final InputStream is, @Context HttpHeaders httpHeaders,
-                                       @Context final UriInfo info) {
+    public StreamingOutput getTextMain(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
         return produceTextMain(is, httpHeaders.getRequestHeaders(), info);
     }
 
@@ -464,16 +461,11 @@ public class TikaResource {
     @Consumes("multipart/form-data")
     @Produces("text/plain")
     @Path("form/main")
-    public StreamingOutput getTextMainFromMultipart(final Attachment att,
-                                                    @Context HttpHeaders httpHeaders,
-                                                    @Context final UriInfo info) {
-        return produceTextMain(att.getObject(InputStream.class),
-                preparePostHeaderMap(att, httpHeaders), info);
+    public StreamingOutput getTextMainFromMultipart(final Attachment att, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
+        return produceTextMain(att.getObject(InputStream.class), preparePostHeaderMap(att, httpHeaders), info);
     }
 
-    public StreamingOutput produceTextMain(final InputStream is,
-                                           MultivaluedMap<String, String> httpHeaders,
-                                           final UriInfo info) {
+    public StreamingOutput produceTextMain(final InputStream is, MultivaluedMap<String, String> httpHeaders, final UriInfo info) {
         final Parser parser = createParser();
         final Metadata metadata = new Metadata();
         final ParseContext context = new ParseContext();
@@ -495,16 +487,12 @@ public class TikaResource {
     @PUT
     @Consumes("*/*")
     @Produces("text/plain")
-    public StreamingOutput getText(final InputStream is, @Context HttpHeaders httpHeaders,
-                                   @Context final UriInfo info) {
+    public StreamingOutput getText(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
         final Metadata metadata = new Metadata();
-        return produceText(getInputStream(is, metadata, httpHeaders, info), metadata,
-                httpHeaders.getRequestHeaders(), info);
+        return produceText(getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info);
     }
 
-    public StreamingOutput produceText(final InputStream is, final Metadata metadata,
-                                       MultivaluedMap<String, String> httpHeaders,
-                                       final UriInfo info) {
+    public StreamingOutput produceText(final InputStream is, final Metadata metadata, MultivaluedMap<String, String> httpHeaders, final UriInfo info) {
         final Parser parser = createParser();
         final ParseContext context = new ParseContext();
 
@@ -526,55 +514,46 @@ public class TikaResource {
     @Consumes("multipart/form-data")
     @Produces("text/html")
     @Path("form")
-    public StreamingOutput getHTMLFromMultipart(Attachment att, @Context HttpHeaders httpHeaders,
-                                                @Context final UriInfo info) {
-        return produceOutput(att.getObject(InputStream.class), new Metadata(),
-                preparePostHeaderMap(att, httpHeaders), info, "html");
+    public StreamingOutput getHTMLFromMultipart(Attachment att, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
+        return produceOutput(att.getObject(InputStream.class), new Metadata(), preparePostHeaderMap(att, httpHeaders), info, "html");
     }
 
     @PUT
     @Consumes("*/*")
     @Produces("text/html")
-    public StreamingOutput getHTML(final InputStream is, @Context HttpHeaders httpHeaders,
-                                   @Context final UriInfo info) {
+    public StreamingOutput getHTML(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
         Metadata metadata = new Metadata();
-        return produceOutput(getInputStream(is, metadata, httpHeaders, info), metadata,
-                httpHeaders.getRequestHeaders(), info, "html");
+        return produceOutput(getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info, "html");
     }
 
     @POST
     @Consumes("multipart/form-data")
     @Produces("text/xml")
     @Path("form")
-    public StreamingOutput getXMLFromMultipart(Attachment att, @Context HttpHeaders httpHeaders,
-                                               @Context final UriInfo info) {
-        return produceOutput(att.getObject(InputStream.class), new Metadata(),
-                preparePostHeaderMap(att, httpHeaders), info, "xml");
+    public StreamingOutput getXMLFromMultipart(Attachment att, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
+        return produceOutput(att.getObject(InputStream.class), new Metadata(), preparePostHeaderMap(att, httpHeaders), info, "xml");
     }
 
     @PUT
     @Consumes("*/*")
     @Produces("text/xml")
-    public StreamingOutput getXML(final InputStream is, @Context HttpHeaders httpHeaders,
-                                  @Context final UriInfo info) {
+    public StreamingOutput getXML(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
         Metadata metadata = new Metadata();
-        return produceOutput(getInputStream(is, metadata, httpHeaders, info), metadata,
-                httpHeaders.getRequestHeaders(), info, "xml");
+        return produceOutput(getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info, "xml");
     }
 
     @POST
     @Consumes("multipart/form-data")
     @Produces("application/json")
     @Path("form{" + HANDLER_TYPE_PARAM + " : (\\w+)?}")
-    public Metadata getJsonFromMultipart(Attachment att, @Context HttpHeaders httpHeaders,
-                                         @Context final UriInfo info,
-                                         @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName)
+    public Metadata getJsonFromMultipart(Attachment att, @Context HttpHeaders httpHeaders, @Context final UriInfo info, @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName)
             throws IOException, TikaException {
         Metadata metadata = new Metadata();
-        parseToMetadata(
-                getInputStream(att.getObject(InputStream.class), metadata, httpHeaders, info),
-                metadata, preparePostHeaderMap(att, httpHeaders), info, handlerTypeName);
-        TikaResource.getConfig().getMetadataFilter().filter(metadata);
+        parseToMetadata(getInputStream(att.getObject(InputStream.class), metadata, httpHeaders, info), metadata, preparePostHeaderMap(att, httpHeaders), info, handlerTypeName);
+        TikaResource
+                .getConfig()
+                .getMetadataFilter()
+                .filter(metadata);
         return metadata;
     }
 
@@ -582,20 +561,18 @@ public class TikaResource {
     @Consumes("*/*")
     @Produces("application/json")
     @Path("{" + HANDLER_TYPE_PARAM + " : (\\w+)?}")
-    public Metadata getJson(final InputStream is, @Context HttpHeaders httpHeaders,
-                            @Context final UriInfo info,
-                            @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName)
+    public Metadata getJson(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info, @PathParam(HANDLER_TYPE_PARAM) String handlerTypeName)
             throws IOException, TikaException {
         Metadata metadata = new Metadata();
-        parseToMetadata(getInputStream(is, metadata, httpHeaders, info), metadata,
-                httpHeaders.getRequestHeaders(), info, handlerTypeName);
-        TikaResource.getConfig().getMetadataFilter().filter(metadata);
+        parseToMetadata(getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info, handlerTypeName);
+        TikaResource
+                .getConfig()
+                .getMetadataFilter()
+                .filter(metadata);
         return metadata;
     }
 
-    private void parseToMetadata(InputStream inputStream, Metadata metadata,
-                                 MultivaluedMap<String, String> httpHeaders, UriInfo info,
-                                 String handlerTypeName) throws IOException {
+    private void parseToMetadata(InputStream inputStream, Metadata metadata, MultivaluedMap<String, String> httpHeaders, UriInfo info, String handlerTypeName) throws IOException {
         final Parser parser = createParser();
         final ParseContext context = new ParseContext();
 
@@ -609,10 +586,8 @@ public class TikaResource {
             writeLimit = Integer.parseInt(httpHeaders.getFirst("writeLimit"));
         }
 
-        BasicContentHandlerFactory.HANDLER_TYPE type =
-                BasicContentHandlerFactory.parseHandlerType(handlerTypeName, DEFAULT_HANDLER_TYPE);
-        BasicContentHandlerFactory fact =
-                new BasicContentHandlerFactory(type, writeLimit, throwOnWriteLimitReached, context);
+        BasicContentHandlerFactory.HANDLER_TYPE type = BasicContentHandlerFactory.parseHandlerType(handlerTypeName, DEFAULT_HANDLER_TYPE);
+        BasicContentHandlerFactory fact = new BasicContentHandlerFactory(type, writeLimit, throwOnWriteLimitReached, context);
         ContentHandler contentHandler = fact.getNewContentHandler();
 
         try {
@@ -626,19 +601,16 @@ public class TikaResource {
             }
             if (TIKA_SERVER_CONFIG.isReturnStackTrace()) {
                 if (cause != null) {
-                    metadata.add(TikaCoreProperties.CONTAINER_EXCEPTION,
-                            ExceptionUtils.getStackTrace(cause));
+                    metadata.add(TikaCoreProperties.CONTAINER_EXCEPTION, ExceptionUtils.getStackTrace(cause));
                 } else {
-                    metadata.add(TikaCoreProperties.CONTAINER_EXCEPTION,
-                            ExceptionUtils.getStackTrace(e));
+                    metadata.add(TikaCoreProperties.CONTAINER_EXCEPTION, ExceptionUtils.getStackTrace(e));
                 }
             } else if (!writeLimitReached) {
                 throw e;
             }
         } catch (OutOfMemoryError e) {
             if (TIKA_SERVER_CONFIG.isReturnStackTrace()) {
-                metadata.add(TikaCoreProperties.CONTAINER_EXCEPTION,
-                        ExceptionUtils.getStackTrace(e));
+                metadata.add(TikaCoreProperties.CONTAINER_EXCEPTION, ExceptionUtils.getStackTrace(e));
             } else {
                 throw e;
             }
@@ -647,9 +619,7 @@ public class TikaResource {
         }
     }
 
-    private StreamingOutput produceOutput(final InputStream is, Metadata metadata,
-                                          final MultivaluedMap<String, String> httpHeaders,
-                                          final UriInfo info, final String format) {
+    private StreamingOutput produceOutput(final InputStream is, Metadata metadata, final MultivaluedMap<String, String> httpHeaders, final UriInfo info, final String format) {
         final Parser parser = createParser();
         final ParseContext context = new ParseContext();
 
@@ -664,13 +634,20 @@ public class TikaResource {
             ContentHandler content;
 
             try {
-                SAXTransformerFactory factory =
-                        (SAXTransformerFactory) SAXTransformerFactory.newInstance();
+                SAXTransformerFactory factory = (SAXTransformerFactory) SAXTransformerFactory.newInstance();
                 TransformerHandler handler = factory.newTransformerHandler();
-                handler.getTransformer().setOutputProperty(OutputKeys.METHOD, format);
-                handler.getTransformer().setOutputProperty(OutputKeys.INDENT, "yes");
-                handler.getTransformer().setOutputProperty(OutputKeys.ENCODING, UTF_8.name());
-                handler.getTransformer().setOutputProperty(OutputKeys.VERSION, "1.1");
+                handler
+                        .getTransformer()
+                        .setOutputProperty(OutputKeys.METHOD, format);
+                handler
+                        .getTransformer()
+                        .setOutputProperty(OutputKeys.INDENT, "yes");
+                handler
+                        .getTransformer()
+                        .setOutputProperty(OutputKeys.ENCODING, UTF_8.name());
+                handler
+                        .getTransformer()
+                        .setOutputProperty(OutputKeys.VERSION, "1.1");
                 handler.setResult(new StreamResult(writer));
                 content = new ExpandedTitleContentHandler(handler);
             } catch (TransformerConfigurationException e) {
@@ -689,8 +666,7 @@ public class TikaResource {
      * @param httpHeaders the http headers, fetched from context.
      * @return the case insensitive MetadataMap containing combined headers.
      */
-    private MetadataMap<String, String> preparePostHeaderMap(Attachment att,
-                                                             HttpHeaders httpHeaders) {
+    private MetadataMap<String, String> preparePostHeaderMap(Attachment att, HttpHeaders httpHeaders) {
         if (att == null && httpHeaders == null) {
             return null;
         }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaWelcome.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaWelcome.java
index 2105492de..b8ff1d4e7 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaWelcome.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TikaWelcome.java
@@ -78,9 +78,13 @@ public class TikaWelcome {
             Path p = endpoint.getAnnotation(Path.class);
             String basePath = null;
             if (p != null) {
-                basePath =
-                        p.value().endsWith("/") ? p.value().substring(0, p.value().length() - 2) :
-                                p.value();
+                basePath = p
+                        .value()
+                        .endsWith("/") ? p
+                        .value()
+                        .substring(0, p
+                                .value()
+                                .length() - 2) : p.value();
             }
 
             for (Method m : endpoint.getMethods()) {
@@ -121,7 +125,9 @@ public class TikaWelcome {
                 }
             }
         }
-        found.sort(Comparator.comparing((Endpoint e) -> e.path).thenComparing(e -> e.methodName));
+        found.sort(Comparator
+                .comparing((Endpoint e) -> e.path)
+                .thenComparing(e -> e.methodName));
         return found;
     }
 
@@ -149,7 +155,11 @@ public class TikaWelcome {
         if (m.find()) {
             String versionNumber = m.group();
             String miredot = "https://tika.apache.org/" + versionNumber + "/miredot/index.html";
-            h.append(" and <a href=\"").append(miredot).append("\">").append(miredot)
+            h
+                    .append(" and <a href=\"")
+                    .append(miredot)
+                    .append("\">")
+                    .append(miredot)
                     .append("</a>");
         }
         h.append("</p>\n");
@@ -213,8 +223,7 @@ public class TikaWelcome {
         public final String httpMethod;
         public final List<String> produces;
 
-        protected Endpoint(Class<?> endpoint, Method method, String path, String httpMethod,
-                           String[] produces) {
+        protected Endpoint(Class<?> endpoint, Method method, String path, String httpMethod, String[] produces) {
             this.className = endpoint.getCanonicalName();
             this.methodName = method.getName();
             this.path = path;
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TranslateResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TranslateResource.java
index ff6e07d0e..d19ead2f7 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TranslateResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/TranslateResource.java
@@ -50,9 +50,10 @@ public class TranslateResource {
     private ServiceLoader loader;
 
     public TranslateResource(ServerStatus serverStatus, long timeoutMillis) {
-        this.loader =
-                new ServiceLoader(ServiceLoader.class.getClassLoader(), LoadErrorHandler.WARN);
-        this.defaultTranslator = TikaResource.getConfig().getTranslator();
+        this.loader = new ServiceLoader(ServiceLoader.class.getClassLoader(), LoadErrorHandler.WARN);
+        this.defaultTranslator = TikaResource
+                .getConfig()
+                .getTranslator();
         this.serverStatus = serverStatus;
         this.timeoutMillis = timeoutMillis;
     }
@@ -62,8 +63,7 @@ public class TranslateResource {
     @Path("/all/{translator}/{src}/{dest}")
     @Consumes("*/*")
     @Produces("text/plain")
-    public String translate(final InputStream is, @PathParam("translator") String translator,
-                            @PathParam("src") String sLang, @PathParam("dest") String dLang)
+    public String translate(final InputStream is, @PathParam("translator") String translator, @PathParam("src") String sLang, @PathParam("dest") String dLang)
             throws TikaException, IOException {
         return doTranslate(IOUtils.toString(is, UTF_8), translator, sLang, dLang);
 
@@ -74,10 +74,11 @@ public class TranslateResource {
     @Path("/all/{translator}/{dest}")
     @Consumes("*/*")
     @Produces("text/plain")
-    public String autoTranslate(final InputStream is, @PathParam("translator") String translator,
-                                @PathParam("dest") String dLang) throws TikaException, IOException {
+    public String autoTranslate(final InputStream is, @PathParam("translator") String translator, @PathParam("dest") String dLang) throws TikaException, IOException {
         final String content = IOUtils.toString(is, UTF_8);
-        LanguageResult language = new OptimaizeLangDetector().loadModels().detect(content);
+        LanguageResult language = new OptimaizeLangDetector()
+                .loadModels()
+                .detect(content);
         if (language.isUnknown()) {
             throw new TikaException("Unable to detect language to use for translation of text");
         }
@@ -88,8 +89,7 @@ public class TranslateResource {
         return doTranslate(content, translator, sLang, dLang);
     }
 
-    private String doTranslate(String content, String translator, String sLang, String dLang)
-            throws TikaException, IOException {
+    private String doTranslate(String content, String translator, String sLang, String dLang) throws TikaException, IOException {
         LOG.info("Using translator: [{}]: src: [{}]: dest: [{}]", translator, sLang, dLang);
         Translator translate = byClassName(translator);
         if (translate == null) {
@@ -111,7 +111,10 @@ public class TranslateResource {
     private Translator byClassName(String className) {
         List<Translator> translators = loader.loadStaticServiceProviders(Translator.class);
         for (Translator t : translators) {
-            if (t.getClass().getName().equals(className)) {
+            if (t
+                    .getClass()
+                    .getName()
+                    .equals(className)) {
                 return t;
             }
         }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/UnpackerResource.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/UnpackerResource.java
index 18316e9f1..79d41dec9 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/UnpackerResource.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/resource/UnpackerResource.java
@@ -78,10 +78,8 @@ public class UnpackerResource {
 
     private static final Logger LOG = LoggerFactory.getLogger(UnpackerResource.class);
 
-    public static void metadataToCsv(Metadata metadata, OutputStream outputStream)
-            throws IOException {
-        CSVPrinter writer =
-                new CSVPrinter(new OutputStreamWriter(outputStream, UTF_8), CSVFormat.EXCEL);
+    public static void metadataToCsv(Metadata metadata, OutputStream outputStream) throws IOException {
+        CSVPrinter writer = new CSVPrinter(new OutputStreamWriter(outputStream, UTF_8), CSVFormat.EXCEL);
 
         for (String name : metadata.names()) {
             String[] values = metadata.getValues(name);
@@ -97,33 +95,28 @@ public class UnpackerResource {
     @Path("/{id:(/.*)?}")
     @PUT
     @Produces({"application/zip", "application/x-tar"})
-    public Map<String, byte[]> unpack(InputStream is, @Context HttpHeaders httpHeaders,
-                                      @Context UriInfo info) throws Exception {
-        return process(TikaResource.getInputStream(is, new Metadata(), httpHeaders, info),
-                httpHeaders, info, false);
+    public Map<String, byte[]> unpack(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
+        return process(TikaResource.getInputStream(is, new Metadata(), httpHeaders, info), httpHeaders, info, false);
     }
 
     @Path("/all{id:(/.*)?}")
     @PUT
     @Produces({"application/zip", "application/x-tar"})
-    public Map<String, byte[]> unpackAll(InputStream is, @Context HttpHeaders httpHeaders,
-                                         @Context UriInfo info) throws Exception {
-        return process(TikaResource.getInputStream(is, new Metadata(), httpHeaders, info),
-                httpHeaders, info, true);
+    public Map<String, byte[]> unpackAll(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
+        return process(TikaResource.getInputStream(is, new Metadata(), httpHeaders, info), httpHeaders, info, true);
     }
 
-    private Map<String, byte[]> process(InputStream is, @Context HttpHeaders httpHeaders,
-                                        @Context UriInfo info, boolean saveAll) throws Exception {
+    private Map<String, byte[]> process(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info, boolean saveAll) throws Exception {
         Metadata metadata = new Metadata();
         ParseContext pc = new ParseContext();
         long unpackMaxBytes = DEFAULT_MAX_ATTACHMENT_BYTES;
-        String unpackMaxBytesString =
-                httpHeaders.getRequestHeaders().getFirst(UNPACK_MAX_BYTES_KEY);
+        String unpackMaxBytesString = httpHeaders
+                .getRequestHeaders()
+                .getFirst(UNPACK_MAX_BYTES_KEY);
         if (!StringUtils.isBlank(unpackMaxBytesString)) {
             unpackMaxBytes = Long.parseLong(unpackMaxBytesString);
             if (unpackMaxBytes > Integer.MAX_VALUE) {
-                throw new IllegalArgumentException(
-                        "Can't request value > than Integer" + ".MAX_VALUE : " + unpackMaxBytes);
+                throw new IllegalArgumentException("Can't request value > than Integer" + ".MAX_VALUE : " + unpackMaxBytes);
             } else if (unpackMaxBytes < 0) {
                 throw new IllegalArgumentException("Can't request value < 0: " + unpackMaxBytes);
             }
@@ -141,11 +134,12 @@ public class UnpackerResource {
         //we need to add this to allow for "inline" use of other parsers.
         pc.set(Parser.class, parser);
         ContentHandler ch;
-        UnsynchronizedByteArrayOutputStream text = UnsynchronizedByteArrayOutputStream.builder().get();
+        UnsynchronizedByteArrayOutputStream text = UnsynchronizedByteArrayOutputStream
+                .builder()
+                .get();
 
         if (saveAll) {
-            ch = new BodyContentHandler(
-                    new RichTextContentHandler(new OutputStreamWriter(text, UTF_8)));
+            ch = new BodyContentHandler(new RichTextContentHandler(new OutputStreamWriter(text, UTF_8)));
         } else {
             ch = new DefaultHandler();
         }
@@ -153,8 +147,7 @@ public class UnpackerResource {
         Map<String, byte[]> files = new HashMap<>();
         MutableInt count = new MutableInt();
 
-        pc.set(EmbeddedDocumentExtractor.class,
-                new MyEmbeddedDocumentExtractor(count, files, unpackMaxBytes));
+        pc.set(EmbeddedDocumentExtractor.class, new MyEmbeddedDocumentExtractor(count, files, unpackMaxBytes));
 
         TikaResource.parse(parser, LOG, info.getPath(), is, ch, metadata, pc);
 
@@ -165,8 +158,9 @@ public class UnpackerResource {
         if (saveAll) {
             files.put(TEXT_FILENAME, text.toByteArray());
 
-            UnsynchronizedByteArrayOutputStream metaStream =
-                    UnsynchronizedByteArrayOutputStream.builder().get();
+            UnsynchronizedByteArrayOutputStream metaStream = UnsynchronizedByteArrayOutputStream
+                    .builder()
+                    .get();
             metadataToCsv(metadata, metaStream);
 
             files.put(META_FILENAME, metaStream.toByteArray());
@@ -180,11 +174,9 @@ public class UnpackerResource {
         private final Map<String, byte[]> zout;
 
         private final long unpackMaxBytes;
-        private final EmbeddedStreamTranslator embeddedStreamTranslator =
-                new DefaultEmbeddedStreamTranslator();
+        private final EmbeddedStreamTranslator embeddedStreamTranslator = new DefaultEmbeddedStreamTranslator();
 
-        MyEmbeddedDocumentExtractor(MutableInt count, Map<String, byte[]> zout,
-                                    long unpackMaxBytes) {
+        MyEmbeddedDocumentExtractor(MutableInt count, Map<String, byte[]> zout, long unpackMaxBytes) {
             this.count = count;
             this.zout = zout;
             this.unpackMaxBytes = unpackMaxBytes;
@@ -194,18 +186,17 @@ public class UnpackerResource {
             return true;
         }
 
-        public void parseEmbedded(InputStream inputStream, ContentHandler contentHandler,
-                                  Metadata metadata, boolean b) throws SAXException, IOException {
-            UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream.builder().get();
+        public void parseEmbedded(InputStream inputStream, ContentHandler contentHandler, Metadata metadata, boolean b) throws SAXException, IOException {
+            UnsynchronizedByteArrayOutputStream bos = UnsynchronizedByteArrayOutputStream
+                    .builder()
+                    .get();
 
             BoundedInputStream bis = new BoundedInputStream(unpackMaxBytes, inputStream);
             IOUtils.copy(bis, bos);
             if (bis.hasHitBound()) {
-                throw new IOException(new TikaMemoryLimitException("An attachment is longer than " +
-                        "'unpackMaxBytes' (default=100MB, actual=" + unpackMaxBytes + "). " +
-                        "If you need to increase this " +
-                        "limit, add a header to your request, such as: unpackMaxBytes: " +
-                        "1073741824.  There is a hard limit of 2GB."));
+                throw new IOException(new TikaMemoryLimitException(
+                        "An attachment is longer than " + "'unpackMaxBytes' (default=100MB, actual=" + unpackMaxBytes + "). " + "If you need to increase this " +
+                                "limit, add a header to your request, such as: unpackMaxBytes: " + "1073741824.  There is a hard limit of 2GB."));
             }
             byte[] data = bos.toByteArray();
 
@@ -218,7 +209,10 @@ public class UnpackerResource {
 
             if (!name.contains(".") && contentType != null) {
                 try {
-                    String ext = TikaResource.getConfig().getMimeRepository().forName(contentType)
+                    String ext = TikaResource
+                            .getConfig()
+                            .getMimeRepository()
+                            .forName(contentType)
                             .getExtension();
 
                     if (ext != null) {
@@ -230,10 +224,10 @@ public class UnpackerResource {
             }
             try (InputStream is = new UnsynchronizedByteArrayInputStream(data)) {
                 if (embeddedStreamTranslator.shouldTranslate(is, metadata)) {
-                    InputStream translated = embeddedStreamTranslator.translate(
-                            new UnsynchronizedByteArrayInputStream(data), metadata);
-                    UnsynchronizedByteArrayOutputStream bos2 =
-                            UnsynchronizedByteArrayOutputStream.builder().get();
+                    InputStream translated = embeddedStreamTranslator.translate(new UnsynchronizedByteArrayInputStream(data), metadata);
+                    UnsynchronizedByteArrayOutputStream bos2 = UnsynchronizedByteArrayOutputStream
+                            .builder()
+                            .get();
                     IOUtils.copy(translated, bos2);
                     data = bos2.toByteArray();
                 }
@@ -264,7 +258,9 @@ public class UnpackerResource {
                 normalizedName = normalizedName.substring(prefixLength);
             }
             if (zout.containsKey(normalizedName)) {
-                return UUID.randomUUID().toString() + "-" + normalizedName;
+                return UUID
+                        .randomUUID()
+                        .toString() + "-" + normalizedName;
             }
             return normalizedName;
         }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/CSVMessageBodyWriter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/CSVMessageBodyWriter.java
index 384250832..ba9ce6727 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/CSVMessageBodyWriter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/CSVMessageBodyWriter.java
@@ -42,25 +42,20 @@ import org.apache.tika.metadata.Metadata;
 @Produces("text/csv")
 public class CSVMessageBodyWriter implements MessageBodyWriter<Metadata> {
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return Metadata.class.isAssignableFrom(type);
     }
 
-    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations,
-                        MediaType mediaType) {
+    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
     @Override
     @SuppressWarnings("resource")
-    public void writeTo(Metadata metadata, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
 
-        CSVPrinter writer =
-                new CSVPrinter(new OutputStreamWriter(entityStream, UTF_8), CSVFormat.EXCEL);
+        CSVPrinter writer = new CSVPrinter(new OutputStreamWriter(entityStream, UTF_8), CSVFormat.EXCEL);
 
         for (String name : metadata.names()) {
             String[] values = metadata.getValues(name);
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONMessageBodyWriter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONMessageBodyWriter.java
index 66676bdb5..942cea29b 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONMessageBodyWriter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONMessageBodyWriter.java
@@ -34,27 +34,23 @@ import jakarta.ws.rs.ext.MessageBodyWriter;
 import jakarta.ws.rs.ext.Provider;
 
 import org.apache.tika.metadata.Metadata;
-import org.apache.tika.metadata.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadata;
 
 @Provider
 @Produces(MediaType.APPLICATION_JSON)
 public class JSONMessageBodyWriter implements MessageBodyWriter<Metadata> {
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return Metadata.class.isAssignableFrom(type);
     }
 
-    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations,
-                        MediaType mediaType) {
+    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
     @Override
-    public void writeTo(Metadata metadata, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
         try (Writer writer = new OutputStreamWriter(entityStream, UTF_8)) {
             JsonMetadata.toJson(metadata, writer);
         }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONObjWriter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONObjWriter.java
index 0243f2884..8ed053678 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONObjWriter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/JSONObjWriter.java
@@ -42,24 +42,22 @@ import org.apache.tika.metadata.Metadata;
 public class JSONObjWriter implements MessageBodyWriter<Map<String, Object>> {
 
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return Map.class.isAssignableFrom(type);
     }
 
-    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations,
-                        MediaType mediaType) {
+    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
     @Override
-    public void writeTo(Map<String, Object> map, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(Map<String, Object> map, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
         try (Writer writer = new OutputStreamWriter(entityStream, UTF_8)) {
             ObjectMapper objectMapper = new ObjectMapper();
-            objectMapper.writerWithDefaultPrettyPrinter().writeValue(writer, map);
+            objectMapper
+                    .writerWithDefaultPrettyPrinter()
+                    .writeValue(writer, map);
         }
     }
 }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/MetadataListMessageBodyWriter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/MetadataListMessageBodyWriter.java
index 4e6814847..1e0a139db 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/MetadataListMessageBodyWriter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/MetadataListMessageBodyWriter.java
@@ -33,31 +33,27 @@ import jakarta.ws.rs.core.MultivaluedMap;
 import jakarta.ws.rs.ext.MessageBodyWriter;
 import jakarta.ws.rs.ext.Provider;
 
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.MetadataList;
 
 @Provider
 @Produces(MediaType.APPLICATION_JSON)
 public class MetadataListMessageBodyWriter implements MessageBodyWriter<MetadataList> {
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         if (!MediaType.APPLICATION_JSON_TYPE.equals(mediaType)) {
             return false;
         }
         return type.isAssignableFrom(MetadataList.class);
     }
 
-    public long getSize(MetadataList data, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType) {
+    public long getSize(MetadataList data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
     @Override
-    public void writeTo(MetadataList list, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(MetadataList list, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
         try (Writer writer = new OutputStreamWriter(entityStream, UTF_8)) {
             JsonMetadataList.toJson(list.getMetadata(), writer);
         }
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TarWriter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TarWriter.java
index a5a3d9388..64f2d20ba 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TarWriter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TarWriter.java
@@ -35,8 +35,7 @@ import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;
 @Provider
 @Produces("application/x-tar")
 public class TarWriter implements MessageBodyWriter<Map<String, byte[]>> {
-    private static void tarStoreBuffer(TarArchiveOutputStream zip, String name, byte[] dataBuffer)
-            throws IOException {
+    private static void tarStoreBuffer(TarArchiveOutputStream zip, String name, byte[] dataBuffer) throws IOException {
         TarArchiveEntry entry = new TarArchiveEntry(name);
 
         entry.setSize(dataBuffer.length);
@@ -48,20 +47,16 @@ public class TarWriter implements MessageBodyWriter<Map<String, byte[]>> {
         zip.closeArchiveEntry();
     }
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return Map.class.isAssignableFrom(type);
     }
 
-    public long getSize(Map<String, byte[]> stringMap, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType) {
+    public long getSize(Map<String, byte[]> stringMap, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
-    public void writeTo(Map<String, byte[]> parts, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(Map<String, byte[]> parts, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
         TarArchiveOutputStream zip = new TarArchiveOutputStream(entityStream);
 
         for (Map.Entry<String, byte[]> entry : parts.entrySet()) {
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TextMessageBodyWriter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TextMessageBodyWriter.java
index 46b4bc98c..037f17679 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TextMessageBodyWriter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/TextMessageBodyWriter.java
@@ -46,22 +46,18 @@ import org.apache.tika.metadata.Metadata;
 @Produces(MediaType.TEXT_PLAIN)
 public class TextMessageBodyWriter implements MessageBodyWriter<Metadata> {
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return mediaType.equals(MediaType.TEXT_PLAIN_TYPE) && Metadata.class.isAssignableFrom(type);
     }
 
-    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations,
-                        MediaType mediaType) {
+    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
     @Override
     @SuppressWarnings("resource")
-    public void writeTo(Metadata metadata, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
 
         if (metadata.names().length != 1) {
             throw new WebApplicationException("Metadata object must only have one entry!");
diff --git a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/ZipWriter.java b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/ZipWriter.java
index 923f15059..98e962e1c 100644
--- a/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/ZipWriter.java
+++ b/tika-server/tika-server-core/src/main/java/org/apache/tika/server/core/writer/ZipWriter.java
@@ -40,9 +40,10 @@ import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;
 @Provider
 @Produces("application/zip")
 public class ZipWriter implements MessageBodyWriter<Map<String, byte[]>> {
-    private static void zipStoreBuffer(ZipArchiveOutputStream zip, String name, byte[] dataBuffer)
-            throws IOException {
-        ZipEntry zipEntry = new ZipEntry(name != null ? name : UUID.randomUUID().toString());
+    private static void zipStoreBuffer(ZipArchiveOutputStream zip, String name, byte[] dataBuffer) throws IOException {
+        ZipEntry zipEntry = new ZipEntry(name != null ? name : UUID
+                .randomUUID()
+                .toString());
         zipEntry.setMethod(ZipOutputStream.STORED);
 
         zipEntry.setSize(dataBuffer.length);
@@ -64,20 +65,16 @@ public class ZipWriter implements MessageBodyWriter<Map<String, byte[]>> {
         zip.closeArchiveEntry();
     }
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return Map.class.isAssignableFrom(type);
     }
 
-    public long getSize(Map<String, byte[]> stringMap, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType) {
+    public long getSize(Map<String, byte[]> stringMap, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
-    public void writeTo(Map<String, byte[]> parts, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(Map<String, byte[]> parts, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
         ZipArchiveOutputStream zip = new ZipArchiveOutputStream(entityStream);
 
         zip.setMethod(ZipArchiveOutputStream.STORED);
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/CXFTestBase.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/CXFTestBase.java
index 387b085a0..808ebb7d2 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/CXFTestBase.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/CXFTestBase.java
@@ -67,8 +67,7 @@ public abstract class CXFTestBase {
     }
 
     public static void assertNotFound(String needle, String haystack) {
-        assertFalse(haystack.contains(needle),
-                needle + " unexpectedly found in:\n" + haystack);
+        assertFalse(haystack.contains(needle), needle + " unexpectedly found in:\n" + haystack);
     }
 
     protected static InputStream copy(InputStream in, int remaining) throws IOException {
@@ -98,15 +97,32 @@ public abstract class CXFTestBase {
         return new ByteArrayInputStream(bos.toByteArray());
     }
 
+    protected static AverageColor getAverageColor(BufferedImage image, int minX, int maxX, int minY, int maxY) {
+        long totalRed = 0;
+        long totalGreen = 0;
+        long totalBlue = 0;
+        int pixels = 0;
+        for (int x = minX; x < maxX; x++) {
+            for (int y = minY; y < maxY; y++) {
+                int clr = image.getRGB(x, y);
+                int red = (clr & 0x00ff0000) >> 16;
+                int green = (clr & 0x0000ff00) >> 8;
+                int blue = clr & 0x000000ff;
+                totalRed += red;
+                totalGreen += green;
+                totalBlue += blue;
+                pixels++;
+            }
+        }
+        return new AverageColor((double) totalRed / (double) pixels, (double) totalGreen / (double) pixels, (double) totalBlue / (double) pixels);
+    }
+
     @BeforeEach
     public void setUp() throws Exception {
 
         this.tika = new TikaConfig(getTikaConfigInputStream());
         TikaServerConfig tikaServerConfig = getTikaServerConfig();
-        TikaResource.init(tika, tikaServerConfig,
-                new CommonsDigester(DIGESTER_READ_LIMIT, "md5," +
-                        "sha1:32"),
-                getInputStreamFactory(getTikaConfigInputStream()),
+        TikaResource.init(tika, tikaServerConfig, new CommonsDigester(DIGESTER_READ_LIMIT, "md5," + "sha1:32"), getInputStreamFactory(getTikaConfigInputStream()),
                 new ServerStatus("", 0, true));
         JAXRSServerFactoryBean sf = new JAXRSServerFactoryBean();
         //set compression interceptors
@@ -118,7 +134,9 @@ public abstract class CXFTestBase {
         sf.setAddress(endPoint + "/");
         sf.setResourceComparator(new ProduceTypeResourceComparator());
 
-        BindingFactoryManager manager = sf.getBus().getExtension(BindingFactoryManager.class);
+        BindingFactoryManager manager = sf
+                .getBus()
+                .getExtension(BindingFactoryManager.class);
 
         JAXRSBindingFactory factory = new JAXRSBindingFactory();
         factory.setBus(sf.getBus());
@@ -139,9 +157,7 @@ public abstract class CXFTestBase {
 
     protected InputStream getTikaConfigInputStream() throws IOException {
         return new ByteArrayInputStream(new String(
-                "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n" + "<properties>\n" +
-                        "    <parsers>\n" +
-                        "        <parser class=\"org.apache.tika.parser.DefaultParser\"/>\n" +
+                "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n" + "<properties>\n" + "    <parsers>\n" + "        <parser class=\"org.apache.tika.parser.DefaultParser\"/>\n" +
                         "    </parsers>\n" + "</properties>").getBytes(UTF_8));
     }
 
@@ -178,7 +194,7 @@ public abstract class CXFTestBase {
             }
             zip.close();
         } finally {
-            if (tempFile != null ) {
+            if (tempFile != null) {
                 Files.delete(tempFile);
             }
         }
@@ -200,7 +216,7 @@ public abstract class CXFTestBase {
             }
             zip.close();
         } finally {
-            if (tempFile != null ) {
+            if (tempFile != null) {
                 Files.delete(tempFile);
             }
         }
@@ -236,8 +252,7 @@ public abstract class CXFTestBase {
         return metadata + "\n\n" + txt;
     }
 
-    protected Map<String, String> readArchiveFromStream(ArchiveInputStream zip)
-            throws IOException {
+    protected Map<String, String> readArchiveFromStream(ArchiveInputStream zip) throws IOException {
         Map<String, String> data = new HashMap<>();
         while (true) {
             ArchiveEntry entry = zip.getNextEntry();
@@ -253,36 +268,12 @@ public abstract class CXFTestBase {
         return data;
     }
 
-    private Path writeTemporaryArchiveFile(InputStream inputStream, String archiveType)
-            throws IOException {
-        Path tmp = Files.createTempFile("apache-tika-server-test-tmp-",
-                "." + archiveType);
+    private Path writeTemporaryArchiveFile(InputStream inputStream, String archiveType) throws IOException {
+        Path tmp = Files.createTempFile("apache-tika-server-test-tmp-", "." + archiveType);
         Files.copy(inputStream, tmp, StandardCopyOption.REPLACE_EXISTING);
         return tmp;
     }
 
-    protected static AverageColor getAverageColor(BufferedImage image, int minX, int maxX, int minY,
-                                                  int maxY) {
-        long totalRed = 0;
-        long totalGreen = 0;
-        long totalBlue = 0;
-        int pixels = 0;
-        for (int x = minX; x < maxX; x++) {
-            for (int y = minY; y < maxY; y++) {
-                int clr = image.getRGB(x, y);
-                int red = (clr & 0x00ff0000) >> 16;
-                int green = (clr & 0x0000ff00) >> 8;
-                int blue = clr & 0x000000ff;
-                totalRed += red;
-                totalGreen += green;
-                totalBlue += blue;
-                pixels++;
-            }
-        }
-        return new AverageColor((double) totalRed / (double) pixels,
-                (double) totalGreen / (double) pixels, (double) totalBlue / (double) pixels);
-    }
-
     public static class AverageColor {
         double red;
         double green;
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/IntegrationTestBase.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/IntegrationTestBase.java
index 858f15243..b5b066207 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/IntegrationTestBase.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/IntegrationTestBase.java
@@ -69,9 +69,7 @@ public class IntegrationTestBase extends TikaTest {
         LogUtils.setLoggerClass(NullWebClientLogger.class);
 
         LOG_FILE = Files.createTempFile(TEMP_WORKING_DIR, "tika-server-integration", ".xml");
-        Files.copy(
-                TikaServerIntegrationTest.class.getResourceAsStream("/logging/log4j2_forked.xml"),
-                LOG_FILE, StandardCopyOption.REPLACE_EXISTING);
+        Files.copy(TikaServerIntegrationTest.class.getResourceAsStream("/logging/log4j2_forked.xml"), LOG_FILE, StandardCopyOption.REPLACE_EXISTING);
         STREAMS_DIR = Files.createTempDirectory(TEMP_WORKING_DIR, "tika-server-integration");
     }
 
@@ -87,8 +85,7 @@ public class IntegrationTestBase extends TikaTest {
     }
 
     public void startProcess(String[] extraArgs) throws IOException {
-        String[] base = new String[]{"java", "-cp", System.getProperty("java.class.path"),
-                "org.apache.tika.server.core.TikaServerCli",};
+        String[] base = new String[]{"java", "-cp", System.getProperty("java.class.path"), "org.apache.tika.server.core.TikaServerCli",};
         List<String> args = new ArrayList<>(Arrays.asList(base));
         args.addAll(Arrays.asList(extraArgs));
         ProcessBuilder pb = new ProcessBuilder(args);
@@ -100,32 +97,38 @@ public class IntegrationTestBase extends TikaTest {
     }
 
     void awaitServerStartup() throws Exception {
-        WebClient client = WebClient.create(endPoint + "/").accept("text/html");
+        WebClient client = WebClient
+                .create(endPoint + "/")
+                .accept("text/html");
         awaitServerStartup(client);
 
     }
 
     void awaitServerStartup(WebClient client) throws Exception {
         Instant started = Instant.now();
-        long elapsed = Duration.between(started, Instant.now()).toMillis();
+        long elapsed = Duration
+                .between(started, Instant.now())
+                .toMillis();
         while (elapsed < MAX_WAIT_MS) {
             try {
                 Response response = client.get();
                 if (response.getStatus() == 200) {
-                    elapsed = Duration.between(started, Instant.now()).toMillis();
-                    LOG.info(
-                            "client observes server successfully started after " + elapsed + " ms");
+                    elapsed = Duration
+                            .between(started, Instant.now())
+                            .toMillis();
+                    LOG.info("client observes server successfully started after " + elapsed + " ms");
                     return;
                 }
-                LOG.debug("tika test client failed to connect to server with status: {}",
-                        response.getStatus());
+                LOG.debug("tika test client failed to connect to server with status: {}", response.getStatus());
 
             } catch (jakarta.ws.rs.ProcessingException e) {
                 LOG.debug("tika test client failed to connect to server", e);
             }
 
             Thread.sleep(1000);
-            elapsed = Duration.between(started, Instant.now()).toMillis();
+            elapsed = Duration
+                    .between(started, Instant.now())
+                    .toMillis();
         }
         throw new TimeoutException("couldn't connect to server after " + elapsed + " ms");
     }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/LanguageResourceTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/LanguageResourceTest.java
index 0ed0df06b..48dff3d7c 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/LanguageResourceTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/LanguageResourceTest.java
@@ -45,8 +45,7 @@ public class LanguageResourceTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(LanguageResource.class);
-        sf.setResourceProvider(LanguageResource.class,
-                new SingletonResourceProvider(new LanguageResource()));
+        sf.setResourceProvider(LanguageResource.class, new SingletonResourceProvider(new LanguageResource()));
 
     }
 
@@ -63,8 +62,11 @@ public class LanguageResourceTest extends CXFTestBase {
     @Test
     public void testDetectEnglishString() throws Exception {
         String url = endPoint + LANG_STRING_PATH;
-        Response response =
-                WebClient.create(url).type("text/plain").accept("text/plain").put(ENGLISH_STRING);
+        Response response = WebClient
+                .create(url)
+                .type("text/plain")
+                .accept("text/plain")
+                .put(ENGLISH_STRING);
         assertNotNull(response);
         String readLang = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals("en", readLang);
@@ -73,8 +75,11 @@ public class LanguageResourceTest extends CXFTestBase {
     @Test
     public void testDetectFrenchString() throws Exception {
         String url = endPoint + LANG_STRING_PATH;
-        Response response =
-                WebClient.create(url).type("text/plain").accept("text/plain").put(FRENCH_STRING);
+        Response response = WebClient
+                .create(url)
+                .type("text/plain")
+                .accept("text/plain")
+                .put(FRENCH_STRING);
         assertNotNull(response);
         String readLang = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals("fr", readLang);
@@ -83,7 +88,10 @@ public class LanguageResourceTest extends CXFTestBase {
     @Test
     public void testDetectEnglishFile() throws Exception {
         String url = endPoint + LANG_STREAM_PATH;
-        Response response = WebClient.create(url).type("text/plain").accept("text/plain")
+        Response response = WebClient
+                .create(url)
+                .type("text/plain")
+                .accept("text/plain")
                 .put(getClass().getResourceAsStream("/test-documents/english.txt"));
         assertNotNull(response);
         String readLang = getStringFromInputStream((InputStream) response.getEntity());
@@ -93,7 +101,10 @@ public class LanguageResourceTest extends CXFTestBase {
     @Test
     public void testDetectFrenchFile() throws Exception {
         String url = endPoint + LANG_STREAM_PATH;
-        Response response = WebClient.create(url).type("text/plain").accept("text/plain")
+        Response response = WebClient
+                .create(url)
+                .type("text/plain")
+                .accept("text/plain")
                 .put(getClass().getResourceAsStream("/test-documents/french.txt"));
         assertNotNull(response);
         String readLang = getStringFromInputStream((InputStream) response.getEntity());
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/RecursiveMetadataResourceTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/RecursiveMetadataResourceTest.java
index 7b5d15d9f..b4c54933a 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/RecursiveMetadataResourceTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/RecursiveMetadataResourceTest.java
@@ -34,21 +34,19 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.resource.RecursiveMetadataResource;
 import org.apache.tika.server.core.writer.MetadataListMessageBodyWriter;
 
 public class RecursiveMetadataResourceTest extends CXFTestBase {
 
-    private static final String META_PATH = "/rmeta";
-
     public static final String TEST_NULL_POINTER = "test-documents/mock/null_pointer.xml";
+    private static final String META_PATH = "/rmeta";
 
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(RecursiveMetadataResource.class);
-        sf.setResourceProvider(RecursiveMetadataResource.class,
-                new SingletonResourceProvider(new RecursiveMetadataResource()));
+        sf.setResourceProvider(RecursiveMetadataResource.class, new SingletonResourceProvider(new RecursiveMetadataResource()));
     }
 
     @Override
@@ -60,7 +58,9 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
 
     @Test
     public void testNPE() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_NULL_POINTER));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
@@ -69,8 +69,7 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         assertEquals("Nikolai Lobachevsky", metadata.get("author"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
         assertContains("some content", metadata.get(TikaCoreProperties.TIKA_CONTENT));
-        assertContains("null pointer message",
-                metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION));
+        assertContains("null pointer message", metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION));
 
     }
     /*
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/ServerStatusTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/ServerStatusTest.java
index 888813867..889413861 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/ServerStatusTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/ServerStatusTest.java
@@ -48,8 +48,7 @@ public class ServerStatusTest {
         int numThreads = 10;
         int filesToProcess = 20;
         ExecutorService service = Executors.newFixedThreadPool(numThreads);
-        ExecutorCompletionService<Integer> completionService =
-                new ExecutorCompletionService<>(service);
+        ExecutorCompletionService<Integer> completionService = new ExecutorCompletionService<>(service);
         ServerStatus serverStatus = new ServerStatus("", 0);
         for (int i = 0; i < numThreads; i++) {
             completionService.submit(new MockTask(serverStatus, filesToProcess));
@@ -65,7 +64,9 @@ public class ServerStatusTest {
             }
         }
         assertEquals(numThreads * filesToProcess, totalProcessed);
-        assertEquals(0, serverStatus.getTasks().size());
+        assertEquals(0, serverStatus
+                .getTasks()
+                .size());
         assertEquals(totalProcessed, serverStatus.getFilesProcessed());
 
     }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceOffTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceOffTest.java
index 32fe5f957..e3576a7c4 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceOffTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceOffTest.java
@@ -50,8 +50,7 @@ public class StackTraceOffTest extends CXFTestBase {
 
     private static final String TEST_HELLO_WORLD = "test-documents/mock/hello_world.xml";
     private static final String TEST_NULL = "test-documents/mock/null_pointer.xml";
-    private static final String TEST_PASSWORD_PROTECTED =
-            "test-documents/mock/encrypted_document_exception.xml";
+    private static final String TEST_PASSWORD_PROTECTED = "test-documents/mock/encrypted_document_exception.xml";
 
 
     private static final String[] PATHS = new String[]{"/tika", "/rmeta", "/unpack", "/meta",};
@@ -62,8 +61,7 @@ public class StackTraceOffTest extends CXFTestBase {
         List<ResourceProvider> rCoreProviders = new ArrayList<>();
         rCoreProviders.add(new SingletonResourceProvider(new MetadataResource()));
         rCoreProviders.add(new SingletonResourceProvider(new RecursiveMetadataResource()));
-        rCoreProviders
-                .add(new SingletonResourceProvider(new DetectorResource(new ServerStatus("", 0))));
+        rCoreProviders.add(new SingletonResourceProvider(new DetectorResource(new ServerStatus("", 0))));
         rCoreProviders.add(new SingletonResourceProvider(new TikaResource()));
         rCoreProviders.add(new SingletonResourceProvider(new UnpackerResource()));
         sf.setResourceProviders(rCoreProviders);
@@ -90,9 +88,10 @@ public class StackTraceOffTest extends CXFTestBase {
             if ("/tika".equals(path)) {
                 accept = "text/plain";
             }
-            Response response = WebClient.create(endPoint + path).accept(accept)
-                    .header("Content-Disposition",
-                            "attachment; filename=" + TEST_PASSWORD_PROTECTED)
+            Response response = WebClient
+                    .create(endPoint + path)
+                    .accept(accept)
+                    .header("Content-Disposition", "attachment; filename=" + TEST_PASSWORD_PROTECTED)
                     .put(ClassLoader.getSystemResourceAsStream(TEST_PASSWORD_PROTECTED));
             assertNotNull(response, "null response: " + path);
             assertEquals(UNPROCESSEABLE, response.getStatus(), "unprocessable: " + path);
@@ -111,7 +110,9 @@ public class StackTraceOffTest extends CXFTestBase {
             if ("/tika".equals(path)) {
                 accept = "text/plain";
             }
-            Response response = WebClient.create(endPoint + path).accept(accept)
+            Response response = WebClient
+                    .create(endPoint + path)
+                    .accept(accept)
                     .put(ClassLoader.getSystemResourceAsStream(TEST_NULL));
             assertNotNull(response, "null response: " + path);
             assertEquals(UNPROCESSEABLE, response.getStatus(), "unprocessable: " + path);
@@ -128,7 +129,9 @@ public class StackTraceOffTest extends CXFTestBase {
         //no stack traces for 415
         for (String path : PATHS) {
 
-            Response response = WebClient.create(endPoint + path).accept("*:*")
+            Response response = WebClient
+                    .create(endPoint + path)
+                    .accept("*:*")
                     .put(getClass().getResourceAsStream("/test-documents/testDigilite.fdf"));
             if (path.equals("/unpack")) {
                 //"NO CONTENT"
@@ -148,9 +151,11 @@ public class StackTraceOffTest extends CXFTestBase {
     public void testMeta() throws Exception {
         InputStream stream = ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD);
 
-        Response response =
-                WebClient.create(endPoint + "/meta" + "/Author").type("application/mock+xml")
-                        .accept(MediaType.TEXT_PLAIN).put(copy(stream, 100));
+        Response response = WebClient
+                .create(endPoint + "/meta" + "/Author")
+                .type("application/mock+xml")
+                .accept(MediaType.TEXT_PLAIN)
+                .put(copy(stream, 100));
         assertEquals(Response.Status.BAD_REQUEST.getStatusCode(), response.getStatus());
         String msg = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals("Failed to get metadata field Author", msg);
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceTest.java
index f95f71f26..172133d85 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/StackTraceTest.java
@@ -46,8 +46,7 @@ public class StackTraceTest extends CXFTestBase {
 
     private static final String TEST_HELLO_WORLD = "test-documents/mock/hello_world.xml";
     private static final String TEST_NULL = "test-documents/mock/null_pointer.xml";
-    private static final String TEST_PASSWORD_PROTECTED =
-            "test-documents/mock/encrypted_document_exception.xml";
+    private static final String TEST_PASSWORD_PROTECTED = "test-documents/mock/encrypted_document_exception.xml";
 
     private static final String[] PATHS = new String[]{"/tika", "/rmeta", "/unpack", "/meta",};
     private static final int UNPROCESSEABLE = 422;
@@ -57,8 +56,7 @@ public class StackTraceTest extends CXFTestBase {
         List<ResourceProvider> rCoreProviders = new ArrayList<>();
         rCoreProviders.add(new SingletonResourceProvider(new MetadataResource()));
         rCoreProviders.add(new SingletonResourceProvider(new RecursiveMetadataResource()));
-        rCoreProviders
-                .add(new SingletonResourceProvider(new DetectorResource(new ServerStatus("", 0))));
+        rCoreProviders.add(new SingletonResourceProvider(new DetectorResource(new ServerStatus("", 0))));
         rCoreProviders.add(new SingletonResourceProvider(new TikaResource()));
         rCoreProviders.add(new SingletonResourceProvider(new UnpackerResource()));
         sf.setResourceProviders(rCoreProviders);
@@ -86,9 +84,10 @@ public class StackTraceTest extends CXFTestBase {
             if ("/tika".equals(path)) {
                 accept = "text/plain";
             }
-            Response response = WebClient.create(endPoint + path).accept(accept)
-                    .header("Content-Disposition",
-                            "attachment; filename=" + TEST_PASSWORD_PROTECTED)
+            Response response = WebClient
+                    .create(endPoint + path)
+                    .accept(accept)
+                    .header("Content-Disposition", "attachment; filename=" + TEST_PASSWORD_PROTECTED)
                     .put(ClassLoader.getSystemResourceAsStream(TEST_PASSWORD_PROTECTED));
             assertNotNull(response, "null response: " + path);
             assertEquals(UNPROCESSEABLE, response.getStatus(), "unprocessable: " + path);
@@ -107,7 +106,9 @@ public class StackTraceTest extends CXFTestBase {
             if ("/tika".equals(path)) {
                 accept = "text/plain";
             }
-            Response response = WebClient.create(endPoint + path).accept(accept)
+            Response response = WebClient
+                    .create(endPoint + path)
+                    .accept(accept)
                     .put(ClassLoader.getSystemResourceAsStream(TEST_NULL));
             assertNotNull(response);
             assertEquals(UNPROCESSEABLE, response.getStatus(), "unprocessable: " + path);
@@ -123,7 +124,9 @@ public class StackTraceTest extends CXFTestBase {
         //no stack traces for 415
         for (String path : PATHS) {
 
-            Response response = WebClient.create(endPoint + path).accept("*:*")
+            Response response = WebClient
+                    .create(endPoint + path)
+                    .accept("*:*")
                     .put(ClassLoader.getSystemResourceAsStream("test-documents/testDigilite.fdf"));
             if (path.equals("/unpack")) {
                 //"NO CONTENT"
@@ -144,9 +147,11 @@ public class StackTraceTest extends CXFTestBase {
     public void testMeta() throws Exception {
         InputStream stream = ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD);
 
-        Response response =
-                WebClient.create(endPoint + "/meta" + "/Author").type("application/mock+xml")
-                        .accept(MediaType.TEXT_PLAIN).put(copy(stream, 100));
+        Response response = WebClient
+                .create(endPoint + "/meta" + "/Author")
+                .type("application/mock+xml")
+                .accept(MediaType.TEXT_PLAIN)
+                .put(copy(stream, 100));
         assertEquals(Response.Status.BAD_REQUEST.getStatusCode(), response.getStatus());
         String msg = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals("Failed to get metadata field Author", msg);
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaMimeTypesTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaMimeTypesTest.java
index 31492afce..108cdbb20 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaMimeTypesTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaMimeTypesTest.java
@@ -35,8 +35,7 @@ public class TikaMimeTypesTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaMimeTypes.class);
-        sf.setResourceProvider(TikaMimeTypes.class,
-                new SingletonResourceProvider(new TikaMimeTypes()));
+        sf.setResourceProvider(TikaMimeTypes.class, new SingletonResourceProvider(new TikaMimeTypes()));
     }
 
     @Override
@@ -45,9 +44,11 @@ public class TikaMimeTypesTest extends CXFTestBase {
 
     @Test
     public void testGetPlainText() throws Exception {
-        Response response =
-                WebClient.create(endPoint + MIMETYPES_PATH).type("text/plain").accept("text/plain")
-                        .get();
+        Response response = WebClient
+                .create(endPoint + MIMETYPES_PATH)
+                .type("text/plain")
+                .accept("text/plain")
+                .get();
 
         String text = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("text/plain", text);
@@ -61,9 +62,11 @@ public class TikaMimeTypesTest extends CXFTestBase {
 
     @Test
     public void testGetHTML() throws Exception {
-        Response response =
-                WebClient.create(endPoint + MIMETYPES_PATH).type("text/html").accept("text/html")
-                        .get();
+        Response response = WebClient
+                .create(endPoint + MIMETYPES_PATH)
+                .type("text/html")
+                .accept("text/html")
+                .get();
 
         String text = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("text/plain", text);
@@ -83,9 +86,11 @@ public class TikaMimeTypesTest extends CXFTestBase {
 
     @Test
     public void testGetHTMLDetails() throws Exception {
-        Response response =
-                WebClient.create(endPoint + MIMETYPES_PATH + "/application/cbor").type("text/html")
-                        .accept("text/html").get();
+        Response response = WebClient
+                .create(endPoint + MIMETYPES_PATH + "/application/cbor")
+                .type("text/html")
+                .accept("text/html")
+                .get();
 
         String text = getStringFromInputStream((InputStream) response.getEntity());
         assertNotFound("text/plain", text);
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaPipesTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaPipesTest.java
index b96e1d2f1..4bb990098 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaPipesTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaPipesTest.java
@@ -51,14 +51,15 @@ import org.junit.jupiter.api.Test;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTuple;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.fetcher.FetchKey;
 import org.apache.tika.pipes.fetcher.FetcherManager;
 import org.apache.tika.sax.BasicContentHandlerFactory;
+import org.apache.tika.serialization.JsonMetadataList;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTuple;
 import org.apache.tika.server.core.resource.PipesResource;
 import org.apache.tika.server.core.writer.JSONObjWriter;
 import org.apache.tika.utils.ProcessUtils;
@@ -96,34 +97,20 @@ public class TikaPipesTest extends CXFTestBase {
         Files.createDirectories(TMP_OUTPUT_DIR);
 
         for (String mockFile : new String[]{"hello_world.xml", "null_pointer.xml"}) {
-            Files.copy(
-                    TikaPipesTest.class.getResourceAsStream("/test-documents/mock/" + mockFile),
-                    inputDir.resolve(mockFile));
+            Files.copy(TikaPipesTest.class.getResourceAsStream("/test-documents/mock/" + mockFile), inputDir.resolve(mockFile));
         }
         TIKA_CONFIG_PATH = Files.createTempFile(TMP_DIR, "tika-pipes-", ".xml");
         TIKA_PIPES_LOG4j2_PATH = Files.createTempFile(TMP_DIR, "log4j2-", ".xml");
-        Files.copy(TikaPipesTest.class.getResourceAsStream("/log4j2.xml"), TIKA_PIPES_LOG4j2_PATH,
-                StandardCopyOption.REPLACE_EXISTING);
-        TIKA_CONFIG_XML =
-                "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" +
-                        "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" +
-                        "<name>fsf</name>" +
-                        "<basePath>" + inputDir.toAbsolutePath() +
-                        "</basePath>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
-                        "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" +
-                        "<name>fse</name>" +
-                        "<basePath>" +
-                        TMP_OUTPUT_DIR.toAbsolutePath() + "</basePath>" +
-                        "</emitter>" +
-                        "</emitters>" + "<pipes><tikaConfig>" +
-                ProcessUtils.escapeCommandLine(TIKA_CONFIG_PATH.toAbsolutePath().toString()) +
-                        "</tikaConfig><numClients>10</numClients>" +
-                        "<forkedJvmArgs>" +
-                        "<arg>-Xmx256m</arg>" +
-                        "<arg>-Dlog4j.configurationFile=file:" +
-                        ProcessUtils.escapeCommandLine(TIKA_PIPES_LOG4j2_PATH.toAbsolutePath().toString()) + "</arg>" +
-                        "</forkedJvmArgs>" +
-                        "</pipes>" + "</properties>";
+        Files.copy(TikaPipesTest.class.getResourceAsStream("/log4j2.xml"), TIKA_PIPES_LOG4j2_PATH, StandardCopyOption.REPLACE_EXISTING);
+        TIKA_CONFIG_XML = "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" + "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" +
+                "<name>fsf</name>" + "<basePath>" + inputDir.toAbsolutePath() + "</basePath>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
+                "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" + "<name>fse</name>" + "<basePath>" + TMP_OUTPUT_DIR.toAbsolutePath() + "</basePath>" +
+                "</emitter>" + "</emitters>" + "<pipes><tikaConfig>" + ProcessUtils.escapeCommandLine(TIKA_CONFIG_PATH
+                .toAbsolutePath()
+                .toString()) + "</tikaConfig><numClients>10</numClients>" + "<forkedJvmArgs>" + "<arg>-Xmx256m</arg>" + "<arg>-Dlog4j.configurationFile=file:" +
+                ProcessUtils.escapeCommandLine(TIKA_PIPES_LOG4j2_PATH
+                        .toAbsolutePath()
+                        .toString()) + "</arg>" + "</forkedJvmArgs>" + "</pipes>" + "</properties>";
         Files.write(TIKA_CONFIG_PATH, TIKA_CONFIG_XML.getBytes(StandardCharsets.UTF_8));
     }
 
@@ -182,16 +169,15 @@ public class TikaPipesTest extends CXFTestBase {
             userMetadata.add("my-key-multi", s);
         }
 
-        FetchEmitTuple t =
-                new FetchEmitTuple("myId",
-                        new FetchKey("fsf", "hello_world.xml"), new EmitKey("fse", ""),
-                        userMetadata);
+        FetchEmitTuple t = new FetchEmitTuple("myId", new FetchKey("fsf", "hello_world.xml"), new EmitKey("fse", ""), userMetadata);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
 
         String getUrl = endPoint + PIPES_PATH;
-        Response response =
-                WebClient.create(getUrl).accept("application/json").post(writer.toString());
+        Response response = WebClient
+                .create(getUrl)
+                .accept("application/json")
+                .post(writer.toString());
         assertEquals(200, response.getStatus());
 
         List<Metadata> metadataList = null;
@@ -200,7 +186,9 @@ public class TikaPipesTest extends CXFTestBase {
         }
         assertEquals(1, metadataList.size());
         Metadata metadata = metadataList.get(0);
-        assertEquals("hello world", metadata.get(TikaCoreProperties.TIKA_CONTENT).trim());
+        assertEquals("hello world", metadata
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim());
         assertEquals("Nikolai Lobachevsky", metadata.get("author"));
         assertEquals("你好，世界", metadata.get("title"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
@@ -216,21 +204,19 @@ public class TikaPipesTest extends CXFTestBase {
         for (String s : VALUE_ARRAY) {
             userMetadata.add("my-key-multi", s);
         }
-
+        ParseContext parseContext = new ParseContext();
+        HandlerConfig handlerConfig = new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML, HandlerConfig.PARSE_MODE.RMETA, -1, -1, true);
+        parseContext.set(HandlerConfig.class, handlerConfig);
         FetchEmitTuple t =
-                new FetchEmitTuple("myId",
-                        new FetchKey("fsf", "hello_world.xml"),
-                        new EmitKey("fse", ""),
-                        userMetadata,
-                        new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.XML,
-                                HandlerConfig.PARSE_MODE.RMETA, -1, -1, true),
-                        FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT);
+                new FetchEmitTuple("myId", new FetchKey("fsf", "hello_world.xml"), new EmitKey("fse", ""), userMetadata, parseContext, FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
 
         String getUrl = endPoint + PIPES_PATH;
-        Response response =
-                WebClient.create(getUrl).accept("application/json").post(writer.toString());
+        Response response = WebClient
+                .create(getUrl)
+                .accept("application/json")
+                .post(writer.toString());
         assertEquals(200, response.getStatus());
 
         List<Metadata> metadataList = null;
@@ -239,7 +225,9 @@ public class TikaPipesTest extends CXFTestBase {
         }
         assertEquals(1, metadataList.size());
         Metadata metadata = metadataList.get(0);
-        assertContains("<p>hello world</p>", metadata.get(TikaCoreProperties.TIKA_CONTENT).trim());
+        assertContains("<p>hello world</p>", metadata
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim());
     }
 
     @Test
@@ -250,31 +238,31 @@ public class TikaPipesTest extends CXFTestBase {
             userMetadata.add("my-key-multi", s);
         }
 
-        FetchEmitTuple t =
-                new FetchEmitTuple("myId",
-                        new FetchKey("fsf", "null_pointer.xml"),
-                        new EmitKey("fse", ""),
-                        userMetadata);
+        FetchEmitTuple t = new FetchEmitTuple("myId", new FetchKey("fsf", "null_pointer.xml"), new EmitKey("fse", ""), userMetadata);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
 
         String getUrl = endPoint + PIPES_PATH;
-        Response response =
-                WebClient.create(getUrl).accept("application/json").post(writer.toString());
+        Response response = WebClient
+                .create(getUrl)
+                .accept("application/json")
+                .post(writer.toString());
         assertEquals(200, response.getStatus());
 
         JsonNode jsonResponse;
-        try (Reader reader = new InputStreamReader((InputStream) response.getEntity(),
-                StandardCharsets.UTF_8)) {
+        try (Reader reader = new InputStreamReader((InputStream) response.getEntity(), StandardCharsets.UTF_8)) {
             jsonResponse = new ObjectMapper().readTree(reader);
         }
-        String parseException = jsonResponse.get("parse_exception").asText();
+        String parseException = jsonResponse
+                .get("parse_exception")
+                .asText();
         assertNotNull(parseException);
         assertContains("NullPointerException", parseException);
-        assertTrue(jsonResponse.get("emitted").asBoolean());
+        assertTrue(jsonResponse
+                .get("emitted")
+                .asBoolean());
         List<Metadata> metadataList;
-        try (Reader reader = Files
-                .newBufferedReader(TMP_OUTPUT_DIR.resolve("null_pointer.xml.json"))) {
+        try (Reader reader = Files.newBufferedReader(TMP_OUTPUT_DIR.resolve("null_pointer.xml.json"))) {
             metadataList = JsonMetadataList.fromJson(reader);
         }
         assertEquals(1, metadataList.size());
@@ -282,34 +270,35 @@ public class TikaPipesTest extends CXFTestBase {
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
         assertEquals("my-value", metadata.get("my-key"));
         assertArrayEquals(VALUE_ARRAY, metadata.getValues("my-key-multi"));
-        assertContains("NullPointerException",
-                metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION));
+        assertContains("NullPointerException", metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION));
     }
 
     @Test
     public void testPostNPENoEmit() throws Exception {
-        FetchEmitTuple t =
-                new FetchEmitTuple("myId",
-                        new FetchKey("fsf", "null_pointer.xml"),
-                        new EmitKey("fse", ""),
-                        FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP);
+        FetchEmitTuple t = new FetchEmitTuple("myId", new FetchKey("fsf", "null_pointer.xml"), new EmitKey("fse", ""), new Metadata(), new ParseContext(),
+                FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
 
         String getUrl = endPoint + PIPES_PATH;
-        Response response =
-                WebClient.create(getUrl).accept("application/json").post(writer.toString());
+        Response response = WebClient
+                .create(getUrl)
+                .accept("application/json")
+                .post(writer.toString());
         assertEquals(200, response.getStatus());
 
         JsonNode jsonResponse;
-        try (Reader reader = new InputStreamReader((InputStream) response.getEntity(),
-                StandardCharsets.UTF_8)) {
+        try (Reader reader = new InputStreamReader((InputStream) response.getEntity(), StandardCharsets.UTF_8)) {
             jsonResponse = new ObjectMapper().readTree(reader);
         }
-        String parseException = jsonResponse.get("parse_exception").asText();
+        String parseException = jsonResponse
+                .get("parse_exception")
+                .asText();
         assertNotNull(parseException);
         assertContains("NullPointerException", parseException);
-        assertFalse(jsonResponse.get("emitted").asBoolean());
+        assertFalse(jsonResponse
+                .get("emitted")
+                .asBoolean());
         assertFalse(Files.isRegularFile(TMP_NPE_OUTPUT_FILE));
     }
 }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceFetcherTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceFetcherTest.java
index f7475e546..47219afa4 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceFetcherTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceFetcherTest.java
@@ -49,8 +49,7 @@ public class TikaResourceFetcherTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaResource.class);
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
     }
 
     @Override
@@ -65,16 +64,16 @@ public class TikaResourceFetcherTest extends CXFTestBase {
     protected InputStream getTikaConfigInputStream() throws IOException {
         Path inputDir = null;
         try {
-            inputDir = Paths.get(
-                    TikaResourceFetcherTest.class.getResource("/test-documents/").toURI());
+            inputDir = Paths.get(TikaResourceFetcherTest.class
+                    .getResource("/test-documents/")
+                    .toURI());
         } catch (URISyntaxException e) {
             throw new RuntimeException(e);
         }
-        String configXML = getStringFromInputStream(
-                TikaResourceFetcherTest.class.getResourceAsStream(
-                        "/configs/tika-config-server-fetcher-template.xml"));
-        configXML = configXML.replace("{FETCHER_BASE_PATH}",
-                inputDir.toAbsolutePath().toString());
+        String configXML = getStringFromInputStream(TikaResourceFetcherTest.class.getResourceAsStream("/configs/tika-config-server-fetcher-template.xml"));
+        configXML = configXML.replace("{FETCHER_BASE_PATH}", inputDir
+                .toAbsolutePath()
+                .toString());
 
         configXML = configXML.replace("{PORT}", "9998");
         return new ByteArrayInputStream(configXML.getBytes(StandardCharsets.UTF_8));
@@ -95,16 +94,23 @@ public class TikaResourceFetcherTest extends CXFTestBase {
         MultivaluedMap<String, String> map = new MultivaluedHashMap<>();
         map.putSingle("fetcherName", "fsf");
         map.putSingle("fetchKey", "mock/hello_world.xml");
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).headers(map).accept("text/xml").put(null);
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .headers(map)
+                .accept("text/xml")
+                .put(null);
         String xml = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("hello world", xml);
     }
 
     @Test
     public void testQueryPart() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).query("fetcherName", "fsf")
-                .query("fetchKey", "mock/hello_world.xml").accept("text/xml").put(null);
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .query("fetcherName", "fsf")
+                .query("fetchKey", "mock/hello_world.xml")
+                .accept("text/xml")
+                .put(null);
         String xml = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("hello world", xml);
     }
@@ -112,8 +118,12 @@ public class TikaResourceFetcherTest extends CXFTestBase {
     @Test
     @Disabled("Apache's Hudson does not like the test file or the utf-8 in this source file")
     public void testNonAsciiInQueryParameters() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).query("fetcherName", "fsf")
-                .query("fetchKey", "mock/中文.xml").accept("text/xml").put(null);
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .query("fetcherName", "fsf")
+                .query("fetchKey", "mock/中文.xml")
+                .accept("text/xml")
+                .put(null);
         String xml = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("你好世界", xml);
     }
@@ -121,8 +131,12 @@ public class TikaResourceFetcherTest extends CXFTestBase {
     @Test
     @Disabled("Apache's Hudson does not like the test file or the utf-8 in this source file")
     public void testNonAsciiUrlEncodedInQueryParameters() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).query("fetcherName", "fsf")
-                .query("fetchKey", "mock/%E4%B8%AD%E6%96%87.xml").accept("text/xml").put(null);
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .query("fetcherName", "fsf")
+                .query("fetchKey", "mock/%E4%B8%AD%E6%96%87.xml")
+                .accept("text/xml")
+                .put(null);
         String xml = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("你好世界", xml);
     }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceMetadataFilterTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceMetadataFilterTest.java
index 1786a0a7e..c7f663128 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceMetadataFilterTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceMetadataFilterTest.java
@@ -34,7 +34,7 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadata;
 import org.apache.tika.server.core.resource.TikaResource;
 import org.apache.tika.server.core.writer.JSONMessageBodyWriter;
 
@@ -52,8 +52,7 @@ public class TikaResourceMetadataFilterTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaResource.class);
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
     }
 
     @Override
@@ -67,12 +66,11 @@ public class TikaResourceMetadataFilterTest extends CXFTestBase {
 
     @Test
     public void testBasic() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept(
-                "application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
-        Metadata metadata =
-                JsonMetadata.fromJson(new InputStreamReader(
-                        ((InputStream)response.getEntity()), StandardCharsets.UTF_8));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
         assertEquals(2, metadata.names().length);
         assertNull(metadata.get("author"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceNoStackTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceNoStackTest.java
index b28c5e2bb..ecb76d8ed 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceNoStackTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceNoStackTest.java
@@ -34,7 +34,7 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadata;
 import org.apache.tika.server.core.resource.TikaResource;
 import org.apache.tika.server.core.writer.JSONMessageBodyWriter;
 
@@ -60,8 +60,7 @@ public class TikaResourceNoStackTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaResource.class);
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
     }
 
     @Override
@@ -74,8 +73,9 @@ public class TikaResourceNoStackTest extends CXFTestBase {
 
     @Test
     public void testJsonNPE() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept(
-                "application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_NULL_POINTER));
         assertEquals(UNPROCESSEABLE, response.getStatus());
         String content = getStringFromInputStream((InputStream) response.getEntity());
@@ -84,17 +84,15 @@ public class TikaResourceNoStackTest extends CXFTestBase {
 
     @Test
     public void testJsonWriteLimit() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH)
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
                 .header("writeLimit", "100")
                 .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD_LONG));
         assertEquals(200, response.getStatus());
-        Metadata metadata =
-                JsonMetadata.fromJson(new InputStreamReader((InputStream) response.getEntity(),
-                        StandardCharsets.UTF_8));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader((InputStream) response.getEntity(), StandardCharsets.UTF_8));
         assertEquals("true", metadata.get(TikaCoreProperties.WRITE_LIMIT_REACHED));
-        assertContains("When in the Course of human events",
-                metadata.get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("When in the Course of human events", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertNotContained("political bands", metadata.get(TikaCoreProperties.TIKA_CONTENT));
     }
 
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceTest.java
index 535c40d39..a471cec39 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaResourceTest.java
@@ -37,7 +37,7 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadata;
 import org.apache.tika.server.core.resource.TikaResource;
 import org.apache.tika.server.core.writer.JSONMessageBodyWriter;
 
@@ -56,8 +56,7 @@ public class TikaResourceTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaResource.class);
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
     }
 
     @Override
@@ -70,18 +69,22 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testHelloWorld() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("text/plain").accept("text/plain")
-                        .get();
-        assertEquals(TikaResource.GREETING,
-                getStringFromInputStream((InputStream) response.getEntity()));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("text/plain")
+                .accept("text/plain")
+                .get();
+        assertEquals(TikaResource.GREETING, getStringFromInputStream((InputStream) response.getEntity()));
     }
 
     @Test
     public void testHeaders() throws Exception {
         MultivaluedMap<String, String> map = new MultivaluedHashMap<>();
         map.addAll("meta_mymeta", "first", "second", "third");
-        Response response = WebClient.create(endPoint + TIKA_PATH).headers(map).accept("text/xml")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .headers(map)
+                .accept("text/xml")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
         String xml = getStringFromInputStream((InputStream) response.getEntity());
         //can't figure out why these values are comma-delimited, rather
@@ -102,14 +105,18 @@ public class TikaResourceTest extends CXFTestBase {
 
         Response response = null;
         try {
-            response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+            response = WebClient
+                    .create(endPoint + TIKA_PATH)
+                    .accept("text/plain")
                     .put(ClassLoader.getSystemResourceAsStream(TEST_OOM));
         } catch (Exception e) {
             //oom may or may not cause an exception depending
             //on the timing
         }
 
-        response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
 
@@ -118,19 +125,21 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testApplicationWadl() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH + "?_wadl").accept("text/plain").get();
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "?_wadl")
+                .accept("text/plain")
+                .get();
         String resp = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(resp.startsWith("<application"));
     }
 
     @Test
     public void testJson() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
 
         assertEquals("Nikolai Lobachevsky", metadata.get("author"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
@@ -139,33 +148,33 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testJsonNPE() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_NULL_POINTER));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
 
         assertEquals("Nikolai Lobachevsky", metadata.get("author"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
         assertContains("some content", metadata.get(TikaCoreProperties.TIKA_CONTENT));
-        assertContains("null pointer message",
-                metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION));
+        assertContains("null pointer message", metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION));
     }
 
     @Test
     public void testJsonWriteLimit() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).header("writeLimit", "100")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .header("writeLimit", "100")
                 .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD_LONG));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
 
         assertEquals("Nikolai Lobachevsky", metadata.get("author"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
         assertContains("Hello world", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertNotFound("dissolve", metadata.get(TikaCoreProperties.TIKA_CONTENT));
-        assertTrue(metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION)
+        assertTrue(metadata
+                .get(TikaCoreProperties.CONTAINER_EXCEPTION)
                 .startsWith("org.apache.tika.exception.WriteLimitReachedException"));
         assertEquals("true", metadata.get(TikaCoreProperties.WRITE_LIMIT_REACHED));
     }
@@ -174,13 +183,17 @@ public class TikaResourceTest extends CXFTestBase {
     public void testNoWriteLimitOnStreamingWrite() throws Exception {
         //this test shows that write limit is not active for
         //text or xhtml or anything that does streaming writes
-        Response response = WebClient.create(endPoint + TIKA_PATH).header("writeLimit", "100")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .header("writeLimit", "100")
                 .accept("text/plain")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD_LONG));
         String content = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("separation.", content);
 
-        response = WebClient.create(endPoint + TIKA_PATH).header("writeLimit", "100")
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .header("writeLimit", "100")
                 .accept("text/html")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD_LONG));
         content = getStringFromInputStream((InputStream) response.getEntity());
@@ -189,11 +202,11 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testJsonHandlerType() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD_LONG));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
 
         assertEquals("Nikolai Lobachevsky", metadata.get("author"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
@@ -201,10 +214,11 @@ public class TikaResourceTest extends CXFTestBase {
         //default is xhtml
         assertContains("<p>", metadata.get(TikaCoreProperties.TIKA_CONTENT));
 
-        response = WebClient.create(endPoint + TIKA_PATH + "/text").accept("application/json")
+        response = WebClient
+                .create(endPoint + TIKA_PATH + "/text")
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD_LONG));
-        metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()),
-                StandardCharsets.UTF_8));
+        metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
 
         assertEquals("Nikolai Lobachevsky", metadata.get("author"));
         assertEquals("application/mock+xml", metadata.get(Metadata.CONTENT_TYPE));
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerAsyncIntegrationTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerAsyncIntegrationTest.java
index 5cc1e22c8..9ec97b3a5 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerAsyncIntegrationTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerAsyncIntegrationTest.java
@@ -45,11 +45,12 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import org.apache.tika.metadata.Metadata;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTupleList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.fetcher.FetchKey;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTupleList;
 import org.apache.tika.utils.ProcessUtils;
 
 @Disabled("useful for development...need to turn it into a real unit test")
@@ -59,8 +60,7 @@ public class TikaServerAsyncIntegrationTest extends IntegrationTestBase {
     private static final int NUM_FILES = 100;
     private static final String EMITTER_NAME = "fse";
     private static final String FETCHER_NAME = "fsf";
-    private static FetchEmitTuple.ON_PARSE_EXCEPTION ON_PARSE_EXCEPTION =
-            FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT;
+    private static FetchEmitTuple.ON_PARSE_EXCEPTION ON_PARSE_EXCEPTION = FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT;
 
     @TempDir
     private static Path TMP_DIR;
@@ -68,12 +68,9 @@ public class TikaServerAsyncIntegrationTest extends IntegrationTestBase {
     private static String TIKA_CONFIG_XML;
     private static Path TIKA_CONFIG;
     private static List<String> FILE_LIST = new ArrayList<>();
-    private static String[] FILES = new String[]{
-            "hello_world.xml",
-            "null_pointer.xml",
+    private static String[] FILES = new String[]{"hello_world.xml", "null_pointer.xml",
             // "heavy_hang_30000.xml", "real_oom.xml",
-            "system_exit.xml"
-    };
+            "system_exit.xml"};
 
     @BeforeAll
     public static void setUpBeforeClass() throws Exception {
@@ -92,33 +89,21 @@ public class TikaServerAsyncIntegrationTest extends IntegrationTestBase {
                 String targetName = i + "-" + mockFile;
                 Path target = inputDir.resolve(targetName);
                 FILE_LIST.add(targetName);
-                Files.copy(TikaPipesTest.class
-                        .getResourceAsStream("/test-documents/mock/" + mockFile), target);
+                Files.copy(TikaPipesTest.class.getResourceAsStream("/test-documents/mock/" + mockFile), target);
 
             }
         }
         TIKA_CONFIG = TMP_DIR.resolve("tika-config.xml");
 
         TIKA_CONFIG_XML =
-                "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" +
-                        "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" +
-                        "<name>" + FETCHER_NAME + "</name>" +
-                        "<basePath>" + inputDir.toAbsolutePath() + "</basePath>" + "</fetcher>" +
-                        "</fetchers>" + "<emitters>" +
-                        "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" +
-                        "<name>" + EMITTER_NAME + "</name>" +
-                        "<basePath>" +
-                        TMP_OUTPUT_DIR.toAbsolutePath() + "</basePath>" +
-                        "</emitter>" +
-                        "</emitters>" +
-                        "<server><endpoints><endpoint>async</endpoint></endpoints>" +
-                        "<enableUnsecureFeatures>true</enableUnsecureFeatures></server>" +
-                        "<async><tikaConfig>" +
-                        ProcessUtils.escapeCommandLine(TIKA_CONFIG.toAbsolutePath().toString()) +
-                        "</tikaConfig><numClients>10</numClients><forkedJvmArgs><arg>-Xmx256m" +
-                        "</arg></forkedJvmArgs><timeoutMillis>5000</timeoutMillis>" +
-                        "</async>" +
-                        "</properties>";
+                "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" + "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" + "<name>" +
+                        FETCHER_NAME + "</name>" + "<basePath>" + inputDir.toAbsolutePath() + "</basePath>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
+                        "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" + "<name>" + EMITTER_NAME + "</name>" + "<basePath>" +
+                        TMP_OUTPUT_DIR.toAbsolutePath() + "</basePath>" + "</emitter>" + "</emitters>" + "<server><endpoints><endpoint>async</endpoint></endpoints>" +
+                        "<enableUnsecureFeatures>true</enableUnsecureFeatures></server>" + "<async><tikaConfig>" + ProcessUtils.escapeCommandLine(TIKA_CONFIG
+                        .toAbsolutePath()
+                        .toString()) + "</tikaConfig><numClients>10</numClients><forkedJvmArgs><arg>-Xmx256m" + "</arg></forkedJvmArgs><timeoutMillis>5000</timeoutMillis>" +
+                        "</async>" + "</properties>";
 
         FileUtils.write(TIKA_CONFIG.toFile(), TIKA_CONFIG_XML, UTF_8);
     }
@@ -143,20 +128,20 @@ public class TikaServerAsyncIntegrationTest extends IntegrationTestBase {
         Thread serverThread = new Thread(() -> TikaServerCli.main(new String[]{
                 //for debugging/development, use no fork; otherwise go with the default
                 //"-noFork",
-                "-p", INTEGRATION_TEST_PORT, "-config",
-                TIKA_CONFIG.toAbsolutePath().toString()}));
+                "-p", INTEGRATION_TEST_PORT, "-config", TIKA_CONFIG.toAbsolutePath().toString()}));
         serverThread.start();
 
         try {
             long start = System.currentTimeMillis();
 
             JsonNode response = sendAsync(FILE_LIST);
-            String status = response.get("status").asText();
-            if (! "ok".equals(status)) {
+            String status = response
+                    .get("status")
+                    .asText();
+            if (!"ok".equals(status)) {
                 fail("bad status: '" + status + "' -> " + response.toPrettyString());
             }
-            int expected = (ON_PARSE_EXCEPTION == FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT) ?
-                    FILE_LIST.size() : FILE_LIST.size() / 3;
+            int expected = (ON_PARSE_EXCEPTION == FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT) ? FILE_LIST.size() : FILE_LIST.size() / 3;
             int targets = 0;
             while (targets < NUM_FILES * 2) {
                 targets = countTargets();
@@ -169,7 +154,9 @@ public class TikaServerAsyncIntegrationTest extends IntegrationTestBase {
     }
 
     private int countTargets() {
-        return TMP_OUTPUT_DIR.toFile().listFiles().length;
+        return TMP_OUTPUT_DIR
+                .toFile()
+                .listFiles().length;
     }
 
     private JsonNode sendAsync(List<String> fileNames) throws Exception {
@@ -180,15 +167,18 @@ public class TikaServerAsyncIntegrationTest extends IntegrationTestBase {
         }
         String json = JsonFetchEmitTupleList.toJson(tuples);
 
-        Response response =
-                WebClient.create(endPoint + "/async").accept("application/json").post(json);
+        Response response = WebClient
+                .create(endPoint + "/async")
+                .accept("application/json")
+                .post(json);
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         return new ObjectMapper().readTree(reader);
     }
 
     private FetchEmitTuple getFetchEmitTuple(String fileName) throws IOException {
-        return new FetchEmitTuple(fileName, new FetchKey(FETCHER_NAME, fileName),
-                new EmitKey(EMITTER_NAME, ""), new Metadata(), HandlerConfig.DEFAULT_HANDLER_CONFIG,
-                ON_PARSE_EXCEPTION);
+        ParseContext parseContext = new ParseContext();
+        parseContext.set(HandlerConfig.class, HandlerConfig.DEFAULT_HANDLER_CONFIG);
+
+        return new FetchEmitTuple(fileName, new FetchKey(FETCHER_NAME, fileName), new EmitKey(EMITTER_NAME, ""), new Metadata(), parseContext, ON_PARSE_EXCEPTION);
     }
 }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerConfigTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerConfigTest.java
index 55411c91b..8b2c51fc8 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerConfigTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerConfigTest.java
@@ -44,12 +44,10 @@ public class TikaServerConfigTest {
         Set<String> settings = new HashSet<>();
         CommandLineParser parser = new DefaultParser();
         CommandLine emptyCommandLine = parser.parse(new Options(), new String[]{});
-        Path path = Paths.get(TikaConfigTest.class.getResource(
-                "/configs/tika-config-server.xml").toURI());
-        TikaServerConfig config = TikaServerConfig
-                .load(path,
-                        emptyCommandLine,
-                        settings);
+        Path path = Paths.get(TikaConfigTest.class
+                .getResource("/configs/tika-config-server.xml")
+                .toURI());
+        TikaServerConfig config = TikaServerConfig.load(path, emptyCommandLine, settings);
         assertEquals(-1, config.getMaxRestarts());
         assertEquals(54321, config.getTaskTimeoutMillis());
         assertEquals(true, config.isEnableUnsecureFeatures());
@@ -63,39 +61,47 @@ public class TikaServerConfigTest {
         Set<String> settings = new HashSet<>();
         CommandLineParser parser = new DefaultParser();
         CommandLine emptyCommandLine = parser.parse(new Options(), new String[]{});
-        Path path = Paths.get(TikaConfigTest.class.getResource(
-                "/configs/tika-config-server-fetchers-emitters.xml").toURI());
-        TikaServerConfig config = TikaServerConfig
-                .load(path,
-                        emptyCommandLine,
-                        settings);
+        Path path = Paths.get(TikaConfigTest.class
+                .getResource("/configs/tika-config-server-fetchers-emitters.xml")
+                .toURI());
+        TikaServerConfig config = TikaServerConfig.load(path, emptyCommandLine, settings);
         assertEquals(-1, config.getMaxRestarts());
         assertEquals(54321, config.getTaskTimeoutMillis());
         assertEquals(true, config.isEnableUnsecureFeatures());
-        assertEquals(1, config.getSupportedFetchers().size());
-        assertEquals(1, config.getSupportedEmitters().size());
-        assertTrue(config.getSupportedFetchers().contains("fsf"));
-        assertTrue(config.getSupportedEmitters().contains("fse"));
+        assertEquals(1, config
+                .getSupportedFetchers()
+                .size());
+        assertEquals(1, config
+                .getSupportedEmitters()
+                .size());
+        assertTrue(config
+                .getSupportedFetchers()
+                .contains("fsf"));
+        assertTrue(config
+                .getSupportedEmitters()
+                .contains("fse"));
     }
 
     @Test
     public void testPorts() throws Exception {
         CommandLineParser parser = new DefaultParser();
-        Path path = Paths.get(TikaConfigTest.class.getResource(
-                "/configs/tika-config-server.xml").toURI());
-        CommandLine commandLine =
-                parser.parse(
-                        new Options()
-                                .addOption(Option.builder("p").longOpt("port").hasArg().build())
-                                .addOption(Option.builder("c").longOpt("config").hasArg().build()
-                                ),
-                        new String[]{
-                                "-p", "9994-9999",
-                                "-c",
-                                ProcessUtils.escapeCommandLine(path.toAbsolutePath().toString())
-                        });
-        TikaServerConfig config = TikaServerConfig
-                .load(commandLine);
+        Path path = Paths.get(TikaConfigTest.class
+                .getResource("/configs/tika-config-server.xml")
+                .toURI());
+        CommandLine commandLine = parser.parse(new Options()
+                .addOption(Option
+                        .builder("p")
+                        .longOpt("port")
+                        .hasArg()
+                        .build())
+                .addOption(Option
+                        .builder("c")
+                        .longOpt("config")
+                        .hasArg()
+                        .build()), new String[]{"-p", "9994-9999", "-c", ProcessUtils.escapeCommandLine(path
+                .toAbsolutePath()
+                .toString())});
+        TikaServerConfig config = TikaServerConfig.load(commandLine);
         int[] ports = config.getPorts();
         assertEquals(6, ports.length);
         assertEquals(9994, ports[0]);
@@ -107,12 +113,10 @@ public class TikaServerConfigTest {
         Set<String> settings = new HashSet<>();
         CommandLineParser parser = new DefaultParser();
         CommandLine emptyCommandLine = parser.parse(new Options(), new String[]{});
-        Path path = Paths.get(TikaConfigTest.class.getResource(
-                "/configs/tika-config-server-tls.xml").toURI());
-        TikaServerConfig config = TikaServerConfig
-                .load(path,
-                        emptyCommandLine,
-                        settings);
+        Path path = Paths.get(TikaConfigTest.class
+                .getResource("/configs/tika-config-server-tls.xml")
+                .toURI());
+        TikaServerConfig config = TikaServerConfig.load(path, emptyCommandLine, settings);
         TlsConfig tlsConfig = config.getTlsConfig();
         assertTrue(tlsConfig.isActive());
         assertFalse(tlsConfig.isClientAuthenticationWanted());
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerIntegrationTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerIntegrationTest.java
index e9c3704db..00f2380db 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerIntegrationTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerIntegrationTest.java
@@ -56,7 +56,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import org.apache.tika.metadata.Metadata;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.config.TimeoutConfig;
 import org.apache.tika.utils.ProcessUtils;
 
@@ -70,23 +70,25 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
     private static Path TIKA_TLS_ONE_WAY_CONFIG;
     private static Path TIKA_TLS_TWO_WAY_CONFIG;
+
     @BeforeAll
     public static void setUpSSL() throws Exception {
-        TLS_KEYS =
-                Paths.get(TikaServerIntegrationTest.class.getResource("/ssl-keys").toURI());
+        TLS_KEYS = Paths.get(TikaServerIntegrationTest.class
+                .getResource("/ssl-keys")
+                .toURI());
 
-        String xml = IOUtils.resourceToString(
-                "/configs/tika-config-server-tls-two-way-template.xml",
-                UTF_8);
-        xml = xml.replace("{SSL_KEYS}", TLS_KEYS.toAbsolutePath().toString());
+        String xml = IOUtils.resourceToString("/configs/tika-config-server-tls-two-way-template.xml", UTF_8);
+        xml = xml.replace("{SSL_KEYS}", TLS_KEYS
+                .toAbsolutePath()
+                .toString());
 
         TIKA_TLS_TWO_WAY_CONFIG = Files.createTempFile(TLS_CONFIG, "tika-config-tls-", ".xml");
         Files.write(TIKA_TLS_TWO_WAY_CONFIG, xml.getBytes(UTF_8));
 
-        xml = IOUtils.resourceToString(
-                "/configs/tika-config-server-tls-one-way-template.xml",
-                UTF_8);
-        xml = xml.replace("{SSL_KEYS}", TLS_KEYS.toAbsolutePath().toString());
+        xml = IOUtils.resourceToString("/configs/tika-config-server-tls-one-way-template.xml", UTF_8);
+        xml = xml.replace("{SSL_KEYS}", TLS_KEYS
+                .toAbsolutePath()
+                .toString());
 
         TIKA_TLS_ONE_WAY_CONFIG = Files.createTempFile(TLS_CONFIG, "tika-config-tls-", ".xml");
         Files.write(TIKA_TLS_ONE_WAY_CONFIG, xml.getBytes(UTF_8));
@@ -109,7 +111,9 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
         Response response = null;
         try {
-            response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+            response = WebClient
+                    .create(endPoint + RMETA_PATH)
+                    .accept("application/json")
                     .put(ClassLoader.getSystemResourceAsStream(TEST_OOM));
         } catch (Exception e) {
             //oom may or may not cause an exception depending
@@ -124,13 +128,14 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     @Test
     public void testSameServerIdAfterOOM() throws Exception {
 
-        startProcess(new String[]{"-config", getConfig(
-                "tika-config-server-basic.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-basic.xml")});
         awaitServerStartup();
         String serverId = getServerId();
         Response response = null;
         try {
-            response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+            response = WebClient
+                    .create(endPoint + RMETA_PATH)
+                    .accept("application/json")
                     .put(ClassLoader.getSystemResourceAsStream(TEST_OOM));
         } catch (Exception e) {
             //oom may or may not cause an exception depending
@@ -146,28 +151,27 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
     @Test
     public void testMinimumTimeoutInHeader() throws Exception {
-        startProcess(new String[]{"-config", getConfig(
-                "tika-config-server-basic.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-basic.xml")});
         awaitServerStartup();
 
-        Response response = WebClient.create(endPoint + RMETA_PATH)
-                    .accept("application/json")
-                    .header(TimeoutConfig.X_TIKA_TIMEOUT_MILLIS, 1)
-                    .put(ClassLoader.getSystemResourceAsStream(TEST_HEAVY_HANG));
-        assertEquals(Response.Status.BAD_REQUEST.getStatusCode(),
-                    response.getStatus());
+        Response response = WebClient
+                .create(endPoint + RMETA_PATH)
+                .accept("application/json")
+                .header(TimeoutConfig.X_TIKA_TIMEOUT_MILLIS, 1)
+                .put(ClassLoader.getSystemResourceAsStream(TEST_HEAVY_HANG));
+        assertEquals(Response.Status.BAD_REQUEST.getStatusCode(), response.getStatus());
     }
 
     @Test
     public void testTaskTimeoutHeader() throws Exception {
 
-        startProcess(new String[]{"-config", getConfig(
-                "tika-config-server-basic.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-basic.xml")});
         awaitServerStartup();
         String serverId = getServerId();
         Response response = null;
         try {
-            response = WebClient.create(endPoint + RMETA_PATH)
+            response = WebClient
+                    .create(endPoint + RMETA_PATH)
                     .accept("application/json")
                     .header(TimeoutConfig.X_TIKA_TIMEOUT_MILLIS, 100)
                     .put(ClassLoader.getSystemResourceAsStream(TEST_HEAVY_HANG));
@@ -186,13 +190,13 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     @Test
     public void testSameDeclaredServerIdAfterOOM() throws Exception {
         String serverId = "qwertyuiop";
-        startProcess(
-                new String[]{"-config", getConfig("tika-config-server-basic.xml"), "-id",
-                        serverId});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-basic.xml"), "-id", serverId});
         awaitServerStartup();
         Response response = null;
         try {
-            response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+            response = WebClient
+                    .create(endPoint + RMETA_PATH)
+                    .accept("application/json")
                     .put(ClassLoader.getSystemResourceAsStream(TEST_OOM));
         } catch (Exception e) {
             //oom may or may not cause an exception depending
@@ -206,21 +210,27 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     }
 
     private String getServerId() throws Exception {
-        Response response =
-                WebClient.create(endPoint + STATUS_PATH).accept("application/json").get();
-        String jsonString =
-                CXFTestBase.getStringFromInputStream((InputStream) response.getEntity());
+        Response response = WebClient
+                .create(endPoint + STATUS_PATH)
+                .accept("application/json")
+                .get();
+        String jsonString = CXFTestBase.getStringFromInputStream((InputStream) response.getEntity());
         JsonNode root = new ObjectMapper().readTree(jsonString);
-        return root.get("server_id").asText();
+        return root
+                .get("server_id")
+                .asText();
     }
 
     private int getNumRestarts() throws Exception {
-        Response response =
-                WebClient.create(endPoint + STATUS_PATH).accept("application/json").get();
-        String jsonString =
-                CXFTestBase.getStringFromInputStream((InputStream) response.getEntity());
+        Response response = WebClient
+                .create(endPoint + STATUS_PATH)
+                .accept("application/json")
+                .get();
+        String jsonString = CXFTestBase.getStringFromInputStream((InputStream) response.getEntity());
         JsonNode root = new ObjectMapper().readTree(jsonString);
-        return root.get("num_restarts").intValue();
+        return root
+                .get("num_restarts")
+                .intValue();
     }
 
     @Test
@@ -230,7 +240,9 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
         awaitServerStartup();
         Response response = null;
         try {
-            response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+            response = WebClient
+                    .create(endPoint + RMETA_PATH)
+                    .accept("application/json")
                     .put(ClassLoader.getSystemResourceAsStream(TEST_SYSTEM_EXIT));
         } catch (Exception e) {
             //sys exit causes catchable problems for the client
@@ -244,12 +256,13 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
     @Test
     public void testTimeoutOk() throws Exception {
-        startProcess(
-                new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
         awaitServerStartup();
         Response response = null;
         try {
-            response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+            response = WebClient
+                    .create(endPoint + RMETA_PATH)
+                    .accept("application/json")
                     .put(ClassLoader.getSystemResourceAsStream(TEST_HEAVY_HANG_SHORT));
         } catch (Exception e) {
             //potential exception depending on timing
@@ -261,12 +274,13 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     @Test
     @Timeout(60000)
     public void testTimeout() throws Exception {
-        startProcess(
-                new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
         awaitServerStartup();
         Response response = null;
         try {
-            response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+            response = WebClient
+                    .create(endPoint + RMETA_PATH)
+                    .accept("application/json")
                     .put(ClassLoader.getSystemResourceAsStream(TEST_HEAVY_HANG));
         } catch (Exception e) {
             //catchable exception when server shuts down.
@@ -278,8 +292,7 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     @Test
     public void testBadJVMArgs() throws Exception {
 
-        startProcess(
-                new String[]{"-config", getConfig("tika-config-server-badjvmargs.xml"),});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-badjvmargs.xml"),});
 
         boolean finished = process.waitFor(10000, TimeUnit.MILLISECONDS);
         if (!finished) {
@@ -295,8 +308,12 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
     private String getConfig(String configName) {
         try {
-            return ProcessUtils.escapeCommandLine(Paths.get(TikaServerIntegrationTest.class.
-                    getResource("/configs/" + configName).toURI()).toAbsolutePath().toString());
+            return ProcessUtils.escapeCommandLine(Paths
+                    .get(TikaServerIntegrationTest.class
+                            .getResource("/configs/" + configName)
+                            .toURI())
+                    .toAbsolutePath()
+                    .toString());
         } catch (URISyntaxException e) {
             throw new RuntimeException(e);
         }
@@ -304,8 +321,12 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
     private String getSSL(String file) {
         try {
-            return Paths.get(TikaServerIntegrationTest.class.
-                    getResource("/ssl-keys/" + file).toURI()).toAbsolutePath().toString();
+            return Paths
+                    .get(TikaServerIntegrationTest.class
+                            .getResource("/ssl-keys/" + file)
+                            .toURI())
+                    .toAbsolutePath()
+                    .toString();
         } catch (URISyntaxException e) {
             throw new RuntimeException(e);
         }
@@ -314,25 +335,28 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
     @Test
     public void testStdErrOutBasic() throws Exception {
-        startProcess(
-                new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
         awaitServerStartup();
 
-        Response response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + RMETA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_STDOUT_STDERR));
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(1, metadataList.size());
-        assertContains("quick brown fox", metadataList.get(0).get("X-TIKA:content"));
+        assertContains("quick brown fox", metadataList
+                .get(0)
+                .get("X-TIKA:content"));
         testBaseline();
 
     }
 
     @Test
     public void test1WayTLS() throws Exception {
-        startProcess(
-                new String[]{"-config",
-                        ProcessUtils.escapeCommandLine(TIKA_TLS_ONE_WAY_CONFIG.toAbsolutePath().toString())});
+        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_TLS_ONE_WAY_CONFIG
+                .toAbsolutePath()
+                .toString())});
 
         String httpsEndpoint = "https://localhost:" + INTEGRATION_TEST_PORT;
         WebClient webClient = WebClient.create(httpsEndpoint);
@@ -344,31 +368,38 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
         webClient = WebClient.create(httpsEndpoint + RMETA_PATH);
         configure1WayTLS(webClient);
 
-        Response response = webClient.accept("application/json")
+        Response response = webClient
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
 
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(1, metadataList.size());
-        assertEquals("Nikolai Lobachevsky", metadataList.get(0).get("author"));
-        assertContains("hello world", metadataList.get(0).get("X-TIKA:content"));
+        assertEquals("Nikolai Lobachevsky", metadataList
+                .get(0)
+                .get("author"));
+        assertContains("hello world", metadataList
+                .get(0)
+                .get("X-TIKA:content"));
 
         //now test no tls config
         webClient = WebClient.create(httpsEndpoint + RMETA_PATH);
 
         try {
-            response = webClient.accept("application/json").put(
-                    ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
+            response = webClient
+                    .accept("application/json")
+                    .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
             fail("bad, bad, bad. this should have failed!");
         } catch (Exception e) {
             assertContains("javax.net.ssl.SSLHandshakeException", e.getMessage());
         }
     }
+
     @Test
     public void test2WayTLS() throws Exception {
-        startProcess(
-                new String[]{"-config",
-                        ProcessUtils.escapeCommandLine(TIKA_TLS_TWO_WAY_CONFIG.toAbsolutePath().toString())});
+        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_TLS_TWO_WAY_CONFIG
+                .toAbsolutePath()
+                .toString())});
 
         String httpsEndpoint = "https://localhost:" + INTEGRATION_TEST_PORT;
         WebClient webClient = WebClient.create(httpsEndpoint);
@@ -380,21 +411,27 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
         webClient = WebClient.create(httpsEndpoint + RMETA_PATH);
         configure2WayTLS(webClient);
 
-        Response response = webClient.accept("application/json")
+        Response response = webClient
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
 
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(1, metadataList.size());
-        assertEquals("Nikolai Lobachevsky", metadataList.get(0).get("author"));
-        assertContains("hello world", metadataList.get(0).get("X-TIKA:content"));
+        assertEquals("Nikolai Lobachevsky", metadataList
+                .get(0)
+                .get("author"));
+        assertContains("hello world", metadataList
+                .get(0)
+                .get("X-TIKA:content"));
 
         //now test that no tls config fails
         webClient = WebClient.create(httpsEndpoint + RMETA_PATH);
 
         try {
-            response = webClient.accept("application/json").put(
-                    ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
+            response = webClient
+                    .accept("application/json")
+                    .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
             fail("bad, bad, bad. this should have failed!");
         } catch (Exception e) {
             assertContains("javax.net.ssl.SSLHandshakeException", e.getMessage());
@@ -404,8 +441,9 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
         webClient = WebClient.create(httpsEndpoint + RMETA_PATH);
         configure1WayTLS(webClient);
         try {
-            response = webClient.accept("application/json").put(
-                    ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
+            response = webClient
+                    .accept("application/json")
+                    .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
             fail("bad, bad, bad. this should have failed!");
         } catch (Exception e) {
             //the messages vary too much between operating systems and
@@ -414,7 +452,8 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     }
 
     private void configure2WayTLS(WebClient webClient) throws GeneralSecurityException, IOException {
-        HTTPConduit conduit = WebClient.getConfig(webClient)
+        HTTPConduit conduit = WebClient
+                .getConfig(webClient)
                 .getHttpConduit();
         KeyStoreType keystore = new KeyStoreType();
         keystore.setType("PKCS12");
@@ -439,9 +478,9 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
 
     }
 
-    private void configure1WayTLS(WebClient webClient) throws GeneralSecurityException,
-            IOException {
-        HTTPConduit conduit = WebClient.getConfig(webClient)
+    private void configure1WayTLS(WebClient webClient) throws GeneralSecurityException, IOException {
+        HTTPConduit conduit = WebClient
+                .getConfig(webClient)
                 .getHttpConduit();
         TLSClientParameters parameters = new TLSClientParameters();
 
@@ -457,19 +496,21 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     }
 
     @Test
-    @Disabled("This works, but prints too much junk to the console.  " +
-            "Figure out how to gobble/redirect.")
+    @Disabled("This works, but prints too much junk to the console.  " + "Figure out how to gobble/redirect.")
     public void testStaticStdErrOutBasic() throws Exception {
-        startProcess(
-                new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
         awaitServerStartup();
 
-        Response response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + RMETA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_STATIC_STDOUT_STDERR));
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(1, metadataList.size());
-        assertContains("quick brown fox", metadataList.get(0).get("X-TIKA:content"));
+        assertContains("quick brown fox", metadataList
+                .get(0)
+                .get("X-TIKA:content"));
         testBaseline();
 
     }
@@ -480,22 +521,21 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
     public void testStdErrOutLogging() throws Exception {
         final AtomicInteger i = new AtomicInteger();
         Thread serverThread = new Thread(() -> TikaServerCli.main(
-                new String[]{
-                        "-p", INTEGRATION_TEST_PORT, "-taskTimeoutMillis",
-                        "10000", "-taskPulseMillis", "500", "-pingPulseMillis", "100",
-                        "-maxRestarts", "0",
-                        "-JDlog4j.configuration=file:" + LOG_FILE.toAbsolutePath(),
-                        "-tmpFilePrefix", "tika-server-stderrlogging"
-                }));
+                new String[]{"-p", INTEGRATION_TEST_PORT, "-taskTimeoutMillis", "10000", "-taskPulseMillis", "500", "-pingPulseMillis", "100", "-maxRestarts", "0",
+                        "-JDlog4j.configuration=file:" + LOG_FILE.toAbsolutePath(), "-tmpFilePrefix", "tika-server-stderrlogging"}));
         serverThread.start();
         awaitServerStartup();
 
-        Response response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + RMETA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_STDOUT_STDERR));
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(1, metadataList.size());
-        assertContains("quick brown fox", metadataList.get(0).get("X-TIKA:content"));
+        assertContains("quick brown fox", metadataList
+                .get(0)
+                .get("X-TIKA:content"));
 
         try {
             testBaseline();
@@ -545,8 +585,7 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
         //Can watch logs at least for confirmation of behavior
         //TODO: convert to real test
 
-        startProcess(
-                new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
+        startProcess(new String[]{"-config", getConfig("tika-config-server-timeout-10000.xml")});
         awaitServerStartup();
 
 
@@ -563,7 +602,9 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
                 } else if (r.nextFloat() < 0.02) {
                     file = TEST_HEAVY_HANG;
                 }
-                response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+                response = WebClient
+                        .create(endPoint + RMETA_PATH)
+                        .accept("application/json")
                         .put(ClassLoader.getSystemResourceAsStream(file));
             } catch (Exception e) {
                 ex = true;
@@ -575,12 +616,15 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
                 continue;
             }
             if (file.equals(TEST_HELLO_WORLD)) {
-                Reader reader =
-                        new InputStreamReader((InputStream) response.getEntity(), UTF_8);
+                Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
                 List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
                 assertEquals(1, metadataList.size());
-                assertEquals("Nikolai Lobachevsky", metadataList.get(0).get("author"));
-                assertContains("hello world", metadataList.get(0).get("X-TIKA:content"));
+                assertEquals("Nikolai Lobachevsky", metadataList
+                        .get(0)
+                        .get("author"));
+                assertContains("hello world", metadataList
+                        .get(0)
+                        .get("X-TIKA:content"));
             }
             //assertEquals("a38e6c7b38541af87148dee9634cb811",
             // metadataList.get(10).get("X-TIKA:digest:MD5"));
@@ -595,7 +639,9 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
             Response response = null;
 
             try {
-                response = WebClient.create(endPoint + RMETA_PATH).accept("application/json")
+                response = WebClient
+                        .create(endPoint + RMETA_PATH)
+                        .accept("application/json")
                         .put(ClassLoader.getSystemResourceAsStream(TEST_HELLO_WORLD));
             } catch (ProcessingException e) {
                 continue;
@@ -606,8 +652,12 @@ public class TikaServerIntegrationTest extends IntegrationTestBase {
             Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
             List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
             assertEquals(1, metadataList.size());
-            assertEquals("Nikolai Lobachevsky", metadataList.get(0).get("author"));
-            assertContains("hello world", metadataList.get(0).get("X-TIKA:content"));
+            assertEquals("Nikolai Lobachevsky", metadataList
+                    .get(0)
+                    .get("author"));
+            assertContains("hello world", metadataList
+                    .get(0)
+                    .get("X-TIKA:content"));
             return;
         }
         fail("should have completed within 3 tries");
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerPipesIntegrationTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerPipesIntegrationTest.java
index 658f9a7e8..00583ad06 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerPipesIntegrationTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerPipesIntegrationTest.java
@@ -43,26 +43,24 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import org.apache.tika.metadata.Metadata;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTuple;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
 import org.apache.tika.pipes.fetcher.FetchKey;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTuple;
 import org.apache.tika.utils.ProcessUtils;
 
 public class TikaServerPipesIntegrationTest extends IntegrationTestBase {
 
-    private static final Logger LOG =
-            LoggerFactory.getLogger(TikaServerPipesIntegrationTest.class);
+    private static final Logger LOG = LoggerFactory.getLogger(TikaServerPipesIntegrationTest.class);
     private static final String EMITTER_NAME = "fse";
     private static final String FETCHER_NAME = "fsf";
 
     private static Path TEMP_OUTPUT_DIR;
     private static Path TIKA_CONFIG;
     private static Path TIKA_CONFIG_TIMEOUT;
-    private static String[] FILES =
-            new String[]{"hello_world.xml", "heavy_hang_30000.xml", "fake_oom.xml",
-                    "system_exit.xml", "null_pointer.xml"};
+    private static String[] FILES = new String[]{"hello_world.xml", "heavy_hang_30000.xml", "fake_oom.xml", "system_exit.xml", "null_pointer.xml"};
 
     @BeforeAll
     public static void setUpBeforeClass() throws Exception {
@@ -72,39 +70,27 @@ public class TikaServerPipesIntegrationTest extends IntegrationTestBase {
         Files.createDirectories(TEMP_OUTPUT_DIR);
 
         for (String mockFile : FILES) {
-            Files.copy(
-                    TikaPipesTest.class.getResourceAsStream("/test-documents/mock/" + mockFile),
-                    inputDir.resolve(mockFile));
+            Files.copy(TikaPipesTest.class.getResourceAsStream("/test-documents/mock/" + mockFile), inputDir.resolve(mockFile));
         }
         TIKA_CONFIG = TEMP_WORKING_DIR.resolve("tika-config.xml");
         TIKA_CONFIG_TIMEOUT = TEMP_WORKING_DIR.resolve("tika-config-timeout.xml");
         //TODO -- clean this up so that port is sufficient and we don't need portString
-        String xml1 = "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" +
-                "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" +
-                "<name>" + FETCHER_NAME + "</name>" +
-                "<basePath>" + inputDir.toAbsolutePath() +
-                "</basePath>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
-                "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" +
-                "<name>" + EMITTER_NAME + "</name>" +
-                "<basePath>" + TEMP_OUTPUT_DIR.toAbsolutePath() +
-                "</basePath>" + "</emitter>" + "</emitters>" + "<server>" +
-                "<enableUnsecureFeatures>true</enableUnsecureFeatures>" + "<port>9999</port>" +
-                "<endpoints>" + "<endpoint>pipes</endpoint>" + "<endpoint>status</endpoint>" +
-                "</endpoints>";
-        String xml2 = "</server>" +
-                "<pipes><tikaConfig>" +
-                ProcessUtils.escapeCommandLine(TIKA_CONFIG.toAbsolutePath().toString()) +
-                "</tikaConfig><numClients>10</numClients><forkedJvmArgs><arg>-Xmx256m" +
-                "</arg>" + //TODO: need to add logging config here
-                "</forkedJvmArgs><timeoutMillis>5000</timeoutMillis>" +
-                "</pipes>" + "</properties>";
+        String xml1 =
+                "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" + "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" + "<name>" +
+                        FETCHER_NAME + "</name>" + "<basePath>" + inputDir.toAbsolutePath() + "</basePath>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
+                        "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" + "<name>" + EMITTER_NAME + "</name>" + "<basePath>" +
+                        TEMP_OUTPUT_DIR.toAbsolutePath() + "</basePath>" + "</emitter>" + "</emitters>" + "<server>" + "<enableUnsecureFeatures>true</enableUnsecureFeatures>" +
+                        "<port>9999</port>" + "<endpoints>" + "<endpoint>pipes</endpoint>" + "<endpoint>status</endpoint>" + "</endpoints>";
+        String xml2 = "</server>" + "<pipes><tikaConfig>" + ProcessUtils.escapeCommandLine(TIKA_CONFIG
+                .toAbsolutePath()
+                .toString()) + "</tikaConfig><numClients>10</numClients><forkedJvmArgs><arg>-Xmx256m" + "</arg>" + //TODO: need to add logging config here
+                "</forkedJvmArgs><timeoutMillis>5000</timeoutMillis>" + "</pipes>" + "</properties>";
 
         String tikaConfigXML = xml1 + xml2;
 
         FileUtils.write(TIKA_CONFIG.toFile(), tikaConfigXML, UTF_8);
 
-        String tikaConfigTimeoutXML = xml1 + "<taskPulseMillis>100</taskPulseMillis>" +
-                "<taskTimeoutMillis>10000</taskTimeoutMillis>" + xml2;
+        String tikaConfigTimeoutXML = xml1 + "<taskPulseMillis>100</taskPulseMillis>" + "<taskTimeoutMillis>10000</taskTimeoutMillis>" + xml2;
         FileUtils.write(TIKA_CONFIG_TIMEOUT.toFile(), tikaConfigTimeoutXML, UTF_8);
 
     }
@@ -129,52 +115,74 @@ public class TikaServerPipesIntegrationTest extends IntegrationTestBase {
 
     @Test
     public void testBasic() throws Exception {
-        startProcess(new String[]{"-config",
-                ProcessUtils.escapeCommandLine(TIKA_CONFIG.toAbsolutePath().toString())});
+        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_CONFIG
+                .toAbsolutePath()
+                .toString())});
         JsonNode node = testOne("hello_world.xml", true);
-        assertEquals("ok", node.get("status").asText());
+        assertEquals("ok", node
+                .get("status")
+                .asText());
 
     }
 
     @Test
     public void testNPEDefault() throws Exception {
 
-        startProcess(new String[]{"-config",
-                ProcessUtils.escapeCommandLine(TIKA_CONFIG.toAbsolutePath().toString())});
+        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_CONFIG
+                .toAbsolutePath()
+                .toString())});
         JsonNode node = testOne("null_pointer.xml", true);
-        assertEquals("ok", node.get("status").asText());
-        assertContains("java.lang.NullPointerException", node.get("parse_exception").asText());
+        assertEquals("ok", node
+                .get("status")
+                .asText());
+        assertContains("java.lang.NullPointerException", node
+                .get("parse_exception")
+                .asText());
     }
 
     @Test
     public void testNPESkip() throws Exception {
 
-        startProcess(new String[]{"-config",
-                ProcessUtils.escapeCommandLine(TIKA_CONFIG.toAbsolutePath().toString())});
-        JsonNode node =
-                testOne("null_pointer.xml", false, FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP);
-        assertEquals("ok", node.get("status").asText());
-        assertContains("java.lang.NullPointerException", node.get("parse_exception").asText());
+        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_CONFIG
+                .toAbsolutePath()
+                .toString())});
+        JsonNode node = testOne("null_pointer.xml", false, FetchEmitTuple.ON_PARSE_EXCEPTION.SKIP);
+        assertEquals("ok", node
+                .get("status")
+                .asText());
+        assertContains("java.lang.NullPointerException", node
+                .get("parse_exception")
+                .asText());
     }
 
     @Test
     public void testSystemExit() throws Exception {
-        startProcess(new String[]{"-config",
-                ProcessUtils.escapeCommandLine(TIKA_CONFIG.toAbsolutePath().toString())});
+        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_CONFIG
+                .toAbsolutePath()
+                .toString())});
         JsonNode node = testOne("system_exit.xml", false);
-        assertEquals("parse_error", node.get("status").asText());
-        assertContains("unknown_crash", node.get("parse_error").asText());
+        assertEquals("parse_error", node
+                .get("status")
+                .asText());
+        assertContains("unknown_crash", node
+                .get("parse_error")
+                .asText());
     }
 
     @Test
     public void testOOM() throws Exception {
 
         try {
-            startProcess(new String[]{"-config",
-                    ProcessUtils.escapeCommandLine(TIKA_CONFIG.toAbsolutePath().toString())});
+            startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_CONFIG
+                    .toAbsolutePath()
+                    .toString())});
             JsonNode node = testOne("fake_oom.xml", false);
-            assertEquals("parse_error", node.get("status").asText());
-            assertContains("oom", node.get("parse_error").asText());
+            assertEquals("parse_error", node
+                    .get("status")
+                    .asText());
+            assertContains("oom", node
+                    .get("parse_error")
+                    .asText());
         } catch (ProcessingException e) {
             //depending on timing, there may be a connection exception --
             // TODO add more of a delay to server shutdown to ensure message is sent
@@ -184,11 +192,16 @@ public class TikaServerPipesIntegrationTest extends IntegrationTestBase {
 
     @Test
     public void testTimeout() throws Exception {
-        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(
-                TIKA_CONFIG_TIMEOUT.toAbsolutePath().toString())});
+        startProcess(new String[]{"-config", ProcessUtils.escapeCommandLine(TIKA_CONFIG_TIMEOUT
+                .toAbsolutePath()
+                .toString())});
         JsonNode node = testOne("heavy_hang_30000.xml", false);
-        assertEquals("parse_error", node.get("status").asText());
-        assertContains("timeout", node.get("parse_error").asText());
+        assertEquals("parse_error", node
+                .get("status")
+                .asText());
+        assertContains("timeout", node
+                .get("parse_error")
+                .asText());
     }
 
 
@@ -196,8 +209,7 @@ public class TikaServerPipesIntegrationTest extends IntegrationTestBase {
         return testOne(fileName, shouldFileExist, FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT);
     }
 
-    private JsonNode testOne(String fileName, boolean shouldFileExist,
-                             FetchEmitTuple.ON_PARSE_EXCEPTION onParseException) throws Exception {
+    private JsonNode testOne(String fileName, boolean shouldFileExist, FetchEmitTuple.ON_PARSE_EXCEPTION onParseException) throws Exception {
 
         awaitServerStartup();
         Response response = WebClient
@@ -217,12 +229,10 @@ public class TikaServerPipesIntegrationTest extends IntegrationTestBase {
         return null;
     }
 
-    private String getJsonString(String fileName,
-                                 FetchEmitTuple.ON_PARSE_EXCEPTION onParseException)
-            throws IOException {
-        FetchEmitTuple t = new FetchEmitTuple(fileName, new FetchKey(FETCHER_NAME, fileName),
-                new EmitKey(EMITTER_NAME, ""), new Metadata(), HandlerConfig.DEFAULT_HANDLER_CONFIG,
-                onParseException);
+    private String getJsonString(String fileName, FetchEmitTuple.ON_PARSE_EXCEPTION onParseException) throws IOException {
+        ParseContext parseContext = new ParseContext();
+        parseContext.set(HandlerConfig.class, HandlerConfig.DEFAULT_HANDLER_CONFIG);
+        FetchEmitTuple t = new FetchEmitTuple(fileName, new FetchKey(FETCHER_NAME, fileName), new EmitKey(EMITTER_NAME, ""), new Metadata(), parseContext, onParseException);
         return JsonFetchEmitTuple.toJson(t);
     }
 }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerStatusTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerStatusTest.java
index 665f2ee8a..33af91858 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerStatusTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaServerStatusTest.java
@@ -38,13 +38,14 @@ import org.apache.tika.server.core.writer.JSONObjWriter;
 public class TikaServerStatusTest extends CXFTestBase {
 
     private final static String STATUS_PATH = "/status";
-    private final static String SERVER_ID = UUID.randomUUID().toString();
+    private final static String SERVER_ID = UUID
+            .randomUUID()
+            .toString();
 
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaServerStatus.class);
-        sf.setResourceProvider(TikaServerStatus.class, new SingletonResourceProvider(
-                new TikaServerStatus(new ServerStatus(SERVER_ID, 0))));
+        sf.setResourceProvider(TikaServerStatus.class, new SingletonResourceProvider(new TikaServerStatus(new ServerStatus(SERVER_ID, 0))));
     }
 
     @Override
@@ -56,17 +57,27 @@ public class TikaServerStatusTest extends CXFTestBase {
 
     @Test
     public void testBasic() throws Exception {
-        Response response = WebClient.create(endPoint + STATUS_PATH).get();
+        Response response = WebClient
+                .create(endPoint + STATUS_PATH)
+                .get();
         String jsonString = getStringFromInputStream((InputStream) response.getEntity());
         JsonNode root = new ObjectMapper().readTree(jsonString);
         assertTrue(root.has("server_id"));
         assertTrue(root.has("status"));
         assertTrue(root.has("millis_since_last_parse_started"));
         assertTrue(root.has("files_processed"));
-        assertEquals("OPERATING", root.get("status").asText());
-        assertEquals(0, root.get("files_processed").intValue());
-        long millis = root.get("millis_since_last_parse_started").longValue();
+        assertEquals("OPERATING", root
+                .get("status")
+                .asText());
+        assertEquals(0, root
+                .get("files_processed")
+                .intValue());
+        long millis = root
+                .get("millis_since_last_parse_started")
+                .longValue();
         assertTrue(millis >= 0 && millis < 360000);
-        assertEquals(SERVER_ID, root.get("server_id").asText());
+        assertEquals(SERVER_ID, root
+                .get("server_id")
+                .asText());
     }
 }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaVersionTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaVersionTest.java
index ed7471f50..95d2ab215 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaVersionTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaVersionTest.java
@@ -45,11 +45,12 @@ public class TikaVersionTest extends CXFTestBase {
 
     @Test
     public void testGetVersion() throws Exception {
-        Response response =
-                WebClient.create(endPoint + VERSION_PATH).type("text/plain").accept("text/plain")
-                        .get();
+        Response response = WebClient
+                .create(endPoint + VERSION_PATH)
+                .type("text/plain")
+                .accept("text/plain")
+                .get();
 
-        assertEquals(Tika.getString(),
-                getStringFromInputStream((InputStream) response.getEntity()));
+        assertEquals(Tika.getString(), getStringFromInputStream((InputStream) response.getEntity()));
     }
 }
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaWelcomeTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaWelcomeTest.java
index 428ec71f0..b50e4c202 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaWelcomeTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TikaWelcomeTest.java
@@ -57,9 +57,11 @@ public class TikaWelcomeTest extends CXFTestBase {
 
     @Test
     public void testGetHTMLWelcome() throws Exception {
-        String html =
-                WebClient.create(endPoint + WELCOME_PATH).type("text/html").accept("text/html")
-                        .get(String.class);
+        String html = WebClient
+                .create(endPoint + WELCOME_PATH)
+                .type("text/html")
+                .accept("text/html")
+                .get(String.class);
 
 
         assertContains(Tika.getString(), html);
@@ -76,9 +78,11 @@ public class TikaWelcomeTest extends CXFTestBase {
 
     @Test
     public void testGetTextWelcome() throws Exception {
-        Response response =
-                WebClient.create(endPoint + WELCOME_PATH).type("text/plain").accept("text/plain")
-                        .get();
+        Response response = WebClient
+                .create(endPoint + WELCOME_PATH)
+                .type("text/plain")
+                .accept("text/plain")
+                .get();
 
         String text = getStringFromInputStream((InputStream) response.getEntity());
         assertContains(Tika.getString(), text);
@@ -94,9 +98,11 @@ public class TikaWelcomeTest extends CXFTestBase {
 
     @Test
     public void testProperPathWelcome() throws Exception {
-        Response response =
-                WebClient.create(endPoint + WELCOME_PATH).type("text/html").accept("text/html")
-                        .get();
+        Response response = WebClient
+                .create(endPoint + WELCOME_PATH)
+                .type("text/html")
+                .accept("text/html")
+                .get();
 
         String html = getStringFromInputStream((InputStream) response.getEntity());
         assertContains(PATH_RESOURCE, html);
diff --git a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TranslateResourceTest.java b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TranslateResourceTest.java
index 20b67b52b..0081c1376 100644
--- a/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TranslateResourceTest.java
+++ b/tika-server/tika-server-core/src/test/java/org/apache/tika/server/core/TranslateResourceTest.java
@@ -39,17 +39,14 @@ public class TranslateResourceTest extends CXFTestBase {
     private static final String TRANSLATE_PATH = "/translate";
     private static final String TRANSLATE_ALL_PATH = TRANSLATE_PATH + "/all";
     private static final String TRANSLATE_TXT = "This won't translate";
-    private static final String LINGO_PATH =
-            "/org.apache.tika.language.translate.impl.Lingo24Translator";
+    private static final String LINGO_PATH = "/org.apache.tika.language.translate.impl.Lingo24Translator";
     private static final String SRCDEST = "/es/en";
     private static final String DEST = "/en";
 
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TranslateResource.class);
-        sf.setResourceProvider(TranslateResource.class,
-                new SingletonResourceProvider(new TranslateResource(
-                        new ServerStatus("", 0), 60000)));
+        sf.setResourceProvider(TranslateResource.class, new SingletonResourceProvider(new TranslateResource(new ServerStatus("", 0), 60000)));
 
     }
 
@@ -66,8 +63,11 @@ public class TranslateResourceTest extends CXFTestBase {
     @Test
     public void testTranslateFull() throws Exception {
         String url = endPoint + TRANSLATE_ALL_PATH + LINGO_PATH + SRCDEST;
-        Response response =
-                WebClient.create(url).type("text/plain").accept("*/*").put(TRANSLATE_TXT);
+        Response response = WebClient
+                .create(url)
+                .type("text/plain")
+                .accept("*/*")
+                .put(TRANSLATE_TXT);
         assertNotNull(response);
         String translated = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals(TRANSLATE_TXT, translated);
@@ -76,8 +76,11 @@ public class TranslateResourceTest extends CXFTestBase {
     @Test
     public void testTranslateAutoLang() throws Exception {
         String url = endPoint + TRANSLATE_ALL_PATH + LINGO_PATH + DEST;
-        Response response =
-                WebClient.create(url).type("text/plain").accept("*/*").put(TRANSLATE_TXT);
+        Response response = WebClient
+                .create(url)
+                .type("text/plain")
+                .accept("*/*")
+                .put(TRANSLATE_TXT);
         assertNotNull(response);
         String translated = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals(TRANSLATE_TXT, translated);
diff --git a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/PDFServerConfig.java b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/PDFServerConfig.java
index ee8aa8685..a95d3b9f8 100644
--- a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/PDFServerConfig.java
+++ b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/PDFServerConfig.java
@@ -48,18 +48,18 @@ public class PDFServerConfig implements ParseContextConfig {
      * @param parseContext the parse context to configure.
      */
     @Override
-    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata,
-                          ParseContext parseContext) {
+    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata, ParseContext parseContext) {
         //lazily initialize configs
         //if a header is submitted, any params set in --tika-config tika-config.xml
         //upon server startup will be ignored.
         PDFParserConfig pdfParserConfig = null;
         for (Map.Entry<String, List<String>> kvp : httpHeaders.entrySet()) {
             if (StringUtils.startsWithIgnoreCase(kvp.getKey(), X_TIKA_PDF_HEADER_PREFIX)) {
-                pdfParserConfig =
-                        (pdfParserConfig == null) ? new PDFParserConfig() : pdfParserConfig;
-                processHeaderConfig(pdfParserConfig, kvp.getKey(), kvp.getValue().get(0).trim(),
-                        X_TIKA_PDF_HEADER_PREFIX);
+                pdfParserConfig = (pdfParserConfig == null) ? new PDFParserConfig() : pdfParserConfig;
+                processHeaderConfig(pdfParserConfig, kvp.getKey(), kvp
+                        .getValue()
+                        .get(0)
+                        .trim(), X_TIKA_PDF_HEADER_PREFIX);
             }
         }
         if (pdfParserConfig != null) {
diff --git a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/TesseractServerConfig.java b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/TesseractServerConfig.java
index cdf1b5d1a..38ac4c08b 100644
--- a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/TesseractServerConfig.java
+++ b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/config/TesseractServerConfig.java
@@ -48,8 +48,7 @@ public class TesseractServerConfig implements ParseContextConfig {
      * @param parseContext the parse context to configure.
      */
     @Override
-    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata,
-                          ParseContext parseContext) {
+    public void configure(MultivaluedMap<String, String> httpHeaders, Metadata metadata, ParseContext parseContext) {
         //lazily initialize configs
         //if a header is submitted, any params set in --tika-config tika-config.xml
         //upon server startup will be ignored.
@@ -57,8 +56,10 @@ public class TesseractServerConfig implements ParseContextConfig {
         for (Map.Entry<String, List<String>> kvp : httpHeaders.entrySet()) {
             if (StringUtils.startsWithIgnoreCase(kvp.getKey(), X_TIKA_OCR_HEADER_PREFIX)) {
                 ocrConfig = (ocrConfig == null) ? new TesseractOCRConfig() : ocrConfig;
-                processHeaderConfig(ocrConfig, kvp.getKey(), kvp.getValue().get(0).trim(),
-                        X_TIKA_OCR_HEADER_PREFIX);
+                processHeaderConfig(ocrConfig, kvp.getKey(), kvp
+                        .getValue()
+                        .get(0)
+                        .trim(), X_TIKA_OCR_HEADER_PREFIX);
             }
         }
         if (ocrConfig != null) {
diff --git a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/resource/XMPMetadataResource.java b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/resource/XMPMetadataResource.java
index 3879f5c0a..a07182add 100644
--- a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/resource/XMPMetadataResource.java
+++ b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/resource/XMPMetadataResource.java
@@ -41,9 +41,7 @@ public class XMPMetadataResource extends MetadataResource implements TikaServerR
     @Path("{field}")
     @Produces({"application/rdf+xml"})
     @Override
-    public Response getMetadataField(InputStream is, @Context HttpHeaders httpHeaders,
-                                     @Context UriInfo info, @PathParam("field") String field)
-            throws Exception {
+    public Response getMetadataField(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info, @PathParam("field") String field) throws Exception {
         return super.getMetadataField(is, httpHeaders, info, field);
     }
 
@@ -51,20 +49,18 @@ public class XMPMetadataResource extends MetadataResource implements TikaServerR
     @Consumes("multipart/form-data")
     @Produces({"application/rdf+xml"})
     @Path("form")
-    public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info)
-            throws Exception {
-        return Response.ok(
-                parseMetadata(att.getObject(InputStream.class), new Metadata(), att.getHeaders(),
-                        info)).build();
+    public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info) throws Exception {
+        return Response
+                .ok(parseMetadata(att.getObject(InputStream.class), new Metadata(), att.getHeaders(), info))
+                .build();
     }
 
     @PUT
     @Produces({"application/rdf+xml"})
-    public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders,
-                                @Context UriInfo info) throws Exception {
+    public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
         Metadata metadata = new Metadata();
-        return Response.ok(
-                parseMetadata(TikaResource.getInputStream(is, metadata, httpHeaders, info),
-                        metadata, httpHeaders.getRequestHeaders(), info)).build();
+        return Response
+                .ok(parseMetadata(TikaResource.getInputStream(is, metadata, httpHeaders, info), metadata, httpHeaders.getRequestHeaders(), info))
+                .build();
     }
 }
diff --git a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/writer/XMPMessageBodyWriter.java b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/writer/XMPMessageBodyWriter.java
index 8a330fd08..7f1f7ac74 100644
--- a/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/writer/XMPMessageBodyWriter.java
+++ b/tika-server/tika-server-standard/src/main/java/org/apache/tika/server/standard/writer/XMPMessageBodyWriter.java
@@ -43,21 +43,17 @@ public class XMPMessageBodyWriter implements TikaServerWriter<Metadata> {
 
     private static MediaType RDF_XML = MediaType.valueOf("application/rdf+xml");
 
-    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations,
-                               MediaType mediaType) {
+    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return mediaType.equals(RDF_XML) && Metadata.class.isAssignableFrom(type);
     }
 
-    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations,
-                        MediaType mediaType) {
+    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
         return -1;
     }
 
     @Override
-    public void writeTo(Metadata metadata, Class<?> type, Type genericType,
-                        Annotation[] annotations, MediaType mediaType,
-                        MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream)
-            throws IOException, WebApplicationException {
+    public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders,
+                        OutputStream entityStream) throws IOException, WebApplicationException {
         try {
             Writer writer = new OutputStreamWriter(entityStream, UTF_8);
             XMPMetadata xmp = new XMPMetadata(metadata);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/DetectorResourceTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/DetectorResourceTest.java
index f3f51ecd1..d85321d81 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/DetectorResourceTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/DetectorResourceTest.java
@@ -47,8 +47,7 @@ public class DetectorResourceTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(DetectorResource.class);
-        sf.setResourceProvider(DetectorResource.class,
-                new SingletonResourceProvider(new DetectorResource(new ServerStatus("", 0))));
+        sf.setResourceProvider(DetectorResource.class, new SingletonResourceProvider(new DetectorResource(new ServerStatus("", 0))));
 
     }
 
@@ -65,10 +64,12 @@ public class DetectorResourceTest extends CXFTestBase {
     @Test
     public void testDetectCsvWithExt() throws Exception {
         String url = endPoint + DETECT_STREAM_PATH;
-        Response response =
-                WebClient.create(endPoint + DETECT_STREAM_PATH).type("text/csv").accept("*/*")
-                        .header("Content-Disposition", "attachment; filename=" + FOO_CSV)
-                        .put(ClassLoader.getSystemResourceAsStream(FOO_CSV));
+        Response response = WebClient
+                .create(endPoint + DETECT_STREAM_PATH)
+                .type("text/csv")
+                .accept("*/*")
+                .header("Content-Disposition", "attachment; filename=" + FOO_CSV)
+                .put(ClassLoader.getSystemResourceAsStream(FOO_CSV));
         assertNotNull(response);
         String readMime = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals("text/csv", readMime);
@@ -78,16 +79,21 @@ public class DetectorResourceTest extends CXFTestBase {
     @Test
     public void testDetectCsvNoExt() throws Exception {
 
-        Response response =
-                WebClient.create(endPoint + DETECT_STREAM_PATH).type("text/csv").accept("*/*")
-                        .header("Content-Disposition", "attachment; filename=" + CDEC_CSV_NO_EXT)
-                        .put(ClassLoader.getSystemResourceAsStream(CDEC_CSV_NO_EXT));
+        Response response = WebClient
+                .create(endPoint + DETECT_STREAM_PATH)
+                .type("text/csv")
+                .accept("*/*")
+                .header("Content-Disposition", "attachment; filename=" + CDEC_CSV_NO_EXT)
+                .put(ClassLoader.getSystemResourceAsStream(CDEC_CSV_NO_EXT));
         assertNotNull(response);
         String readMime = getStringFromInputStream((InputStream) response.getEntity());
         assertEquals("text/plain", readMime);
 
         // now trick it by adding .csv to the end
-        response = WebClient.create(endPoint + DETECT_STREAM_PATH).type("text/csv").accept("*/*")
+        response = WebClient
+                .create(endPoint + DETECT_STREAM_PATH)
+                .type("text/csv")
+                .accept("*/*")
                 .header("Content-Disposition", "attachment; filename=" + CDEC_CSV_NO_EXT + ".csv")
                 .put(ClassLoader.getSystemResourceAsStream(CDEC_CSV_NO_EXT));
         assertNotNull(response);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/FetcherTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/FetcherTest.java
index 79fc3e81d..0139e7104 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/FetcherTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/FetcherTest.java
@@ -37,8 +37,8 @@ import org.junit.jupiter.api.Test;
 import org.apache.tika.io.TikaInputStream;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
 import org.apache.tika.pipes.fetcher.FetcherManager;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.FetcherStreamFactory;
 import org.apache.tika.server.core.InputStreamFactory;
@@ -57,8 +57,7 @@ public class FetcherTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(RecursiveMetadataResource.class);
-        sf.setResourceProvider(RecursiveMetadataResource.class,
-                new SingletonResourceProvider(new RecursiveMetadataResource()));
+        sf.setResourceProvider(RecursiveMetadataResource.class, new SingletonResourceProvider(new RecursiveMetadataResource()));
     }
 
     @Override
@@ -85,12 +84,15 @@ public class FetcherTest extends CXFTestBase {
 
     @Test
     public void testBasic() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
-                .acceptEncoding("gzip").header("fetcherName", "url")
-                .header("fetchKey", "https://tika.apache.org").put("");
-
-        Reader reader = new InputStreamReader(
-                new GzipCompressorInputStream((InputStream) response.getEntity()), UTF_8);
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
+                .acceptEncoding("gzip")
+                .header("fetcherName", "url")
+                .header("fetchKey", "https://tika.apache.org")
+                .put("");
+
+        Reader reader = new InputStreamReader(new GzipCompressorInputStream((InputStream) response.getEntity()), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         Metadata parent = metadataList.get(0);
         String txt = parent.get(TikaCoreProperties.TIKA_CONTENT);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/JsonMaxFieldLengthTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/JsonMaxFieldLengthTest.java
index 84958c168..ddb45ec37 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/JsonMaxFieldLengthTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/JsonMaxFieldLengthTest.java
@@ -14,6 +14,7 @@ package org.apache.tika.server.standard;/*
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 import static java.nio.charset.StandardCharsets.UTF_8;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 
@@ -34,7 +35,7 @@ import org.junit.jupiter.api.io.TempDir;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadata;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.TikaServerParseExceptionMapper;
 import org.apache.tika.server.core.resource.TikaResource;
@@ -47,8 +48,7 @@ public class JsonMaxFieldLengthTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaResource.class);
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
     }
 
     @Override
@@ -72,14 +72,17 @@ public class JsonMaxFieldLengthTest extends CXFTestBase {
             sb.append("v");
         }
         Path tmp = Files.createTempFile(dir, "long-json-", ".txt");
-        Files.write(tmp, sb.toString().getBytes(UTF_8));
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH + "/text").accept("application/json")
-                        .put(Files.newInputStream(tmp));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Files.write(tmp, sb
+                .toString()
+                .getBytes(UTF_8));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "/text")
+                .accept("application/json")
+                .put(Files.newInputStream(tmp));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
         String t = metadata.get(TikaCoreProperties.TIKA_CONTENT);
-        assertEquals(30000000, t.trim().length());
+        assertEquals(30000000, t
+                .trim()
+                .length());
     }
 }
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/MetadataResourceTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/MetadataResourceTest.java
index a66ecfae5..e4bfc416c 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/MetadataResourceTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/MetadataResourceTest.java
@@ -42,7 +42,7 @@ import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadata;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.resource.MetadataResource;
 import org.apache.tika.server.core.writer.CSVMessageBodyWriter;
@@ -58,10 +58,8 @@ public class MetadataResourceTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(MetadataResource.class, XMPMetadataResource.class);
-        sf.setResourceProvider(MetadataResource.class,
-                new SingletonResourceProvider(new MetadataResource()));
-        sf.setResourceProvider(XMPMetadataResource.class,
-                new SingletonResourceProvider(new XMPMetadataResource()));
+        sf.setResourceProvider(MetadataResource.class, new SingletonResourceProvider(new MetadataResource()));
+        sf.setResourceProvider(XMPMetadataResource.class, new SingletonResourceProvider(new XMPMetadataResource()));
     }
 
     @Override
@@ -76,9 +74,11 @@ public class MetadataResourceTest extends CXFTestBase {
 
     @Test
     public void testSimpleWord() throws Exception {
-        Response response =
-                WebClient.create(endPoint + META_PATH).type("application/msword").accept("text/csv")
-                        .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC));
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/msword")
+                .accept("text/csv")
+                .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
 
@@ -93,32 +93,37 @@ public class MetadataResourceTest extends CXFTestBase {
 
         assertNotNull(metadata.get(TikaCoreProperties.CREATOR.getName()));
         assertEquals("Maxim Valyanskiy", metadata.get(TikaCoreProperties.CREATOR.getName()));
-        assertEquals("f8be45c34e8919eedba48cc8d207fbf0", metadata.get("X-TIKA:digest:MD5"),
-                "X-TIKA:digest:MD5");
+        assertEquals("f8be45c34e8919eedba48cc8d207fbf0", metadata.get("X-TIKA:digest:MD5"), "X-TIKA:digest:MD5");
     }
 
     @Test
     public void testPasswordProtected() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).type("application/vnd.ms-excel")
-                .accept("text/csv").put(ClassLoader.getSystemResourceAsStream(
-                        TikaResourceTest.TEST_PASSWORD_PROTECTED));
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/vnd.ms-excel")
+                .accept("text/csv")
+                .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_PASSWORD_PROTECTED));
 
         // Won't work, no password given
         assertEquals(500, response.getStatus());
 
         // Try again, this time with the wrong password
-        response = WebClient.create(endPoint + META_PATH).type("application/vnd.ms-excel")
-                .accept("text/csv").header("Password", "wrong password")
-                .put(ClassLoader.getSystemResourceAsStream(
-                        TikaResourceTest.TEST_PASSWORD_PROTECTED));
+        response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/vnd.ms-excel")
+                .accept("text/csv")
+                .header("Password", "wrong password")
+                .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_PASSWORD_PROTECTED));
 
         assertEquals(500, response.getStatus());
 
         // Try again, this time with the password
-        response = WebClient.create(endPoint + META_PATH).type("application/vnd.ms-excel")
-                .accept("text/csv").header("Password", "password")
-                .put(ClassLoader.getSystemResourceAsStream(
-                        TikaResourceTest.TEST_PASSWORD_PROTECTED));
+        response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/vnd.ms-excel")
+                .accept("text/csv")
+                .header("Password", "password")
+                .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_PASSWORD_PROTECTED));
 
         // Will work
         assertEquals(200, response.getStatus());
@@ -140,7 +145,9 @@ public class MetadataResourceTest extends CXFTestBase {
 
     @Test
     public void testJSON() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).type("application/msword")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/msword")
                 .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC));
 
@@ -153,7 +160,9 @@ public class MetadataResourceTest extends CXFTestBase {
 
     @Test
     public void testXMP() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).type("application/msword")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/msword")
                 .accept("application/rdf+xml")
                 .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC));
 
@@ -164,10 +173,11 @@ public class MetadataResourceTest extends CXFTestBase {
     //Now test requesting one field
     @Test
     public void testGetField_XXX_NotFound() throws Exception {
-        Response response =
-                WebClient.create(endPoint + META_PATH + "/xxx").type("application/msword")
-                        .accept(MediaType.APPLICATION_JSON)
-                        .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC));
+        Response response = WebClient
+                .create(endPoint + META_PATH + "/xxx")
+                .type("application/msword")
+                .accept(MediaType.APPLICATION_JSON)
+                .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC));
         assertEquals(Response.Status.NOT_FOUND.getStatusCode(), response.getStatus());
     }
 
@@ -176,9 +186,11 @@ public class MetadataResourceTest extends CXFTestBase {
 
         InputStream stream = ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC);
 
-        Response response =
-                WebClient.create(endPoint + META_PATH + "/Author").type("application/msword")
-                        .accept(MediaType.TEXT_PLAIN).put(copy(stream, 8000));
+        Response response = WebClient
+                .create(endPoint + META_PATH + "/Author")
+                .type("application/msword")
+                .accept(MediaType.TEXT_PLAIN)
+                .put(copy(stream, 8000));
         assertEquals(Response.Status.BAD_REQUEST.getStatusCode(), response.getStatus());
     }
 
@@ -187,10 +199,11 @@ public class MetadataResourceTest extends CXFTestBase {
 
         InputStream stream = ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC);
 
-        Response response =
-                WebClient.create(endPoint + META_PATH + "/" + TikaCoreProperties.CREATOR.getName())
-                        .type("application/msword").accept(MediaType.TEXT_PLAIN)
-                        .put(copy(stream, 12000));
+        Response response = WebClient
+                .create(endPoint + META_PATH + "/" + TikaCoreProperties.CREATOR.getName())
+                .type("application/msword")
+                .accept(MediaType.TEXT_PLAIN)
+                .put(copy(stream, 12000));
         assertEquals(Response.Status.OK.getStatusCode(), response.getStatus());
         String s = IOUtils.readStringFromStream((InputStream) response.getEntity());
         assertEquals("Maxim Valyanskiy", s);
@@ -201,13 +214,13 @@ public class MetadataResourceTest extends CXFTestBase {
 
         InputStream stream = ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC);
 
-        Response response =
-                WebClient.create(endPoint + META_PATH + "/" + TikaCoreProperties.CREATOR.getName())
-                        .type("application/msword").accept(MediaType.APPLICATION_JSON)
-                        .put(copy(stream, 12000));
+        Response response = WebClient
+                .create(endPoint + META_PATH + "/" + TikaCoreProperties.CREATOR.getName())
+                .type("application/msword")
+                .accept(MediaType.APPLICATION_JSON)
+                .put(copy(stream, 12000));
         assertEquals(Response.Status.OK.getStatusCode(), response.getStatus());
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader((InputStream) response.getEntity(), UTF_8));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader((InputStream) response.getEntity(), UTF_8));
         assertEquals("Maxim Valyanskiy", metadata.get(TikaCoreProperties.CREATOR));
         assertEquals(1, metadata.names().length);
     }
@@ -217,9 +230,11 @@ public class MetadataResourceTest extends CXFTestBase {
 
         InputStream stream = ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_DOC);
 
-        Response response =
-                WebClient.create(endPoint + META_PATH + "/dc:creator").type("application/msword")
-                        .accept("application/rdf+xml").put(copy(stream, 12000));
+        Response response = WebClient
+                .create(endPoint + META_PATH + "/dc:creator")
+                .type("application/msword")
+                .accept("application/rdf+xml")
+                .put(copy(stream, 12000));
         assertEquals(Response.Status.OK.getStatusCode(), response.getStatus());
         String s = IOUtils.readStringFromStream((InputStream) response.getEntity());
         assertContains("<rdf:li>Maxim Valyanskiy</rdf:li>", s);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OpenNLPMetadataFilterTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OpenNLPMetadataFilterTest.java
index 43f967e03..45d27c942 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OpenNLPMetadataFilterTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OpenNLPMetadataFilterTest.java
@@ -35,8 +35,8 @@ import org.junit.jupiter.api.Test;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.OfficeOpenXMLExtended;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.resource.RecursiveMetadataResource;
 import org.apache.tika.server.core.resource.TikaResource;
@@ -52,10 +52,8 @@ public class OpenNLPMetadataFilterTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(RecursiveMetadataResource.class, TikaResource.class);
-        sf.setResourceProvider(RecursiveMetadataResource.class,
-                new SingletonResourceProvider(new RecursiveMetadataResource()));
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(RecursiveMetadataResource.class, new SingletonResourceProvider(new RecursiveMetadataResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
 
     }
 
@@ -74,28 +72,39 @@ public class OpenNLPMetadataFilterTest extends CXFTestBase {
 
     @Test
     public void testMeta() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
 
         assertEquals(12, metadataList.size());
-        assertEquals("Microsoft Office Word",
-                metadataList.get(0).get(OfficeOpenXMLExtended.APPLICATION));
-        assertContains("plundered our seas", metadataList.get(6).get("X-TIKA:content"));
-
-        assertEquals("a38e6c7b38541af87148dee9634cb811",
-                metadataList.get(10).get("X-TIKA:digest:MD5"));
-
-        assertEquals("eng", metadataList.get(6).get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE));
-        assertEquals("LOW",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE_CONFIDENCE));
+        assertEquals("Microsoft Office Word", metadataList
+                .get(0)
+                .get(OfficeOpenXMLExtended.APPLICATION));
+        assertContains("plundered our seas", metadataList
+                .get(6)
+                .get("X-TIKA:content"));
+
+        assertEquals("a38e6c7b38541af87148dee9634cb811", metadataList
+                .get(10)
+                .get("X-TIKA:digest:MD5"));
+
+        assertEquals("eng", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE));
+        assertEquals("LOW", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE_CONFIDENCE));
     }
 
     @Test
     public void testTika() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OptimaizeMetadataFilterTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OptimaizeMetadataFilterTest.java
index d79e429b0..0ea81742e 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OptimaizeMetadataFilterTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/OptimaizeMetadataFilterTest.java
@@ -35,8 +35,8 @@ import org.junit.jupiter.api.Test;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.OfficeOpenXMLExtended;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadata;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.resource.RecursiveMetadataResource;
 import org.apache.tika.server.core.resource.TikaResource;
@@ -52,10 +52,8 @@ public class OptimaizeMetadataFilterTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(RecursiveMetadataResource.class, TikaResource.class);
-        sf.setResourceProvider(RecursiveMetadataResource.class,
-                new SingletonResourceProvider(new RecursiveMetadataResource()));
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(RecursiveMetadataResource.class, new SingletonResourceProvider(new RecursiveMetadataResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
 
     }
 
@@ -69,34 +67,44 @@ public class OptimaizeMetadataFilterTest extends CXFTestBase {
 
     @Override
     protected InputStream getTikaConfigInputStream() {
-        return getClass().getResourceAsStream(
-                "/config/tika-config-langdetect-optimaize-filter.xml");
+        return getClass().getResourceAsStream("/config/tika-config-langdetect-optimaize-filter.xml");
     }
 
     @Test
     public void testMeta() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
 
         assertEquals(12, metadataList.size());
-        assertEquals("Microsoft Office Word",
-                metadataList.get(0).get(OfficeOpenXMLExtended.APPLICATION));
-        assertContains("plundered our seas", metadataList.get(6).get("X-TIKA:content"));
-
-        assertEquals("a38e6c7b38541af87148dee9634cb811",
-                metadataList.get(10).get("X-TIKA:digest:MD5"));
-
-        assertEquals("en", metadataList.get(6).get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE));
-        assertEquals("HIGH",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE_CONFIDENCE));
+        assertEquals("Microsoft Office Word", metadataList
+                .get(0)
+                .get(OfficeOpenXMLExtended.APPLICATION));
+        assertContains("plundered our seas", metadataList
+                .get(6)
+                .get("X-TIKA:content"));
+
+        assertEquals("a38e6c7b38541af87148dee9634cb811", metadataList
+                .get(10)
+                .get("X-TIKA:digest:MD5"));
+
+        assertEquals("en", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE));
+        assertEquals("HIGH", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_DETECTED_LANGUAGE_CONFIDENCE));
     }
 
     @Test
     public void testTika() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataFilterTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataFilterTest.java
index cdc271150..5b11121bc 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataFilterTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataFilterTest.java
@@ -38,7 +38,7 @@ import org.apache.cxf.jaxrs.lifecycle.SingletonResourceProvider;
 import org.junit.jupiter.api.Test;
 
 import org.apache.tika.metadata.Metadata;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.resource.RecursiveMetadataResource;
 import org.apache.tika.server.core.writer.MetadataListMessageBodyWriter;
@@ -57,8 +57,7 @@ public class RecursiveMetadataFilterTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(RecursiveMetadataResource.class);
-        sf.setResourceProvider(RecursiveMetadataResource.class,
-                new SingletonResourceProvider(new RecursiveMetadataResource()));
+        sf.setResourceProvider(RecursiveMetadataResource.class, new SingletonResourceProvider(new RecursiveMetadataResource()));
     }
 
     @Override
@@ -70,12 +69,13 @@ public class RecursiveMetadataFilterTest extends CXFTestBase {
 
     @Test
     public void testBasicFilter() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .acceptEncoding("gzip")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
-        Reader reader = new InputStreamReader(
-                new GzipCompressorInputStream((InputStream) response.getEntity()), UTF_8);
+        Reader reader = new InputStreamReader(new GzipCompressorInputStream((InputStream) response.getEntity()), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(5, metadataList.size());
 
@@ -84,10 +84,14 @@ public class RecursiveMetadataFilterTest extends CXFTestBase {
         expectedKeys.add("extended-properties:Application");
         expectedKeys.add("Content-Type");
         for (Metadata m : metadataList) {
-            if (m.get(Metadata.CONTENT_TYPE).equals("image/emf")) {
+            if (m
+                    .get(Metadata.CONTENT_TYPE)
+                    .equals("image/emf")) {
                 fail("emf should have been filtered out");
             }
-            if (m.get(Metadata.CONTENT_TYPE).startsWith("text/plain")) {
+            if (m
+                    .get(Metadata.CONTENT_TYPE)
+                    .startsWith("text/plain")) {
                 fail("text/plain should have been filtered out");
             }
             assertTrue(m.names().length >= 2);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataResourceTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataResourceTest.java
index fde709e4e..029b429ed 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataResourceTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/RecursiveMetadataResourceTest.java
@@ -43,7 +43,7 @@ import org.apache.tika.TikaTest;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.OfficeOpenXMLExtended;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.serialization.JsonMetadataList;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.config.DocumentSelectorConfig;
 import org.apache.tika.server.core.resource.RecursiveMetadataResource;
@@ -64,8 +64,7 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(RecursiveMetadataResource.class);
-        sf.setResourceProvider(RecursiveMetadataResource.class,
-                new SingletonResourceProvider(new RecursiveMetadataResource()));
+        sf.setResourceProvider(RecursiveMetadataResource.class, new SingletonResourceProvider(new RecursiveMetadataResource()));
     }
 
     @Override
@@ -82,32 +81,41 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
 
     @Test
     public void testGZOut() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .acceptEncoding("gzip")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
-        Reader reader = new InputStreamReader(
-                new GzipCompressorInputStream((InputStream) response.getEntity()), UTF_8);
+        Reader reader = new InputStreamReader(new GzipCompressorInputStream((InputStream) response.getEntity()), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        assertEquals("Microsoft Office Word",
-                metadataList.get(0).get(OfficeOpenXMLExtended.APPLICATION));
-        assertContains("plundered our seas", metadataList.get(6).get("X-TIKA:content"));
-
-        assertEquals("a38e6c7b38541af87148dee9634cb811",
-                metadataList.get(10).get("X-TIKA:digest:MD5"));
+        assertEquals("Microsoft Office Word", metadataList
+                .get(0)
+                .get(OfficeOpenXMLExtended.APPLICATION));
+        assertContains("plundered our seas", metadataList
+                .get(6)
+                .get("X-TIKA:content"));
+
+        assertEquals("a38e6c7b38541af87148dee9634cb811", metadataList
+                .get(10)
+                .get("X-TIKA:digest:MD5"));
     }
 
     @Test
     public void testGZIn() throws Exception {
 
-        Response response =
-                WebClient.create(endPoint + META_PATH).accept("application/json").encoding("gzip")
-                        .put(gzip(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC)));
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
+                .encoding("gzip")
+                .put(gzip(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC)));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
-        String[] parsedBy = metadataList.get(0).getValues(TikaCoreProperties.TIKA_PARSED_BY);
+        String[] parsedBy = metadataList
+                .get(0)
+                .getValues(TikaCoreProperties.TIKA_PARSED_BY);
         //make sure the CompressorParser doesn't show up here
         assertEquals(3, parsedBy.length);
         assertEquals("org.apache.tika.parser.CompositeParser", parsedBy[0]);
@@ -116,30 +124,40 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
 
         //test that the rest is as it should be
         assertEquals(12, metadataList.size());
-        assertEquals("Microsoft Office Word",
-                metadataList.get(0).get(OfficeOpenXMLExtended.APPLICATION));
-        assertContains("plundered our seas", metadataList.get(6).get("X-TIKA:content"));
+        assertEquals("Microsoft Office Word", metadataList
+                .get(0)
+                .get(OfficeOpenXMLExtended.APPLICATION));
+        assertContains("plundered our seas", metadataList
+                .get(6)
+                .get("X-TIKA:content"));
 
-        assertEquals("a38e6c7b38541af87148dee9634cb811",
-                metadataList.get(10).get("X-TIKA:digest:MD5"));
+        assertEquals("a38e6c7b38541af87148dee9634cb811", metadataList
+                .get(10)
+                .get("X-TIKA:digest:MD5"));
 
     }
 
     @Test
     public void testSimpleWord() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
 
         assertEquals(12, metadataList.size());
-        assertEquals("Microsoft Office Word",
-                metadataList.get(0).get(OfficeOpenXMLExtended.APPLICATION));
-        assertContains("plundered our seas", metadataList.get(6).get("X-TIKA:content"));
-
-        assertEquals("a38e6c7b38541af87148dee9634cb811",
-                metadataList.get(10).get("X-TIKA:digest:MD5"));
+        assertEquals("Microsoft Office Word", metadataList
+                .get(0)
+                .get(OfficeOpenXMLExtended.APPLICATION));
+        assertContains("plundered our seas", metadataList
+                .get(6)
+                .get("X-TIKA:content"));
+
+        assertEquals("a38e6c7b38541af87148dee9634cb811", metadataList
+                .get(10)
+                .get("X-TIKA:digest:MD5"));
     }
 
     @Test
@@ -147,20 +165,26 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         MultivaluedMap<String, String> map = new MultivaluedHashMap<>();
         map.addAll("meta_mymeta", "first", "second", "third");
 
-        Response response =
-                WebClient.create(endPoint + META_PATH).headers(map).accept("application/json")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .headers(map)
+                .accept("application/json")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
-        assertEquals("first,second,third", metadataList.get(0).get("mymeta"));
+        assertEquals("first,second,third", metadataList
+                .get(0)
+                .get("mymeta"));
     }
 
     @Test
     public void testPasswordProtected() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).type("application/vnd.ms-excel")
-                .accept("application/json").put(ClassLoader.getSystemResourceAsStream(
-                        TikaResourceTest.TEST_PASSWORD_PROTECTED));
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/vnd.ms-excel")
+                .accept("application/json")
+                .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_PASSWORD_PROTECTED));
 
         // Won't work, no password given
         assertEquals(200, response.getStatus());
@@ -168,14 +192,19 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
 
-        assertNotNull(metadataList.get(0).get(TikaCoreProperties.CREATOR));
-        assertContains("org.apache.tika.exception.EncryptedDocumentException",
-                metadataList.get(0).get(TikaCoreProperties.CONTAINER_EXCEPTION));
+        assertNotNull(metadataList
+                .get(0)
+                .get(TikaCoreProperties.CREATOR));
+        assertContains("org.apache.tika.exception.EncryptedDocumentException", metadataList
+                .get(0)
+                .get(TikaCoreProperties.CONTAINER_EXCEPTION));
         // Try again, this time with the password
-        response = WebClient.create(endPoint + META_PATH).type("application/vnd.ms-excel")
-                .accept("application/json").header("Password", "password")
-                .put(ClassLoader.getSystemResourceAsStream(
-                        TikaResourceTest.TEST_PASSWORD_PROTECTED));
+        response = WebClient
+                .create(endPoint + META_PATH)
+                .type("application/vnd.ms-excel")
+                .accept("application/json")
+                .header("Password", "password")
+                .put(ClassLoader.getSystemResourceAsStream(TikaResourceTest.TEST_PASSWORD_PROTECTED));
 
         // Will work
         assertEquals(200, response.getStatus());
@@ -183,145 +212,199 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         // Check results
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
-        assertNotNull(metadataList.get(0).get(TikaCoreProperties.CREATOR));
-        assertEquals("pavel", metadataList.get(0).get(TikaCoreProperties.CREATOR));
+        assertNotNull(metadataList
+                .get(0)
+                .get(TikaCoreProperties.CREATOR));
+        assertEquals("pavel", metadataList
+                .get(0)
+                .get(TikaCoreProperties.CREATOR));
     }
 
     @Test
     public void testHandlerType() throws Exception {
         //default unspecified
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        String content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        String content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("<html xmlns=\"http://www.w3.org/1999/xhtml\">"));
 
         //extra slash
-        response = WebClient.create(endPoint + META_PATH + SLASH).accept("application/json")
+        response = WebClient
+                .create(endPoint + META_PATH + SLASH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("<html xmlns=\"http://www.w3.org/1999/xhtml\">"));
 
         //unparseable
-        response =
-                WebClient.create(endPoint + META_PATH + UNPARSEABLE_PATH).accept("application/json")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        response = WebClient
+                .create(endPoint + META_PATH + UNPARSEABLE_PATH)
+                .accept("application/json")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("<html xmlns=\"http://www.w3.org/1999/xhtml\">"));
 
         //xml
-        response = WebClient.create(endPoint + META_PATH + XML_PATH).accept("application/json")
+        response = WebClient
+                .create(endPoint + META_PATH + XML_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("<html xmlns=\"http://www.w3.org/1999/xhtml\">"));
 
         //text
-        response = WebClient.create(endPoint + META_PATH + TEXT_PATH).accept("application/json")
+        response = WebClient
+                .create(endPoint + META_PATH + TEXT_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("embed_3"));
 
         //ignore
-        response = WebClient.create(endPoint + META_PATH + IGNORE_PATH).accept("application/json")
+        response = WebClient
+                .create(endPoint + META_PATH + IGNORE_PATH)
+                .accept("application/json")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        assertNull(metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
+        assertNull(metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
 
     }
 
     @Test
     public void testHandlerTypeInMultipartXML() throws Exception {
         //default unspecified
-        Attachment attachmentPart = new Attachment("myworddocx",
-                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
-                ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        Attachment attachmentPart =
+                new Attachment("myworddocx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         WebClient webClient = WebClient.create(endPoint + META_PATH + FORM_PATH);
 
-        Response response = webClient.type("multipart/form-data").accept("application/json")
+        Response response = webClient
+                .type("multipart/form-data")
+                .accept("application/json")
                 .post(attachmentPart);
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        String content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        String content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("<html xmlns=\"http://www.w3.org/1999/xhtml\">"));
 
         //unparseable
-        attachmentPart = new Attachment("myworddocx",
-                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
-                ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        attachmentPart =
+                new Attachment("myworddocx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         webClient = WebClient.create(endPoint + META_PATH + FORM_PATH + UNPARSEABLE_PATH);
 
-        response = webClient.type("multipart/form-data").accept("application/json")
+        response = webClient
+                .type("multipart/form-data")
+                .accept("application/json")
                 .post(attachmentPart);
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("<html xmlns=\"http://www.w3.org/1999/xhtml\">"));
 
         //xml
-        attachmentPart = new Attachment("myworddocx",
-                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
-                ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        attachmentPart =
+                new Attachment("myworddocx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         webClient = WebClient.create(endPoint + META_PATH + FORM_PATH + XML_PATH);
 
-        response = webClient.type("multipart/form-data").accept("application/json")
+        response = webClient
+                .type("multipart/form-data")
+                .accept("application/json")
                 .post(attachmentPart);
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("<html xmlns=\"http://www.w3.org/1999/xhtml\">"));
 
         //text
-        attachmentPart = new Attachment("myworddocx",
-                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
-                ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        attachmentPart =
+                new Attachment("myworddocx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         webClient = WebClient.create(endPoint + META_PATH + FORM_PATH + TEXT_PATH);
 
-        response = webClient.type("multipart/form-data").accept("application/json")
+        response = webClient
+                .type("multipart/form-data")
+                .accept("application/json")
                 .post(attachmentPart);
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        content = metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT).trim();
+        content = metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT)
+                .trim();
         assertTrue(content.startsWith("embed_3"));
 
         //ignore -- no content
-        attachmentPart = new Attachment("myworddocx",
-                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
-                ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        attachmentPart =
+                new Attachment("myworddocx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         webClient = WebClient.create(endPoint + META_PATH + FORM_PATH + IGNORE_PATH);
 
-        response = webClient.type("multipart/form-data").accept("application/json")
-                .query("handler", "ignore").post(attachmentPart);
+        response = webClient
+                .type("multipart/form-data")
+                .accept("application/json")
+                .query("handler", "ignore")
+                .post(attachmentPart);
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        assertNull(metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
+        assertNull(metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
     }
 
     @Test
     public void testEmbeddedResourceLimit() throws Exception {
         for (int i : new int[]{0, 1, 5}) {
-            Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+            Response response = WebClient
+                    .create(endPoint + META_PATH)
+                    .accept("application/json")
                     .header("maxEmbeddedResources", Integer.toString(i))
                     .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
@@ -336,14 +419,18 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
     // TIKA-3227
     @Test
     public void testSkipEmbedded() throws Exception {
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .header(DocumentSelectorConfig.X_TIKA_SKIP_EMBEDDED_HEADER, "false")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
 
-        response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .header(DocumentSelectorConfig.X_TIKA_SKIP_EMBEDDED_HEADER, "true")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
@@ -354,7 +441,9 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
     @Test
     public void testWriteLimit() throws Exception {
         int writeLimit = 10;
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .header("writeLimit", Integer.toString(writeLimit))
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
@@ -363,11 +452,15 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(1, metadataList.size());
-        assertEquals("true", metadataList.get(0).get(TikaCoreProperties.WRITE_LIMIT_REACHED));
+        assertEquals("true", metadataList
+                .get(0)
+                .get(TikaCoreProperties.WRITE_LIMIT_REACHED));
 
         //now try with a write limit of 500
         writeLimit = 550;
-        response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .header("writeLimit", Integer.toString(writeLimit))
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
 
@@ -376,21 +469,26 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(10, metadataList.size());
-        assertEquals("true", metadataList.get(6).get(TikaCoreProperties.WRITE_LIMIT_REACHED));
-        assertContains("When in the Course of human events it becomes necessary for one people",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
-        TikaTest.assertNotContained("We hold these truths",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
+        assertEquals("true", metadataList
+                .get(6)
+                .get(TikaCoreProperties.WRITE_LIMIT_REACHED));
+        assertContains("When in the Course of human events it becomes necessary for one people", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
+        TikaTest.assertNotContained("We hold these truths", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
 
     }
 
     @Test
     public void testWriteLimitInPDF() throws Exception {
         int writeLimit = 10;
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .header("writeLimit", Integer.toString(writeLimit))
-                .put(ClassLoader.getSystemResourceAsStream(
-                        "test-documents/testPDFTwoTextBoxes" + ".pdf"));
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testPDFTwoTextBoxes" + ".pdf"));
 
         assertEquals(200, response.getStatus());
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
@@ -402,7 +500,9 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
     @Test
     public void testNoThrowOnWriteLimitReached() throws Exception {
         int writeLimit = 100;
-        Response response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .header("writeLimit", Integer.toString(writeLimit))
                 .header("throwOnWriteLimitReached", "false")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
@@ -412,11 +512,15 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         List<Metadata> metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        assertEquals("true", metadataList.get(0).get(TikaCoreProperties.WRITE_LIMIT_REACHED));
+        assertEquals("true", metadataList
+                .get(0)
+                .get(TikaCoreProperties.WRITE_LIMIT_REACHED));
 
         //now try with a write limit of 550
         writeLimit = 550;
-        response = WebClient.create(endPoint + META_PATH).accept("application/json")
+        response = WebClient
+                .create(endPoint + META_PATH)
+                .accept("application/json")
                 .header("writeLimit", Integer.toString(writeLimit))
                 .header("throwOnWriteLimitReached", "false")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
@@ -426,11 +530,15 @@ public class RecursiveMetadataResourceTest extends CXFTestBase {
         reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
         metadataList = JsonMetadataList.fromJson(reader);
         assertEquals(12, metadataList.size());
-        assertEquals("true", metadataList.get(0).get(TikaCoreProperties.WRITE_LIMIT_REACHED));
-        assertContains("When in the Course of human events it becomes necessary for one people",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
-        TikaTest.assertNotContained("We hold these truths",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
+        assertEquals("true", metadataList
+                .get(0)
+                .get(TikaCoreProperties.WRITE_LIMIT_REACHED));
+        assertContains("When in the Course of human events it becomes necessary for one people", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
+        TikaTest.assertNotContained("We hold these truths", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
 
     }
 
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaDetectorsTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaDetectorsTest.java
index 9ff0474f5..4e482bd39 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaDetectorsTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaDetectorsTest.java
@@ -45,8 +45,7 @@ public class TikaDetectorsTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaDetectors.class);
-        sf.setResourceProvider(TikaDetectors.class,
-                new SingletonResourceProvider(new TikaDetectors()));
+        sf.setResourceProvider(TikaDetectors.class, new SingletonResourceProvider(new TikaDetectors()));
     }
 
     @Override
@@ -55,9 +54,11 @@ public class TikaDetectorsTest extends CXFTestBase {
 
     @Test
     public void testGetPlainText() throws Exception {
-        Response response =
-                WebClient.create(endPoint + DETECTORS_PATH).type("text/plain").accept("text/plain")
-                        .get();
+        Response response = WebClient
+                .create(endPoint + DETECTORS_PATH)
+                .type("text/plain")
+                .accept("text/plain")
+                .get();
 
         String text = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("org.apache.tika.detect.DefaultDetector (Composite Detector)", text);
@@ -69,9 +70,11 @@ public class TikaDetectorsTest extends CXFTestBase {
 
     @Test
     public void testGetHTML() throws Exception {
-        Response response =
-                WebClient.create(endPoint + DETECTORS_PATH).type("text/html").accept("text/html")
-                        .get();
+        Response response = WebClient
+                .create(endPoint + DETECTORS_PATH)
+                .type("text/html")
+                .accept("text/html")
+                .get();
 
         String text = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("<h2>DefaultDetector</h2>", text);
@@ -90,12 +93,16 @@ public class TikaDetectorsTest extends CXFTestBase {
     @Test
     @SuppressWarnings("unchecked")
     public void testGetJSON() throws Exception {
-        Response response = WebClient.create(endPoint + DETECTORS_PATH)
+        Response response = WebClient
+                .create(endPoint + DETECTORS_PATH)
                 .type(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
-                .accept(jakarta.ws.rs.core.MediaType.APPLICATION_JSON).get();
+                .accept(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
+                .get();
 
         String jsonStr = getStringFromInputStream((InputStream) response.getEntity());
-        Map<String, Object> json = new ObjectMapper().readerFor(Map.class).readValue(jsonStr);
+        Map<String, Object> json = new ObjectMapper()
+                .readerFor(Map.class)
+                .readValue(jsonStr);
 
         // Should have a nested structure
         assertTrue(json.containsKey("name"));
@@ -116,16 +123,24 @@ public class TikaDetectorsTest extends CXFTestBase {
             assertEquals(false, d.containsKey("children"));
 
             String name = (String) d.get("name");
-            if (OggDetector.class.getName().equals(name)) {
+            if (OggDetector.class
+                    .getName()
+                    .equals(name)) {
                 hasOgg = true;
             }
-            if (POIFSContainerDetector.class.getName().equals(name)) {
+            if (POIFSContainerDetector.class
+                    .getName()
+                    .equals(name)) {
                 hasPOIFS = true;
             }
-            if (DefaultZipContainerDetector.class.getName().equals(name)) {
+            if (DefaultZipContainerDetector.class
+                    .getName()
+                    .equals(name)) {
                 hasZIP = true;
             }
-            if (MimeTypes.class.getName().equals(name)) {
+            if (MimeTypes.class
+                    .getName()
+                    .equals(name)) {
                 hasMime = true;
             }
         }
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaMimeTypesTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaMimeTypesTest.java
index db9d441ea..e1a511072 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaMimeTypesTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaMimeTypesTest.java
@@ -42,8 +42,7 @@ public class TikaMimeTypesTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaMimeTypes.class);
-        sf.setResourceProvider(TikaMimeTypes.class,
-                new SingletonResourceProvider(new TikaMimeTypes()));
+        sf.setResourceProvider(TikaMimeTypes.class, new SingletonResourceProvider(new TikaMimeTypes()));
     }
 
     @Override
@@ -53,13 +52,16 @@ public class TikaMimeTypesTest extends CXFTestBase {
     @Test
     @SuppressWarnings("unchecked")
     public void testGetJSON() throws Exception {
-        Response response = WebClient.create(endPoint + MIMETYPES_PATH)
+        Response response = WebClient
+                .create(endPoint + MIMETYPES_PATH)
                 .type(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
-                .accept(jakarta.ws.rs.core.MediaType.APPLICATION_JSON).get();
+                .accept(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
+                .get();
 
         String jsonStr = getStringFromInputStream((InputStream) response.getEntity());
-        Map<String, Map<String, Object>> json =
-                new ObjectMapper().readerFor(Map.class).readValue(jsonStr);
+        Map<String, Map<String, Object>> json = new ObjectMapper()
+                .readerFor(Map.class)
+                .readValue(jsonStr);
 
         assertEquals(true, json.containsKey("text/plain"));
         assertEquals(true, json.containsKey("application/xml"));
@@ -74,9 +76,10 @@ public class TikaMimeTypesTest extends CXFTestBase {
         assertEquals("image/x-bmp", aliases.get(0));
         assertEquals("image/x-ms-bmp", aliases.get(1));
 
-        String whichParser = bmp.get("parser").toString();
-        assertTrue(whichParser.equals("org.apache.tika.parser.ocr.TesseractOCRParser") ||
-                whichParser.equals("org.apache.tika.parser.image.ImageParser"), "Which parser");
+        String whichParser = bmp
+                .get("parser")
+                .toString();
+        assertTrue(whichParser.equals("org.apache.tika.parser.ocr.TesseractOCRParser") || whichParser.equals("org.apache.tika.parser.image.ImageParser"), "Which parser");
 
         Map<String, Object> ogm = json.get("video/x-ogm");
         assertEquals("video/ogg", ogm.get("supertype"));
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaParsersTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaParsersTest.java
index 4a87ee358..7b464f8da 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaParsersTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaParsersTest.java
@@ -64,8 +64,11 @@ public class TikaParsersTest extends CXFTestBase {
     @Test
     public void testGetPlainText() throws Exception {
         for (boolean details : new boolean[]{false, true}) {
-            Response response = WebClient.create(endPoint + getPath(details)).type("text/plain")
-                    .accept("text/plain").get();
+            Response response = WebClient
+                    .create(endPoint + getPath(details))
+                    .type("text/plain")
+                    .accept("text/plain")
+                    .get();
 
             String text = getStringFromInputStream((InputStream) response.getEntity());
             assertContains("org.apache.tika.parser.DefaultParser (Composite Parser)", text);
@@ -90,8 +93,11 @@ public class TikaParsersTest extends CXFTestBase {
     @Test
     public void testGetHTML() throws Exception {
         for (boolean details : new boolean[]{false, true}) {
-            Response response = WebClient.create(endPoint + getPath(details)).type("text/html")
-                    .accept("text/html").get();
+            Response response = WebClient
+                    .create(endPoint + getPath(details))
+                    .type("text/html")
+                    .accept("text/html")
+                    .get();
 
             String text = getStringFromInputStream((InputStream) response.getEntity());
             assertContains("<h3>DefaultParser</h3>", text);
@@ -123,13 +129,16 @@ public class TikaParsersTest extends CXFTestBase {
     @SuppressWarnings("unchecked")
     public void testGetJSON() throws Exception {
         for (boolean details : new boolean[]{false, true}) {
-            Response response = WebClient.create(endPoint + getPath(details))
+            Response response = WebClient
+                    .create(endPoint + getPath(details))
                     .type(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
-                    .accept(jakarta.ws.rs.core.MediaType.APPLICATION_JSON).get();
+                    .accept(jakarta.ws.rs.core.MediaType.APPLICATION_JSON)
+                    .get();
 
             String jsonStr = getStringFromInputStream((InputStream) response.getEntity());
-            Map<String, Map<String, Object>> json =
-                    new ObjectMapper().readerFor(Map.class).readValue(jsonStr);
+            Map<String, Map<String, Object>> json = new ObjectMapper()
+                    .readerFor(Map.class)
+                    .readValue(jsonStr);
 
             // Should have a nested structure
             assertEquals(true, json.containsKey("name"));
@@ -169,13 +178,19 @@ public class TikaParsersTest extends CXFTestBase {
                     }
 
                     String name = (String) grandChildren.get("name");
-                    if (OpusParser.class.getName().equals(name)) {
+                    if (OpusParser.class
+                            .getName()
+                            .equals(name)) {
                         hasOpus = true;
                     }
-                    if (OOXMLParser.class.getName().equals(name)) {
+                    if (OOXMLParser.class
+                            .getName()
+                            .equals(name)) {
                         hasOOXML = true;
                     }
-                    if (PackageParser.class.getName().equals(name)) {
+                    if (PackageParser.class
+                            .getName()
+                            .equals(name)) {
                         hasZip = true;
                     }
                 }
@@ -184,8 +199,7 @@ public class TikaParsersTest extends CXFTestBase {
             assertEquals(true, hasOOXML);
             assertEquals(true, hasZip);
             assertTrue(nonComposite > 20);
-            assertTrue(composite == 0 ||
-                    composite == 1); // if CompositeExternalParser is available it will be 1
+            assertTrue(composite == 0 || composite == 1); // if CompositeExternalParser is available it will be 1
         }
     }
 }
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaPipesTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaPipesTest.java
index 391e67fee..d84c18d41 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaPipesTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaPipesTest.java
@@ -23,6 +23,7 @@ import java.io.ByteArrayInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.Reader;
+import java.io.StringReader;
 import java.io.StringWriter;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.FileVisitResult;
@@ -49,8 +50,7 @@ import org.junit.jupiter.api.io.TempDir;
 import org.apache.tika.exception.TikaConfigException;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonFetchEmitTuple;
-import org.apache.tika.metadata.serialization.JsonMetadataList;
+import org.apache.tika.parser.ParseContext;
 import org.apache.tika.pipes.FetchEmitTuple;
 import org.apache.tika.pipes.HandlerConfig;
 import org.apache.tika.pipes.emitter.EmitKey;
@@ -58,6 +58,8 @@ import org.apache.tika.pipes.extractor.EmbeddedDocumentBytesConfig;
 import org.apache.tika.pipes.fetcher.FetchKey;
 import org.apache.tika.pipes.fetcher.FetcherManager;
 import org.apache.tika.sax.BasicContentHandlerFactory;
+import org.apache.tika.serialization.JsonMetadataList;
+import org.apache.tika.serialization.pipes.JsonFetchEmitTuple;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.FetcherStreamFactory;
 import org.apache.tika.server.core.InputStreamFactory;
@@ -93,35 +95,25 @@ public class TikaPipesTest extends CXFTestBase {
 
         Files.createDirectories(inputDir);
         Files.createDirectories(TMP_OUTPUT_DIR);
-        Files.copy(TikaPipesTest.class.getResourceAsStream("/test-documents/" + TEST_RECURSIVE_DOC),
-                inputDir.resolve("test_recursive_embedded.docx"),
+        Files.copy(TikaPipesTest.class.getResourceAsStream("/test-documents/" + TEST_RECURSIVE_DOC), inputDir.resolve("test_recursive_embedded.docx"),
                 StandardCopyOption.REPLACE_EXISTING);
 
         TIKA_CONFIG_PATH = Files.createTempFile(TMP_WORKING_DIR, "tika-pipes-", ".xml");
         TIKA_PIPES_LOG4j2_PATH = Files.createTempFile(TMP_WORKING_DIR, "log4j2-", ".xml");
-        Files.copy(TikaPipesTest.class.getResourceAsStream("/log4j2.xml"), TIKA_PIPES_LOG4j2_PATH,
-                StandardCopyOption.REPLACE_EXISTING);
+        Files.copy(TikaPipesTest.class.getResourceAsStream("/log4j2.xml"), TIKA_PIPES_LOG4j2_PATH, StandardCopyOption.REPLACE_EXISTING);
 
         //TODO: templatify this config
-        TIKA_CONFIG_XML =
-                "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" +
-                        "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" +
-                        "<params>" + "<name>fsf</name>" + "<basePath>" + inputDir.toAbsolutePath() +
-                        "</basePath>" + "</params>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
-                        "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" +
-                        "<params>" + "<name>fse</name>" + "<basePath>" +
-                        TMP_OUTPUT_DIR.toAbsolutePath() + "</basePath>" + "</params>" +
-                        "</emitter>" + "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" +
-                        "<params>" + "<name>bytes</name>" + "<basePath>" +
-                        TMP_BYTES_DIR.toAbsolutePath() + "</basePath>" + "</params>" +
-                        "</emitter>" + "</emitters>" + "<pipes><params><tikaConfig>" +
-                        ProcessUtils.escapeCommandLine(
-                                TIKA_CONFIG_PATH.toAbsolutePath().toString()) +
-                        "</tikaConfig><numClients>10</numClients>" + "<forkedJvmArgs>" +
-                        "<arg>-Xmx256m</arg>" + "<arg>-Dlog4j.configurationFile=file:" +
-                        ProcessUtils.escapeCommandLine(
-                                TIKA_PIPES_LOG4j2_PATH.toAbsolutePath().toString()) + "</arg>" +
-                        "</forkedJvmArgs>" + "</params></pipes>" + "</properties>";
+        TIKA_CONFIG_XML = "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<properties>" + "<fetchers>" + "<fetcher class=\"org.apache.tika.pipes.fetcher.fs.FileSystemFetcher\">" +
+                "<params>" + "<name>fsf</name>" + "<basePath>" + inputDir.toAbsolutePath() + "</basePath>" + "</params>" + "</fetcher>" + "</fetchers>" + "<emitters>" +
+                "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" + "<params>" + "<name>fse</name>" + "<basePath>" + TMP_OUTPUT_DIR.toAbsolutePath() +
+                "</basePath>" + "</params>" + "</emitter>" + "<emitter class=\"org.apache.tika.pipes.emitter.fs.FileSystemEmitter\">" + "<params>" + "<name>bytes</name>" +
+                "<basePath>" + TMP_BYTES_DIR.toAbsolutePath() + "</basePath>" + "</params>" + "</emitter>" + "</emitters>" + "<pipes><params><tikaConfig>" +
+                ProcessUtils.escapeCommandLine(TIKA_CONFIG_PATH
+                        .toAbsolutePath()
+                        .toString()) + "</tikaConfig><numClients>10</numClients>" + "<forkedJvmArgs>" + "<arg>-Xmx256m</arg>" + "<arg>-Dlog4j.configurationFile=file:" +
+                ProcessUtils.escapeCommandLine(TIKA_PIPES_LOG4j2_PATH
+                        .toAbsolutePath()
+                        .toString()) + "</arg>" + "</forkedJvmArgs>" + "</params></pipes>" + "</properties>";
         Files.write(TIKA_CONFIG_PATH, TIKA_CONFIG_XML.getBytes(StandardCharsets.UTF_8));
     }
 
@@ -169,15 +161,15 @@ public class TikaPipesTest extends CXFTestBase {
     @Test
     public void testBasic() throws Exception {
 
-        FetchEmitTuple t =
-                new FetchEmitTuple("myId", new FetchKey("fsf", "test_recursive_embedded.docx"),
-                        new EmitKey("fse", ""));
+        FetchEmitTuple t = new FetchEmitTuple("myId", new FetchKey("fsf", "test_recursive_embedded.docx"), new EmitKey("fse", ""));
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
 
         String getUrl = endPoint + PIPES_PATH;
-        Response response =
-                WebClient.create(getUrl).accept("application/json").post(writer.toString());
+        Response response = WebClient
+                .create(getUrl)
+                .accept("application/json")
+                .post(writer.toString());
         assertEquals(200, response.getStatus());
 
         List<Metadata> metadataList = null;
@@ -185,25 +177,31 @@ public class TikaPipesTest extends CXFTestBase {
             metadataList = JsonMetadataList.fromJson(reader);
         }
         assertEquals(12, metadataList.size());
-        assertContains("When in the Course",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("When in the Course", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
     }
 
     @Test
     public void testConcatenated() throws Exception {
+        ParseContext parseContext = new ParseContext();
+        HandlerConfig handlerConfig = new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.TEXT, HandlerConfig.PARSE_MODE.CONCATENATE, -1, -1, true);
+        parseContext.set(HandlerConfig.class, handlerConfig);
 
-        FetchEmitTuple t =
-                new FetchEmitTuple("myId", new FetchKey("fsf", "test_recursive_embedded.docx"),
-                        new EmitKey("fse", ""), new Metadata(),
-                        new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.TEXT,
-                                HandlerConfig.PARSE_MODE.CONCATENATE, -1, -1000, true),
-                        FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT);
+        FetchEmitTuple t = new FetchEmitTuple("myId", new FetchKey("fsf", "test_recursive_embedded.docx"),
+                new EmitKey("fse", ""), new Metadata(), parseContext,
+                FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
 
+        FetchEmitTuple deserialized = JsonFetchEmitTuple.fromJson(new StringReader(writer.toString()));
+        assertEquals(t, deserialized);
+
         String getUrl = endPoint + PIPES_PATH;
-        Response response =
-                WebClient.create(getUrl).accept("application/json").post(writer.toString());
+        Response response = WebClient
+                .create(getUrl)
+                .accept("application/json")
+                .post(writer.toString());
         assertEquals(200, response.getStatus());
 
         List<Metadata> metadataList = null;
@@ -211,8 +209,9 @@ public class TikaPipesTest extends CXFTestBase {
             metadataList = JsonMetadataList.fromJson(reader);
         }
         assertEquals(1, metadataList.size());
-        assertContains("When in the Course",
-                metadataList.get(0).get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("When in the Course", metadataList
+                .get(0)
+                .get(TikaCoreProperties.TIKA_CONTENT));
     }
 
     @Test
@@ -221,21 +220,24 @@ public class TikaPipesTest extends CXFTestBase {
         config.setEmitter("bytes");
         config.setIncludeOriginal(true);
         config.setEmbeddedIdPrefix("-");
-        config.setZeroPadNameLength(10);
+        config.setZeroPadName(10);
         config.setSuffixStrategy(EmbeddedDocumentBytesConfig.SUFFIX_STRATEGY.EXISTING);
-
+        ParseContext parseContext = new ParseContext();
+        parseContext.set(HandlerConfig.class, HandlerConfig.DEFAULT_HANDLER_CONFIG);
+        parseContext.set(EmbeddedDocumentBytesConfig.class, config);
         FetchEmitTuple t =
-                new FetchEmitTuple("myId", new FetchKey("fsf", "test_recursive_embedded.docx"),
-                        new EmitKey("fse", "test_recursive_embedded.docx"), new Metadata(),
-                        new HandlerConfig(BasicContentHandlerFactory.HANDLER_TYPE.TEXT,
-                                HandlerConfig.PARSE_MODE.RMETA, -1, -1, false),
-                        FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT, config);
+                new FetchEmitTuple("myId", new FetchKey("fsf", "test_recursive_embedded.docx"), new EmitKey("fse", "test_recursive_embedded.docx"), new Metadata(), parseContext,
+                        FetchEmitTuple.ON_PARSE_EXCEPTION.EMIT);
         StringWriter writer = new StringWriter();
         JsonFetchEmitTuple.toJson(t, writer);
+        FetchEmitTuple deserialized = JsonFetchEmitTuple.fromJson(new StringReader(writer.toString()));
 
+        assertEquals(t, deserialized);
         String getUrl = endPoint + PIPES_PATH;
-        Response response =
-                WebClient.create(getUrl).accept("application/json").post(writer.toString());
+        Response response = WebClient
+                .create(getUrl)
+                .accept("application/json")
+                .post(writer.toString());
         assertEquals(200, response.getStatus());
 
         List<Metadata> metadataList = null;
@@ -243,8 +245,9 @@ public class TikaPipesTest extends CXFTestBase {
             metadataList = JsonMetadataList.fromJson(reader);
         }
         assertEquals(12, metadataList.size());
-        assertContains("When in the Course",
-                metadataList.get(6).get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("When in the Course", metadataList
+                .get(6)
+                .get(TikaCoreProperties.TIKA_CONTENT));
         Map<String, Long> expected = loadExpected();
         Map<String, Long> byteFileNames = getFileNames(TMP_BYTES_DIR);
         assertEquals(expected, byteFileNames);
@@ -271,15 +274,15 @@ public class TikaPipesTest extends CXFTestBase {
         final Map<String, Long> ret = new HashMap<>();
         Files.walkFileTree(TMP_BYTES_DIR, new FileVisitor<Path>() {
             @Override
-            public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs)
-                    throws IOException {
+            public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException {
                 return FileVisitResult.CONTINUE;
             }
 
             @Override
-            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs)
-                    throws IOException {
-                ret.put(file.getFileName().toString(), Files.size(file));
+            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
+                ret.put(file
+                        .getFileName()
+                        .toString(), Files.size(file));
                 return FileVisitResult.CONTINUE;
             }
 
@@ -289,8 +292,7 @@ public class TikaPipesTest extends CXFTestBase {
             }
 
             @Override
-            public FileVisitResult postVisitDirectory(Path dir, IOException exc)
-                    throws IOException {
+            public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException {
                 return FileVisitResult.CONTINUE;
             }
         });
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaResourceTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaResourceTest.java
index 6d9d7cc2b..0b264dad2 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaResourceTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/TikaResourceTest.java
@@ -50,8 +50,8 @@ import org.apache.tika.TikaTest;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.metadata.OfficeOpenXMLExtended;
 import org.apache.tika.metadata.TikaCoreProperties;
-import org.apache.tika.metadata.serialization.JsonMetadata;
 import org.apache.tika.parser.ocr.TesseractOCRParser;
+import org.apache.tika.serialization.JsonMetadata;
 import org.apache.tika.server.core.CXFTestBase;
 import org.apache.tika.server.core.TikaServerParseExceptionMapper;
 import org.apache.tika.server.core.config.DocumentSelectorConfig;
@@ -76,8 +76,7 @@ public class TikaResourceTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(TikaResource.class);
-        sf.setResourceProvider(TikaResource.class,
-                new SingletonResourceProvider(new TikaResource()));
+        sf.setResourceProvider(TikaResource.class, new SingletonResourceProvider(new TikaResource()));
     }
 
     @Override
@@ -95,25 +94,32 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testHelloWorld() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("text/plain").accept("text/plain")
-                        .get();
-        assertEquals(TikaResource.GREETING,
-                getStringFromInputStream((InputStream) response.getEntity()));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("text/plain")
+                .accept("text/plain")
+                .get();
+        assertEquals(TikaResource.GREETING, getStringFromInputStream((InputStream) response.getEntity()));
     }
 
     @Test
     public void testSimpleWord() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).type("application/msword")
-                .accept("text/plain").put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/msword")
+                .accept("text/plain")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("test"));
     }
 
     @Test
     public void testWordGzipIn() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).type("application/msword")
-                .accept("text/plain").encoding("gzip")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/msword")
+                .accept("text/plain")
+                .encoding("gzip")
                 .put(gzip(ClassLoader.getSystemResourceAsStream(TEST_DOC)));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("test"));
@@ -123,13 +129,16 @@ public class TikaResourceTest extends CXFTestBase {
     public void testLongGzipOut() throws Exception {
         //if the output is long enough, jax-rs will compress it, otherwise it won't
         //this output is long enough, and should be compressed
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).accept("text/plain").acceptEncoding("gzip")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
-        assertTrue(response.getHeaders().containsKey(CONTENT_ENCODING));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
+                .acceptEncoding("gzip")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        assertTrue(response
+                .getHeaders()
+                .containsKey(CONTENT_ENCODING));
         assertEquals("gzip", response.getHeaderString(CONTENT_ENCODING));
-        String responseMsg = getStringFromInputStream(
-                new GzipCompressorInputStream((InputStream) response.getEntity()));
+        String responseMsg = getStringFromInputStream(new GzipCompressorInputStream((InputStream) response.getEntity()));
         assertTrue(responseMsg.contains("Course of human"));
     }
 
@@ -137,10 +146,14 @@ public class TikaResourceTest extends CXFTestBase {
     public void testShortGzipOut() throws Exception {
         //if the output is long enough, jax-rs will compress it, otherwise it won't
         //this output is short enough, and should not be compressed
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).accept("text/plain").acceptEncoding("gzip")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
-        assertFalse(response.getHeaders().containsKey(CONTENT_ENCODING));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
+                .acceptEncoding("gzip")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
+        assertFalse(response
+                .getHeaders()
+                .containsKey(CONTENT_ENCODING));
 
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("test"));
@@ -149,7 +162,9 @@ public class TikaResourceTest extends CXFTestBase {
     @Test
     public void testTextMain() throws Exception {
         //boilerpipe
-        Response response = WebClient.create(endPoint + TIKA_PATH + "/main").accept("text/plain")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "/main")
+                .accept("text/plain")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/testHTML.html"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("Title : Test Indexation Html"));
@@ -159,13 +174,14 @@ public class TikaResourceTest extends CXFTestBase {
     @Test
     public void testTextMainMultipart() throws Exception {
         //boilerpipe
-        Attachment attachmentPart = new Attachment("myhtml", "text/html",
-                ClassLoader.getSystemResourceAsStream("test-documents/testHTML.html"));
+        Attachment attachmentPart = new Attachment("myhtml", "text/html", ClassLoader.getSystemResourceAsStream("test-documents/testHTML.html"));
 
 
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH + "/form/main").type("multipart/form-data")
-                        .accept("text/plain").post(attachmentPart);
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "/form/main")
+                .type("multipart/form-data")
+                .accept("text/plain")
+                .post(attachmentPart);
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("Title : Test Indexation Html"));
         assertFalse(responseMsg.contains("Indexation du fichier"));
@@ -174,7 +190,9 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testPasswordXLS() {
-        Response response = WebClient.create(endPoint + TIKA_PATH).type("application/vnd.ms-excel")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/vnd.ms-excel")
                 .accept("text/plain")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/password.xls"));
 
@@ -183,21 +201,22 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testSimpleWordHTML() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).type("application/msword")
-                .accept("text/html").put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/msword")
+                .accept("text/html")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("test"));
-        assertContains(
-                "<meta name=\"X-TIKA:digest:MD5\" content=\"f8be45c34e8919eedba48cc8d207fbf0\"/>",
-                responseMsg);
-        assertContains(
-                "<meta name=\"X-TIKA:digest:SHA1\" content=\"N4EBCE7EGTIGZWETEJ6WD3W4KN32TLPG\"/>",
-                responseMsg);
+        assertContains("<meta name=\"X-TIKA:digest:MD5\" content=\"f8be45c34e8919eedba48cc8d207fbf0\"/>", responseMsg);
+        assertContains("<meta name=\"X-TIKA:digest:SHA1\" content=\"N4EBCE7EGTIGZWETEJ6WD3W4KN32TLPG\"/>", responseMsg);
     }
 
     @Test
     public void testPasswordXLSHTML() {
-        Response response = WebClient.create(endPoint + TIKA_PATH).type("application/vnd.ms-excel")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/vnd.ms-excel")
                 .accept("text/html")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/password.xls"));
 
@@ -206,16 +225,20 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testSimpleWordXML() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/msword").accept("text/xml")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/msword")
+                .accept("text/xml")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("test"));
     }
 
     @Test
     public void testPasswordXLSXML() {
-        Response response = WebClient.create(endPoint + TIKA_PATH).type("application/vnd.ms-excel")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/vnd.ms-excel")
                 .accept("text/xml")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/password.xls"));
 
@@ -225,16 +248,15 @@ public class TikaResourceTest extends CXFTestBase {
     @Test
     public void testSimpleWordMultipartXML() throws Exception {
         ClassLoader.getSystemResourceAsStream(TEST_DOC);
-        Attachment attachmentPart = new Attachment("myworddoc", "application/msword",
-                ClassLoader.getSystemResourceAsStream(TEST_DOC));
+        Attachment attachmentPart = new Attachment("myworddoc", "application/msword", ClassLoader.getSystemResourceAsStream(TEST_DOC));
         WebClient webClient = WebClient.create(endPoint + TIKA_PATH + "/form");
-        Response response =
-                webClient.type("multipart/form-data").accept("text/xml").post(attachmentPart);
+        Response response = webClient
+                .type("multipart/form-data")
+                .accept("text/xml")
+                .post(attachmentPart);
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("test"));
-        assertContains(
-                "<meta name=\"X-TIKA:digest:MD5\" content=\"f8be45c34e8919eedba48cc8d207fbf0\"/>",
-                responseMsg);
+        assertContains("<meta name=\"X-TIKA:digest:MD5\" content=\"f8be45c34e8919eedba48cc8d207fbf0\"/>", responseMsg);
 
     }
 
@@ -247,29 +269,32 @@ public class TikaResourceTest extends CXFTestBase {
     @Test
     public void testEmbedded() throws Exception {
         //first try text
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("Course of human events"));
 
         //now go for xml -- different call than text
-        response = WebClient.create(endPoint + TIKA_PATH).accept("text/xml")
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/xml")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("Course of human events"));
-        assertContains(
-                "<meta name=\"X-TIKA:digest:MD5\" content=\"59f626e09a8c16ab6dbc2800c685f772\"/>",
-                responseMsg);
+        assertContains("<meta name=\"X-TIKA:digest:MD5\" content=\"59f626e09a8c16ab6dbc2800c685f772\"/>", responseMsg);
 
     }
 
     //TIKA-1845
     @Test
     public void testWMFInRTF() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/rtf").accept("text/plain")
-                        .put(ClassLoader.getSystemResourceAsStream(
-                                "test-documents/testRTF_npeFromWMFInTikaServer.rtf"));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/rtf")
+                .accept("text/plain")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testRTF_npeFromWMFInTikaServer.rtf"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("Example text"));
     }
@@ -281,12 +306,13 @@ public class TikaResourceTest extends CXFTestBase {
             return;
         }
 
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "OcrStrategy", "ocr_only")
                 .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "Language", "eng+fra")
                 .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "MinFileSizeToOcr", "10")
-                .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "MaxFileSizeToOcr",
-                        "1000000000")
+                .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "MaxFileSizeToOcr", "1000000000")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("Happy New Year 2003!", responseMsg);
@@ -299,37 +325,43 @@ public class TikaResourceTest extends CXFTestBase {
             return;
         }
 
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "OcrStrategy", "no_ocr")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "OcrStrategy", "no_ocr")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
 
         assertEquals("", responseMsg.trim());
 
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "skipOcr", "true")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "skipOcr", "true")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
 
         assertEquals("", responseMsg.trim());
 
 
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "OcrStrategy",
-                                "ocr_only")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "OcrStrategy", "ocr_only")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("Happy New Year 2003!", responseMsg);
 
         //now try a bad value
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "OcrStrategy",
-                                "non-sense-value")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "OcrStrategy", "non-sense-value")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         assertEquals(400, response.getStatus());
     }
 
@@ -340,39 +372,43 @@ public class TikaResourceTest extends CXFTestBase {
             return;
         }
 
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) +
-                                "ocrstrategy", "no_ocr")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) + "ocrstrategy", "no_ocr")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
 
         assertEquals("", responseMsg.trim());
 
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX.toLowerCase(
-                                Locale.ROOT) + "skipocr", "true")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX.toLowerCase(Locale.ROOT) + "skipocr", "true")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
 
         assertEquals("", responseMsg.trim());
 
 
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) +
-                                "ocrstrategy", "ocr_only")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) + "ocrstrategy", "ocr_only")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("Happy New Year 2003!", responseMsg);
 
         //now try a bad value
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) +
-                                "ocrstrategy", "non-sense-value")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) + "ocrstrategy", "non-sense-value")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         assertEquals(400, response.getStatus());
     }
 
@@ -380,37 +416,40 @@ public class TikaResourceTest extends CXFTestBase {
     @Test
     public void testPDFConfig() throws Exception {
 
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .put(ClassLoader.getSystemResourceAsStream(
-                                "test-documents/testPDFTwoTextBoxes.pdf"));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testPDFTwoTextBoxes.pdf"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
-        responseMsg = responseMsg.replaceAll("[\r\n ]+", " ").trim();
-        assertEquals(
-                "Left column line 1 Right column line 1 Left colu mn line 2 Right column line 2",
-                responseMsg);
-
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "sortByPosition",
-                                "false").put(ClassLoader.getSystemResourceAsStream(
-                                "test-documents/testPDFTwoTextBoxes.pdf"));
+        responseMsg = responseMsg
+                .replaceAll("[\r\n ]+", " ")
+                .trim();
+        assertEquals("Left column line 1 Right column line 1 Left colu mn line 2 Right column line 2", responseMsg);
+
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "sortByPosition", "false")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testPDFTwoTextBoxes.pdf"));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
-        responseMsg = responseMsg.replaceAll("[\r\n ]+", " ").trim();
-        assertEquals(
-                "Left column line 1 Left column line 2 Right column line 1 Right column line 2",
-                responseMsg);
+        responseMsg = responseMsg
+                .replaceAll("[\r\n ]+", " ")
+                .trim();
+        assertEquals("Left column line 1 Left column line 2 Right column line 1 Right column line 2", responseMsg);
 
         //make sure that default reverts to initial config option
-        response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .put(ClassLoader.getSystemResourceAsStream(
-                                "test-documents/testPDFTwoTextBoxes.pdf"));
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testPDFTwoTextBoxes.pdf"));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
-        responseMsg = responseMsg.replaceAll("[\r\n ]+", " ").trim();
-        assertEquals(
-                "Left column line 1 Right column line 1 Left colu mn line 2 Right column line 2",
-                responseMsg);
+        responseMsg = responseMsg
+                .replaceAll("[\r\n ]+", " ")
+                .trim();
+        assertEquals("Left column line 1 Right column line 1 Left colu mn line 2 Right column line 2", responseMsg);
 
     }
 
@@ -418,13 +457,14 @@ public class TikaResourceTest extends CXFTestBase {
     @Test
     public void testExtractTextAcceptPlainText() throws Exception {
         //TIKA-2384
-        Attachment attachmentPart = new Attachment("my-docx-file",
-                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
+        Attachment attachmentPart = new Attachment("my-docx-file", "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                 ClassLoader.getSystemResourceAsStream("test-documents/2pic.docx"));
 
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH + "/form").type("multipart/form-data")
-                        .accept("text/plain").post(attachmentPart);
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "/form")
+                .type("multipart/form-data")
+                .accept("text/plain")
+                .post(attachmentPart);
 
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertTrue(responseMsg.contains("P1040893.JPG"));
@@ -435,10 +475,11 @@ public class TikaResourceTest extends CXFTestBase {
     public void testDataIntegrityCheck() {
         Response response;
         try {
-            response = WebClient.create(endPoint + TIKA_PATH).type("application/pdf")
+            response = WebClient
+                    .create(endPoint + TIKA_PATH)
+                    .type("application/pdf")
                     .accept("text/plain")
-                    .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "tesseractPath",
-                            "C://tmp//hello.bat\u0000")
+                    .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "tesseractPath", "C://tmp//hello.bat\u0000")
                     .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
             assertEquals(400, response.getStatus());
         } catch (ProcessingException e) {
@@ -447,10 +488,11 @@ public class TikaResourceTest extends CXFTestBase {
         }
 
         try {
-            response = WebClient.create(endPoint + TIKA_PATH).type("application/pdf")
+            response = WebClient
+                    .create(endPoint + TIKA_PATH)
+                    .type("application/pdf")
                     .accept("text/plain")
-                    .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "tesseractPath",
-                            "bogus path")
+                    .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "tesseractPath", "bogus path")
                     .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
             assertEquals(400, response.getStatus());
         } catch (ProcessingException e) {
@@ -460,22 +502,24 @@ public class TikaResourceTest extends CXFTestBase {
 
     @Test
     public void testTrustedMethodPrevention() {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX +
-                                "trustedPageSeparator", "\u0020")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(TesseractServerConfig.X_TIKA_OCR_HEADER_PREFIX + "trustedPageSeparator", "\u0020")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         assertEquals(400, response.getStatus());
 
     }
 
     @Test
     public void testFloatInHeader() {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH).type("application/pdf").accept("text/plain")
-                        .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "averageCharTolerance",
-                                "2.0")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .type("application/pdf")
+                .accept("text/plain")
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "averageCharTolerance", "2.0")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
         assertEquals(200, response.getStatus());
 
     }
@@ -484,12 +528,12 @@ public class TikaResourceTest extends CXFTestBase {
     public void testUnicodePasswordProtectedSpaces() throws Exception {
         //TIKA-2858
         final String password = "    ";
-        final String encoded =
-                new Base64().encodeAsString(password.getBytes(StandardCharsets.UTF_8));
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+        final String encoded = new Base64().encodeAsString(password.getBytes(StandardCharsets.UTF_8));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
                 .header(PasswordProviderConfig.PASSWORD_BASE64_UTF8, encoded)
-                .put(ClassLoader.getSystemResourceAsStream(
-                        "test-documents/testPassword4Spaces.pdf"));
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testPassword4Spaces.pdf"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("Just some text.", responseMsg);
     }
@@ -497,14 +541,13 @@ public class TikaResourceTest extends CXFTestBase {
     @Test
     public void testUnicodePasswordProtectedUnicode() throws Exception {
         //TIKA-2858
-        final String password = "  ! < > \" \\ \u20AC \u0153 \u00A4 \u0031\u2044\u0034 " +
-                "\u0031\u2044\u0032 \uD841\uDF0E \uD867\uDD98 \uD83D\uDE00  ";
-        final String encoded =
-                new Base64().encodeAsString(password.getBytes(StandardCharsets.UTF_8));
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+        final String password = "  ! < > \" \\ \u20AC \u0153 \u00A4 \u0031\u2044\u0034 " + "\u0031\u2044\u0032 \uD841\uDF0E \uD867\uDD98 \uD83D\uDE00  ";
+        final String encoded = new Base64().encodeAsString(password.getBytes(StandardCharsets.UTF_8));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
                 .header(PasswordProviderConfig.PASSWORD_BASE64_UTF8, encoded)
-                .put(ClassLoader.getSystemResourceAsStream(
-                        "test-documents/testUnicodePassword.pdf"));
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testUnicodePassword.pdf"));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("Just some text.", responseMsg);
     }
@@ -512,13 +555,17 @@ public class TikaResourceTest extends CXFTestBase {
     // TIKA-3227
     @Test
     public void testSkipEmbedded() throws Exception {
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
                 .header(DocumentSelectorConfig.X_TIKA_SKIP_EMBEDDED_HEADER, "false")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("embed4.txt", responseMsg);
 
-        response = WebClient.create(endPoint + TIKA_PATH).accept("text/plain")
+        response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("text/plain")
                 .header(DocumentSelectorConfig.X_TIKA_SKIP_EMBEDDED_HEADER, "true")
                 .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
@@ -532,98 +579,103 @@ public class TikaResourceTest extends CXFTestBase {
             return;
         }
 
-        Response response = WebClient.create(endPoint + TIKA_POST_PATH).type("application/pdf")
-                .accept(MediaType.TEXT_PLAIN).type(MediaType.MULTIPART_FORM_DATA)
-                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) +
-                        "ocrstrategy", "no_ocr").post(testPDFLowerCaseOCRConfigPOSTBody());
+        Response response = WebClient
+                .create(endPoint + TIKA_POST_PATH)
+                .type("application/pdf")
+                .accept(MediaType.TEXT_PLAIN)
+                .type(MediaType.MULTIPART_FORM_DATA)
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) + "ocrstrategy", "no_ocr")
+                .post(testPDFLowerCaseOCRConfigPOSTBody());
         String responseMsg = getStringFromInputStream((InputStream) response.getEntity());
 
         assertEquals("", responseMsg.trim());
 
-        response = WebClient.create(endPoint + TIKA_POST_PATH).type("application/pdf")
-                .accept(MediaType.TEXT_PLAIN).type(MediaType.MULTIPART_FORM_DATA)
-                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) +
-                        "ocrstrategy", "ocr_only").post(testPDFLowerCaseOCRConfigPOSTBody());
+        response = WebClient
+                .create(endPoint + TIKA_POST_PATH)
+                .type("application/pdf")
+                .accept(MediaType.TEXT_PLAIN)
+                .type(MediaType.MULTIPART_FORM_DATA)
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) + "ocrstrategy", "ocr_only")
+                .post(testPDFLowerCaseOCRConfigPOSTBody());
         responseMsg = getStringFromInputStream((InputStream) response.getEntity());
         assertContains("Happy New Year 2003!", responseMsg);
 
         //now try a bad value
-        response = WebClient.create(endPoint + TIKA_POST_PATH).type("application/pdf")
-                .accept(MediaType.TEXT_PLAIN).type(MediaType.MULTIPART_FORM_DATA)
-                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) +
-                        "ocrstrategy", "non-sense-value").post(testPDFLowerCaseOCRConfigPOSTBody());
+        response = WebClient
+                .create(endPoint + TIKA_POST_PATH)
+                .type("application/pdf")
+                .accept(MediaType.TEXT_PLAIN)
+                .type(MediaType.MULTIPART_FORM_DATA)
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX.toLowerCase(Locale.ROOT) + "ocrstrategy", "non-sense-value")
+                .post(testPDFLowerCaseOCRConfigPOSTBody());
         assertEquals(400, response.getStatus());
     }
 
     private MultipartBody testPDFLowerCaseOCRConfigPOSTBody() {
-        ContentDisposition cd =
-                new ContentDisposition("form-data; name=\"input\"; filename=\"testOCR.pdf\"");
-        Attachment att = new Attachment("upload",
-                ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"), cd);
+        ContentDisposition cd = new ContentDisposition("form-data; name=\"input\"; filename=\"testOCR.pdf\"");
+        Attachment att = new Attachment("upload", ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"), cd);
         return new MultipartBody(att);
     }
 
     @Test
     public void testJson() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH + "/text").accept("application/json")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "/text")
+                .accept("application/json")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
         assertContains("embed4.txt", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertContains("General Congress", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertNotFound("<p", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertEquals("Microsoft Office Word", metadata.get(OfficeOpenXMLExtended.APPLICATION));
         //test that embedded parsers are appearing in full set of "parsed bys"
-        TikaTest.assertContains("org.apache.tika.parser.microsoft.EMFParser",
-                Arrays.asList(metadata.getValues(TikaCoreProperties.TIKA_PARSED_BY_FULL_SET)));
+        TikaTest.assertContains("org.apache.tika.parser.microsoft.EMFParser", Arrays.asList(metadata.getValues(TikaCoreProperties.TIKA_PARSED_BY_FULL_SET)));
     }
 
     @Test
     public void testJsonWriteLimitEmbedded() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH + "/html").accept("application/json")
-                        .header("writeLimit", "500")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "/html")
+                .accept("application/json")
+                .header("writeLimit", "500")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
         assertContains("embed2a.txt", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertContains("When in the Course", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertNotFound("declare the causes", metadata.get(TikaCoreProperties.TIKA_CONTENT));
         assertEquals("Microsoft Office Word", metadata.get(OfficeOpenXMLExtended.APPLICATION));
-        assertTrue(metadata.get(TikaCoreProperties.CONTAINER_EXCEPTION)
+        assertTrue(metadata
+                .get(TikaCoreProperties.CONTAINER_EXCEPTION)
                 .startsWith("org.apache.tika.exception.WriteLimitReachedException"));
         assertNotFound("embed4.txt", metadata.get(TikaCoreProperties.TIKA_CONTENT));
     }
 
     @Test
     public void testJsonNoThrowWriteLimitEmbedded() throws Exception {
-        Response response =
-                WebClient.create(endPoint + TIKA_PATH + "/html").accept("application/json")
-                        .header("writeLimit", "500").header("throwOnWriteLimitReached", "false")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
-        Metadata metadata = JsonMetadata.fromJson(
-                new InputStreamReader(((InputStream) response.getEntity()),
-                        StandardCharsets.UTF_8));
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH + "/html")
+                .accept("application/json")
+                .header("writeLimit", "500")
+                .header("throwOnWriteLimitReached", "false")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_RECURSIVE_DOC));
+        Metadata metadata = JsonMetadata.fromJson(new InputStreamReader(((InputStream) response.getEntity()), StandardCharsets.UTF_8));
         String txt = metadata.get(TikaCoreProperties.TIKA_CONTENT);
         assertContains("embed2a.txt", txt);
         assertContains("When in the Course", txt);
         assertNotFound("declare the causes", txt);
         assertEquals("Microsoft Office Word", metadata.get(OfficeOpenXMLExtended.APPLICATION));
         assertEquals("true", metadata.get(TikaCoreProperties.WRITE_LIMIT_REACHED));
-        assertContains("<div class=\"embedded\" id=\"embed4.txt",
-                metadata.get(TikaCoreProperties.TIKA_CONTENT));
+        assertContains("<div class=\"embedded\" id=\"embed4.txt", metadata.get(TikaCoreProperties.TIKA_CONTENT));
     }
 
     @Test
     public void testWriteLimitInPDF() throws Exception {
         int writeLimit = 10;
-        Response response = WebClient.create(endPoint + TIKA_PATH).accept("application/json")
+        Response response = WebClient
+                .create(endPoint + TIKA_PATH)
+                .accept("application/json")
                 .header("writeLimit", Integer.toString(writeLimit))
-                .put(ClassLoader.getSystemResourceAsStream(
-                        "test-documents/testPDFTwoTextBoxes.pdf"));
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testPDFTwoTextBoxes.pdf"));
 
         assertEquals(200, response.getStatus());
         Reader reader = new InputStreamReader((InputStream) response.getEntity(), UTF_8);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceTest.java
index c99ade885..bd9934dc6 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceTest.java
@@ -80,8 +80,7 @@ public class UnpackerResourceTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(UnpackerResource.class);
-        sf.setResourceProvider(UnpackerResource.class,
-                new SingletonResourceProvider(new UnpackerResource()));
+        sf.setResourceProvider(UnpackerResource.class, new SingletonResourceProvider(new UnpackerResource()));
     }
 
     @Override
@@ -95,10 +94,11 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testDocWAV() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).type(APPLICATION_MSWORD)
-                        .accept("application/zip")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .type(APPLICATION_MSWORD)
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
 
         Map<String, String> data = readZipArchive((InputStream) response.getEntity());
         assertEquals(WAV1_MD5, data.get(WAV1_NAME));
@@ -108,10 +108,11 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testDocWAVText() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + ALL_PATH).type(APPLICATION_MSWORD)
-                        .accept("application/zip")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + ALL_PATH)
+                .type(APPLICATION_MSWORD)
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
 
         Map<String, String> data = readZipArchive((InputStream) response.getEntity());
         assertEquals(WAV1_MD5, data.get(WAV1_NAME));
@@ -121,10 +122,11 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testDocPicture() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).type(APPLICATION_MSWORD)
-                        .accept("application/zip")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .type(APPLICATION_MSWORD)
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
 
         Map<String, String> data = readZipArchive((InputStream) response.getEntity());
 
@@ -133,10 +135,11 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testDocPictureNoOle() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).type(APPLICATION_MSWORD)
-                        .accept("application/zip")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/2pic.doc"));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .type(APPLICATION_MSWORD)
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/2pic.doc"));
 
         Map<String, String> data = readZipArchive((InputStream) response.getEntity());
         assertEquals(JPG2_MD5, data.get(JPG2_NAME));
@@ -144,9 +147,10 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testImageDOCX() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).accept("application/zip")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOCX_IMAGE));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOCX_IMAGE));
 
         Map<String, String> data = readZipArchive((InputStream) response.getEntity());
         assertEquals(DOCX_IMAGE1_MD5, data.get(DOCX_IMAGE1_NAME));
@@ -155,9 +159,11 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void test204() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).type("xxx/xxx").accept("*/*")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .type("xxx/xxx")
+                .accept("*/*")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
 
         assertEquals(204, response.getStatus());
     }
@@ -165,9 +171,10 @@ public class UnpackerResourceTest extends CXFTestBase {
     @Test
     public void testExeDOCX() throws Exception {
         String TEST_DOCX_EXE = "test-documents/2exe.docx";
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).accept("application/zip")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOCX_EXE));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOCX_EXE));
 
         Map<String, String> data = readZipArchive((InputStream) response.getEntity());
 
@@ -177,9 +184,10 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testImageXSL() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).accept("application/zip")
-                        .put(ClassLoader.getSystemResourceAsStream("test-documents/pic.xls"));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/pic.xls"));
 
         Map<String, String> data = readZipArchive((InputStream) response.getEntity());
         assertEquals(XSL_IMAGE1_MD5, data.get("0.jpg"));
@@ -188,21 +196,23 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testTarDocPicture() throws Exception {
-        Response response =
-                WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH).type(APPLICATION_MSWORD)
-                        .accept("application/x-tar")
-                        .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
+                .type(APPLICATION_MSWORD)
+                .accept("application/x-tar")
+                .put(ClassLoader.getSystemResourceAsStream(TEST_DOC_WAV));
 
-        Map<String, String> data = readArchiveFromStream(
-                new TarArchiveInputStream((InputStream) response.getEntity()));
+        Map<String, String> data = readArchiveFromStream(new TarArchiveInputStream((InputStream) response.getEntity()));
 
         assertEquals(JPG_MD5, data.get(JPG_NAME));
     }
 
     @Test
     public void testText() throws Exception {
-        Response response = WebClient.create(CXFTestBase.endPoint + ALL_PATH)
-                .header(CONTENT_TYPE, APPLICATION_XML).accept("application/zip")
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + ALL_PATH)
+                .header(CONTENT_TYPE, APPLICATION_XML)
+                .accept("application/zip")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/test.doc"));
 
         String responseMsg = readArchiveMetadataAndText((InputStream) response.getEntity());
@@ -213,16 +223,19 @@ public class UnpackerResourceTest extends CXFTestBase {
 
     @Test
     public void testMaxBytes() throws Exception {
-        Response response = WebClient.create(CXFTestBase.endPoint + ALL_PATH)
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + ALL_PATH)
                 .header(CONTENT_TYPE, APPLICATION_XML)
-                .header(UnpackerResource.UNPACK_MAX_BYTES_KEY, 100).accept("application/zip")
+                .header(UnpackerResource.UNPACK_MAX_BYTES_KEY, 100)
+                .accept("application/zip")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/pic.xls"));
         assertEquals(422, response.getStatus());
     }
 
     @Test
     public void testPDFImages() throws Exception {
-        Response response = WebClient.create(CXFTestBase.endPoint + UNPACKER_PATH)
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + UNPACKER_PATH)
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "ExtractInlineImages", "true")
                 .accept("application/zip")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
@@ -240,7 +253,8 @@ public class UnpackerResourceTest extends CXFTestBase {
     public void testPDFRenderOCR() throws Exception {
         assumeTrue(new TesseractOCRParser().hasTesseract());
 
-        Response response = WebClient.create(CXFTestBase.endPoint + ALL_PATH)
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + ALL_PATH)
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "ocrStrategy", "ocr_only")
                 .accept("application/zip")
                 .put(ClassLoader.getSystemResourceAsStream("test-documents/testOCR.pdf"));
@@ -251,46 +265,46 @@ public class UnpackerResourceTest extends CXFTestBase {
     @Test
     public void testPDFPerPageRenderColor() throws Exception {
 
-        Response response = WebClient.create(CXFTestBase.endPoint + ALL_PATH)
-                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "imageStrategy",
-                        "RenderPagesAtPageEnd")
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + ALL_PATH)
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "imageStrategy", "RenderPagesAtPageEnd")
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "ocrImageType", "rgb")
-                .accept("application/zip").put(ClassLoader.getSystemResourceAsStream(
-                        "test-documents/testColorRendering.pdf"));
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testColorRendering.pdf"));
         Map<String, byte[]> results = readZipArchiveBytes((InputStream) response.getEntity());
         byte[] renderedImage = null;
         for (Map.Entry<String, byte[]> e : results.entrySet()) {
-            if (e.getKey().startsWith("tika-pdfbox-rendering")) {
+            if (e
+                    .getKey()
+                    .startsWith("tika-pdfbox-rendering")) {
                 renderedImage = e.getValue();
                 break;
             }
         }
-        assertEquals("image/png", TikaConfig.getDefaultConfig().getDetector()
-                .detect(new ByteArrayInputStream(renderedImage), new Metadata()).toString());
+        assertEquals("image/png", TikaConfig
+                .getDefaultConfig()
+                .getDetector()
+                .detect(new ByteArrayInputStream(renderedImage), new Metadata())
+                .toString());
 
         try (InputStream is = new ByteArrayInputStream(renderedImage)) {
             BufferedImage image = ImageIO.read(is);
             //top left
-            AverageColor averageColor =
-                    getAverageColor(image, 0, image.getWidth() / 5, 0, image.getHeight() / 10);
+            AverageColor averageColor = getAverageColor(image, 0, image.getWidth() / 5, 0, image.getHeight() / 10);
             assertTrue(averageColor.getRed() > 250);
             assertTrue(averageColor.getGreen() < 1);
             assertTrue(averageColor.getBlue() < 1);
 
             //bottom left = green
-            averageColor = getAverageColor(image, 0, image.getWidth() / 5,
-                    image.getHeight() / 2 + image.getHeight() / 10,
-                    image.getHeight() / 2 + 2 * image.getHeight() / 10);
+            averageColor = getAverageColor(image, 0, image.getWidth() / 5, image.getHeight() / 2 + image.getHeight() / 10, image.getHeight() / 2 + 2 * image.getHeight() / 10);
 
             assertTrue(averageColor.getRed() < 1);
             assertTrue(averageColor.getGreen() > 250);
             assertTrue(averageColor.getBlue() < 1);
 
             //bottom right = blue
-            averageColor = getAverageColor(image, image.getWidth() / 2 + image.getWidth() / 10,
-                    image.getWidth() / 2 + 2 * image.getWidth() / 10,
-                    image.getHeight() / 2 + image.getHeight() / 10,
-                    image.getHeight() / 2 + 2 * image.getHeight() / 10);
+            averageColor = getAverageColor(image, image.getWidth() / 2 + image.getWidth() / 10, image.getWidth() / 2 + 2 * image.getWidth() / 10,
+                    image.getHeight() / 2 + image.getHeight() / 10, image.getHeight() / 2 + 2 * image.getHeight() / 10);
 
             assertTrue(averageColor.getRed() < 1);
             assertTrue(averageColor.getGreen() < 1);
diff --git a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceWithConfigTest.java b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceWithConfigTest.java
index bb636b5b4..a8172d5fa 100644
--- a/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceWithConfigTest.java
+++ b/tika-server/tika-server-standard/src/test/java/org/apache/tika/server/standard/UnpackerResourceWithConfigTest.java
@@ -51,8 +51,7 @@ public class UnpackerResourceWithConfigTest extends CXFTestBase {
     @Override
     protected void setUpResources(JAXRSServerFactoryBean sf) {
         sf.setResourceClasses(UnpackerResource.class);
-        sf.setResourceProvider(UnpackerResource.class,
-                new SingletonResourceProvider(new UnpackerResource()));
+        sf.setResourceProvider(UnpackerResource.class, new SingletonResourceProvider(new UnpackerResource()));
     }
 
     @Override
@@ -66,7 +65,9 @@ public class UnpackerResourceWithConfigTest extends CXFTestBase {
 
     @Override
     protected InputStream getTikaConfigInputStream() throws IOException {
-        return this.getClass().getResourceAsStream("/config/tika-config-unpacker.xml");
+        return this
+                .getClass()
+                .getResourceAsStream("/config/tika-config-unpacker.xml");
     }
 
     //Test that the PDFParser's renderer can be configured at parse time
@@ -75,47 +76,47 @@ public class UnpackerResourceWithConfigTest extends CXFTestBase {
     public void testPDFPerPageRenderColor() throws Exception {
 
         //default is gray scale png; change to rgb and tiff
-        Response response = WebClient.create(CXFTestBase.endPoint + ALL_PATH)
-                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "imageStrategy",
-                        "RenderPagesAtPageEnd")
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + ALL_PATH)
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "imageStrategy", "RenderPagesAtPageEnd")
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "ocrImageType", "rgb")
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "ocrImageFormatName", "tiff")
-                .accept("application/zip").put(ClassLoader.getSystemResourceAsStream(
-                        "test-documents/testColorRendering.pdf"));
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testColorRendering.pdf"));
         Map<String, byte[]> results = readZipArchiveBytes((InputStream) response.getEntity());
         byte[] renderedImage = null;
         for (Map.Entry<String, byte[]> e : results.entrySet()) {
-            if (e.getKey().startsWith("tika-pdfbox-rendering")) {
+            if (e
+                    .getKey()
+                    .startsWith("tika-pdfbox-rendering")) {
                 renderedImage = e.getValue();
                 break;
             }
         }
-        assertEquals("image/tiff", TikaConfig.getDefaultConfig().getDetector()
-                .detect(new ByteArrayInputStream(renderedImage), new Metadata()).toString());
+        assertEquals("image/tiff", TikaConfig
+                .getDefaultConfig()
+                .getDetector()
+                .detect(new ByteArrayInputStream(renderedImage), new Metadata())
+                .toString());
 
         try (InputStream is = new ByteArrayInputStream(renderedImage)) {
             BufferedImage image = ImageIO.read(is);
             //top left
-            AverageColor averageColor =
-                    getAverageColor(image, 0, image.getWidth() / 5, 0, image.getHeight() / 10);
+            AverageColor averageColor = getAverageColor(image, 0, image.getWidth() / 5, 0, image.getHeight() / 10);
             assertTrue(averageColor.getRed() > 250);
             assertTrue(averageColor.getGreen() < 1);
             assertTrue(averageColor.getBlue() < 1);
 
             //bottom left = green
-            averageColor = getAverageColor(image, 0, image.getWidth() / 5,
-                    image.getHeight() / 2 + image.getHeight() / 10,
-                    image.getHeight() / 2 + 2 * image.getHeight() / 10);
+            averageColor = getAverageColor(image, 0, image.getWidth() / 5, image.getHeight() / 2 + image.getHeight() / 10, image.getHeight() / 2 + 2 * image.getHeight() / 10);
 
             assertTrue(averageColor.getRed() < 1);
             assertTrue(averageColor.getGreen() > 250);
             assertTrue(averageColor.getBlue() < 1);
 
             //bottom right = blue
-            averageColor = getAverageColor(image, image.getWidth() / 2 + image.getWidth() / 10,
-                    image.getWidth() / 2 + 2 * image.getWidth() / 10,
-                    image.getHeight() / 2 + image.getHeight() / 10,
-                    image.getHeight() / 2 + 2 * image.getHeight() / 10);
+            averageColor = getAverageColor(image, image.getWidth() / 2 + image.getWidth() / 10, image.getWidth() / 2 + 2 * image.getWidth() / 10,
+                    image.getHeight() / 2 + image.getHeight() / 10, image.getHeight() / 2 + 2 * image.getHeight() / 10);
 
             assertTrue(averageColor.getRed() < 1);
             assertTrue(averageColor.getGreen() < 1);
@@ -127,48 +128,48 @@ public class UnpackerResourceWithConfigTest extends CXFTestBase {
     public void testPDFPerPageRenderGray() throws Exception {
 
 
-        Response response = WebClient.create(CXFTestBase.endPoint + ALL_PATH)
-                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "imageStrategy",
-                        "RenderPagesAtPageEnd")
+        Response response = WebClient
+                .create(CXFTestBase.endPoint + ALL_PATH)
+                .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "imageStrategy", "RenderPagesAtPageEnd")
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "ocrImageType", "gray")
                 .header(PDFServerConfig.X_TIKA_PDF_HEADER_PREFIX + "ocrImageFormatName", "jpeg")
-                .accept("application/zip").put(ClassLoader.getSystemResourceAsStream(
-                        "test-documents/testColorRendering.pdf"));
+                .accept("application/zip")
+                .put(ClassLoader.getSystemResourceAsStream("test-documents/testColorRendering.pdf"));
         Map<String, byte[]> results = readZipArchiveBytes((InputStream) response.getEntity());
         byte[] renderedImage = null;
         for (Map.Entry<String, byte[]> e : results.entrySet()) {
-            if (e.getKey().startsWith("tika-pdfbox-rendering")) {
+            if (e
+                    .getKey()
+                    .startsWith("tika-pdfbox-rendering")) {
                 renderedImage = e.getValue();
                 break;
             }
         }
-        assertEquals("image/jpeg", TikaConfig.getDefaultConfig().getDetector()
-                .detect(new ByteArrayInputStream(renderedImage), new Metadata()).toString());
+        assertEquals("image/jpeg", TikaConfig
+                .getDefaultConfig()
+                .getDetector()
+                .detect(new ByteArrayInputStream(renderedImage), new Metadata())
+                .toString());
 
         try (InputStream is = new ByteArrayInputStream(renderedImage)) {
             BufferedImage image = ImageIO.read(is);
             //top left
-            AverageColor averageColor =
-                    getAverageColor(image, 0, image.getWidth() / 5, 0, image.getHeight() / 10);
+            AverageColor averageColor = getAverageColor(image, 0, image.getWidth() / 5, 0, image.getHeight() / 10);
 
             assertTrue(averageColor.getRed() > 140 && averageColor.getRed() < 160);
             assertTrue(averageColor.getGreen() > 140 && averageColor.getGreen() < 160);
             assertTrue(averageColor.getBlue() > 140 && averageColor.getBlue() < 160);
 
             //bottom left = green
-            averageColor = getAverageColor(image, 0, image.getWidth() / 5,
-                    image.getHeight() / 2 + image.getHeight() / 10,
-                    image.getHeight() / 2 + 2 * image.getHeight() / 10);
+            averageColor = getAverageColor(image, 0, image.getWidth() / 5, image.getHeight() / 2 + image.getHeight() / 10, image.getHeight() / 2 + 2 * image.getHeight() / 10);
 
             assertTrue(averageColor.getRed() < 210 && averageColor.getRed() > 190);
             assertTrue(averageColor.getGreen() < 210 && averageColor.getGreen() > 190);
             assertTrue(averageColor.getBlue() < 210 && averageColor.getBlue() > 190);
 
             //bottom right = blue
-            averageColor = getAverageColor(image, image.getWidth() / 2 + image.getWidth() / 10,
-                    image.getWidth() / 2 + 2 * image.getWidth() / 10,
-                    image.getHeight() / 2 + image.getHeight() / 10,
-                    image.getHeight() / 2 + 2 * image.getHeight() / 10);
+            averageColor = getAverageColor(image, image.getWidth() / 2 + image.getWidth() / 10, image.getWidth() / 2 + 2 * image.getWidth() / 10,
+                    image.getHeight() / 2 + image.getHeight() / 10, image.getHeight() / 2 + 2 * image.getHeight() / 10);
             assertTrue(averageColor.getRed() < 100 && averageColor.getRed() > 90);
             assertTrue(averageColor.getGreen() < 100 && averageColor.getGreen() > 90);
             assertTrue(averageColor.getBlue() < 100 && averageColor.getBlue() > 90);
diff --git a/tika-server/tika-server-standard/src/test/resources/config/TIKA-3137-include.xml b/tika-server/tika-server-standard/src/test/resources/config/TIKA-3137-include.xml
index da1182aa3..056e64c6f 100644
--- a/tika-server/tika-server-standard/src/test/resources/config/TIKA-3137-include.xml
+++ b/tika-server/tika-server-standard/src/test/resources/config/TIKA-3137-include.xml
@@ -16,23 +16,23 @@
   limitations under the License.
 -->
 <properties>
-    <metadataFilters>
-        <metadataFilter class="org.apache.tika.metadata.filter.IncludeFieldMetadataFilter">
-            <params>
-                <include>
-                    <field>X-TIKA:content</field>
-                    <field>extended-properties:Application</field>
-                    <field>Content-Type</field>
-                </include>
-            </params>
-        </metadataFilter>
-        <metadataFilter class="org.apache.tika.metadata.filter.ClearByMimeMetadataFilter">
-            <params>
-                <mimes>
-                    <mime>image/emf</mime>
-                    <mime>text/plain</mime>
-                </mimes>
-            </params>
-        </metadataFilter>
-    </metadataFilters>
+  <metadataFilters>
+    <metadataFilter class="org.apache.tika.metadata.filter.IncludeFieldMetadataFilter">
+      <params>
+        <include>
+          <field>X-TIKA:content</field>
+          <field>extended-properties:Application</field>
+          <field>Content-Type</field>
+        </include>
+      </params>
+    </metadataFilter>
+    <metadataFilter class="org.apache.tika.metadata.filter.ClearByMimeMetadataFilter">
+      <params>
+        <mimes>
+          <mime>image/emf</mime>
+          <mime>text/plain</mime>
+        </mimes>
+      </params>
+    </metadataFilter>
+  </metadataFilters>
 </properties>
diff --git a/tika-server/tika-server-standard/src/test/resources/config/tika-config-for-server-tests.xml b/tika-server/tika-server-standard/src/test/resources/config/tika-config-for-server-tests.xml
index 8867655f7..5e86586ae 100644
--- a/tika-server/tika-server-standard/src/test/resources/config/tika-config-for-server-tests.xml
+++ b/tika-server/tika-server-standard/src/test/resources/config/tika-config-for-server-tests.xml
@@ -16,14 +16,14 @@
   limitations under the License.
 -->
 <properties>
-    <parsers>
-        <parser class="org.apache.tika.parser.DefaultParser">
-            <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
-        </parser>
-        <parser class="org.apache.tika.parser.pdf.PDFParser">
-            <params>
-                <param name="sortByPosition" type="bool">true</param>
-            </params>
-        </parser>
-    </parsers>
+  <parsers>
+    <parser class="org.apache.tika.parser.DefaultParser">
+      <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
+    </parser>
+    <parser class="org.apache.tika.parser.pdf.PDFParser">
+      <params>
+        <param name="sortByPosition" type="bool">true</param>
+      </params>
+    </parser>
+  </parsers>
 </properties>
\ No newline at end of file
diff --git a/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-opennlp-filter.xml b/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-opennlp-filter.xml
index 8448e42bf..7dd1d5fe6 100644
--- a/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-opennlp-filter.xml
+++ b/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-opennlp-filter.xml
@@ -16,17 +16,17 @@
   limitations under the License.
 -->
 <properties>
-    <parsers>
-        <parser class="org.apache.tika.parser.DefaultParser">
-            <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
-        </parser>
-        <parser class="org.apache.tika.parser.pdf.PDFParser">
-            <params>
-                <param name="sortByPosition" type="bool">true</param>
-            </params>
-        </parser>
-    </parsers>
-    <metadataFilters>
-        <metadataFilter class="org.apache.tika.langdetect.opennlp.metadatafilter.OpenNLPMetadataFilter"/>
-    </metadataFilters>
+  <parsers>
+    <parser class="org.apache.tika.parser.DefaultParser">
+      <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
+    </parser>
+    <parser class="org.apache.tika.parser.pdf.PDFParser">
+      <params>
+        <param name="sortByPosition" type="bool">true</param>
+      </params>
+    </parser>
+  </parsers>
+  <metadataFilters>
+    <metadataFilter class="org.apache.tika.langdetect.opennlp.metadatafilter.OpenNLPMetadataFilter"/>
+  </metadataFilters>
 </properties>
\ No newline at end of file
diff --git a/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-optimaize-filter.xml b/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-optimaize-filter.xml
index 82c623b24..faaed6fcf 100644
--- a/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-optimaize-filter.xml
+++ b/tika-server/tika-server-standard/src/test/resources/config/tika-config-langdetect-optimaize-filter.xml
@@ -16,17 +16,17 @@
   limitations under the License.
 -->
 <properties>
-    <parsers>
-        <parser class="org.apache.tika.parser.DefaultParser">
-            <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
-        </parser>
-        <parser class="org.apache.tika.parser.pdf.PDFParser">
-            <params>
-                <param name="sortByPosition" type="bool">true</param>
-            </params>
-        </parser>
-    </parsers>
-    <metadataFilters>
-        <metadataFilter class="org.apache.tika.langdetect.optimaize.metadatafilter.OptimaizeMetadataFilter"/>
-    </metadataFilters>
+  <parsers>
+    <parser class="org.apache.tika.parser.DefaultParser">
+      <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
+    </parser>
+    <parser class="org.apache.tika.parser.pdf.PDFParser">
+      <params>
+        <param name="sortByPosition" type="bool">true</param>
+      </params>
+    </parser>
+  </parsers>
+  <metadataFilters>
+    <metadataFilter class="org.apache.tika.langdetect.optimaize.metadatafilter.OptimaizeMetadataFilter"/>
+  </metadataFilters>
 </properties>
\ No newline at end of file
diff --git a/tika-server/tika-server-standard/src/test/resources/config/tika-config-unpacker.xml b/tika-server/tika-server-standard/src/test/resources/config/tika-config-unpacker.xml
index 0e3c103b8..40150947f 100644
--- a/tika-server/tika-server-standard/src/test/resources/config/tika-config-unpacker.xml
+++ b/tika-server/tika-server-standard/src/test/resources/config/tika-config-unpacker.xml
@@ -16,17 +16,17 @@
   limitations under the License.
 -->
 <properties>
-    <parsers>
-        <parser class="org.apache.tika.parser.DefaultParser">
-            <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
-        </parser>
-        <parser class="org.apache.tika.parser.pdf.PDFParser">
-            <params>
-                <param name="sortByPosition" type="bool">true</param>
-            </params>
-        </parser>
-    </parsers>
-    <renderers>
-        <renderer class="org.apache.tika.renderer.pdf.pdfbox.PDFBoxRenderer"/>
-    </renderers>
+  <parsers>
+    <parser class="org.apache.tika.parser.DefaultParser">
+      <parser-exclude class="org.apache.tika.parser.pdf.PDFParser"/>
+    </parser>
+    <parser class="org.apache.tika.parser.pdf.PDFParser">
+      <params>
+        <param name="sortByPosition" type="bool">true</param>
+      </params>
+    </parser>
+  </parsers>
+  <renderers>
+    <renderer class="org.apache.tika.renderer.pdf.pdfbox.PDFBoxRenderer"/>
+  </renderers>
 </properties>
\ No newline at end of file
diff --git a/tika-server/tika-server-standard/src/test/resources/config/tika-config-url-fetcher.xml b/tika-server/tika-server-standard/src/test/resources/config/tika-config-url-fetcher.xml
index d8a4321e4..25f01b87d 100644
--- a/tika-server/tika-server-standard/src/test/resources/config/tika-config-url-fetcher.xml
+++ b/tika-server/tika-server-standard/src/test/resources/config/tika-config-url-fetcher.xml
@@ -18,14 +18,14 @@
   under the License.
 -->
 <properties>
-    <parsers>
-        <parser class="org.apache.tika.parser.DefaultParser"/>
-    </parsers>
-    <fetchers>
-        <fetcher class="org.apache.tika.pipes.fetcher.url.UrlFetcher">
-            <params>
-                <name>url</name>
-            </params>
-        </fetcher>
-    </fetchers>
+  <parsers>
+    <parser class="org.apache.tika.parser.DefaultParser"/>
+  </parsers>
+  <fetchers>
+    <fetcher class="org.apache.tika.pipes.fetcher.url.UrlFetcher">
+      <params>
+        <name>url</name>
+      </params>
+    </fetcher>
+  </fetchers>
 </properties>
\ No newline at end of file
