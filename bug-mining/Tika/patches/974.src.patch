diff --git a/CHANGES.txt b/CHANGES.txt
index 1a375bbab..4f4ccdaa4 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -1,5 +1,7 @@
 Release 1.17 - ???
 
+  * Fix thread-safety in ChmExtractor (TIKA-2519).
+
   * Upgrade cxf to 3.0.16 (TIKA-2516).
 
   * Allow users to configure maxMainMemoryBytes for PDFs via shrike (PR-213).
diff --git a/tika-core/src/test/java/org/apache/tika/MultiThreadedTikaTest.java b/tika-core/src/test/java/org/apache/tika/MultiThreadedTikaTest.java
new file mode 100644
index 000000000..b5b60a97b
--- /dev/null
+++ b/tika-core/src/test/java/org/apache/tika/MultiThreadedTikaTest.java
@@ -0,0 +1,251 @@
+package org.apache.tika;
+
+import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.AutoDetectParser;
+import org.apache.tika.parser.ParseContext;
+import org.apache.tika.parser.Parser;
+import org.apache.tika.parser.RecursiveParserWrapper;
+import org.apache.tika.sax.BasicContentHandlerFactory;
+import org.junit.Test;
+import org.xml.sax.helpers.DefaultHandler;
+
+import java.io.FileFilter;
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URISyntaxException;
+import java.nio.file.FileVisitResult;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.nio.file.SimpleFileVisitor;
+import java.nio.file.attribute.BasicFileAttributes;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Random;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorCompletionService;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+
+import static org.junit.Assert.assertEquals;
+
+public class MultiThreadedTikaTest extends TikaTest {
+    //TODO: figure out how to make failures reproducible a la Lucene/Solr with a seed
+    //TODO: Consider randomizing the Locale and timezone, like Lucene/Solr...
+
+    /**
+     * This calls {@link #testEach(Path[], int, int)} and then {@link #testAll(Path[], int, int)}
+     *
+     * @param numThreads number of threads to use
+     * @param numIterations number of iterations per thread
+     * @param filter file filter to select files from "/test-documents"; if <code>null</code>,
+     *               all files will be used
+     * @throws Exception
+     */
+    protected void testMultiThreaded(int numThreads, int numIterations, FileFilter filter) throws Exception {
+        Path[] allFiles = getTestFiles(filter);
+        testEach(allFiles, numThreads, numIterations);
+        testAll(allFiles, numThreads, numIterations);
+    }
+
+    /**
+     *  Test each file, one at a time in multiple threads.
+     *  This was required to test TIKA-2519 in a reasonable
+     *  amount of time.  This forced the parser to use the
+     *  same underlying memory structurees because it was the same file.
+     *  This is stricter than I think our agreement with clients is
+     *  because this run tests on literally the same file and
+     *  not a copy of the file per thread.  Let's leave this as is
+     *  unless there's a good reason to create a separate copy per thread.
+     *
+     * @param files files to test, one at a time
+     * @param numThreads number of threads to use
+     * @param numIterations number of iterations per thread
+     */
+    protected void testEach(Path[] files, int numThreads, int numIterations) {
+        for (Path p : files) {
+            Path[] toTest = new Path[1];
+            toTest[0] = p;
+            testAll(toTest, numThreads, numIterations);
+        }
+    }
+
+    /**
+     * This tests all files together.  Each parser randomly selects
+     * a file from the array.  Two parsers could wind up parsing the
+     * same file at the same time.  Good.
+     *
+     * In the current implementation, this gets ground truth only
+     * from files that do not throw exceptions.  This will ignore
+     * files that cause exceptions.
+     *
+     * @param files files to parse
+     * @param numThreads number of parser threads
+     * @param numIterations number of iterations per parser
+     */
+    protected void testAll(Path[] files, int numThreads, int numIterations) {
+
+        Map<Path, Extract> truth = getBaseline(files);
+        //if all files caused an exception
+        if (truth.size() == 0) {
+            return;
+        }
+        //only those that parsed without exception
+        Path[] testFiles = new Path[truth.size()];
+        int j = 0;
+        for (Path testFile : truth.keySet()) {
+            testFiles[j++] = testFile;
+        }
+
+        ExecutorService ex = Executors.newFixedThreadPool(numThreads);
+        try {
+            _testAll(files, numThreads, numIterations, truth, ex);
+        } finally {
+            ex.shutdown();
+            ex.shutdownNow();
+        }
+    }
+
+    private void _testAll(Path[] testFiles, int numThreads, int numIterations,
+                          Map<Path, Extract> truth, ExecutorService ex) {
+
+        ExecutorCompletionService<Integer> executorCompletionService = new ExecutorCompletionService<>(ex);
+
+        //use the same parser in all threads
+        Parser parser = new AutoDetectParser();
+        for (int i = 0; i < numThreads; i++) {
+            executorCompletionService.submit(new TikaRunner(parser, numIterations, testFiles, truth));
+        }
+
+        int completed = 0;
+        while (completed < numThreads) {
+            //TODO: add a maximum timeout threshold
+
+            Future<Integer> future = null;
+            try {
+                future = executorCompletionService.poll(1000, TimeUnit.MILLISECONDS);
+                if (future != null) {
+                    future.get();//trigger exceptions from thread
+                    completed++;
+                }
+            } catch (InterruptedException|ExecutionException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+
+    private static Path[] getTestFiles(final FileFilter fileFilter) throws URISyntaxException, IOException {
+        Path root = Paths.get(
+                MultiThreadedTikaTest.class.getResource("/test-documents").toURI());
+        final List<Path> files = new ArrayList<>();
+        Files.walkFileTree(root, new SimpleFileVisitor<Path>() {
+            @Override
+            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
+                if (fileFilter != null && ! fileFilter.accept(file.toFile())) {
+                    return FileVisitResult.CONTINUE;
+                }
+                if (!attrs.isDirectory()) {
+                    if (files.size() < 20) {
+                        files.add(file);
+                    }
+                }
+                return FileVisitResult.CONTINUE;
+            }
+        });
+        return files.toArray(new Path[files.size()]);
+    }
+
+    private static ConcurrentHashMap<Path, Extract> getBaseline(Path[] files) {
+        ConcurrentHashMap<Path, Extract> baseline = new ConcurrentHashMap<>();
+        for (Path f : files) {
+
+            try {
+                Parser p = new AutoDetectParser();
+                RecursiveParserWrapper wrapper = new RecursiveParserWrapper(p,
+                        new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.TEXT, -1));
+                try (InputStream is = Files.newInputStream(f)) {
+                    wrapper.parse(is, new DefaultHandler(), new Metadata(), new ParseContext());
+                }
+                List<Metadata> metadataList = wrapper.getMetadata();
+                baseline.put(f, new Extract(metadataList));
+            } catch (Exception e) {
+                //swallow
+            }
+        }
+        return baseline;
+    }
+
+    private static List<Metadata> getRecursiveMetadata(InputStream is, Parser p) throws Exception {
+        //different from parent TikaTest in that this extracts text.
+        //can't extract xhtml because "tmp" file names wind up in
+        //content's metadata and they'll differ by file.
+        RecursiveParserWrapper wrapper = new RecursiveParserWrapper(p,
+                new BasicContentHandlerFactory(BasicContentHandlerFactory.HANDLER_TYPE.TEXT, -1));
+        wrapper.parse(is, new DefaultHandler(), new Metadata(), new ParseContext());
+        return wrapper.getMetadata();
+    }
+
+    //TODO: make this return something useful besides an integer
+    private class TikaRunner implements Callable<Integer> {
+        private final Parser parser;
+        private final int iterations;
+        private final Path[] files;
+        private final Map<Path, Extract> truth;
+
+        private final Random random = new Random();
+
+        private TikaRunner(Parser parser, int iterations, Path[] files, Map<Path, Extract> truth) {
+            this.parser = parser;
+            this.iterations = iterations;
+            this.files = files;
+            this.truth = truth;
+        }
+
+        @Override
+        public Integer call() throws Exception {
+            for (int i = 0; i < iterations; i++) {
+                int randIndex = random.nextInt(files.length);
+                Path testFile = files[randIndex];
+                try (InputStream is = Files.newInputStream(testFile)) {
+                    List<Metadata> metadataList = getRecursiveMetadata(is, parser);
+                    assertExtractEquals(truth.get(testFile), new Extract(metadataList));
+                } catch (Exception e) {
+                    throw new RuntimeException(testFile+" triggered this exception", e);
+                }
+            }
+            return 1;
+        }
+
+    }
+
+    private void assertExtractEquals(Extract extractA, Extract extractB) {
+        //this currently only checks the basics
+        //might want to add more checks
+
+        assertEquals("number of embedded files", extractA.metadataList.size(),
+                extractB.metadataList.size());
+
+        for (int i = 0; i < extractA.metadataList.size(); i++) {
+            assertEquals("number of metadata elements in attachment: " + i,
+                    extractA.metadataList.get(i).size(), extractB.metadataList.get(i).size());
+
+            assertEquals("content in attachment: " + i,
+                    extractA.metadataList.get(i).get(RecursiveParserWrapper.TIKA_CONTENT),
+                    extractB.metadataList.get(i).get(RecursiveParserWrapper.TIKA_CONTENT));
+        }
+    }
+
+    private static class Extract {
+        final List<Metadata> metadataList;
+
+        private Extract(List<Metadata> metadataList) {
+            this.metadataList = metadataList;
+        }
+    }
+}
diff --git a/tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmExtractor.java b/tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmExtractor.java
index faadc4d5d..6105436f4 100644
--- a/tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmExtractor.java
+++ b/tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmExtractor.java
@@ -58,6 +58,7 @@ public class ChmExtractor {
     private int indexOfContent;
     private long lzxBlockOffset;
     private long lzxBlockLength;
+    private final ChmBlockInfo chmBlockInfo = new ChmBlockInfo();
 
     /**
      * Returns lzxc control data.
@@ -266,9 +267,9 @@ public class ChmExtractor {
             } else if (directoryListingEntry.getEntryType() == EntryType.COMPRESSED
                     && !ChmCommons.hasSkip(directoryListingEntry)) {
                 /* Gets a chm hit_cache info */
-                ChmBlockInfo bb = ChmBlockInfo.getChmBlockInfoInstance(
+                ChmBlockInfo.resetChmBlockInfoInstance(
                         directoryListingEntry, (int) getChmLzxcResetTable()
-                                .getBlockLen(), getChmLzxcControlData());
+                                .getBlockLen(), getChmLzxcControlData(), chmBlockInfo);
 
                 int i = 0, start = 0, hit_cache = 0;
 
@@ -281,7 +282,7 @@ public class ChmExtractor {
                         for (i = 0; i < getLzxBlocksCache().size(); i++) {
                             //lzxBlock = getLzxBlocksCache().get(i);
                             int bn = getLzxBlocksCache().get(i).getBlockNumber();
-                            for (int j = bb.getIniBlock(); j <= bb.getStartBlock(); j++) {
+                            for (int j = chmBlockInfo.getIniBlock(); j <= chmBlockInfo.getStartBlock(); j++) {
                                 if (bn == j) {
                                     if (j > start) {
                                         start = j;
@@ -289,14 +290,14 @@ public class ChmExtractor {
                                     }
                                 }
                             }
-                            if (start == bb.getStartBlock())
+                            if (start == chmBlockInfo.getStartBlock())
                                 break;
                         }
                     }
 
 //                    if (i == getLzxBlocksCache().size() && i == 0) {
                     if (start<0) {
-                        start = bb.getIniBlock();
+                        start = chmBlockInfo.getIniBlock();
 
                         byte[] dataSegment = ChmCommons.getChmBlockSegment(
                                 getData(),
@@ -312,25 +313,25 @@ public class ChmExtractor {
                         lzxBlock = getLzxBlocksCache().get(hit_cache);
                     }
 
-                    for (i = start; i <= bb.getEndBlock();) {
-                        if (i == bb.getStartBlock() && i == bb.getEndBlock()) {
+                    for (i = start; i <= chmBlockInfo.getEndBlock();) {
+                        if (i == chmBlockInfo.getStartBlock() && i == chmBlockInfo.getEndBlock()) {
                             buffer.write(lzxBlock.getContent(
-                                    bb.getStartOffset(), bb.getEndOffset()));
+                                    chmBlockInfo.getStartOffset(), chmBlockInfo.getEndOffset()));
                             break;
                         }
 
-                        if (i == bb.getStartBlock()) {
+                        if (i == chmBlockInfo.getStartBlock()) {
                             buffer.write(lzxBlock.getContent(
-                                    bb.getStartOffset()));
+                                    chmBlockInfo.getStartOffset()));
                         }
 
-                        if (i > bb.getStartBlock() && i < bb.getEndBlock()) {
+                        if (i > chmBlockInfo.getStartBlock() && i < chmBlockInfo.getEndBlock()) {
                             buffer.write(lzxBlock.getContent());
                         }
 
-                        if (i == bb.getEndBlock()) {
+                        if (i == chmBlockInfo.getEndBlock()) {
                             buffer.write(lzxBlock.getContent(
-                                    0, bb.getEndOffset()));
+                                    0, chmBlockInfo.getEndOffset()));
                             break;
                         }
 
diff --git a/tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmBlockInfo.java b/tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmBlockInfo.java
index cda829ce5..8bf9d5053 100644
--- a/tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmBlockInfo.java
+++ b/tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmBlockInfo.java
@@ -37,9 +37,7 @@ public class ChmBlockInfo {
     private int startOffset;
     private int endOffset;
 
-    private static ChmBlockInfo chmBlockInfo = null;
-
-    private ChmBlockInfo() {
+    public ChmBlockInfo() {
 
     }
 
@@ -79,23 +77,30 @@ public class ChmBlockInfo {
         return chmBlockInfo;
     }
 
+    @Deprecated
     public static ChmBlockInfo getChmBlockInfoInstance(
             DirectoryListingEntry dle, int bytesPerBlock,
             ChmLzxcControlData clcd) {
-        setChmBlockInfo(new ChmBlockInfo());
-        getChmBlockInfo().setStartBlock(dle.getOffset() / bytesPerBlock);
-        getChmBlockInfo().setEndBlock(
+        return resetChmBlockInfoInstance(dle, bytesPerBlock, clcd, new ChmBlockInfo());
+    }
+
+    public static ChmBlockInfo resetChmBlockInfoInstance(
+                DirectoryListingEntry dle, int bytesPerBlock,
+        ChmLzxcControlData clcd, ChmBlockInfo chmBlockInfo) {
+
+        chmBlockInfo.setStartBlock(dle.getOffset() / bytesPerBlock);
+        chmBlockInfo.setEndBlock(
                 (dle.getOffset() + dle.getLength()) / bytesPerBlock);
-        getChmBlockInfo().setStartOffset(dle.getOffset() % bytesPerBlock);
-        getChmBlockInfo().setEndOffset(
+        chmBlockInfo.setStartOffset(dle.getOffset() % bytesPerBlock);
+        chmBlockInfo.setEndOffset(
                 (dle.getOffset() + dle.getLength()) % bytesPerBlock);
         // potential problem with casting long to int
-        getChmBlockInfo().setIniBlock(
-                getChmBlockInfo().startBlock - getChmBlockInfo().startBlock
+        chmBlockInfo.setIniBlock(
+                chmBlockInfo.startBlock - chmBlockInfo.startBlock
                         % (int) clcd.getResetInterval());
 //                (getChmBlockInfo().startBlock - getChmBlockInfo().startBlock)
 //                        % (int) clcd.getResetInterval());
-        return getChmBlockInfo();
+        return chmBlockInfo;
     }
 
     /**
@@ -225,11 +230,4 @@ public class ChmBlockInfo {
         this.endOffset = endOffset;
     }
 
-    public static void setChmBlockInfo(ChmBlockInfo chmBlockInfo) {
-        ChmBlockInfo.chmBlockInfo = chmBlockInfo;
-    }
-
-    public static ChmBlockInfo getChmBlockInfo() {
-        return chmBlockInfo;
-    }
 }
diff --git a/tika-parsers/src/test/java/org/apache/tika/TestParsers.java b/tika-parsers/src/test/java/org/apache/tika/TestParsers.java
index ddd671daf..173555bdc 100644
--- a/tika-parsers/src/test/java/org/apache/tika/TestParsers.java
+++ b/tika-parsers/src/test/java/org/apache/tika/TestParsers.java
@@ -29,13 +29,14 @@ import org.apache.tika.metadata.TikaCoreProperties;
 import org.apache.tika.parser.ParseContext;
 import org.apache.tika.parser.Parser;
 import org.junit.Before;
+import org.junit.Ignore;
 import org.junit.Test;
 import org.xml.sax.helpers.DefaultHandler;
 
 /**
  * Junit test class for Tika {@link Parser}s.
  */
-public class TestParsers extends TikaTest {
+public class TestParsers extends MultiThreadedTikaTest {
 
     private TikaConfig tc;
 
@@ -76,16 +77,16 @@ public class TestParsers extends TikaTest {
     @Test
     public void testOptionalHyphen() throws Exception {
         String[] extensions =
-                new String[] { "ppt", "pptx", "doc", "docx", "rtf", "pdf"};
+                new String[]{"ppt", "pptx", "doc", "docx", "rtf", "pdf"};
         for (String extension : extensions) {
             File file = getResourceAsFile("/test-documents/testOptionalHyphen." + extension);
             String content = tika.parseToString(file);
             assertTrue("optional hyphen was not handled for '" + extension + "' file type: " + content,
-                       content.contains("optionalhyphen") ||
-                       content.contains("optional\u00adhyphen") ||   // soft hyphen
-                       content.contains("optional\u200bhyphen") ||   // zero width space
-                       content.contains("optional\u2027"));          // hyphenation point
-            
+                    content.contains("optionalhyphen") ||
+                            content.contains("optional\u00adhyphen") ||   // soft hyphen
+                            content.contains("optional\u200bhyphen") ||   // zero width space
+                            content.contains("optional\u2027"));          // hyphenation point
+
         }
     }
 
@@ -93,17 +94,25 @@ public class TestParsers extends TikaTest {
         File file = getResourceAsFile("/test-documents/" + fileName + "." + extension);
         String content = tika.parseToString(file);
         assertTrue(extension + ": content=" + content + " did not extract text",
-                   content.contains("Here is some text"));
+                content.contains("Here is some text"));
         assertTrue(extension + ": content=" + content + " did not extract comment",
-                   content.contains("Here is a comment"));
+                content.contains("Here is a comment"));
     }
 
     @Test
     public void testComment() throws Exception {
-        final String[] extensions = new String[] {"ppt", "pptx", "doc", 
-            "docx", "xls", "xlsx", "pdf", "rtf"};
-        for(String extension : extensions) {
+        final String[] extensions = new String[]{"ppt", "pptx", "doc",
+                "docx", "xls", "xlsx", "pdf", "rtf"};
+        for (String extension : extensions) {
             verifyComment(extension, "testComment");
         }
     }
+
+    //TODO: add a @smoketest tag or something similar to run this occasionally automatically
+    @Test
+    @Ignore("ignore for regular builds; run occasionally")
+    public void testAllMultiThreaded() throws Exception {
+        //this runs against all files in /test-documents
+        testMultiThreaded(10, 100, null);
+    }
 }
diff --git a/tika-parsers/src/test/java/org/apache/tika/parser/chm/TestChmExtraction.java b/tika-parsers/src/test/java/org/apache/tika/parser/chm/TestChmExtraction.java
index 42e54a73f..7c35d48a2 100644
--- a/tika-parsers/src/test/java/org/apache/tika/parser/chm/TestChmExtraction.java
+++ b/tika-parsers/src/test/java/org/apache/tika/parser/chm/TestChmExtraction.java
@@ -16,11 +16,21 @@
  */
 package org.apache.tika.parser.chm;
 
-import static java.nio.charset.StandardCharsets.ISO_8859_1;
-import static org.junit.Assert.assertTrue;
+import org.apache.tika.MultiThreadedTikaTest;
+import org.apache.tika.exception.TikaException;
+import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.ParseContext;
+import org.apache.tika.parser.Parser;
+import org.apache.tika.parser.chm.accessor.ChmDirectoryListingSet;
+import org.apache.tika.parser.chm.accessor.DirectoryListingEntry;
+import org.apache.tika.parser.chm.core.ChmExtractor;
+import org.apache.tika.sax.BodyContentHandler;
+import org.junit.Test;
+import org.xml.sax.SAXException;
 
 import java.io.ByteArrayInputStream;
 import java.io.File;
+import java.io.FileFilter;
 import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.InputStream;
@@ -34,18 +44,10 @@ import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.regex.Pattern;
 
-import org.apache.tika.exception.TikaException;
-import org.apache.tika.metadata.Metadata;
-import org.apache.tika.parser.ParseContext;
-import org.apache.tika.parser.Parser;
-import org.apache.tika.parser.chm.accessor.ChmDirectoryListingSet;
-import org.apache.tika.parser.chm.accessor.DirectoryListingEntry;
-import org.apache.tika.parser.chm.core.ChmExtractor;
-import org.apache.tika.sax.BodyContentHandler;
-import org.junit.Test;
-import org.xml.sax.SAXException;
+import static java.nio.charset.StandardCharsets.ISO_8859_1;
+import static org.junit.Assert.assertTrue;
 
-public class TestChmExtraction {
+public class TestChmExtraction extends MultiThreadedTikaTest {
 
     private final Parser parser = new ChmParser();
 
@@ -161,7 +163,7 @@ public class TestChmExtraction {
     }
     
 
-    @Test
+    @Test //TODO: redo with new MultiThreadedTikaTest
     public void testMultiThreadedChmExtraction() throws InterruptedException {
         ExecutorService executor = Executors.newFixedThreadPool(TestParameters.NTHREADS);
         for (int i = 0; i < TestParameters.NTHREADS; i++) {
@@ -204,4 +206,20 @@ public class TestChmExtraction {
             testingChm(stream);
         }
     }
+
+
+    @Test
+    public void testMultiThreaded() throws Exception {
+        testMultiThreaded(10, 100, null);
+/*                new FileFilter() {
+                    @Override
+                    public boolean accept(File pathname) {
+                        if (pathname.getName().toLowerCase(Locale.ENGLISH).endsWith(".chm")) {
+                            return true;
+                        } else {
+                            return false;
+                        }
+                    }
+                });*/
+    }
 }
